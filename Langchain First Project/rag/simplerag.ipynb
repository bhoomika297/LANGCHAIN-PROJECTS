{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever And Chain With Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Speech.txt'}, page_content=\"Good Morning /Afternoon sir. First of all thankyou so much for giving me this opportunity to let me talk about my-self. My name is Bhoomika Agrahari and I am a final year student of MCA Specialization in Data Science and Artificial Intelligence from BBDU. \\n\\nI am a determined and self motivated person.I tend to have a creative and solution oriented approach in my life.\\nFrom the regular subjects of my MCA related curriculum I have always had a keen interest for Java and Python for machine learning and model deployement  and I have also learnt some technologies related to the same. \\nAs I am now shifting from my academic year towards a professional one I am really looking forward to getting to know the latest technologies and working on the same so that they cater to the development of my interpersonal skills and lead to a positive growth on my career.\\nThank you so much.\\n\\n1. Why is Java a platform independent language?\\nJava language was developed in such a way that it does not depend on any hardware or software due to the fact that the compiler compiles the code and then converts it to platform-independent byte code which can be run on multiple systems.\\n\\nThe only condition to run that byte code is for the machine to have a runtime environment (JRE) installed in it\\n\\n2. Why is Java not a pure object oriented language?\\nJava supports primitive data types - byte, boolean, char, short, int, float, long, and double and hence it is not a pure object oriented language.\\n\\n3. Difference between Heap and Stack Memory in java. And how java utilizes this.\\nStack memory is the portion of memory that was assigned to every individual program. And it was fixed. On the other hand, Heap memory is the portion that was not allocated to the java program but it will be available for use by the java program when it is required, mostly during the runtime of the program.\\n\\n4. Can java be said to be the complete object-oriented programming language?\\nIt is not wrong if we claim that java is the complete object-oriented programming language. Because Everything in Java is under the classes. And we can access that by creating the objects.\\n\\nBut also if we say that java is not a completely object-oriented programming language because it has the support of primitive data types like int, float, char, boolean, double, etc.\\n\\nNow for the question: Is java a completely object-oriented programming language? We can say that - Java is not a pure object-oriented programming language, because it has direct access to primitive data types. And these primitive data types don't directly belong to the Integer classes.\\n\\n5. How is Java different from C++?\\nC++ is only a  compiled language, whereas Java is compiled as well as an interpreted language.\\nJava programs are machine-independent whereas a c++ program can run only in the machine in which it is compiled. \\nC++ allows users to use pointers in the program. Whereas java doesn’t allow it. Java internally uses pointers. \\nC++ supports the concept of Multiple inheritances whereas Java doesn't support this. And it is due to avoiding the complexity of name ambiguity that causes the diamond problem.\\n\\n6. Pointers are used in C/ C++. Why does Java not make use of pointers?\\nPointers are quite complicated and unsafe to use by beginner programmers. Java focuses on code simplicity, and the usage of pointers can make it challenging. Pointer utilization can also cause potential errors. Moreover, security is also compromised if pointers are used because the users can directly access memory with the help of pointers.\\n\\nThus, a certain level of abstraction is furnished by not including pointers in Java. Moreover, the usage of pointers can make the procedure of garbage collection quite slow and erroneous. Java makes use of references as these cannot be manipulated, unlike pointers.\\n\\n7. What do you understand by an instance variable and a local variable?\\nInstance variables are those variables that are accessible by all the methods in the class. They are declared outside the methods and inside the class. These variables describe the properties of an object and remain bound to it at any cost.\\n\\nAll the objects of the class will have their copy of the variables for utilization. If any modification is done on these variables, then only that instance will be impacted by it, and all other class instances continue to remain unaffected.\\nExample:\\n\\nclass Athlete {\\npublic String athleteName;\\npublic double athleteSpeed;\\npublic int athleteAge;\\n}\\nLocal variables are those variables present within a block, function, or constructor and can be accessed only inside them. The utilization of the variable is restricted to the block scope. Whenever a local variable is declared inside a method, the other class methods don’t have any knowledge about the local variable.\\n\\nExample:\\n\\npublic void athlete() {\\nString athleteName;\\ndouble athleteSpeed;\\nint athleteAge;\\n}\\n\\n8. What are the default values assigned to variables and instances in java?\\nThere are no default values assigned to the variables in java. We need to initialize the value before using it. Otherwise, it will throw a compilation error of (Variable might not be initialized). \\nBut for instance, if we create the object, then the default value will be initialized by the default constructor depending on the data type. \\nIf it is a reference, then it will be assigned to null. \\nIf it is numeric, then it will assign to 0.\\nIf it is a boolean, then it will be assigned to false. Etc.\\n\\n9. What do you mean by data encapsulation?\\nData Encapsulation is an Object-Oriented Programming concept of hiding the data attributes and their behaviours in a single unit.\\nIt helps developers to follow modularity while developing software by ensuring that each object is independent of other objects by having its own methods, attributes, and functionalities.\\nIt is used for the security of the private properties of an object and hence serves the purpose of data hiding.\\n\\n\\nAloha Technology is a global IT services outsourcing firm delivering digital transformation by developing technology to create revenue growth and improve agility by reinventing business functionality. Aloha Technology's domain expertise spans across several business verticals like Retail, Healthcare, Telecommunication, Business Intelligence, BFSI, Enterprise Collaboration, Supply Chain, Internet Advertising and CRM.\\n\\nLeading ISVs look at us as a strategic IT outsourcing and end-to-end product development choice. Aloha Technology has a global footprint which showcases a workforce of application developers, designers, computer scientists and engineers, and subject matter experts ready to serve the growing 21st century information and technology needs of clients.\\n\\nCan you describe the working culture of the company?\\nWhat are some challenges I might face in this position?\\nDo you have any concerns or questions about my qualifications?\\nIs there anything we haven’t yet covered that you think is important to know about working here?\\nMy strength lies in my solid educational background, holding a master's degree in data science and artificial intelligence. I've applied this knowledge practically through many projecs during my academic years, demonstrating my hands-on skills in machine learning and model Deployement. Moreover, my effective communication and presentation skills were recognized during a recent public presentation at my college, where I received positive feedback from teachers. This combination of education, practical experience, and communication skills positions me well for roles in machine learning software development and data science.\\n\\n\\nCertainly! Here are some machine learning project ideas considering the feasibility, viability, and direct monetization:\\n\\n1. **Personalized Health Assistant:**\\n   - Develop a system that analyzes user health data (e.g., activity, sleep, heart rate) to provide personalized health recommendations.\\n   - Monetization: Subscription model for premium health insights and recommendations.\\n\\n2. **Predictive Maintenance for Industrial Equipment:**\\n   - Implement machine learning models to predict when industrial machinery or equipment is likely to fail.\\n   - Monetization: Provide a subscription-based service to industries for predictive maintenance solutions.\\n\\n3. **AI-Powered Content Recommendation System:**\\n   - Create a content recommendation system using machine learning to suggest personalized movies, books, or articles to users.\\n   - Monetization: Partner with content providers and earn a commission on subscriptions or purchases through recommendations.\\n\\n4. **Fraud Detection for Financial Transactions:**\\n   - Build a fraud detection system for financial transactions using anomaly detection and machine learning algorithms.\\n   - Monetization: Charge financial institutions based on the volume of transactions processed.\\n\\n5. **AI-Powered Job Matching Platform:**\\n   - Develop a platform that uses AI to match job seekers with suitable job opportunities based on their skills and preferences.\\n   - Monetization: Charge employers for premium access to candidate profiles and advanced matching algorithms.\\n\\n6. **Smart Energy Consumption Monitoring:**\\n   - Create a system that monitors and optimizes energy consumption in households or businesses using machine learning.\\n   - Monetization: Offer a subscription service for energy optimization recommendations.\\n\\n7. **E-commerce Sales Prediction:**\\n   - Build a machine learning model to predict future sales for e-commerce businesses, helping them optimize inventory and marketing strategies.\\n   - Monetization: Charge e-commerce businesses for access to the sales prediction service.\\n\\n8. **AI-Powered Customer Support Chatbot:**\\n   - Develop an advanced chatbot using natural language processing to handle customer support queries for businesses.\\n   - Monetization: Charge businesses based on the volume of customer interactions handled by the chatbot.\\n\\n9. **Language Translation Service:**\\n   - Create an AI-powered language translation service that offers accurate and context-aware translations.\\n   - Monetization: Charge users based on the number of translated words or offer subscription plans for frequent users.\\n\\n10. **Real-time Stock Market Prediction:**\\n    - Build machine learning models to predict stock market movements in real-time, assisting traders and investors.\\n    - Monetization: Offer premium access to advanced prediction models and analytics.\\n\\nRemember to thoroughly research and validate the market demand for your chosen project before investing significant time and resources.\\n\\n\\n\\n\\nAIzaSyC1wVixp51Iha1T0UhUiYnsuJYGi7tfNMk\\n\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"Speech.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition'}, page_content='')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition\",),\n",
    "                       bs_kwargs=dict(parse_only = bs4.SoupStrainer(\n",
    "                           class_ = (\"css-1wtu5xw\")\n",
    "                       )))\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 0}, page_content='Deep Learning for \\nNatural Language Processing\\nDevelop Deep Learning Models for \\nNatural Language in Python\\nJason Brownlee'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 1}, page_content='i\\nDisclaimer\\nThe information contained within this eBook is strictly for educational purposes. If you wish to apply\\nideas contained in this eBook, you are taking full responsibility for your actions.\\nThe author has made every eﬀort to ensure the accuracy of the information within this book was\\ncorrect at time of publication. The author does not assume and hereby disclaims any liability to any\\nparty for any loss, damage, or disruption caused by errors or omissions, whether such errors or\\nomissions result from accident, negligence, or any other cause.\\nNo part of this eBook may be reproduced or transmitted in any form or by any means, electronic or\\nmechanical, recording or by any information storage and retrieval system, without written permission\\nfrom the author.\\nAcknowledgements\\nSpecial thanks to my copy editor Sarah Martin and my technical editors Arun Koshy and Andrei\\nCheremskoy.\\nCopyright\\nDeep Learning for Natural Language Processing\\n©Copyright 2017 Jason Brownlee. All Rights Reserved.\\nEdition: v1.1'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 2}, page_content='Contents\\nCopyright i\\nContents ii\\nPreface iii\\nI Introductions iv\\nWelcome v\\nWho Is This Book For? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v\\nAbout Your Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\\nHow to Read This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\\nAbout the Book Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\\nAbout Python Code Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\nAbout Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\nAbout Getting Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\\nII Foundations 1\\n1 Natural Language Processing 2\\n1.1 Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.2 Challenge of Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 From Linguistics to Natural Language Processing . . . . . . . . . . . . . . . . . 3\\n1.4 Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2 Deep Learning 8\\n2.1 Deep Learning is Large Neural Networks . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Deep Learning is Hierarchical Feature Learning . . . . . . . . . . . . . . . . . . 11\\n2.3 Deep Learning as Scalable Learning Across Domains . . . . . . . . . . . . . . . 12\\n2.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\nii'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 3}, page_content='CONTENTS iii\\n3 Promise of Deep Learning for Natural Language 16\\n3.1 Promise of Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.2 Promise of Drop-in Replacement Models . . . . . . . . . . . . . . . . . . . . . . 17\\n3.3 Promise of New NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.4 Promise of Feature Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.5 Promise of Continued Improvement . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.6 Promise of End-to-End Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n3.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4 How to Develop Deep Learning Models With Keras 21\\n4.1 Keras Model Life-Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4.2 Keras Functional Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n4.3 Standard Network Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\nIII Data Preparation 34\\n5 How to Clean Text Manually and with NLTK 35\\n5.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n5.2 Metamorphosis by Franz Kafka . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n5.3 Text Cleaning Is Task Speciﬁc . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n5.4 Manual Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n5.5 Tokenization and Cleaning with NLTK . . . . . . . . . . . . . . . . . . . . . . . 41\\n5.6 Additional Text Cleaning Considerations . . . . . . . . . . . . . . . . . . . . . . 46\\n5.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n5.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n6 How to Prepare Text Data with scikit-learn 48\\n6.1 The Bag-of-Words Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n6.2 Word Counts with CountVectorizer . . . . . . . . . . . . . . . . . . . . . . . . 49\\n6.3 Word Frequencies with TfidfVectorizer . . . . . . . . . . . . . . . . . . . . . . 50\\n6.4 Hashing with HashingVectorizer . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n6.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\n6.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n7 How to Prepare Text Data With Keras 54\\n7.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n7.2 Split Words with text toword sequence . . . . . . . . . . . . . . . . . . . . . 54\\n7.3 Encoding with onehot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n7.4 Hash Encoding with hashing trick . . . . . . . . . . . . . . . . . . . . . . . . 56\\n7.5 Tokenizer API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n7.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 4}, page_content='CONTENTS iv\\nIV Bag-of-Words 61\\n8 The Bag-of-Words Model 62\\n8.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n8.2 The Problem with Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n8.3 What is a Bag-of-Words? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n8.4 Example of the Bag-of-Words Model . . . . . . . . . . . . . . . . . . . . . . . . 63\\n8.5 Managing Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n8.6 Scoring Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n8.7 Limitations of Bag-of-Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n8.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n8.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n9 How to Prepare Movie Review Data for Sentiment Analysis 69\\n9.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n9.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n9.3 Load Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n9.4 Clean Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\n9.5 Develop Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n9.6 Save Prepared Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n9.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n9.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n10 Project: Develop a Neural Bag-of-Words Model for Sentiment Analysis 85\\n10.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n10.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n10.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n10.4 Bag-of-Words Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n10.5 Sentiment Analysis Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\\n10.6 Comparing Word Scoring Methods . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n10.7 Predicting Sentiment for New Reviews . . . . . . . . . . . . . . . . . . . . . . . 108\\n10.8 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\\n10.9 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\\n10.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\\nV Word Embeddings 114\\n11 The Word Embedding Model 115\\n11.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n11.2 What Are Word Embeddings? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n11.3 Word Embedding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n11.4 Using Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n11.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 5}, page_content='CONTENTS v\\n12 How to Develop Word Embeddings with Gensim 122\\n12.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n12.2 Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.3 Gensim Python Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.4 Develop Word2Vec Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.5 Visualize Word Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\n12.6 Load Google’s Word2Vec Embedding . . . . . . . . . . . . . . . . . . . . . . . . 128\\n12.7 Load Stanford’s GloVe Embedding . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n12.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\\n12.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n13 How to Learn and Load Word Embeddings in Keras 133\\n13.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\\n13.2 Word Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\\n13.3 Keras Embedding Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\\n13.4 Example of Learning an Embedding . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n13.5 Example of Using Pre-Trained GloVe Embedding . . . . . . . . . . . . . . . . . 138\\n13.6 Tips for Cleaning Text for Word Embedding . . . . . . . . . . . . . . . . . . . . 142\\n13.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\\n13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\\nVI Text Classiﬁcation 144\\n14 Neural Models for Document Classiﬁcation 145\\n14.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\\n14.2 Word Embeddings + CNN = Text Classiﬁcation . . . . . . . . . . . . . . . . . . 146\\n14.3 Use a Single Layer CNN Architecture . . . . . . . . . . . . . . . . . . . . . . . . 147\\n14.4 Dial in CNN Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n14.5 Consider Character-Level CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n14.6 Consider Deeper CNNs for Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . 151\\n14.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n14.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n15 Project: Develop an Embedding + CNN Model for Sentiment Analysis 153\\n15.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n15.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n15.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n15.4 Train CNN With Embedding Layer . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n15.5 Evaluate Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n15.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n15.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n15.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 6}, page_content='CONTENTS vi\\n16 Project: Develop an n-gram CNN Model for Sentiment Analysis 174\\n16.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n16.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n16.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\\n16.4 Develop Multichannel Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n16.5 Evaluate Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n16.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n16.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\\n16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\\nVII Language Modeling 189\\n17 Neural Language Modeling 190\\n17.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\\n17.2 Problem of Modeling Language . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\\n17.3 Statistical Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n17.4 Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n17.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\\n17.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\\n18 How to Develop a Character-Based Neural Language Model 197\\n18.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\\n18.2 Sing a Song of Sixpence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\n18.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\n18.4 Train Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\\n18.5 Generate Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\\n18.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\\n18.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\\n19 How to Develop a Word-Based Neural Language Model 211\\n19.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\\n19.2 Framing Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\\n19.3 Jack and Jill Nursery Rhyme . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\\n19.4 Model 1: One-Word-In, One-Word-Out Sequences . . . . . . . . . . . . . . . . . 212\\n19.5 Model 2: Line-by-Line Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\\n19.6 Model 3: Two-Words-In, One-Word-Out Sequence . . . . . . . . . . . . . . . . . 222\\n19.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\\n19.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\\n20 Project: Develop a Neural Language Model for Text Generation 226\\n20.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\\n20.2 The Republic by Plato . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\\n20.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\\n20.4 Train Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\\n20.5 Use Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\\n20.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 7}, page_content='CONTENTS vii\\n20.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\\n20.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\\nVIII Image Captioning 245\\n21 Neural Image Caption Generation 246\\n21.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\\n21.2 Describing an Image with Text . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\\n21.3 Neural Captioning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\n21.4 Encoder-Decoder Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\\n21.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\\n21.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\\n22 Neural Network Models for Caption Generation 252\\n22.1 Image Caption Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\\n22.2 Inject Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\\n22.3 Merge Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\\n22.4 More on the Merge Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\\n22.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\\n22.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\\n23 How to Load and Use a Pre-Trained Object Recognition Model 257\\n23.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\\n23.2 ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\\n23.3 The Oxford VGG Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\\n23.4 Load the VGG Model in Keras . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\\n23.5 Develop a Simple Photo Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n23.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\\n23.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\\n24 How to Evaluate Generated Text With the BLEU Score 268\\n24.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\\n24.2 Bilingual Evaluation Understudy Score . . . . . . . . . . . . . . . . . . . . . . . 268\\n24.3 Calculate BLEU Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\\n24.4 Cumulative and Individual BLEU Scores . . . . . . . . . . . . . . . . . . . . . . 271\\n24.5 Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\\n24.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\\n24.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\\n25 How to Prepare a Photo Caption Dataset For Modeling 277\\n25.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\\n25.2 Download the Flickr8K Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\\n25.3 How to Load Photographs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\\n25.4 Pre-Calculate Photo Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\\n25.5 How to Load Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n25.6 Prepare Description Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 8}, page_content='CONTENTS viii\\n25.7 Whole Description Sequence Model . . . . . . . . . . . . . . . . . . . . . . . . . 287\\n25.8 Word-By-Word Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\\n25.9 Progressive Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\\n25.10Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\\n25.11Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\\n26 Project: Develop a Neural Image Caption Generation Model 299\\n26.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\\n26.2 Photo and Caption Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\\n26.3 Prepare Photo Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\\n26.4 Prepare Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\\n26.5 Develop Deep Learning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\\n26.6 Evaluate Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\\n26.7 Generate New Captions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\\n26.8 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\\n26.9 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\\n26.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\\nIX Machine Translation 332\\n27 Neural Machine Translation 333\\n27.1 What is Machine Translation? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\\n27.2 What is Statistical Machine Translation? . . . . . . . . . . . . . . . . . . . . . . 334\\n27.3 What is Neural Machine Translation? . . . . . . . . . . . . . . . . . . . . . . . . 335\\n27.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\\n27.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\\n28 What are Encoder-Decoder Models for Neural Machine Translation 339\\n28.1 Encoder-Decoder Architecture for NMT . . . . . . . . . . . . . . . . . . . . . . 339\\n28.2 Sutskever NMT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\\n28.3 Cho NMT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\\n28.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\\n28.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\\n29 How to Conﬁgure Encoder-Decoder Models for Machine Translation 347\\n29.1 Encoder-Decoder Model for Neural Machine Translation . . . . . . . . . . . . . 347\\n29.2 Baseline Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\\n29.3 Word Embedding Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\\n29.4 RNN Cell Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\\n29.5 Encoder-Decoder Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\\n29.6 Direction of Encoder Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\\n29.7 Attention Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\\n29.8 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\\n29.9 Final Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\\n29.10Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\\n29.11Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 9}, page_content='CONTENTS ix\\n30 Project: Develop a Neural Machine Translation Model 354\\n30.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\n30.2 German to English Translation Dataset . . . . . . . . . . . . . . . . . . . . . . . 354\\n30.3 Preparing the Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\\n30.4 Train Neural Translation Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\\n30.5 Evaluate Neural Translation Model . . . . . . . . . . . . . . . . . . . . . . . . . 366\\n30.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\\n30.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n30.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\\nX Appendix 373\\nA Getting Help 374\\nA.1 Oﬃcial Keras Destinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\\nA.2 Where to Get Help with Keras . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\\nA.3 Where to Get Help with Natural Language . . . . . . . . . . . . . . . . . . . . . 375\\nA.4 How to Ask Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\\nA.5 Contact the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\\nB How to Setup a Workstation for Deep Learning 376\\nB.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\\nB.2 Download Anaconda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\\nB.3 Install Anaconda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378\\nB.4 Start and Update Anaconda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\\nB.5 Install Deep Learning Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\\nB.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\nB.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\nC How to Use Deep Learning in the Cloud 385\\nC.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\\nC.2 Setup Your AWS Account . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386\\nC.3 Launch Your Server Instance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\\nC.4 Login, Conﬁgure and Run . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\\nC.5 Build and Run Models on AWS . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\\nC.6 Close Your EC2 Instance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\\nC.7 Tips and Tricks for Using Keras on AWS . . . . . . . . . . . . . . . . . . . . . . 395\\nC.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\\nC.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\\nXI Conclusions 396\\nHow Far You Have Come 397'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 10}, page_content='Preface\\nWe are awash with text, from books, papers, blogs, tweets, news, and increasingly text from\\nspoken utterances. Every day, I get questions asking how to develop machine learning models\\nfor text data. Working with text is hard as it requires drawing upon knowledge from diverse\\ndomains such as linguistics, machine learning, statistical natural language processing, and these\\ndays, deep learning.\\nI have done my best to write blog posts to answer frequently asked questions on the topic\\nand decided to pull together my best knowledge on the matter into this book. I designed this\\nbook to teach you step-by-step how to bring modern deep learning methods to your natural\\nlanguage processing projects. I chose the programming language, programming libraries, and\\ntutorial topics to give you the skills you need.\\nPython is the go-to language for applied machine learning and deep learning, both in terms\\nof demand from employers and employees. This is not least because it could be a renaissance\\nfor machine learning tools. I have focused on showing you how to use the best of breed Python\\ntools for natural language processing such as Gensim and NLTK, and even a little scikit-learn.\\nKey to getting results is speed of development, and for this reason, we use the Keras deep\\nlearning library as you can deﬁne, train, and use complex deep learning models with just a few\\nlines of Python code.\\nThere are three key areas that you must know when working with text:\\n1.How to clean text. This includes loading, analyzing, ﬁltering and cleaning tasks required\\nprior to modeling.\\n2.How to represent text. This includes the classical bag-of-words model and the modern\\nand powerful distributed representation in word embeddings.\\n3.How to generate text. This includes the range of most interesting problems, such as image\\ncaptioning and translation.\\nThese key topics provide the backbone for the book and the tutorials you will work through.\\nI believe that after completing this book, you will have the skills that you need to both work\\nthrough your own natural language processing projects and bring modern deep learning methods\\nto bare.\\nJason Brownlee\\n2017\\nx'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 11}, page_content='Part I\\nIntroductions\\nxi'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 12}, page_content='Welcome\\nWelcome to Deep Learning for Natural Language Processing . Natural language processing is the\\narea of study dedicated to the automatic manipulation of speech and text by software. It is\\nan old ﬁeld of study, originally dominated by rule-based methods designed by linguists, then\\nstatistical methods, and, more recently, deep learning methods that show great promise in the\\nﬁeld. So much so that the heart of the Google Translate service uses a deep learning method, a\\ntopic that you will learn more about in this book.\\nI designed this book to teach you step-by-step how to bring modern deep learning models to\\nyour own natural language processing projects.\\nWho Is This Book For?\\nBefore we get started, let’s make sure you are in the right place. This book is for developers\\nthat know some applied machine learning and some deep learning. Maybe you want or need\\nto start using deep learning for text on your research project or on a project at work. This\\nguide was written to help you do that quickly and eﬃciently by compressing years worth of\\nknowledge and experience into a laser-focused course of hands-on tutorials. The lessons in this\\nbook assume a few things about you, such as:\\n\\x88You know your way around basic Python for programming.\\n\\x88You know your way around basic NumPy for array manipulation.\\n\\x88You know your way around basic scikit-learn for machine learning.\\n\\x88You know your way around basic Keras for deep learning.\\nFor some bonus points, perhaps some of the below points apply to you. Don’t panic if they\\ndon’t.\\n\\x88You may know how to work through a predictive modeling problem end-to-end.\\n\\x88You may know a little bit of natural language processing.\\n\\x88You may know a little bit of natural language libraries such as NLTK or Gensim.\\nThis guide was written in the top-down and results-ﬁrst machine learning style that you’re\\nused to from MachineLearningMastery.com.\\nxii'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 13}, page_content='xiii\\nAbout Your Outcomes\\nThis book will teach you how to get results as a machine learning practitioner interested in\\nusing deep learning on your natural language processing project. After reading and working\\nthrough this book, you will know:\\n\\x88What natural language processing is and why it is challenging.\\n\\x88What deep learning is and how it is diﬀerent from other machine learning methods.\\n\\x88The promise of deep learning methods for natural language processing problems.\\n\\x88How to prepare text data for modeling using best-of-breed Python libraries.\\n\\x88How to develop distributed representations of text using word embedding models.\\n\\x88How to develop a bag-of-words model, a representation technique that can be used for\\nmachine learning and deep learning methods.\\n\\x88How to develop a neural sentiment analysis model for automatically predicting the class\\nlabel for a text document.\\n\\x88How to develop a neural language model, required for any text generating neural network.\\n\\x88How to develop a photo captioning system to automatically generate textual descriptions\\nof photographs.\\n\\x88How to develop a neural machine translation system for translating text from one language\\nto another.\\nThis book will NOT teach you how to be a research scientist and all the theory behind why\\nspeciﬁc methods work. For that, I would recommend good research papers and textbooks. See\\ntheFurther Reading section at the end of each tutorial for a good starting point.\\nHow to Read This Book\\nThis book was written to be read linearly, from start to ﬁnish. That being said, if you know\\nthe basics and need help with a speciﬁc problem type or technique, then you can ﬂip straight\\nto that section and get started. This book was designed for you to read on your workstation,\\non the screen, not on an eReader. My hope is that you have the book open right next to your\\neditor and run the examples as you read about them.\\nThis book is not intended to be read passively or be placed in a folder as a reference text.\\nIt is a playbook, a workbook, and a guidebook intended for you to learn by doing and then\\napply your new understanding to your own natural language projects. To get the most out of\\nthe book, I would recommend playing with the examples in each tutorial. Extend them, break\\nthem, then ﬁx them. Try some of the extensions presented at the end of each lesson and let me\\nknow how you do.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 14}, page_content='xiv\\nAbout the Book Structure\\nThis book was designed around major activities, techniques, and natural language processing\\nproblems. There are a lot of things you could learn about deep learning and natural language\\nprocessing, from theory to applications to APIs. My goal is to take you straight to getting\\nresults with laser-focused tutorials. I designed the tutorials to focus on how to get things done.\\nThey give you the tools to both rapidly understand and apply each technique to your own\\nnatural language processing prediction problems.\\nEach of the tutorials are designed to take you about one hour to read through and complete,\\nexcluding the extensions and further reading. You can choose to work through the lessons one\\nper day, one per week, or at your own pace. I think momentum is critically important, and this\\nbook is intended to be read and used, not to sit idle. I would recommend picking a schedule\\nand sticking to it. The tutorials are divided into eight parts:\\n\\x88Part 1: Foundations . Discover a gentle introduction to natural language processing,\\ndeep learning, and the promise of combining the two, as well as tutorials on how to get\\nstarted with Keras.\\n\\x88Part 2: Data Preparation : Discover tutorials that show how to clean, prepare and\\nencode text ready for modeling with neural networks.\\n\\x88Part 3: Bag-of-Words . Discover the bag-of-words model, a staple representation for\\nmachine learning and a good starting point for neural networks for sentiment analysis.\\n\\x88Part 4: Word Embeddings . Discover a more powerful word representation in word\\nembeddings, how to develop them as standalone models, and how to learn them as part of\\nneural network models.\\n\\x88Part 5: Text Classiﬁcation . Discover how to leverage word embeddings and convolu-\\ntional neural networks to learn spatial invariant models of text for sentiment analysis, a\\nsuccessor to the bag-of-words model.\\n\\x88Part 6: Language Modeling . Discover how to develop character-based and word-based\\nlanguage models, a technique that is required as part of any modern text generating model.\\n\\x88Part 7: Image Captioning . Discover how to combine a pre-trained object recognition\\nmodel with a language model to automatically caption images.\\n\\x88Part 8: Machine Translation . Discover how to combine two language models to\\nautomatically translate text from one language to another.\\nEach part targets a speciﬁc learning outcome, and so does each tutorial within each part.\\nThis acts as a ﬁlter to ensure you are only focused on the things you need to know to get to a\\nspeciﬁc result and do not get bogged down in the math or near-inﬁnite number of conﬁguration\\nparameters. The tutorials were not designed to teach you everything there is to know about\\neach of the techniques or natural language processing problems. They were designed to give you\\nan understanding of how they work, how to use them on your projects the fastest way I know\\nhow: to learn by doing.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 15}, page_content='xv\\nAbout Python Code Examples\\nThe code examples were carefully designed to demonstrate the purpose of a given lesson. For\\nthis reason, the examples are highly targeted.\\n\\x88Models were demonstrated on real-world datasets to give you the context and conﬁdence\\nto bring the techniques to your own natural language processing problems.\\n\\x88Model conﬁgurations used were discovered through trial and error are skillful, but not\\noptimized. This leaves the door open for you to explore new and possibly better conﬁgu-\\nrations.\\n\\x88Code examples are complete and standalone. The code for each lesson will run as-is with\\nno code from prior lessons or third parties required beyond the installation of the required\\npackages.\\nA complete working example is presented with each tutorial for you to inspect and copy-\\nand-paste. All source code is also provided with the book and I would recommend running\\nthe provided ﬁles whenever possible to avoid any copy-paste issues. The provided code was\\ndeveloped in a text editor and intended to be run on the command line. No special IDE or\\nnotebooks are required. If you are using a more advanced development environment and are\\nhaving trouble, try running the example from the command line instead.\\nNeural network algorithms are stochastic. This means that they will make diﬀerent predictions\\nwhen the same model conﬁguration is trained on the same training data. On top of that, each\\nexperimental problem in this book is based around generating stochastic predictions. As a\\nresult, this means you will not get exactly the same sample output presented in this book. This\\nis by design. I want you to get used to the stochastic nature of the neural network algorithms.\\nIf this bothers you, please note:\\n\\x88You can re-run a given example a few times and your results should be close to the values\\nreported.\\n\\x88You can make the output consistent by ﬁxing the NumPy random number seed.\\n\\x88You can develop a robust estimate of the skill of a model by ﬁtting and evaluating it\\nmultiple times and taking the average of the ﬁnal skill score (highly recommended).\\nAll code examples were tested on a POSIX-compaitable machine with Python 3 and Keras\\n2. All code examples will run on modest and modern computer hardware and were executed on\\na CPU. No GPUs are required to run the presented examples, although a GPU would make the\\ncode run faster. I am only human and there may be a bug in the sample code. If you discover a\\nbug, please let me know so I can ﬁx it and update the book and send out a free update.\\nAbout Further Reading\\nEach lesson includes a list of further reading resources. This may include:\\n\\x88Research papers.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 16}, page_content='xvi\\n\\x88Books and book chapters.\\n\\x88Webpages.\\n\\x88API documentation.\\nWherever possible, I try to list and link to the relevant API documentation for key objects\\nand functions used in each lesson so you can learn more about them. When it comes to research\\npapers, I try to list papers that are ﬁrst to use a speciﬁc technique or ﬁrst in a speciﬁc problem\\ndomain. These are not required reading, but can give you more technical details, theory, and\\nconﬁguration details if you’re looking for it. Wherever possible, I have tried to link to the freely\\navailable version of the paper on arxiv.org . You can search for and download any of the papers\\nlisted on Google Scholar Search scholar.google.com . Wherever possible, I have tried to link\\nto books on Amazon. I don’t know everything, and if you discover a good resource related to a\\ngiven lesson, please let me know so I can update the book.\\nAbout Getting Help\\nYou might need help along the way. Don’t worry, you are not alone.\\n\\x88Help with a Technique? If you need help with the technical aspects of a speciﬁc model or\\nmethod, see the Further Reading sections at the end of each lesson.\\n\\x88Help with Keras? If you need help with using the Keras library, see the list of resources\\nin Appendix A.\\n\\x88Help with your workstation? If you need help setting up your environment, I would\\nrecommend using Anaconda and following my tutorial in Appendix B.\\n\\x88Help running large models? I recommend renting time on Amazon Web Service (AWS) to\\nrun large models. If you need help getting started on AWS, see the tutorial in Appendix\\nC.\\n\\x88Help in general? You can shoot me an email. My details are in Appendix A.\\nSummary\\nAre you ready? Let’s dive in!\\nNext up you will discover a concrete idea of what natural language processing actually\\nmeans.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 17}, page_content='Part II\\nFoundations\\n1'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 18}, page_content='Chapter 1\\nNatural Language Processing\\nNatural Language Processing, or NLP for short, is broadly deﬁned as the automatic manipulation\\nof natural language, like speech and text, by software. The study of natural language processing\\nhas been around for more than 50 years and grew out of the ﬁeld of linguistics with the rise of\\ncomputers. In this chapter, you will discover what natural language processing is and why it is\\nso important. After reading this chapter, you will know:\\n\\x88What natural language is and how it is diﬀerent from other types of data.\\n\\x88What makes working with natural language so challenging.\\n\\x88Where the ﬁeld of NLP came from and how it is deﬁned by modern practitioners.\\nLet’s get started.\\n1.1 Natural Language\\nNatural language refers to the way we, humans, communicate with each other. Namely, speech\\nand text. We are surrounded by text. Think about how much text you see each day:\\n\\x88Signs\\n\\x88Menus\\n\\x88Email\\n\\x88SMS\\n\\x88Web Pages\\n\\x88and so much more...\\nThe list is endless. Now think about speech. We may speak to each other, as a species, more\\nthan we write. It may even be easier to learn to speak than to write. Voice and text are how we\\ncommunicate with each other. Given the importance of this type of data, we must have methods\\nto understand and reason about natural language, just like we do for other types of data.\\n2'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 19}, page_content='1.2. Challenge of Natural Language 3\\n1.2 Challenge of Natural Language\\nWorking with natural language data is not solved. It has been studied for half a century, and it\\nis really hard.\\nIt is hard from the standpoint of the child, who must spend many years acquiring\\na language ... it is hard for the adult language learner, it is hard for the scientist\\nwho attempts to model the relevant phenomena, and it is hard for the engineer who\\nattempts to build systems that deal with natural language input or output. These\\ntasks are so hard that Turing could rightly make ﬂuent conversation in natural\\nlanguage the centerpiece of his test for intelligence.\\n— Page 248, Mathematical Linguistics , 2010.\\nNatural language is primarily hard because it is messy. There are few rules. And yet we can\\neasily understand each other most of the time.\\nHuman language is highly ambiguous ... It is also ever changing and evolving. People\\nare great at producing language and understanding language, and are capable of\\nexpressing, perceiving, and interpreting very elaborate and nuanced meanings. At\\nthe same time, while we humans are great users of language, we are also very poor\\nat formally understanding and describing the rules that govern language.\\n— Page 1, Neural Network Methods in Natural Language Processing , 2017.\\n1.3 From Linguistics to Natural Language Processing\\n1.3.1 Linguistics\\nLinguistics is the scientiﬁc study of language, including its grammar, semantics, and phonetics.\\nClassical linguistics involved devising and evaluating rules of language. Great progress was made\\non formal methods for syntax and semantics, but for the most part, the interesting problems in\\nnatural language understanding resist clean mathematical formalisms.\\nBroadly, a linguist is anyone who studies language, but perhaps more colloquially, a self-\\ndeﬁning linguist may be more focused on being out in the ﬁeld. Mathematics is the tool of\\nscience. Mathematicians working on natural language may refer to their study as mathematical\\nlinguistics, focusing exclusively on the use of discrete mathematical formalisms and theory for\\nnatural language (e.g. formal languages and automata theory).\\n1.3.2 Computational Linguistics\\nComputational linguistics is the modern study of linguistics using the tools of computer science.\\nYesterday’s linguistics may be today’s computational linguist as the use of computational tools\\nand thinking has overtaken most ﬁelds of study.\\nComputational linguistics is the study of computer systems for understanding and\\ngenerating natural language. ... One natural function for computational linguistics\\nwould be the testing of grammars proposed by theoretical linguists.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 20}, page_content='1.3. From Linguistics to Natural Language Processing 4\\n— Pages 4-5, Computational Linguistics: An Introduction , 1986.\\nLarge data and fast computers mean that new and diﬀerent things can be discovered from\\nlarge datasets of text by writing and running software. In the 1990s, statistical methods and\\nstatistical machine learning began to and eventually replaced the classical top-down rule-based\\napproaches to language, primarily because of their better results, speed, and robustness. The\\nstatistical approach to studying natural language now dominates the ﬁeld; it may deﬁne the\\nﬁeld.\\nData-Driven methods for natural language processing have now become so popular\\nthat they must be considered mainstream approaches to computational linguistics.\\n... A strong contributing factor to this development is undoubtedly the increase\\namount of available electronically stored data to which these methods can be applied;\\nanother factor might be a certain disenchantment with approaches relying exclusively\\non hand-crafted rules, due to their observed brittleness.\\n— Page 358, The Oxford Handbook of Computational Linguistics , 2005.\\nThe statistical approach to natural language is not limited to statistics per-se, but also to\\nadvanced inference methods like those used in applied machine learning.\\n... understanding natural language require large amounts of knowledge about\\nmorphology, syntax, semantics and pragmatics as well as general knowledge about\\nthe world. Acquiring and encoding all of this knowledge is one of the fundamental\\nimpediments to developing eﬀective and robust language systems. Like the statistical\\nmethods ... machine learning methods oﬀ the promise of te automatic acquisition of\\nthis knowledge from annotated or unannotated language corpora.\\n— Page 377, The Oxford Handbook of Computational Linguistics , 2005.\\n1.3.3 Statistical Natural Language Processing\\nComputational linguistics also became known by the name of natural language process, or\\nNLP, to reﬂect the more engineer-based or empirical approach of the statistical methods. The\\nstatistical dominance of the ﬁeld also often leads to NLP being described as Statistical Natural\\nLanguage Processing, perhaps to distance it from the classical computational linguistics methods.\\nI view computational linguistics as having both a scientiﬁc and an engineering side.\\nThe engineering side of computational linguistics, often called natural language\\nprocessing (NLP), is largely concerned with building computational tools that do\\nuseful things with language, e.g., machine translation, summarization, question-\\nanswering, etc. Like any engineering discipline, natural language processing draws\\non a variety of diﬀerent scientiﬁc disciplines.\\n—How the statistical revolution changes (computational) linguistics , 2009.\\nLinguistics is a large topic of study, and, although the statistical approach to NLP has shown\\ngreat success in some areas, there is still room and great beneﬁt from the classical top-down\\nmethods.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 21}, page_content='1.4. Natural Language Processing 5\\nRoughly speaking, statistical NLP associates probabilities with the alternatives\\nencountered in the course of analyzing an utterance or a text and accepts the\\nmost probable outcome as the correct one. ... Not surprisingly, words that name\\nphenomena that are closely related in the world, or our perception of it, frequently\\noccur close to one another so that crisp facts about the world are reﬂected in\\nsomewhat fuzzier facts about texts. There is much room for debate in this view.\\n— Page xix, The Oxford Handbook of Computational Linguistics , 2005.\\n1.4 Natural Language Processing\\nAs machine learning practitioners interested in working with text data, we are concerned with\\nthe tools and methods from the ﬁeld of Natural Language Processing. We have seen the path\\nfrom linguistics to NLP in the previous section. Now, let’s take a look at how modern researchers\\nand practitioners deﬁne what NLP is all about. In perhaps one of the more widely known\\ntextbooks written by top researchers in the ﬁeld, they refer to the subject as linguistic science ,\\npermitting discussion of both classical linguistics and modern statistical methods.\\nThe aim of a linguistic science is to be able to characterize and explain the multitude\\nof linguistic observations circling around us, in conversations, writing, and other\\nmedia. Part of that has to do with the cognitive size of how humans acquire, produce\\nand understand language, part of it has to do with understanding the relationship\\nbetween linguistic utterances and the world, and part of it has to do with understand\\nthe linguistic structures by which language communicates.\\n— Page 3, Foundations of Statistical Natural Language Processing , 1999.\\nThey go on to focus on inference through the use of statistical methods in natural language\\nprocessing.\\nStatistical NLP aims to do statistical inference for the ﬁeld of natural language.\\nStatistical inference in general consists of taking some data (generated in accordance\\nwith some unknown probability distribution) and then making some inference about\\nthis distribution.\\n— Page 191, Foundations of Statistical Natural Language Processing , 1999.\\nIn their text on applied natural language processing, the authors and contributors to the\\npopular NLTK Python library for NLP describe the ﬁeld broadly as using computers to work\\nwith natural language data.\\nWe will take Natural Language Processing - or NLP for short - in a wide sense to\\ncover any kind of computer manipulation of natural language. At one extreme, it\\ncould be as simple as counting word frequencies to compare diﬀerent writing styles.\\nAt the other extreme, NLP involves “understanding” complete human utterances,\\nat least to the extent of being able to give useful responses to them.\\n— Page ix, Natural Language Processing with Python , 2009.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 22}, page_content='1.5. Further Reading 6\\nStatistical NLP has turned another corner and is now strongly focused on the use of deep\\nlearning neural networks to both perform inference on speciﬁc tasks and for developing robust\\nend-to-end systems. In one of the ﬁrst textbooks dedicated to this emerging topic, Yoav Goldberg\\nsuccinctly deﬁnes NLP as automatic methods that take natural language as input or produce\\nnatural language as output.\\nNatural language processing (NLP) is a collective term referring to automatic\\ncomputational processing of human languages. This includes both algorithms that\\ntake human-produced text as input, and algorithms that produce natural looking\\ntext as outputs.\\n— Page xvii, Neural Network Methods in Natural Language Processing , 2017.\\n1.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n1.5.1 Books\\n\\x88Mathematical Linguistics , 2010.\\nhttp://amzn.to/2tO1cOO\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2u0JtPl\\n\\x88Computational Linguistics: An Introduction , 1986.\\nhttp://amzn.to/2h6U4qY\\n\\x88The Oxford Handbook of Computational Linguistics , 2005.\\nhttp://amzn.to/2uHeERE\\n\\x88Foundations of Statistical Natural Language Processing , 1999.\\nhttp://amzn.to/2uzwxDE\\n\\x88Natural Language Processing with Python , 2009.\\nhttp://amzn.to/2uZMF27\\n1.5.2 Wikipedia\\n\\x88Linguistics.\\nhttps://en.wikipedia.org/wiki/Linguistics\\n\\x88Computational linguistics.\\nhttps://en.wikipedia.org/wiki/Computational_linguistics\\n\\x88Natural language processing.\\nhttps://en.wikipedia.org/wiki/Natural_language_processing'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 23}, page_content='1.6. Summary 7\\n\\x88History of natural language processing.\\nhttps://en.wikipedia.org/wiki/History_of_natural_language_processing\\n\\x88Outline of natural language processing.\\nhttps://en.wikipedia.org/wiki/Outline_of_natural_language_processing\\n1.6 Summary\\nIn this chapter, you discovered what natural language processing is why it is so important.\\nSpeciﬁcally, you learned:\\n\\x88What natural language is and how it is diﬀerent from other types of data.\\n\\x88What makes working with natural language so challenging.\\n\\x88Where the ﬁeld of NLP came from and how it is deﬁned by modern practitioners.\\n1.6.1 Next\\nIn the next chapter, you will discover what deep learning is and the motivation behind using\\ndeep artiﬁcial neural networks.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 24}, page_content='Chapter 2\\nDeep Learning\\nDeep Learning is a subﬁeld of machine learning concerned with algorithms inspired by the\\nstructure and function of the brain called artiﬁcial neural networks. If you are just starting out\\nin the ﬁeld of deep learning or you had some experience with neural networks some time ago,\\nyou may be confused. I know I was confused initially and so were many of my colleagues and\\nfriends who learned and used neural networks in the 1990s and early 2000s.\\nThe leaders and experts in the ﬁeld have ideas of what deep learning is and these speciﬁc\\nand nuanced perspectives shed a lot of light on what deep learning is all about. In this chapter,\\nyou will discover exactly what deep learning is by hearing from a range of experts and leaders\\nin the ﬁeld. After reading this chapter, you will know:\\n\\x88The motivation for exploring and adopting large neural network models.\\n\\x88The perspective on deep learning as hierarchical feature learning.\\n\\x88The promise of scalability of deep learning with the size of data.\\nLet’s dive in.\\n2.1 Deep Learning is Large Neural Networks\\nAndrew Ng from Coursera and formally Chief Scientist at Baidu Research and founder of Google\\nBrain that eventually resulted in the productization of deep learning technologies across a large\\nnumber of Google services. He has spoken and written a lot about what deep learning is and\\nis a good place to start. In early talks on deep learning, Andrew described deep learning in\\nthe context of traditional artiﬁcial neural networks. In the 2013 talk titled Deep Learning,\\nSelf-Taught Learning and Unsupervised Feature Learning he described the idea of deep learning\\nas:\\nUsing brain simulations, hope to:\\n- Make learning algorithms much better and easier to use.\\n- Make revolutionary advances in machine learning and AI.\\nI believe this is our best shot at progress towards real AI\\n—Deep Learning, Self-Taught Learning and Unsupervised Feature Learning , 2013.\\n8'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 25}, page_content='2.1. Deep Learning is Large Neural Networks 9\\nLater his comments became more nuanced. The core of deep learning according to Andrew\\nis that we now have fast enough computers and enough data to actually train large neural\\nnetworks. When discussing why now is the time that deep learning is taking oﬀ at ExtractConf\\n2015 in a talk titled What data scientists should know about deep learning , he commented:\\n... very large neural networks we can now have and ... huge amounts of data that\\nwe have access to\\n—What data scientists should know about deep learning , 2015.\\nHe also commented on the important point that it is all about scale. That as we construct\\nlarger neural networks and train them with more and more data, their performance continues to\\nincrease. This is generally diﬀerent to other machine learning techniques that reach a plateau in\\nperformance.\\n... for most ﬂavors of the old generations of learning algorithms ... performance will\\nplateau. ... deep learning ... is the ﬁrst class of algorithms ... that is scalable. ...\\nperformance just keeps getting better as you feed them more data\\n—What data scientists should know about deep learning , 2015.\\nHe provides a nice cartoon of this in his slides:\\nFigure 2.1: Why Deep Learning? Slide by Andrew Ng, taken from What data scientists should\\nknow about deep learning .\\nFinally, he is clear to point out that the beneﬁts from deep learning that we are seeing in\\npractice come from supervised learning. From the 2015 ExtractConf talk, he commented:\\n... almost all the value today of deep learning is through supervised learning or\\nlearning from labeled data'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 26}, page_content='2.1. Deep Learning is Large Neural Networks 10\\n—What data scientists should know about deep learning , 2015.\\nEarlier at a talk to Stanford University titled Deep Learning in 2014 he made a similar\\ncomment:\\n... one reason that deep learning has taken oﬀ like crazy is because it is fantastic at\\nsupervised learning\\n—Invited Talk: Andrew Ng (Stanford University): Deep Learning , 2014.\\nAndrew often mentions that we should and will see more beneﬁts coming from the unsu-\\npervised side of the tracks as the ﬁeld matures to deal with the abundance of unlabeled data\\navailable. Jeﬀ Dean is a Wizard and Google Senior Fellow in the Systems and Infrastructure\\nGroup at Google and has been involved and perhaps partially responsible for the scaling and\\nadoption of deep learning within Google. Jeﬀ was involved in the Google Brain project and the\\ndevelopment of large-scale deep learning software DistBelief and later TensorFlow. In a 2016\\ntalk titled Deep Learning for Building Intelligent Computer Systems he made a comment in the\\nsimilar vein, that deep learning is really all about large neural networks.\\nWhen you hear the term deep learning, just think of a large deep neural net. Deep\\nrefers to the number of layers typically and so this kind of the popular term that’s\\nbeen adopted in the press. I think of them as deep neural networks generally.\\n—Deep Learning for Building Intelligent Computer Systems , 2016.\\nHe has given this talk a few times, and in a modiﬁed set of slides for the same talk, he\\nhighlights the scalability of neural networks indicating that results get better with more data\\nand larger models, that in turn require more computation to train.\\nFigure 2.2: Results Get Better With More Data, Larger Models, More Compute. Taken from\\nDeep Learning for Building Intelligent Computer Systems .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 27}, page_content='2.2. Deep Learning is Hierarchical Feature Learning 11\\n2.2 Deep Learning is Hierarchical Feature Learning\\nIn addition to scalability, another often cited beneﬁt of deep learning models is their ability to\\nperform automatic feature extraction from raw data, also called feature learning. Yoshua Bengio\\nis another leader in deep learning although began with a strong interest in the automatic feature\\nlearning that large neural networks are capable of achieving. He describes deep learning in terms\\nof the algorithms ability to discover and learn good representations using feature learning. In\\nhis 2012 paper titled Deep Learning of Representations for Unsupervised and Transfer Learning\\nhe commented:\\nDeep learning algorithms seek to exploit the unknown structure in the input dis-\\ntribution in order to discover good representations, often at multiple levels, with\\nhigher-level learned features deﬁned in terms of lower-level features\\n—Deep Learning of Representations for Unsupervised and Transfer Learning , 2012.\\nAn elaborated perspective of deep learning along these lines is provided in his 2009 technical\\nreport titled Learning deep architectures for AI where he emphasizes the importance the\\nhierarchy in feature learning.\\nDeep learning methods aim at learning feature hierarchies with features from higher\\nlevels of the hierarchy formed by the composition of lower level features. Auto-\\nmatically learning features at multiple levels of abstraction allow a system to learn\\ncomplex functions mapping the input to the output directly from data, without\\ndepending completely on human-crafted features.\\n—Learning deep architectures for AI , 2009.\\nIn the published book titled Deep Learning co-authored with Ian Goodfellow and Aaron\\nCourville, they deﬁne deep learning in terms of the depth of the architecture of the models.\\nThe hierarchy of concepts allows the computer to learn complicated concepts by\\nbuilding them out of simpler ones. If we draw a graph showing how these concepts\\nare built on top of each other, the graph is deep, with many layers. For this reason,\\nwe call this approach to AI deep learning.\\n—Deep Learning , 2016.\\nThis is an important book and will likely may be the deﬁnitive resource for the ﬁeld for some\\ntime. The book goes on to describe Multilayer Perceptrons as an algorithm used in the ﬁeld of\\ndeep learning, giving the idea that deep learning has subsumed artiﬁcial neural networks.\\nThe quintessential example of a deep learning model is the feedforward deep network\\nor multilayer perceptron (MLP).\\n—Deep Learning , 2016.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 28}, page_content='2.3. Deep Learning as Scalable Learning Across Domains 12\\nPeter Norvig is the Director of Research at Google and famous for his textbook on AI titled\\nArtiﬁcial Intelligence: A Modern Approach . In a 2016 talk he gave titled Deep Learning and\\nUnderstandability versus Software Engineering and Veriﬁcation he deﬁned deep learning in a\\nvery similar way to Yoshua, focusing on the power of abstraction permitted by using a deeper\\nnetwork structure.\\na kind of learning where the representation you form have several levels of abstraction,\\nrather than a direct input to output\\n—Deep Learning and Understandability versus Software Engineering and Veriﬁcation , 2016.\\n2.3 Deep Learning as Scalable Learning Across Domains\\nDeep learning excels on problem domains where the inputs (and even output) are analog.\\nMeaning, they are not a few quantities in a tabular format but instead are images of pixel\\ndata, documents of text data or ﬁles of audio data. Yann LeCun is the director of Facebook\\nResearch and is the father of the network architecture that excels at object recognition in image\\ndata called the Convolutional Neural Network (CNN). This technique is seeing great success\\nbecause like multilayer perceptron feedforward neural networks, the technique scales with data\\nand model size and can be trained with backpropagation.\\nThis biases his deﬁnition of deep learning as the development of very large CNNs, which have\\nhad great success on object recognition in photographs. In a 2016 talk at Lawrence Livermore\\nNational Laboratory titled Accelerating Understanding: Deep Learning, Intelligent Applications,\\nand GPUs he described deep learning generally as learning hierarchical representations and\\ndeﬁnes it as a scalable approach to building object recognition systems:\\ndeep learning [is] ... a pipeline of modules all of which are trainable. ... deep because\\n[has] multiple stages in the process of recognizing an object and all of those stages\\nare part of the training\\n—Accelerating Understanding: Deep Learning, Intelligent Applications, and GPUs , 2016.\\nJurgen Schmidhuber is the father of another popular algorithm that like MLPs and CNNs\\nalso scales with model size and dataset size and can be trained with backpropagation, but\\nis instead tailored to learning sequence data, called the Long Short-Term Memory Network\\n(LSTM), a type of recurrent neural network. We do see some confusion in the phrasing of the\\nﬁeld as deep learning . In his 2014 paper titled Deep Learning in Neural Networks: An Overview\\nhe does comment on the problematic naming of the ﬁeld and the diﬀerentiation of deep from\\nshallow learning. He also interestingly describes depth in terms of the complexity of the problem\\nrather than the model used to solve the problem.\\nAt which problem depth does Shallow Learning end, and Deep Learning begin?\\nDiscussions with DL experts have not yet yielded a conclusive response to this\\nquestion. [...], let me just deﬁne for the purposes of this overview: problems of depth\\n>10 require Very Deep Learning.\\n—Deep Learning in Neural Networks: An Overview , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 29}, page_content='2.4. Further Reading 13\\nDemis Hassabis is the founder of DeepMind, later acquired by Google. DeepMind made\\nthe breakthrough of combining deep learning techniques with reinforcement learning to handle\\ncomplex learning problems like game playing, famously demonstrated in playing Atari games\\nand the game Go with Alpha Go. In keeping with the naming, they called their new technique\\na Deep Q-Network, combining Deep Learning with Q-Learning. They also name the broader\\nﬁeld of study Deep Reinforcement Learning .\\nIn their 2015 nature paper titled Human-level control through deep reinforcement learning\\nthey comment on the important role of deep neural networks in their breakthrough and highlight\\nthe need for hierarchical abstraction.\\nTo achieve this, we developed a novel agent, a deep Q-network (DQN), which is\\nable to combine reinforcement learning with a class of artiﬁcial neural network\\nknown as deep neural networks. Notably, recent advances in deep neural networks,\\nin which several layers of nodes are used to build up progressively more abstract\\nrepresentations of the data, have made it possible for artiﬁcial neural networks to\\nlearn concepts such as object categories directly from raw sensory data.\\n—Human-level control through deep reinforcement learning , 2015.\\nFinally, in what may be considered a deﬁning paper in the ﬁeld, Yann LeCun, Yoshua Bengio\\nand Geoﬀrey Hinton published a paper in Nature titled simply Deep Learning . In it, they open\\nwith a clean deﬁnition of deep learning highlighting the multilayered approach.\\nDeep learning allows computational models that are composed of multiple processing\\nlayers to learn representations of data with multiple levels of abstraction.\\n—Deep Learning , 2015.\\nLater the multilayered approach is described in terms of representation learning and abstrac-\\ntion.\\nDeep-learning methods are representation-learning methods with multiple levels of\\nrepresentation, obtained by composing simple but non-linear modules that each\\ntransform the representation at one level (starting with the raw input) into a\\nrepresentation at a higher, slightly more abstract level. [...] The key aspect of deep\\nlearning is that these layers of features are not designed by human engineers: they\\nare learned from data using a general-purpose learning procedure.\\n—Deep Learning , 2015.\\nThis is a nice and generic description, and could easily describe most artiﬁcial neural network\\nalgorithms. It is also a good note to end on.\\n2.4 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 30}, page_content='2.5. Summary 14\\n2.4.1 Videos\\n\\x88Deep Learning, Self-Taught Learning and Unsupervised Feature Learning , 2013.\\nhttps://www.youtube.com/watch?v=n1ViNeWhC24\\n\\x88What data scientists should know about deep learning , 2015.\\nhttps://www.youtube.com/watch?v=O0VN0pGgBZM\\n\\x88Invited Talk: Andrew Ng (Stanford University): Deep Learning 2014.\\nhttps://www.youtube.com/watch?v=W15K9PegQt0\\n\\x88Deep Learning for Building Intelligent Computer Systems , 2016.\\nhttps://www.youtube.com/watch?v=QSaZGT4-6EY\\n\\x88Deep Learning and Understandability versus Software Engineering and Veriﬁcation , 2016.\\nhttps://www.youtube.com/watch?v=X769cyzBNVw\\n\\x88Accelerating Understanding: Deep Learning, Intelligent Applications, and GPUs , 2016.\\nhttps://www.youtube.com/watch?v=Qk4SqF9FT-M\\n2.4.2 Books\\n\\x88Deep Learning , 2016.\\nhttp://amzn.to/2goLnbO\\n2.4.3 Articles\\n\\x88Deep Learning of Representations for Unsupervised and Transfer Learning , 2012.\\nhttp://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf\\n\\x88Learning deep architectures for AI , 2009.\\nhttp://www.iro.umontreal.ca/ ~lisa/publications2/index.php/publications/show/\\n239\\n\\x88Deep Learning in Neural Networks: An Overview , 2014.\\nhttp://arxiv.org/pdf/1404.7828v4.pdf\\n\\x88Human-level control through deep reinforcement learning , 2015.\\nhttp://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\\n\\x88Deep Learning , 2015.\\nhttp://www.nature.com/nature/journal/v521/n7553/full/nature14539.html\\n2.5 Summary\\nIn this chapter you discovered that deep learning is just very big neural networks on a lot\\nmore data, requiring bigger computers. Although early approaches published by Hinton and\\ncollaborators focus on greedy layer-wise training and unsupervised methods like autoencoders,\\nmodern state-of-the-art deep learning is focused on training deep (many layered) neural network'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 31}, page_content='2.5. Summary 15\\nmodels using the backpropagation algorithm. The most popular techniques that we will focus\\non are:\\n\\x88Multilayer Perceptron Networks (MLP).\\n\\x88Convolutional Neural Networks (CNN).\\n\\x88Long Short-Term Memory Recurrent Neural Networks (LSTM).\\nI hope this has cleared up what deep learning is and how leading deﬁnitions ﬁt together\\nunder the one umbrella.\\n2.5.1 Next\\nIn the next chapter, you will discover the promise of deep learning neural networks for the ﬁeld\\nof natural language processing.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 32}, page_content='Chapter 3\\nPromise of Deep Learning for Natural\\nLanguage\\nThe promise of deep learning in the ﬁeld of natural language processing is the better performance\\nby models that may require more data but less linguistic expertise to train and operate. There\\nis a lot of hype and large claims around deep learning methods, but beyond the hype, deep\\nlearning methods are achieving state-of-the-art results on challenging problems. Notably in\\nnatural language processing. In this chapter, you will discover the speciﬁc promises that deep\\nlearning methods have for tackling natural language processing problems. After reading this\\nchapter, you will know:\\n\\x88The promises of deep learning for natural language processing.\\n\\x88What practitioners and research scientists have to say about the promise of deep learning\\nin NLP.\\n\\x88Key deep learning methods and applications for natural language processing.\\nLet’s get started.\\n3.1 Promise of Deep Learning\\nDeep learning methods are popular, primarily because they are delivering on their promise.\\nThat is not to say that there is no hype around the technology, but that the hype is based\\non very real results that are being demonstrated across a suite of very challenging artiﬁcial\\nintelligence problems from computer vision and natural language processing. Some of the\\nﬁrst large demonstrations of the power of deep learning were in natural language processing,\\nspeciﬁcally speech recognition. More recently in machine translation.\\nIn this chapter, we will look at ﬁve speciﬁc promises of deep learning methods in the ﬁeld of\\nnatural language processing. Promises highlighted recently by researchers and practitioners in\\nthe ﬁeld, people who may be more tempered than the average reported in what the promises\\nmay be. In summary, they are:\\n\\x88The Promise of Drop-in Replacement Models . That is, deep learning methods can\\nbe dropped into existing natural language systems as replacement models that can achieve\\ncommensurate or better performance.\\n16'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 33}, page_content='3.2. Promise of Drop-in Replacement Models 17\\n\\x88The Promise of New NLP Models . That is, deep learning methods oﬀer the op-\\nportunity of new modeling approaches to challenging natural language problems like\\nsequence-to-sequence prediction.\\n\\x88The Promise of Feature Learning . That is, that deep learning methods can learn\\nthe features from natural language required by the model, rather than requiring that the\\nfeatures be speciﬁed and extracted by an expert.\\n\\x88The Promise of Continued Improvement . That is, that the performance of deep\\nlearning in natural language processing is based on real results and that the improvements\\nappear to be continuing and perhaps speeding up.\\n\\x88The Promise of End-to-End Models . That is, that large end-to-end deep learning\\nmodels can be ﬁt on natural language problems oﬀering a more general and better-\\nperforming approach.\\nWe will now take a closer look at each. There are other promises of deep learning for natural\\nlanguage processing; these were just the 5 that I chose to highlight.\\n3.2 Promise of Drop-in Replacement Models\\nThe ﬁrst promise for deep learning in natural language processing is the ability to replace\\nexisting linear models with better performing models capable of learning and exploiting nonlinear\\nrelationships. Yoav Goldberg, in his primer on neural networks for NLP researchers, highlights\\nboth that deep learning methods are achieving impressive results.\\nMore recently, neural network models started to be applied also to textual natural\\nlanguage signals, again with very promising results.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\nHe goes on to highlight that the methods are easy to use and can sometimes be used to\\nwholesale replace existing linear methods.\\nRecently, the ﬁeld has seen some success in switching from such linear models over\\nsparse inputs to non-linear neural-network models over dense inputs. While most\\nof the neural network techniques are easy to apply, sometimes as almost drop-in\\nreplacements of the old linear classiﬁers, there is in many cases a strong barrier of\\nentry.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\n3.3 Promise of New NLP Models\\nAnother promise is that deep learning methods facilitate developing entirely new models. One\\nstrong example is the use of recurrent neural networks that are able learn and condition output\\nover very long sequences. The approach is suﬃciently diﬀerent in that they allow the practitioner'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 34}, page_content='3.4. Promise of Feature Learning 18\\nto break free of traditional modeling assumptions and in turn achieve state-of-the-art results.\\nIn his book expanding on deep learning for NLP, Yoav Goldberg comments that sophisticated\\nneural network models like recurrent neural networks allow for wholly new NLP modeling\\nopportunities.\\nAround 2014, the ﬁeld has started to see some success in switching from such\\nlinear models over sparse inputs to nonlinear neural network models over dense\\ninputs. ... Others are more advanced, require a change of mindset, and provide new\\nmodeling opportunities, In particular, a family of approaches based on recurrent\\nneural networks (RNNs) alleviates the reliance on the Markov Assumption that was\\nprevalent in sequence models, allowing to condition on arbitrary long sequences\\nand produce eﬀective feature extractors. These advance lead to breakthroughs in\\nlanguage modeling, automatic machine translations and other applications.\\n— Page xvii, Neural Network Methods in Natural Language Processing , 2017.\\n3.4 Promise of Feature Learning\\nDeep learning methods have the ability to learn feature representations rather than requiring\\nexperts to manually specify and extract features from natural language. The NLP researcher\\nChris Manning, in the ﬁrst lecture of his course on deep learning for natural language processing,\\nhighlights a diﬀerent perspective. He describes the limitations of manually deﬁned input features,\\nwhere prior applications of machine learning in statistical NLP were really a testament to the\\nhumans deﬁning the features and that the computers did very little learning.\\nChris suggests that the promise of deep learning methods is the automatic feature learning.\\nHe highlights that feature learning is automatic rather than manual, easy to adapt rather than\\nbrittle, and can continually and automatically improve.\\nIn general our manually designed features tend to be overspeciﬁed, incomplete, take a\\nlong time to design and validated, and only get you to a certain level of performance\\nat the end of the day. Where the learned features are easy to adapt, fast to train\\nand they can keep on learning so that they get to a better level of performance they\\nwe’ve been able to achieve previously.\\n— Chris Manning, Lecture 1 – Natural Language Processing with Deep Learning, 2017.\\n3.5 Promise of Continued Improvement\\nAnother promise of deep learning for NLP is continued and rapid improvement on challenging\\nproblems. In the same initial lecture on deep learning for NLP, Chris Manning goes on to\\ndescribe that deep learning methods are popular for natural language because they are working.\\nThe real reason why deep learning is so exciting to most people is it has been\\nworking.\\n— Chris Manning, Lecture 1 – Natural Language Processing with Deep Learning, 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 35}, page_content='3.6. Promise of End-to-End Models 19\\nHe highlights that initial results were impressive and achieved results in speech better than\\nany other methods in the last 30 years. Chris goes on to mention that it is not just the\\nstate-of-the-art results being achieved, but also the rate of improvement.\\n... what has just been totally stunning is over the last 6 or 7 years, there’s just\\nbeen this amazing ramp in which deep learning methods have been keeping on being\\nimproved and getting better at just an amazing speed. ... I’d actually just say\\nit unprecedented, in terms of seeming a ﬁeld that has been progressing quite so\\nquickly in its ability to be sort of rolling out better methods of doing things month\\non month.\\n— Chris Manning, Lecture 1 – Natural Language Processing with Deep Learning, 2017.\\n3.6 Promise of End-to-End Models\\nA ﬁnal promise of deep learning is the ability to develop and train end-to-end models for natural\\nlanguage problems instead of developing pipelines of specialized models. This is desirable both\\nfor the speed and simplicity of development in addition to the improved performance of these\\nmodels.\\nNeural machine translation, or NMT for short, refers to large neural networks that attempt\\nto learn to translate one language to another. This was a task traditionally handled by a pipeline\\nof classical hand-tuned models, each of which required specialized expertise. This is described\\nby Chris Manning in lecture 10 of his Stanford course on deep learning for NLP.\\nNeural machine translation is used to mean what we want to do is build one big\\nneural network which we can train entire end-to-end machine translation process in\\nand optimize end-to-end.\\n[...]\\nThis move away from hand customized piecewise models towards end-to-end sequence-\\nto-sequence prediction models has been the trend in speech recognition. Systems\\nthat do that are referred to as an NMT [neural machine translation] system.\\n— Chris Manning, Lecture 10: Neural Machine Translation and Models with Attention, 2017.\\nThis trend towards end-to-end models rather than pipelines of specialized systems is also\\na trend in speech recognition. In his presentation of speech recognition in the Stanford NLP\\ncourse, the NLP researcher Navdeep Jaitly, now at Nvidia, highlights that each component of a\\nspeech recognition can be replaced with a neural network. The large blocks of an automatic\\nspeech recognition pipeline are speech processing, acoustic models, pronunciation models, and\\nlanguage models. The problem is, the properties and importantly the errors of each sub-system\\nare diﬀerent. This motivates the need to develop one neural network to learn the whole problem\\nend-to-end.\\nOver time people starting noticing that each of these components could be done\\nbetter if we used a neural network. ... However, there’s still a problem. There’s\\nneural networks in every component, but errors in each one are diﬀerent, so they\\nmay not play well together. So that is the basic motivation for trying to go to a\\nprocess where you train entire model as one big model itself.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 36}, page_content='3.7. Further Reading 20\\n— Navdeep Jaitly, Lecture 12: End-to-End Models for Speech Processing, Natural Language\\nProcessing with Deep Learning, 2017.\\n3.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88A Primer on Neural Network Models for Natural Language Processing , 2015.\\nhttps://arxiv.org/abs/1510.00726\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2eScGtY\\n\\x88Stanford CS224n: Natural Language Processing with Deep Learning , 2017.\\nhttp://web.stanford.edu/class/cs224n/\\n3.8 Summary\\nIn this chapter, you discovered the promise of deep learning neural networks for natural language\\nprocessing. Speciﬁcally, you learned:\\n\\x88The promises of deep learning for natural language processing.\\n\\x88What practitioners and research scientists have to say about the promise of deep learning\\nin NLP.\\n\\x88Key deep learning methods and applications for natural language processing.\\n3.8.1 Next\\nIn the next chapter, you will discover how you can develop deep learning neural networks using\\nthe Keras Python library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 37}, page_content='Chapter 4\\nHow to Develop Deep Learning Models\\nWith Keras\\nDeep learning neural networks are very easy to create and evaluate in Python with Keras, but\\nyou must follow a strict model life-cycle. In this chapter you will discover the step-by-step\\nlife-cycle for creating, training and evaluating deep learning neural networks in Keras and how\\nto make predictions with a trained model. You will also discover how to use the functional API\\nthat provides more ﬂexibility when designing models. After reading this chapter you will know:\\n\\x88How to deﬁne, compile, ﬁt and evaluate a deep learning neural network in Keras.\\n\\x88How to select standard defaults for regression and classiﬁcation predictive modeling\\nproblems.\\n\\x88How to use the functional API to develop standard Multilayer Perceptron, convolutional\\nand recurrent neural networks.\\nLet’s get started.\\nNote: It is assumed that you have a basic familiarity with deep learning and Keras, this chapter\\nshould provide a refresher for the Keras API, and perhaps an introduction to the Keras\\nfunctional API. See the Appendix for installation instructions. Most code snippets in this\\ntutorial are just for reference and are not complete examples.\\n4.1 Keras Model Life-Cycle\\nBelow is an overview of the 5 steps in the neural network model life-cycle in Keras:\\n1. Deﬁne Network.\\n2. Compile Network.\\n3. Fit Network.\\n4. Evaluate Network.\\n5. Make Predictions.\\n21'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 38}, page_content='4.1. Keras Model Life-Cycle 22\\nFigure 4.1: 5 Step Life-Cycle for Neural Network Models in Keras.\\nLet’s take a look at each step in turn using the easy-to-use Keras Sequential API.\\n4.1.1 Step 1. Deﬁne Network\\nThe ﬁrst step is to deﬁne your neural network. Neural networks are deﬁned in Keras as a\\nsequence of layers. The container for these layers is the Sequential class. The ﬁrst step is to\\ncreate an instance of the Sequential class. Then you can create your layers and add them in\\nthe order that they should be connected. For example, we can do this in two steps:\\nmodel = Sequential()\\nmodel.add(Dense(2))\\nListing 4.1: Sequential model with one Dense layer with 2 neurons.\\nBut we can also do this in one step by creating an array of layers and passing it to the\\nconstructor of the Sequential class.\\nlayers = [Dense(2)]\\nmodel = Sequential(layers)\\nListing 4.2: Layers for a Sequential model deﬁned as an array.\\nThe ﬁrst layer in the network must deﬁne the number of inputs to expect. The way that this\\nis speciﬁed can diﬀer depending on the network type, but for a Multilayer Perceptron model\\nthis is speciﬁed by the input dimattribute. For example, a small Multilayer Perceptron model\\nwith 2 inputs in the visible layer, 5 neurons in the hidden layer and one neuron in the output\\nlayer can be deﬁned as:\\nmodel = Sequential()\\nmodel.add(Dense(5, input_dim=2))\\nmodel.add(Dense(1))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 39}, page_content=\"4.1. Keras Model Life-Cycle 23\\nListing 4.3: Sequential model with 2 inputs.\\nThink of a Sequential model as a pipeline with your raw data fed in at the bottom and\\npredictions that come out at the top. This is a helpful conception in Keras as concerns that were\\ntraditionally associated with a layer can also be split out and added as separate layers, clearly\\nshowing their role in the transform of data from input to prediction. For example, activation\\nfunctions that transform a summed signal from each neuron in a layer can be extracted and\\nadded to the Sequential as a layer-like object called the Activation class.\\nmodel = Sequential()\\nmodel.add(Dense(5, input_dim=2))\\nmodel.add(Activation( 'relu '))\\nmodel.add(Dense(1))\\nmodel.add(Activation( 'sigmoid '))\\nListing 4.4: Sequential model with Activation functions deﬁned separately from layers.\\nThe choice of activation function is most important for the output layer as it will deﬁne the\\nformat that predictions will take. For example, below are some common predictive modeling\\nproblem types and the structure and standard activation function that you can use in the output\\nlayer:\\n\\x88Regression : Linear activation function, or linear , and the number of neurons matching\\nthe number of outputs.\\n\\x88Binary Classiﬁcation (2 class) : Logistic activation function, or sigmoid , and one\\nneuron the output layer.\\n\\x88Multiclass Classiﬁcation ( >2 class) : Softmax activation function, or softmax , and\\none output neuron per class value, assuming a one hot encoded output pattern.\\n4.1.2 Step 2. Compile Network\\nOnce we have deﬁned our network, we must compile it. Compilation is an eﬃciency step. It\\ntransforms the simple sequence of layers that we deﬁned into a highly eﬃcient series of matrix\\ntransforms in a format intended to be executed on your GPU or CPU, depending on how Keras\\nis conﬁgured. Think of compilation as a precompute step for your network. It is always required\\nafter deﬁning a model.\\nCompilation requires a number of parameters to be speciﬁed, speciﬁcally tailored to training\\nyour network. Speciﬁcally, the optimization algorithm to use to train the network and the loss\\nfunction used to evaluate the network that is minimized by the optimization algorithm. For\\nexample, below is a case of compiling a deﬁned model and specifying the stochastic gradient\\ndescent ( sgd) optimization algorithm and the mean squared error ( mean squared error ) loss\\nfunction, intended for a regression type problem.\\nmodel.compile(optimizer= 'sgd ', loss= 'mean_squared_error ')\\nListing 4.5: Example of compiling a deﬁned model.\\nAlternately, the optimizer can be created and conﬁgured before being provided as an argument\\nto the compilation step.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 40}, page_content=\"4.1. Keras Model Life-Cycle 24\\nalgorithm = SGD(lr=0.1, momentum=0.3)\\nmodel.compile(optimizer=algorithm, loss= 'mean_squared_error ')\\nListing 4.6: Example of deﬁning the optimization algorithm separately.\\nThe type of predictive modeling problem imposes constraints on the type of loss function\\nthat can be used. For example, below are some standard loss functions for diﬀerent predictive\\nmodel types:\\n\\x88Regression : Mean Squared Error or mean squared error .\\n\\x88Binary Classiﬁcation (2 class) : Logarithmic Loss, also called cross entropy or\\nbinary crossentropy .\\n\\x88Multiclass Classiﬁcation ( >2 class) : Multiclass Logarithmic Loss or\\ncategorical crossentropy .\\nThe most common optimization algorithm is stochastic gradient descent, but Keras also\\nsupports a suite of other state-of-the-art optimization algorithms that work well with little or\\nno conﬁguration. Perhaps the most commonly used optimization algorithms because of their\\ngenerally better performance are:\\n\\x88Stochastic Gradient Descent , orsgd, that requires the tuning of a learning rate and\\nmomentum.\\n\\x88Adam , oradam , that requires the tuning of learning rate.\\n\\x88RMSprop , orrmsprop , that requires the tuning of learning rate.\\nFinally, you can also specify metrics to collect while ﬁtting your model in addition to the\\nloss function. Generally, the most useful additional metric to collect is accuracy for classiﬁcation\\nproblems. The metrics to collect are speciﬁed by name in an array. For example:\\nmodel.compile(optimizer= 'sgd ', loss= 'mean_squared_error ', metrics=[ 'accuracy '])\\nListing 4.7: Example of deﬁning metrics when compiling the model.\\n4.1.3 Step 3. Fit Network\\nOnce the network is compiled, it can be ﬁt, which means adapt the weights on a training dataset.\\nFitting the network requires the training data to be speciﬁed, both a matrix of input patterns, X,\\nand an array of matching output patterns, y. The network is trained using the backpropagation\\nalgorithm and optimized according to the optimization algorithm and loss function speciﬁed\\nwhen compiling the model.\\nThe backpropagation algorithm requires that the network be trained for a speciﬁed number\\nof epochs or exposures to the training dataset. Each epoch can be partitioned into groups\\nof input-output pattern pairs called batches. This deﬁnes the number of patterns that the\\nnetwork is exposed to before the weights are updated within an epoch. It is also an eﬃciency\\noptimization, ensuring that not too many input patterns are loaded into memory at a time. A\\nminimal example of ﬁtting a network is as follows:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 41}, page_content='4.1. Keras Model Life-Cycle 25\\nhistory = model.fit(X, y, batch_size=10, epochs=100)\\nListing 4.8: Example of ﬁtting a compiled model.\\nOnce ﬁt, a history object is returned that provides a summary of the performance of the\\nmodel during training. This includes both the loss and any additional metrics speciﬁed when\\ncompiling the model, recorded each epoch. Training can take a long time, from seconds to hours\\nto days depending on the size of the network and the size of the training data.\\nBy default, a progress bar is displayed on the command line for each epoch. This may create\\ntoo much noise for you, or may cause problems for your environment, such as if you are in an\\ninteractive notebook or IDE. You can reduce the amount of information displayed to just the\\nloss each epoch by setting the verbose argument to 2. You can turn oﬀ all output by setting\\nverbose to 0. For example:\\nhistory = model.fit(X, y, batch_size=10, epochs=100, verbose=0)\\nListing 4.9: Example of turning oﬀ verbose output when ﬁtting the model.\\n4.1.4 Step 4. Evaluate Network\\nOnce the network is trained, it can be evaluated. The network can be evaluated on the training\\ndata, but this will not provide a useful indication of the performance of the network as a\\npredictive model, as it has seen all of this data before. We can evaluate the performance of\\nthe network on a separate dataset, unseen during testing. This will provide an estimate of the\\nperformance of the network at making predictions for unseen data in the future.\\nThe model evaluates the loss across all of the test patterns, as well as any other metrics\\nspeciﬁed when the model was compiled, like classiﬁcation accuracy. A list of evaluation metrics\\nis returned. For example, for a model compiled with the accuracy metric, we could evaluate it\\non a new dataset as follows:\\nloss, accuracy = model.evaluate(X, y)\\nListing 4.10: Example of evaluating a ﬁt model.\\nAs with ﬁtting the network, verbose output is provided to give an idea of the progress of\\nevaluating the model. We can turn this oﬀ by setting the verbose argument to 0.\\nloss, accuracy = model.evaluate(X, y, verbose=0)\\nListing 4.11: Example of turning oﬀ verbose output when evaluating a ﬁt model.\\n4.1.5 Step 5. Make Predictions\\nOnce we are satisﬁed with the performance of our ﬁt model, we can use it to make predictions\\non new data. This is as easy as calling the predict() function on the model with an array of\\nnew input patterns. For example:\\npredictions = model.predict(X)\\nListing 4.12: Example of making a prediction with a ﬁt model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 42}, page_content='4.2. Keras Functional Models 26\\nThe predictions will be returned in the format provided by the output layer of the network.\\nIn the case of a regression problem, these predictions may be in the format of the problem\\ndirectly, provided by a linear activation function. For a binary classiﬁcation problem, the\\npredictions may be an array of probabilities for the ﬁrst class that can be converted to a 1 or 0\\nby rounding.\\nFor a multiclass classiﬁcation problem, the results may be in the form of an array of\\nprobabilities (assuming a one hot encoded output variable) that may need to be converted to a\\nsingle class output prediction using the argmax() NumPy function. Alternately, for classiﬁcation\\nproblems, we can use the predict classes() function that will automatically convert uncrisp\\npredictions to crisp integer class values.\\npredictions = model.predict_classes(X)\\nListing 4.13: Example of predicting classes with a ﬁt model.\\nAs with ﬁtting and evaluating the network, verbose output is provided to give an idea of the\\nprogress of the model making predictions. We can turn this oﬀ by setting the verbose argument\\nto 0.\\npredictions = model.predict(X, verbose=0)\\nListing 4.14: Example of disabling verbose output when making predictions.\\n4.2 Keras Functional Models\\nThe sequential API allows you to create models layer-by-layer for most problems. It is limited\\nin that it does not allow you to create models that share layers or have multiple inputs or\\noutputs. The functional API in Keras is an alternate way of creating models that oﬀers a lot\\nmore ﬂexibility, including creating more complex models.\\nIt speciﬁcally allows you to deﬁne multiple input or output models as well as models that\\nshare layers. More than that, it allows you to deﬁne ad hoc acyclic network graphs. Models are\\ndeﬁned by creating instances of layers and connecting them directly to each other in pairs, then\\ndeﬁning a Model that speciﬁes the layers to act as the input and output to the model. Let’s\\nlook at the three unique aspects of Keras functional API in turn:\\n4.2.1 Deﬁning Input\\nUnlike the Sequential model, you must create and deﬁne a standalone Input layer that speciﬁes\\nthe shape of input data. The input layer takes a shape argument that is a tuple that indicates the\\ndimensionality of the input data. When input data is one-dimensional, such as for a Multilayer\\nPerceptron, the shape must explicitly leave room for the shape of the mini-batch size used when\\nsplitting the data when training the network. Therefore, the shape tuple is always deﬁned with\\na hanging last dimension (2,) , for example:\\nfrom keras.layers import Input\\nvisible = Input(shape=(2,))\\nListing 4.15: Example of deﬁning input for a functional model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 43}, page_content='4.3. Standard Network Models 27\\n4.2.2 Connecting Layers\\nThe layers in the model are connected pairwise. This is done by specifying where the input\\ncomes from when deﬁning each new layer. A bracket notation is used, such that after the layer\\nis created, the layer from which the input to the current layer comes from is speciﬁed. Let’s\\nmake this clear with a short example. We can create the input layer as above, then create a\\nhidden layer as a Dense that receives input only from the input layer.\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nvisible = Input(shape=(2,))\\nhidden = Dense(2)(visible)\\nListing 4.16: Example of connecting a hidden layer to the visible layer.\\nNote it is the visible after the creation of the Dense layer that connects the input layer’s\\noutput as the input to the Dense hidden layer. It is this way of connecting layers piece by piece\\nthat gives the functional API its ﬂexibility. For example, you can see how easy it would be to\\nstart deﬁning ad hoc graphs of layers.\\n4.2.3 Creating the Model\\nAfter creating all of your model layers and connecting them together, you must deﬁne the model.\\nAs with the Sequential API, the model is the thing you can summarize, ﬁt, evaluate, and use to\\nmake predictions. Keras provides a Model class that you can use to create a model from your\\ncreated layers. It requires that you only specify the input and output layers. For example:\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nvisible = Input(shape=(2,))\\nhidden = Dense(2)(visible)\\nmodel = Model(inputs=visible, outputs=hidden)\\nListing 4.17: Example of creating a full model with the functional API.\\nNow that we know all of the key pieces of the Keras functional API, let’s work through\\ndeﬁning a suite of diﬀerent models and build up some practice with it. Each example is\\nexecutable and prints the structure and creates a diagram of the graph. I recommend doing\\nthis for your own models to make it clear what exactly you have deﬁned. My hope is that\\nthese examples provide templates for you when you want to deﬁne your own models using the\\nfunctional API in the future.\\n4.3 Standard Network Models\\nWhen getting started with the functional API, it is a good idea to see how some standard\\nneural network models are deﬁned. In this section, we will look at deﬁning a simple Multilayer\\nPerceptron, convolutional neural network, and recurrent neural network. These examples will\\nprovide a foundation for understanding the more elaborate examples later.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 44}, page_content=\"4.3. Standard Network Models 28\\n4.3.1 Multilayer Perceptron\\nIn this section, we deﬁne a Multilayer Perceptron model for binary classiﬁcation. The model\\nhas 10 inputs, 3 hidden layers with 10, 20, and 10 neurons, and an output layer with 1 output.\\nRectiﬁed linear activation functions are used in each hidden layer and a sigmoid activation\\nfunction is used in the output layer, for binary classiﬁcation.\\n# Multilayer Perceptron\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nvisible = Input(shape=(10,))\\nhidden1 = Dense(10, activation= 'relu ')(visible)\\nhidden2 = Dense(20, activation= 'relu ')(hidden1)\\nhidden3 = Dense(10, activation= 'relu ')(hidden2)\\noutput = Dense(1, activation= 'sigmoid ')(hidden3)\\nmodel = Model(inputs=visible, outputs=output)\\n# summarize layers\\nmodel.summary()\\n# plot graph\\nplot_model(model, to_file= 'multilayer_perceptron_graph.png ')\\nListing 4.18: Example of deﬁning an MLP with the functional API.\\nRunning the example prints the structure of the network.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 10) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 10) 110\\n_________________________________________________________________\\ndense_2 (Dense) (None, 20) 220\\n_________________________________________________________________\\ndense_3 (Dense) (None, 10) 210\\n_________________________________________________________________\\ndense_4 (Dense) (None, 1) 11\\n=================================================================\\nTotal params: 551\\nTrainable params: 551\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 4.19: Summary of MLP model deﬁned with the functional API.\\nA plot of the model graph is also created and saved to ﬁle.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 45}, page_content=\"4.3. Standard Network Models 29\\nFigure 4.2: Plot of the MLP Model Graph.\\nNote, creating plots of Keras models requires that you install pydot andpygraphviz (the\\ngraphviz library and the python wrapper). Instructions for installing these libraries vary for\\ndiﬀerent systems. If this is a challenge for you (e.g. you’re on windows), consider commenting\\nout the calls to plot model() when you see them.\\n4.3.2 Convolutional Neural Network\\nIn this section, we will deﬁne a convolutional neural network for image classiﬁcation. The model\\nreceives black and white 64 x 64 images as input, then has a sequence of two convolutional and\\npooling layers as feature extractors, followed by a fully connected layer to interpret the features\\nand an output layer with a sigmoid activation for two-class predictions.\\n# Convolutional Neural Network\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers.convolutional import Conv2D\\nfrom keras.layers.pooling import MaxPooling2D\\nvisible = Input(shape=(64,64,1))\\nconv1 = Conv2D(32, kernel_size=4, activation= 'relu ')(visible)\\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\\nconv2 = Conv2D(16, kernel_size=4, activation= 'relu ')(pool1)\\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\\nhidden1 = Dense(10, activation= 'relu ')(pool2)\\noutput = Dense(1, activation= 'sigmoid ')(hidden1)\\nmodel = Model(inputs=visible, outputs=output)\\n# summarize layers\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 46}, page_content=\"4.3. Standard Network Models 30\\nmodel.summary()\\n# plot graph\\nplot_model(model, to_file= 'convolutional_neural_network.png ')\\nListing 4.20: Example of deﬁning an CNN with the functional API.\\nRunning the example summarizes the model layers.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 64, 64, 1) 0\\n_________________________________________________________________\\nconv2d_1 (Conv2D) (None, 61, 61, 32) 544\\n_________________________________________________________________\\nmax_pooling2d_1 (MaxPooling2 (None, 30, 30, 32) 0\\n_________________________________________________________________\\nconv2d_2 (Conv2D) (None, 27, 27, 16) 8208\\n_________________________________________________________________\\nmax_pooling2d_2 (MaxPooling2 (None, 13, 13, 16) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 13, 13, 10) 170\\n_________________________________________________________________\\ndense_2 (Dense) (None, 13, 13, 1) 11\\n=================================================================\\nTotal params: 8,933\\nTrainable params: 8,933\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 4.21: Summary of CNN model deﬁned with the functional API.\\nA plot of the model graph is also created and saved to ﬁle.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 47}, page_content=\"4.3. Standard Network Models 31\\nFigure 4.3: Plot of the CNN Model Graph.\\n4.3.3 Recurrent Neural Network\\nIn this section, we will deﬁne a long short-term memory recurrent neural network for sequence\\nclassiﬁcation. The model expects 100 time steps of one feature as input. The model has a single\\nLSTM hidden layer to extract features from the sequence, followed by a fully connected layer to\\ninterpret the LSTM output, followed by an output layer for making binary predictions.\\n# Recurrent Neural Network\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers.recurrent import LSTM\\nvisible = Input(shape=(100,1))\\nhidden1 = LSTM(10)(visible)\\nhidden2 = Dense(10, activation= 'relu ')(hidden1)\\noutput = Dense(1, activation= 'sigmoid ')(hidden2)\\nmodel = Model(inputs=visible, outputs=output)\\n# summarize layers\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 48}, page_content=\"4.4. Further Reading 32\\nmodel.summary()\\n# plot graph\\nplot_model(model, to_file= 'recurrent_neural_network.png ')\\nListing 4.22: Example of deﬁning an RNN with the functional API.\\nRunning the example summarizes the model layers.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 100, 1) 0\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 10) 480\\n_________________________________________________________________\\ndense_1 (Dense) (None, 10) 110\\n_________________________________________________________________\\ndense_2 (Dense) (None, 1) 11\\n=================================================================\\nTotal params: 601\\nTrainable params: 601\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 4.23: Summary of RNN model deﬁned with the functional API.\\nA plot of the model graph is also created and saved to ﬁle.\\nFigure 4.4: Plot of the RNN Model Graph.\\n4.4 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Keras documentation for Sequential Models.\\nhttps://keras.io/models/sequential/\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 49}, page_content='4.5. Summary 33\\n\\x88Keras documentation for Functional Models.\\nhttps://keras.io/models/model/\\n\\x88Getting started with the Keras Sequential model.\\nhttps://keras.io/models/model/\\n\\x88Getting started with the Keras functional API.\\nhttps://keras.io/models/model/\\n\\x88Keras documentation for optimization algorithms.\\nhttps://keras.io/optimizers/\\n\\x88Keras documentation for loss functions.\\nhttps://keras.io/losses/\\n4.5 Summary\\nIn this tutorial, you discovered the step-by-step life-cycle for creating, training and evaluating\\ndeep learning neural networks in Keras and how to use the functional API that provides more\\nﬂexibility when deigning models. Speciﬁcally, you learned:\\n\\x88How to deﬁne, compile, ﬁt and evaluate a deep learning neural network in Keras.\\n\\x88How to select standard defaults for regression and classiﬁcation predictive modeling\\nproblems.\\n\\x88How to use the functional API to develop standard Multilayer Perceptron, convolutional\\nand recurrent neural networks.\\n4.5.1 Next\\nThis is the last chapter in the foundations part. In the next part, you will discover how you can\\nprepare text data ready for modeling.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 50}, page_content='Part III\\nData Preparation\\n34'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 51}, page_content='Chapter 5\\nHow to Clean Text Manually and with\\nNLTK\\nYou cannot go straight from raw text to ﬁtting a machine learning or deep learning model. You\\nmust clean your text ﬁrst, which means splitting it into words and handling punctuation and\\ncase. In fact, there is a whole suite of text preparation methods that you may need to use, and\\nthe choice of methods really depends on your natural language processing task. In this tutorial,\\nyou will discover how you can clean and prepare your text ready for modeling with machine\\nlearning. After completing this tutorial, you will know:\\n\\x88How to get started by developing your own very simple text cleaning tools.\\n\\x88How to take a step up and use the more sophisticated methods in the NLTK library.\\n\\x88Considerations when preparing text for natural language processing models.\\nLet’s get started.\\n5.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Metamorphosis by Franz Kafka\\n2. Text Cleaning is Task Speciﬁc\\n3. Manual Tokenization\\n4. Tokenization and Cleaning with NLTK\\n5. Additional Text Cleaning Considerations\\n35'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 52}, page_content='5.2. Metamorphosis by Franz Kafka 36\\n5.2 Metamorphosis by Franz Kafka\\nLet’s start oﬀ by selecting a dataset. In this tutorial, we will use the text from the book\\nMetamorphosis by Franz Kafka. No speciﬁc reason, other than it’s short, I like it, and you may\\nlike it too. I expect it’s one of those classics that most students have to read in school. The full\\ntext for Metamorphosis is available for free from Project Gutenberg. You can download the\\nASCII text version of the text here:\\n\\x88Metamorphosis by Franz Kafka Plain Text UTF-8 (may need to load the page twice).\\nhttp://www.gutenberg.org/cache/epub/5200/pg5200.txt\\nDownload the ﬁle and place it in your current working directory with the ﬁle name\\nmetamorphosis.txt . The ﬁle contains header and footer information that we are not in-\\nterested in, speciﬁcally copyright and license information. Open the ﬁle and delete the header\\nand footer information and save the ﬁle as metamorphosis clean.txt . The start of the clean\\nﬁle should look like:\\nOne morning, when Gregor Samsa woke from troubled dreams, he found himself\\ntransformed in his bed into a horrible vermin.\\nThe ﬁle should end with:\\nAnd, as if in conﬁrmation of their new dreams and good intentions, as soon as they\\nreached their destination Grete was the ﬁrst to get up and stretch out her young\\nbody.\\nPoor Gregor...\\n5.3 Text Cleaning Is Task Speciﬁc\\nAfter actually getting a hold of your text data, the ﬁrst step in cleaning up text data is to have\\na strong idea about what you’re trying to achieve, and in that context review your text to see\\nwhat exactly might help. Take a moment to look at the text. What do you notice? Here’s what\\nI see:\\n\\x88It’s plain text so there is no markup to parse (yay!).\\n\\x88The translation of the original German uses UK English (e.g. travelling ).\\n\\x88The lines are artiﬁcially wrapped with new lines at about 70 characters (meh).\\n\\x88There are no obvious typos or spelling mistakes.\\n\\x88There’s punctuation like commas, apostrophes, quotes, question marks, and more.\\n\\x88There’s hyphenated descriptions like armour-like .\\n\\x88There’s a lot of use of the em dash ( -) to continue sentences (maybe replace with commas?).\\n\\x88There are names (e.g. Mr. Samsa )'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 53}, page_content=\"5.4. Manual Tokenization 37\\n\\x88There does not appear to be numbers that require handling (e.g. 1999)\\n\\x88There are section markers (e.g. IIand III).\\nI’m sure there is a lot more going on to the trained eye. We are going to look at general\\ntext cleaning steps in this tutorial. Nevertheless, consider some possible objectives we may have\\nwhen working with this text document. For example:\\n\\x88If we were interested in developing a Kafkaesque language model, we may want to keep all\\nof the case, quotes, and other punctuation in place.\\n\\x88If we were interested in classifying documents as Kafka and Not Kafka , maybe we would\\nwant to strip case, punctuation, and even trim words back to their stem.\\nUse your task as the lens by which to choose how to ready your text data.\\n5.4 Manual Tokenization\\nText cleaning is hard, but the text we have chosen to work with is pretty clean already. We\\ncould just write some Python code to clean it up manually, and this is a good exercise for those\\nsimple problems that you encounter. Tools like regular expressions and splitting strings can get\\nyou a long way.\\n5.4.1 Load Data\\nLet’s load the text data so that we can work with it. The text is small and will load quickly\\nand easily ﬁt into memory. This will not always be the case and you may need to write code\\nto memory map the ﬁle. Tools like NLTK (covered in the next section) will make working\\nwith large ﬁles much easier. We can load the entire metamorphosis clean.txt into memory as\\nfollows:\\n# load text\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\nListing 5.1: Manually load the ﬁle.\\n5.4.2 Split by Whitespace\\nClean text often means a list of words or tokens that we can work with in our machine learning\\nmodels. This means converting the raw text into a list of words and saving it again. A very\\nsimple way to do this would be to split the document by white space, including “ ” (space), new\\nlines, tabs and more. We can do this in Python with the split() function on the loaded string.\\n# load text\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 54}, page_content='5.4. Manual Tokenization 38\\nfile.close()\\n# split into words by white space\\nwords = text.split()\\nprint(words[:100])\\nListing 5.2: Manually split words by white space.\\nRunning the example splits the document into a long list of words and prints the ﬁrst 100 for\\nus to review. We can see that punctuation is preserved (e.g. wasn’t and armour-like ), which is\\nnice. We can also see that end of sentence punctuation is kept with the last word (e.g. thought .),\\nwhich is not great.\\n[ \\'One \\', \\'morning, \\', \\'when \\', \\'Gregor \\', \\'Samsa \\', \\'woke \\', \\'from \\', \\'troubled \\', \\'dreams, \\', \\'he \\',\\n\\'found \\', \\'himself \\', \\'transformed \\', \\'in \\', \\'his \\', \\'bed \\', \\'into \\', \\'a \\', \\'horrible \\',\\n\\'vermin. \\', \\'He \\', \\'lay \\', \\'on \\', \\'his \\', \\'armour-like \\', \\'back, \\', \\'and \\', \\'if \\', \\'he \\',\\n\\'lifted \\', \\'his \\', \\'head \\', \\'a \\', \\'little \\', \\'he \\', \\'could \\', \\'see \\', \\'his \\', \\'brown \\', \\'belly, \\',\\n\\'slightly \\', \\'domed \\', \\'and \\', \\'divided \\', \\'by \\', \\'arches \\', \\'into \\', \\'stiff \\', \\'sections. \\',\\n\\'The \\', \\'bedding \\', \\'was \\', \\'hardly \\', \\'able \\', \\'to \\', \\'cover \\', \\'it \\', \\'and \\', \\'seemed \\',\\n\\'ready \\', \\'to \\', \\'slide \\', \\'off \\', \\'any \\', \\'moment. \\', \\'His \\', \\'many \\', \\'legs, \\', \\'pitifully \\',\\n\\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him, \\', \\'waved \\',\\n\\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked. \\', \\'\"What\\\\ \\'s \\', \\'happened \\', \\'to \\', \\'me?\" \\',\\n\\'he \\', \\'thought. \\', \\'It \\', \"wasn \\'t\", \\'a \\', \\'dream. \\', \\'His \\', \\'room, \\', \\'a \\', \\'proper \\', \\'human \\']\\nListing 5.3: Example output of splitting words by white space.\\n5.4.3 Select Words\\nAnother approach might be to use the regex model ( re) and split the document into words by\\nselecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and ‘ ’). For example:\\nimport re\\n# load text\\nfilename = \\'metamorphosis_clean.txt \\'\\nfile = open(filename, \\'rt \\')\\ntext = file.read()\\nfile.close()\\n# split based on words only\\nwords = re.split(r \\'\\\\W+ \\', text)\\nprint(words[:100])\\nListing 5.4: Manually select words with regex.\\nAgain, running the example we can see that we get our list of words. This time, we can see\\nthat armour-like is now two words armour and like(ﬁne) but contractions like What’s is also\\ntwo words What and s(not great).\\n[ \\'One \\', \\'morning \\', \\'when \\', \\'Gregor \\', \\'Samsa \\', \\'woke \\', \\'from \\', \\'troubled \\', \\'dreams \\', \\'he \\',\\n\\'found \\', \\'himself \\', \\'transformed \\', \\'in \\', \\'his \\', \\'bed \\', \\'into \\', \\'a \\', \\'horrible \\',\\n\\'vermin \\', \\'He \\', \\'lay \\', \\'on \\', \\'his \\', \\'armour \\', \\'like \\', \\'back \\', \\'and \\', \\'if \\', \\'he \\',\\n\\'lifted \\', \\'his \\', \\'head \\', \\'a \\', \\'little \\', \\'he \\', \\'could \\', \\'see \\', \\'his \\', \\'brown \\', \\'belly \\',\\n\\'slightly \\', \\'domed \\', \\'and \\', \\'divided \\', \\'by \\', \\'arches \\', \\'into \\', \\'stiff \\', \\'sections \\',\\n\\'The \\', \\'bedding \\', \\'was \\', \\'hardly \\', \\'able \\', \\'to \\', \\'cover \\', \\'it \\', \\'and \\', \\'seemed \\',\\n\\'ready \\', \\'to \\', \\'slide \\', \\'off \\', \\'any \\', \\'moment \\', \\'His \\', \\'many \\', \\'legs \\', \\'pitifully \\',\\n\\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him \\', \\'waved \\',\\n\\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked \\', \\'What \\', \\'s \\', \\'happened \\', \\'to \\', \\'me \\', \\'he \\',\\n\\'thought \\', \\'It \\', \\'wasn \\', \\'t \\', \\'a \\', \\'dream \\', \\'His \\', \\'room \\']'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 55}, page_content='5.4. Manual Tokenization 39\\nListing 5.5: Example output of selecting words with regex.\\n5.4.4 Split by Whitespace and Remove Punctuation\\nWe may want the words, but without the punctuation like commas and quotes. We also want to\\nkeep contractions together. One way would be to split the document into words by white space\\n(as in the section Split by Whitespace ), then use string translation to replace all punctuation with\\nnothing (e.g. remove it). Python provides a constant called string.punctuation that provides a\\ngreat list of punctuation characters. For example:\\nprint(string.punctuation)\\nListing 5.6: Print the known punctuation characters.\\nResults in:\\n!\"# $%& \\'()*+,-./:;<=>?@[\\\\]^_ `{|}~\\nListing 5.7: Example output of printing the known punctuation characters.\\nWe can use regular expressions to select for the punctuation characters and use the sub()\\nfunction to replace them with nothing. For example:\\nre_punc = re.compile( \\'[%s] \\'% re.escape(string.punctuation))\\n# remove punctuation from each word\\nstripped = [re_punc.sub( \\'\\', w) for w in words]\\nListing 5.8: Example of constructing a translation table that will remove punctuation.\\nWe can put all of this together, load the text ﬁle, split it into words by white space, then\\ntranslate each word to remove the punctuation.\\nimport string\\nimport re\\n# load text\\nfilename = \\'metamorphosis_clean.txt \\'\\nfile = open(filename, \\'rt \\')\\ntext = file.read()\\nfile.close()\\n# split into words by white space\\nwords = text.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( \\'[%s] \\'% re.escape(string.punctuation))\\n# remove punctuation from each word\\nstripped = [re_punc.sub( \\'\\', w) for w in words]\\nprint(stripped[:100])\\nListing 5.9: Manually remove punctuation.\\nWe can see that this has had the desired eﬀect, mostly. Contractions like What’s have\\nbecome Whats butarmour-like has become armourlike .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 56}, page_content='5.4. Manual Tokenization 40\\n[ \\'One \\', \\'morning \\', \\'when \\', \\'Gregor \\', \\'Samsa \\', \\'woke \\', \\'from \\', \\'troubled \\', \\'dreams \\', \\'he \\',\\n\\'found \\', \\'himself \\', \\'transformed \\', \\'in \\', \\'his \\', \\'bed \\', \\'into \\', \\'a \\', \\'horrible \\',\\n\\'vermin \\', \\'He \\', \\'lay \\', \\'on \\', \\'his \\', \\'armourlike \\', \\'back \\', \\'and \\', \\'if \\', \\'he \\', \\'lifted \\',\\n\\'his \\', \\'head \\', \\'a \\', \\'little \\', \\'he \\', \\'could \\', \\'see \\', \\'his \\', \\'brown \\', \\'belly \\',\\n\\'slightly \\', \\'domed \\', \\'and \\', \\'divided \\', \\'by \\', \\'arches \\', \\'into \\', \\'stiff \\', \\'sections \\',\\n\\'The \\', \\'bedding \\', \\'was \\', \\'hardly \\', \\'able \\', \\'to \\', \\'cover \\', \\'it \\', \\'and \\', \\'seemed \\',\\n\\'ready \\', \\'to \\', \\'slide \\', \\'off \\', \\'any \\', \\'moment \\', \\'His \\', \\'many \\', \\'legs \\', \\'pitifully \\',\\n\\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him \\', \\'waved \\',\\n\\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked \\', \\'Whats \\', \\'happened \\', \\'to \\', \\'me \\', \\'he \\',\\n\\'thought \\', \\'It \\', \\'wasnt \\', \\'a \\', \\'dream \\', \\'His \\', \\'room \\', \\'a \\', \\'proper \\', \\'human \\']\\nListing 5.10: Example output of removing punctuation with translation tables.\\nSometimes text data may contain non-printable characters. We can use a similar approach to\\nﬁlter out all non-printable characters by selecting the inverse of the string.printable constant.\\nFor example:\\n...\\nre_print = re.compile( \\'[^%s] \\'% re.escape(string.printable))\\nresult = [re_print.sub( \\'\\', w) for w in words]\\nListing 5.11: Example of removing non-printable characters.\\n5.4.5 Normalizing Case\\nIt is common to convert all words to one case. This means that the vocabulary will shrink in\\nsize, but some distinctions are lost (e.g. Apple the company vs apple the fruit is a commonly\\nused example). We can convert all words to lowercase by calling the lower() function on each\\nword. For example:\\nfilename = \\'metamorphosis_clean.txt \\'\\nfile = open(filename, \\'rt \\')\\ntext = file.read()\\nfile.close()\\n# split into words by white space\\nwords = text.split()\\n# convert to lower case\\nwords = [word.lower() for word in words]\\nprint(words[:100])\\nListing 5.12: Manually normalize case.\\nRunning the example, we can see that all words are now lowercase.\\n[ \\'one \\', \\'morning, \\', \\'when \\', \\'gregor \\', \\'samsa \\', \\'woke \\', \\'from \\', \\'troubled \\', \\'dreams, \\', \\'he \\',\\n\\'found \\', \\'himself \\', \\'transformed \\', \\'in \\', \\'his \\', \\'bed \\', \\'into \\', \\'a \\', \\'horrible \\',\\n\\'vermin. \\', \\'he \\', \\'lay \\', \\'on \\', \\'his \\', \\'armour-like \\', \\'back, \\', \\'and \\', \\'if \\', \\'he \\',\\n\\'lifted \\', \\'his \\', \\'head \\', \\'a \\', \\'little \\', \\'he \\', \\'could \\', \\'see \\', \\'his \\', \\'brown \\', \\'belly, \\',\\n\\'slightly \\', \\'domed \\', \\'and \\', \\'divided \\', \\'by \\', \\'arches \\', \\'into \\', \\'stiff \\', \\'sections. \\',\\n\\'the \\', \\'bedding \\', \\'was \\', \\'hardly \\', \\'able \\', \\'to \\', \\'cover \\', \\'it \\', \\'and \\', \\'seemed \\',\\n\\'ready \\', \\'to \\', \\'slide \\', \\'off \\', \\'any \\', \\'moment. \\', \\'his \\', \\'many \\', \\'legs, \\', \\'pitifully \\',\\n\\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him, \\', \\'waved \\',\\n\\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked. \\', \\'\"what\\\\ \\'s \\', \\'happened \\', \\'to \\', \\'me?\" \\',\\n\\'he \\', \\'thought. \\', \\'it \\', \"wasn \\'t\", \\'a \\', \\'dream. \\', \\'his \\', \\'room, \\', \\'a \\', \\'proper \\', \\'human \\']\\nListing 5.13: Example output of removing punctuation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 57}, page_content=\"5.5. Tokenization and Cleaning with NLTK 41\\n5.4.6 Note on Cleaning Text\\nCleaning text is really hard, problem speciﬁc, and full of tradeoﬀs. Remember, simple is better.\\nSimpler text data, simpler models, smaller vocabularies. You can always make things more\\ncomplex later to see if it results in better model skill. Next, we’ll look at some of the tools in\\nthe NLTK library that oﬀer more than simple string splitting.\\n5.5 Tokenization and Cleaning with NLTK\\nThe Natural Language Toolkit, or NLTK for short, is a Python library written for working and\\nmodeling text. It provides good tools for loading and cleaning text that we can use to get our\\ndata ready for working with machine learning and deep learning algorithms.\\n5.5.1 Install NLTK\\nYou can install NLTK using your favorite package manager, such as pip. On a POSIX-compaitable\\nmachine, this would be:\\nsudo pip install -U nltk\\nListing 5.14: Command to install the NLTK library.\\nAfter installation, you will need to install the data used with the library, including a great\\nset of documents that you can use later for testing other tools in NLTK. There are few ways to\\ndo this, such as from within a script:\\nimport nltk\\nnltk.download()\\nListing 5.15: NLTK script to download required text data.\\nOr from the command line:\\npython -m nltk.downloader all\\nListing 5.16: Command to download NLTK required text data.\\n5.5.2 Split into Sentences\\nA good useful ﬁrst step is to split the text into sentences. Some modeling tasks prefer input\\nto be in the form of paragraphs or sentences, such as Word2Vec. You could ﬁrst split your\\ntext into sentences, split each sentence into words, then save each sentence to ﬁle, one per line.\\nNLTK provides the sent tokenize() function to split text into sentences. The example below\\nloads the metamorphosis clean.txt ﬁle into memory, splits it into sentences, and prints the\\nﬁrst sentence.\\nfrom nltk import sent_tokenize\\n# load data\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 58}, page_content='5.5. Tokenization and Cleaning with NLTK 42\\n# split into sentences\\nsentences = sent_tokenize(text)\\nprint(sentences[0])\\nListing 5.17: NLTK script to split text into sentences.\\nRunning the example, we can see that although the document is split into sentences, that\\neach sentence still preserves the new line from the artiﬁcial wrap of the lines in the original\\ndocument.\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.\\nListing 5.18: Example output of splitting text into sentences.\\n5.5.3 Split into Words\\nNLTK provides a function called word tokenize() for splitting strings into tokens (nominally\\nwords). It splits tokens based on white space and punctuation. For example, commas and\\nperiods are taken as separate tokens. Contractions are split apart (e.g. What’s becomes What\\nand ’s). Quotes are kept, and so on. For example:\\nfrom nltk.tokenize import word_tokenize\\n# load data\\nfilename = \\'metamorphosis_clean.txt \\'\\nfile = open(filename, \\'rt \\')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\nprint(tokens[:100])\\nListing 5.19: NLTK script to split text into words.\\nRunning the code, we can see that punctuation are now tokens that we could then decide to\\nspeciﬁcally ﬁlter out.\\n[ \\'One \\', \\'morning \\', \\', \\', \\'when \\', \\'Gregor \\', \\'Samsa \\', \\'woke \\', \\'from \\', \\'troubled \\', \\'dreams \\',\\n\\', \\', \\'he \\', \\'found \\', \\'himself \\', \\'transformed \\', \\'in \\', \\'his \\', \\'bed \\', \\'into \\', \\'a \\',\\n\\'horrible \\', \\'vermin \\', \\'. \\', \\'He \\', \\'lay \\', \\'on \\', \\'his \\', \\'armour-like \\', \\'back \\', \\', \\', \\'and \\',\\n\\'if \\', \\'he \\', \\'lifted \\', \\'his \\', \\'head \\', \\'a \\', \\'little \\', \\'he \\', \\'could \\', \\'see \\', \\'his \\',\\n\\'brown \\', \\'belly \\', \\', \\', \\'slightly \\', \\'domed \\', \\'and \\', \\'divided \\', \\'by \\', \\'arches \\', \\'into \\',\\n\\'stiff \\', \\'sections \\', \\'. \\', \\'The \\', \\'bedding \\', \\'was \\', \\'hardly \\', \\'able \\', \\'to \\', \\'cover \\',\\n\\'it \\', \\'and \\', \\'seemed \\', \\'ready \\', \\'to \\', \\'slide \\', \\'off \\', \\'any \\', \\'moment \\', \\'. \\', \\'His \\',\\n\\'many \\', \\'legs \\', \\', \\', \\'pitifully \\', \\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\',\\n\\'the \\', \\'rest \\', \\'of \\', \\'him \\', \\', \\', \\'waved \\', \\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked \\',\\n\\'. \\', \\'``\\' , \\'What \\', \" \\'s\", \\'happened \\', \\'to \\']\\nListing 5.20: Example output of splitting text into words.\\n5.5.4 Filter Out Punctuation\\nWe can ﬁlter out all tokens that we are not interested in, such as all standalone punctuation. This\\ncan be done by iterating over all tokens and only keeping those tokens that are all alphabetic.\\nPython has the function isalpha() that can be used. For example:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 59}, page_content=\"5.5. Tokenization and Cleaning with NLTK 43\\nfrom nltk.tokenize import word_tokenize\\n# load data\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\n# remove all tokens that are not alphabetic\\nwords = [word for word in tokens if word.isalpha()]\\nprint(words[:100])\\nListing 5.21: NLTK script to remove punctuation.\\nRunning the example, you can see that not only punctuation tokens, but examples like\\narmour-like and ’swere also ﬁltered out.\\n[ 'One ', 'morning ', 'when ', 'Gregor ', 'Samsa ', 'woke ', 'from ', 'troubled ', 'dreams ', 'he ',\\n'found ', 'himself ', 'transformed ', 'in ', 'his ', 'bed ', 'into ', 'a ', 'horrible ',\\n'vermin ', 'He ', 'lay ', 'on ', 'his ', 'back ', 'and ', 'if ', 'he ', 'lifted ', 'his ', 'head ',\\n'a ', 'little ', 'he ', 'could ', 'see ', 'his ', 'brown ', 'belly ', 'slightly ', 'domed ',\\n'and ', 'divided ', 'by ', 'arches ', 'into ', 'stiff ', 'sections ', 'The ', 'bedding ', 'was ',\\n'hardly ', 'able ', 'to ', 'cover ', 'it ', 'and ', 'seemed ', 'ready ', 'to ', 'slide ', 'off ',\\n'any ', 'moment ', 'His ', 'many ', 'legs ', 'pitifully ', 'thin ', 'compared ', 'with ', 'the ',\\n'size ', 'of ', 'the ', 'rest ', 'of ', 'him ', 'waved ', 'about ', 'helplessly ', 'as ', 'he ',\\n'looked ', 'What ', 'happened ', 'to ', 'me ', 'he ', 'thought ', 'It ', 'was ', 'a ', 'dream ',\\n'His ', 'room ', 'a ', 'proper ', 'human ', 'room ']\\nListing 5.22: Example output of removing punctuation.\\n5.5.5 Filter out Stop Words (and Pipeline)\\nStop words are those words that do not contribute to the deeper meaning of the phrase. They\\nare the most common words such as: the,a, and is. For some applications like documentation\\nclassiﬁcation, it may make sense to remove stop words. NLTK provides a list of commonly\\nagreed upon stop words for a variety of languages, such as English. They can be loaded as\\nfollows:\\nfrom nltk.corpus import stopwords\\nstop_words = stopwords.words( 'english ')\\nprint(stop_words)\\nListing 5.23: NLTK script print stop words.\\nYou can see the full list as follows:\\n[ 'i ', 'me ', 'my ', 'myself ', 'we ', 'our ', 'ours ', 'ourselves ', 'you ', 'your ', 'yours ',\\n'yourself ', 'yourselves ', 'he ', 'him ', 'his ', 'himself ', 'she ', 'her ', 'hers ',\\n'herself ', 'it ', 'its ', 'itself ', 'they ', 'them ', 'their ', 'theirs ', 'themselves ',\\n'what ', 'which ', 'who ', 'whom ', 'this ', 'that ', 'these ', 'those ', 'am ', 'is ', 'are ',\\n'was ', 'were ', 'be ', 'been ', 'being ', 'have ', 'has ', 'had ', 'having ', 'do ', 'does ',\\n'did ', 'doing ', 'a ', 'an ', 'the ', 'and ', 'but ', 'if ', 'or ', 'because ', 'as ', 'until ',\\n'while ', 'of ', 'at ', 'by ', 'for ', 'with ', 'about ', 'against ', 'between ', 'into ',\\n'through ', 'during ', 'before ', 'after ', 'above ', 'below ', 'to ', 'from ', 'up ', 'down ',\\n'in ', 'out ', 'on ', 'off ', 'over ', 'under ', 'again ', 'further ', 'then ', 'once ', 'here ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 60}, page_content=\"5.5. Tokenization and Cleaning with NLTK 44\\n'there ', 'when ', 'where ', 'why ', 'how ', 'all ', 'any ', 'both ', 'each ', 'few ', 'more ',\\n'most ', 'other ', 'some ', 'such ', 'no ', 'nor ', 'not ', 'only ', 'own ', 'same ', 'so ',\\n'than ', 'too ', 'very ', 's ', 't ', 'can ', 'will ', 'just ', 'don ', 'should ', 'now ', 'd ',\\n'll ', 'm ', 'o ', 're ', 've ', 'y ', 'ain ', 'aren ', 'couldn ', 'didn ', 'doesn ', 'hadn ',\\n'hasn ', 'haven ', 'isn ', 'ma ', 'mightn ', 'mustn ', 'needn ', 'shan ', 'shouldn ', 'wasn ',\\n'weren ', 'won ', 'wouldn ']\\nListing 5.24: Example output of printing stop words.\\nYou can see that they are all lower case and have punctuation removed. You could compare\\nyour tokens to the stop words and ﬁlter them out, but you must ensure that your text is prepared\\nthe same way. Let’s demonstrate this with a small pipeline of text preparation including:\\n\\x88Load the raw text.\\n\\x88Split into tokens.\\n\\x88Convert to lowercase.\\n\\x88Remove punctuation from each token.\\n\\x88Filter out remaining tokens that are not alphabetic.\\n\\x88Filter out tokens that are stop words.\\nimport string\\nimport re\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n# load data\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\n# convert to lower case\\ntokens = [w.lower() for w in tokens]\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\nstripped = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\nwords = [word for word in stripped if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\nwords = [w for w in words if not w in stop_words]\\nprint(words[:100])\\nListing 5.25: NLTK script ﬁlter out stop words.\\nRunning this example, we can see that in addition to all of the other transforms, stop words\\nlikeaand tohave been removed. I note that we are still left with tokens like nt. The rabbit\\nhole is deep; there’s always more we can do.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 61}, page_content='5.5. Tokenization and Cleaning with NLTK 45\\n[ \\'one \\', \\'morning \\', \\'gregor \\', \\'samsa \\', \\'woke \\', \\'troubled \\', \\'dreams \\', \\'found \\', \\'transformed \\',\\n\\'bed \\', \\'horrible \\', \\'vermin \\', \\'lay \\', \\'armourlike \\', \\'back \\', \\'lifted \\', \\'head \\', \\'little \\',\\n\\'could \\', \\'see \\', \\'brown \\', \\'belly \\', \\'slightly \\', \\'domed \\', \\'divided \\', \\'arches \\', \\'stiff \\',\\n\\'sections \\', \\'bedding \\', \\'hardly \\', \\'able \\', \\'cover \\', \\'seemed \\', \\'ready \\', \\'slide \\', \\'moment \\',\\n\\'many \\', \\'legs \\', \\'pitifully \\', \\'thin \\', \\'compared \\', \\'size \\', \\'rest \\', \\'waved \\', \\'helplessly \\',\\n\\'looked \\', \\'happened \\', \\'thought \\', \\'nt \\', \\'dream \\', \\'room \\', \\'proper \\', \\'human \\', \\'room \\',\\n\\'although \\', \\'little \\', \\'small \\', \\'lay \\', \\'peacefully \\', \\'four \\', \\'familiar \\', \\'walls \\',\\n\\'collection \\', \\'textile \\', \\'samples \\', \\'lay \\', \\'spread \\', \\'table \\', \\'samsa \\', \\'travelling \\',\\n\\'salesman \\', \\'hung \\', \\'picture \\', \\'recently \\', \\'cut \\', \\'illustrated \\', \\'magazine \\', \\'housed \\',\\n\\'nice \\', \\'gilded \\', \\'frame \\', \\'showed \\', \\'lady \\', \\'fitted \\', \\'fur \\', \\'hat \\', \\'fur \\', \\'boa \\',\\n\\'sat \\', \\'upright \\', \\'raising \\', \\'heavy \\', \\'fur \\', \\'muff \\', \\'covered \\', \\'whole \\', \\'lower \\',\\n\\'arm \\', \\'towards \\', \\'viewer \\']\\nListing 5.26: Example output of ﬁltering out stop words.\\n5.5.6 Stem Words\\nStemming refers to the process of reducing each word to its root or base. For example ﬁshing ,\\nﬁshed ,ﬁsher all reduce to the stem ﬁsh. Some applications, like document classiﬁcation, may\\nbeneﬁt from stemming in order to both reduce the vocabulary and to focus on the sense or\\nsentiment of a document rather than deeper meaning. There are many stemming algorithms,\\nalthough a popular and long-standing method is the Porter Stemming algorithm. This method\\nis available in NLTK via the PorterStemmer class. For example:\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem.porter import PorterStemmer\\n# load data\\nfilename = \\'metamorphosis_clean.txt \\'\\nfile = open(filename, \\'rt \\')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\n# stemming of words\\nporter = PorterStemmer()\\nstemmed = [porter.stem(word) for word in tokens]\\nprint(stemmed[:100])\\nListing 5.27: NLTK script stem words.\\nRunning the example, you can see that words have been reduced to their stems, such as\\ntrouble has become troubl . You can also see that the stemming implementation has also reduced\\nthe tokens to lowercase, likely for internal look-ups in word tables.\\n[ \\'one \\', \\'morn \\', \\', \\', \\'when \\', \\'gregor \\', \\'samsa \\', \\'woke \\', \\'from \\', \\'troubl \\', \\'dream \\', \\', \\',\\n\\'he \\', \\'found \\', \\'himself \\', \\'transform \\', \\'in \\', \\'hi \\', \\'bed \\', \\'into \\', \\'a \\', \\'horribl \\',\\n\\'vermin \\', \\'. \\', \\'He \\', \\'lay \\', \\'on \\', \\'hi \\', \\'armour-lik \\', \\'back \\', \\', \\', \\'and \\', \\'if \\', \\'he \\',\\n\\'lift \\', \\'hi \\', \\'head \\', \\'a \\', \\'littl \\', \\'he \\', \\'could \\', \\'see \\', \\'hi \\', \\'brown \\', \\'belli \\', \\', \\',\\n\\'slightli \\', \\'dome \\', \\'and \\', \\'divid \\', \\'by \\', \\'arch \\', \\'into \\', \\'stiff \\', \\'section \\', \\'. \\',\\n\\'the \\', \\'bed \\', \\'wa \\', \\'hardli \\', \\'abl \\', \\'to \\', \\'cover \\', \\'it \\', \\'and \\', \\'seem \\', \\'readi \\', \\'to \\',\\n\\'slide \\', \\'off \\', \\'ani \\', \\'moment \\', \\'. \\', \\'hi \\', \\'mani \\', \\'leg \\', \\', \\', \\'piti \\', \\'thin \\',\\n\\'compar \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him \\', \\', \\', \\'wave \\',\\n\\'about \\', \\'helplessli \\', \\'as \\', \\'he \\', \\'look \\', \\'. \\', \\'``\\' , \\'what \\', \" \\'s\", \\'happen \\', \\'to \\''),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 62}, page_content='5.6. Additional Text Cleaning Considerations 46\\nListing 5.28: Example output of stemming words.\\nThere is a nice suite of stemming and lemmatization algorithms to choose from in NLTK, if\\nreducing words to their root is something you need for your project.\\n5.6 Additional Text Cleaning Considerations\\nWe are only getting started. Because the source text for this tutorial was reasonably clean to\\nbegin with, we skipped many concerns of text cleaning that you may need to deal with in your\\nown project. Here is a shortlist of additional considerations when cleaning text:\\n\\x88Handling large documents and large collections of text documents that do not ﬁt into\\nmemory.\\n\\x88Extracting text from markup like HTML, PDF, or other structured document formats.\\n\\x88Transliteration of characters from other languages into English.\\n\\x88Decoding Unicode characters into a normalized form, such as UTF8.\\n\\x88Handling of domain speciﬁc words, phrases, and acronyms.\\n\\x88Handling or removing numbers, such as dates and amounts.\\n\\x88Locating and correcting common typos and misspellings.\\n\\x88And much more...\\nThe list could go on. Hopefully, you can see that getting truly clean text is impossible, that\\nwe are really doing the best we can based on the time, resources, and knowledge we have. The\\nidea of clean is really deﬁned by the speciﬁc task or concern of your project.\\nA pro tip is to continually review your tokens after every transform. I have tried to show\\nthat in this tutorial and I hope you take that to heart. Ideally, you would save a new ﬁle after\\neach transform so that you can spend time with all of the data in the new form. Things always\\njump out at you when to take the time to review your data.\\n5.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Metamorphosis by Franz Kafka on Project Gutenberg.\\nhttp://www.gutenberg.org/ebooks/5200\\n\\x88Installing NLTK.\\nhttp://www.nltk.org/install.html\\n\\x88Installing NLTK Data.\\nhttp://www.nltk.org/data.html'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 63}, page_content='5.8. Summary 47\\n\\x88Python isalpha() function.\\nhttps://docs.python.org/3/library/stdtypes.html#str.isalpha\\n\\x88Stop Words on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Stop_words\\n\\x88Stemming on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Stemming\\n\\x88nltk.tokenize package API.\\nhttp://www.nltk.org/api/nltk.tokenize.html\\n\\x88Porer Stemming algorithm.\\nhttps://tartarus.org/martin/PorterStemmer/\\n\\x88nltk.stem package API.\\nhttp://www.nltk.org/api/nltk.stem.html\\n\\x88Processing Raw Text ,Natural Language Processing with Python .\\nhttp://www.nltk.org/book/ch03.html\\n5.8 Summary\\nIn this tutorial, you discovered how to clean text or machine learning in Python.\\nSpeciﬁcally, you learned:\\n\\x88How to get started by developing your own very simple text cleaning tools.\\n\\x88How to take a step up and use the more sophisticated methods in the NLTK library.\\n\\x88Considerations when preparing text for natural language processing models.\\n5.8.1 Next\\nIn the next chapter, you will discover how you can encode text data using the scikit-learn\\nPython library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 64}, page_content='Chapter 6\\nHow to Prepare Text Data with\\nscikit-learn\\nText data requires special preparation before you can start using it for predictive modeling. The\\ntext must be parsed to remove words, called tokenization. Then the words need to be encoded\\nas integers or ﬂoating point values for use as input to a machine learning algorithm, called\\nfeature extraction (or vectorization). The scikit-learn library oﬀers easy-to-use tools to perform\\nboth tokenization and feature extraction of your text data. In this tutorial, you will discover\\nexactly how you can prepare your text data for predictive modeling in Python with scikit-learn.\\nAfter completing this tutorial, you will know:\\n\\x88How to convert text to word count vectors with CountVectorizer .\\n\\x88How to convert text to word frequency vectors with TfidfVectorizer .\\n\\x88How to convert text to unique integers with HashingVectorizer .\\nLet’s get started.\\n6.1 The Bag-of-Words Model\\nWe cannot work with text directly when using machine learning algorithms. Instead, we need\\nto convert the text to numbers. We may want to perform classiﬁcation of documents, so each\\ndocument is an input and a class label is the output for our predictive algorithm. Algorithms\\ntake vectors of numbers as input, therefore we need to convert documents to ﬁxed-length vectors\\nof numbers.\\nA simple and eﬀective model for thinking about text documents in machine learning is called\\nthe Bag-of-Words Model, or BoW. Note, that we cover the BoW model in great detail in the\\nnext part, starting with Chapter 8. The model is simple in that it throws away all of the order\\ninformation in the words and focuses on the occurrence of words in a document. This can be\\ndone by assigning each word a unique number. Then any document we see can be encoded\\nas a ﬁxed-length vector with the length of the vocabulary of known words. The value in each\\nposition in the vector could be ﬁlled with a count or frequency of each word in the encoded\\ndocument.\\n48'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 65}, page_content='6.2. Word Counts with CountVectorizer 49\\nThis is the bag-of-words model, where we are only concerned with encoding schemes that\\nrepresent what words are present or the degree to which they are present in encoded documents\\nwithout any information about order. There are many ways to extend this simple method, both\\nby better clarifying what a word is and in deﬁning what to encode about each word in the\\nvector. The scikit-learn library provides 3 diﬀerent schemes that we can use, and we will brieﬂy\\nlook at each.\\n6.2 Word Counts with CountVectorizer\\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents\\nand build a vocabulary of known words, but also to encode new documents using that vocabulary.\\nYou can use it as follows:\\n\\x88Create an instance of the CountVectorizer class.\\n\\x88Call the fit() function in order to learn a vocabulary from one or more documents.\\n\\x88Call the transform() function on one or more documents as needed to encode each as a\\nvector.\\nAn encoded vector is returned with a length of the entire vocabulary and an integer count\\nfor the number of times each word appeared in the document. Because these vectors will\\ncontain a lot of zeros, we call them sparse. Python provides an eﬃcient way of handling sparse\\nvectors in the scipy.sparse package. The vectors returned from a call to transform() will\\nbe sparse vectors, and you can transform them back to NumPy arrays to look and better\\nunderstand what is going on by calling the toarray() function. Below is an example of using\\ntheCountVectorizer to tokenize, build a vocabulary, and then encode a document.\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n# list of text documents\\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\\n# create the transform\\nvectorizer = CountVectorizer()\\n# tokenize and build vocab\\nvectorizer.fit(text)\\n# summarize\\nprint(vectorizer.vocabulary_)\\n# encode document\\nvector = vectorizer.transform(text)\\n# summarize encoded vector\\nprint(vector.shape)\\nprint(type(vector))\\nprint(vector.toarray())\\nListing 6.1: Example of training a CountVectorizer .\\nAbove, you can see that we access the vocabulary to see what exactly was tokenized by\\ncalling:\\nprint(vectorizer.vocabulary_)\\nListing 6.2: Print the learned vocabulary.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 66}, page_content='6.3. Word Frequencies with TfidfVectorizer 50\\nWe can see that all words were made lowercase by default and that the punctuation was\\nignored. These and other aspects of tokenizing can be conﬁgured and I encourage you to review\\nall of the options in the API documentation. Running the example ﬁrst prints the vocabulary,\\nthen the shape of the encoded document. We can see that there are 8 words in the vocab, and\\ntherefore encoded vectors have a length of 8. We can then see that the encoded vector is a\\nsparse matrix. Finally, we can see an array version of the encoded vector showing a count of 1\\noccurrence for each word except the (index and id 7) that has an occurrence of 2.\\n{ \\'dog \\': 1, \\'fox \\': 2, \\'over \\': 5, \\'brown \\': 0, \\'quick \\': 6, \\'the \\': 7, \\'lazy \\': 4, \\'jumped \\': 3}\\n(1, 8)\\n<class \\'scipy.sparse.csr.csr_matrix \\'>\\n[[1 1 1 1 1 1 1 2]]\\nListing 6.3: Example output of training a CountVectorizer .\\nImportantly, the same vectorizer can be used on documents that contain words not included\\nin the vocabulary. These words are ignored and no count is given in the resulting vector. For\\nexample, below is an example of using the vectorizer above to encode a document with one\\nword in the vocab and one word that is not.\\n# encode another document\\ntext2 = [\"the puppy\"]\\nvector = vectorizer.transform(text2)\\nprint(vector.toarray())\\nListing 6.4: Example of encoding another document with the ﬁt CountVectorizer .\\nRunning this example prints the array version of the encoded sparse vector showing one\\noccurrence of the one word in the vocab and the other word not in the vocab completely ignored.\\n[[0 0 0 0 0 0 0 1]]\\nListing 6.5: Example output of encoding another document.\\nThe encoded vectors can then be used directly with a machine learning algorithm.\\n6.3 Word Frequencies with TfidfVectorizer\\nWord counts are a good starting point, but are very basic. One issue with simple counts is that\\nsome words like thewill appear many times and their large counts will not be very meaningful\\nin the encoded vectors. An alternative is to calculate word frequencies, and by far the most\\npopular method is called TF-IDF. This is an acronym that stands for Term Frequency - Inverse\\nDocument Frequency which are the components of the resulting scores assigned to each word.\\n\\x88Term Frequency : This summarizes how often a given word appears within a document.\\n\\x88Inverse Document Frequency : This downscales words that appear a lot across docu-\\nments.\\nWithout going into the math, TF-IDF are word frequency scores that try to highlight\\nwords that are more interesting, e.g. frequent in a document but not across documents.\\nThe TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 67}, page_content='6.4. Hashing with HashingVectorizer 51\\nfrequency weightings, and allow you to encode new documents. Alternately, if you already have a\\nlearned CountVectorizer , you can use it with a TfidfTransformer to just calculate the inverse\\ndocument frequencies and start encoding documents. The same create, ﬁt, and transform process\\nis used as with the CountVectorizer . Below is an example of using the TfidfVectorizer to\\nlearn vocabulary and inverse document frequencies across 3 small documents and then encode\\none of those documents.\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n# list of text documents\\ntext = [\"The quick brown fox jumped over the lazy dog.\",\\n\"The dog.\",\\n\"The fox\"]\\n# create the transform\\nvectorizer = TfidfVectorizer()\\n# tokenize and build vocab\\nvectorizer.fit(text)\\n# summarize\\nprint(vectorizer.vocabulary_)\\nprint(vectorizer.idf_)\\n# encode document\\nvector = vectorizer.transform([text[0]])\\n# summarize encoded vector\\nprint(vector.shape)\\nprint(vector.toarray())\\nListing 6.6: Example of training a TfidfVectorizer .\\nA vocabulary of 8 words is learned from the documents and each word is assigned a unique\\ninteger index in the output vector. The inverse document frequencies are calculated for each\\nword in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word:\\ntheat index 7. Finally, the ﬁrst document is encoded as an 8-element sparse array and we can\\nreview the ﬁnal scorings of each word with diﬀerent values for the,fox, and dogfrom the other\\nwords in the vocabulary.\\n{ \\'fox \\': 2, \\'lazy \\': 4, \\'dog \\': 1, \\'quick \\': 6, \\'the \\': 7, \\'over \\': 5, \\'brown \\': 0, \\'jumped \\': 3}\\n[ 1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\\n1.69314718 1. ]\\n(1, 8)\\n[[ 0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\\n0.36388646 0.42983441]]\\nListing 6.7: Example output of training a TfidfVectorizer .\\nThe scores are normalized to values between 0 and 1 and the encoded document vectors can\\nthen be used directly with most machine learning algorithms.\\n6.4 Hashing with HashingVectorizer\\nCounts and frequencies can be very useful, but one limitation of these methods is that the\\nvocabulary can become very large. This, in turn, will require large vectors for encoding\\ndocuments and impose large requirements on memory and slow down algorithms. A clever work\\naround is to use a one way hash of words to convert them to integers. The clever part is that\\nno vocabulary is required and you can choose an arbitrary-long ﬁxed length vector. A downside'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 68}, page_content='6.5. Further Reading 52\\nis that the hash is a one-way function so there is no way to convert the encoding back to a word\\n(which may not matter for many supervised learning tasks).\\nThe HashingVectorizer class implements this approach that can be used to consistently\\nhash words, then tokenize and encode documents as needed. The example below demonstrates\\ntheHashingVectorizer for encoding a single document. An arbitrary ﬁxed-length vector size\\nof 20 was chosen. This corresponds to the range of the hash function, where small values (like\\n20) may result in hash collisions. Remembering back to Computer Science classes, I believe\\nthere are heuristics that you can use to pick the hash length and probability of collision based\\non estimated vocabulary size (e.g. a load factor of 75%). See any good textbook on the topic.\\nNote that this vectorizer does not require a call to ﬁt on the training data documents. Instead,\\nafter instantiation, it can be used directly to start encoding documents.\\nfrom sklearn.feature_extraction.text import HashingVectorizer\\n# list of text documents\\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\\n# create the transform\\nvectorizer = HashingVectorizer(n_features=20)\\n# encode document\\nvector = vectorizer.transform(text)\\n# summarize encoded vector\\nprint(vector.shape)\\nprint(vector.toarray())\\nListing 6.8: Example of training a HashingVectorizer .\\nRunning the example encodes the sample document as a 20-element sparse array. The values\\nof the encoded document correspond to normalized word counts by default in the range of -1 to\\n1, but could be made simple integer counts by changing the default conﬁguration.\\n(1, 20)\\n[[ 0. 0. 0. 0. 0. 0.33333333\\n0. -0.33333333 0.33333333 0. 0. 0.33333333\\n0. 0. 0. -0.33333333 0. 0.\\n-0.66666667 0. ]]\\nListing 6.9: Example output of training a HashingVectorizer .\\n6.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n6.5.1 Natural Language Processing\\n\\x88Bag-of-words model on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Bag-of-words_model\\n\\x88Tokenization on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\\n\\x88TF-IDF on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Tf%E2%80%93idf'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 69}, page_content='6.6. Summary 53\\n6.5.2 sciki-learn\\n\\x88Section 4.2. Feature extraction, scikit-learn User Guide.\\nhttp://scikit-learn.org/stable/modules/feature_extraction.html\\n\\x88sckit-learn Feature Extraction API.\\nhttp://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_\\nextraction\\n\\x88Working With Text Data, scikit-learn Tutorial.\\nhttp://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.\\nhtml\\n6.5.3 Class APIs\\n\\x88CountVectorizer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.CountVectorizer.html\\n\\x88TfidfVectorizer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.TfidfVectorizer.html\\n\\x88TfidfTransformer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.TfidfTransformer.html\\n\\x88HashingVectorizer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.HashingVectorizer.html\\n6.6 Summary\\nIn this tutorial, you discovered how to prepare text documents for machine learning with\\nscikit-learn for bag-of-words models. Speciﬁcally, you learned:\\n\\x88How to convert text to word count vectors with CountVectorizer .\\n\\x88How to convert text to word frequency vectors with TfidfVectorizer .\\n\\x88How to convert text to unique integers with HashingVectorizer .\\nWe have only scratched the surface in these examples and I want to highlight that there are\\nmany conﬁguration details for these classes to inﬂuence the tokenizing of documents that are\\nworth exploring.\\n6.6.1 Next\\nIn the next chapter, you will discover how you can prepare text data using the Keras deep\\nlearning library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 70}, page_content='Chapter 7\\nHow to Prepare Text Data With Keras\\nYou cannot feed raw text directly into deep learning models. Text data must be encoded as\\nnumbers to be used as input or output for machine learning and deep learning models, such\\nas word embeddings. The Keras deep learning library provides some basic tools to help you\\nprepare your text data. In this tutorial, you will discover how you can use Keras to prepare\\nyour text data. After completing this tutorial, you will know:\\n\\x88About the convenience methods that you can use to quickly prepare text data.\\n\\x88TheTokenizer API that can be ﬁt on training data and used to encode training, validation,\\nand test documents.\\n\\x88The range of 4 diﬀerent document encoding schemes oﬀered by the Tokenizer API.\\nLet’s get started.\\n7.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Split words with text toword sequence .\\n2. Encoding with onehot.\\n3. Hash Encoding with hashing trick .\\n4.Tokenizer API\\n7.2 Split Words with text toword sequence\\nA good ﬁrst step when working with text is to split it into words. Words are called to-\\nkens and the process of splitting text into tokens is called tokenization. Keras provides the\\ntext toword sequence() function that you can use to split text into a list of words. By\\ndefault, this function automatically does 3 things:\\n\\x88Splits words by space.\\n54'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 71}, page_content=\"7.3. Encoding with onehot 55\\n\\x88Filters out punctuation.\\n\\x88Converts text to lowercase ( lower=True ).\\nYou can change any of these defaults by passing arguments to the function. Below is an\\nexample of using the text toword sequence() function to split a document (in this case a\\nsimple string) into a list of words.\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\\n# tokenize the document\\nresult = text_to_word_sequence(text)\\nprint(result)\\nListing 7.1: Example splitting words with the Tokenizer .\\nRunning the example creates an array containing all of the words in the document. The list\\nof words is printed for review.\\n[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']\\nListing 7.2: Example output for splitting words with the Tokenizer .\\nThis is a good ﬁrst step, but further pre-processing is required before you can work with the\\ntext.\\n7.3 Encoding with onehot\\nIt is popular to represent a document as a sequence of integer values, where each word in the\\ndocument is represented as a unique integer. Keras provides the onehot() function that you\\ncan use to tokenize and integer encode a text document in one step. The name suggests that it\\nwill create a one hot encoding of the document, which is not the case. Instead, the function\\nis a wrapper for the hashing trick() function described in the next section. The function\\nreturns an integer encoded version of the document. The use of a hash function means that\\nthere may be collisions and not all words will be assigned unique integer values. As with the\\ntext toword sequence() function in the previous section, the onehot() function will make\\nthe text lower case, ﬁlter out punctuation, and split words based on white space.\\nIn addition to the text, the vocabulary size (total words) must be speciﬁed. This could be the\\ntotal number of words in the document or more if you intend to encode additional documents\\nthat contains additional words. The size of the vocabulary deﬁnes the hashing space from which\\nwords are hashed. By default, the hash function is used, although as we will see in the next\\nsection, alternate hash functions can be speciﬁed when calling the hashing trick() function\\ndirectly.\\nWe can use the text toword sequence() function from the previous section to split the\\ndocument into words and then use a set to represent only the unique words in the document.\\nThe size of this set can be used to estimate the size of the vocabulary for one document. For\\nexample:\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 72}, page_content=\"7.4. Hash Encoding with hashing trick 56\\n# estimate the size of the vocabulary\\nwords = set(text_to_word_sequence(text))\\nvocab_size = len(words)\\nprint(vocab_size)\\nListing 7.3: Example of preparing a vocabulary.\\nWe can put this together with the onehot() function and encode the words in the document.\\nThe complete example is listed below. The vocabulary size is increased by one-third to minimize\\ncollisions when hashing words.\\nfrom keras.preprocessing.text import one_hot\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\\n# estimate the size of the vocabulary\\nwords = set(text_to_word_sequence(text))\\nvocab_size = len(words)\\nprint(vocab_size)\\n# integer encode the document\\nresult = one_hot(text, round(vocab_size*1.3))\\nprint(result)\\nListing 7.4: Example of one hot encoding.\\nRunning the example ﬁrst prints the size of the vocabulary as 8. The encoded document is\\nthen printed as an array of integer encoded words.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n8\\n[5, 9, 8, 7, 9, 1, 5, 3, 8]\\nListing 7.5: Example output for one hot encoding with the Tokenizer .\\n7.4 Hash Encoding with hashing trick\\nA limitation of integer and count base encodings is that they must maintain a vocabulary of\\nwords and their mapping to integers. An alternative to this approach is to use a one-way hash\\nfunction to convert words to integers. This avoids the need to keep track of a vocabulary, which\\nis faster and requires less memory.\\nKeras provides the hashing trick() function that tokenizes and then integer encodes the\\ndocument, just like the onehot() function. It provides more ﬂexibility, allowing you to specify\\nthe hash function as either hash (the default) or other hash functions such as the built in md5\\nfunction or your own function. Below is an example of integer encoding a document using the\\nmd5 hash function.\\nfrom keras.preprocessing.text import hashing_trick\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 73}, page_content=\"7.5.Tokenizer API 57\\n# estimate the size of the vocabulary\\nwords = set(text_to_word_sequence(text))\\nvocab_size = len(words)\\nprint(vocab_size)\\n# integer encode the document\\nresult = hashing_trick(text, round(vocab_size*1.3), hash_function= 'md5 ')\\nprint(result)\\nListing 7.6: Example of hash encoding.\\nRunning the example prints the size of the vocabulary and the integer encoded document.\\nWe can see that the use of a diﬀerent hash function results in consistent, but diﬀerent integers\\nfor words as the onehot() function in the previous section.\\n8\\n[6, 4, 1, 2, 7, 5, 6, 2, 6]\\nListing 7.7: Example output for hash encoding with the Tokenizer .\\n7.5 Tokenizer API\\nSo far we have looked at one-oﬀ convenience methods for preparing text with Keras. Keras\\nprovides a more sophisticated API for preparing text that can be ﬁt and reused to prepare\\nmultiple text documents. This may be the preferred approach for large projects. Keras provides\\ntheTokenizer class for preparing text documents for deep learning. The Tokenizer must be\\nconstructed and then ﬁt on either raw text documents or integer encoded text documents. For\\nexample:\\nfrom keras.preprocessing.text import Tokenizer\\n# define 5 documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ']\\n# create the tokenizer\\nt = Tokenizer()\\n# fit the tokenizer on the documents\\nt.fit_on_texts(docs)\\nListing 7.8: Example of ﬁtting a Tokenizer .\\nOnce ﬁt, the Tokenizer provides 4 attributes that you can use to query what has been\\nlearned about your documents:\\n\\x88word counts : A dictionary of words and their counts.\\n\\x88word docs : An integer count of the total number of documents that were used to ﬁt the\\nTokenizer .\\n\\x88word index : A dictionary of words and their uniquely assigned integers.\\n\\x88document count : A dictionary of words and how many documents each appeared in.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 74}, page_content=\"7.5.Tokenizer API 58\\nFor example:\\n# summarize what was learned\\nprint(t.word_counts)\\nprint(t.document_count)\\nprint(t.word_index)\\nprint(t.word_docs)\\nListing 7.9: Summarize the output of the ﬁt Tokenizer .\\nOnce the Tokenizer has been ﬁt on training data, it can be used to encode documents in\\nthe train or test datasets. The texts tomatrix() function on the Tokenizer can be used to\\ncreate one vector per document provided per input. The length of the vectors is the total size\\nof the vocabulary. This function provides a suite of standard bag-of-words model text encoding\\nschemes that can be provided via a mode argument to the function. The modes available\\ninclude:\\n\\x88binary : Whether or not each word is present in the document. This is the default.\\n\\x88count : The count of each word in the document.\\n\\x88tfidf : The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word\\nin the document.\\n\\x88freq : The frequency of each word as a ratio of words within each document.\\nWe can put all of this together with a worked example.\\nfrom keras.preprocessing.text import Tokenizer\\n# define 5 documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ']\\n# create the tokenizer\\nt = Tokenizer()\\n# fit the tokenizer on the documents\\nt.fit_on_texts(docs)\\n# summarize what was learned\\nprint(t.word_counts)\\nprint(t.document_count)\\nprint(t.word_index)\\nprint(t.word_docs)\\n# integer encode documents\\nencoded_docs = t.texts_to_matrix(docs, mode= 'count ')\\nprint(encoded_docs)\\nListing 7.10: Example of ﬁtting and encoding with the Tokenizer .\\nRunning the example ﬁts the Tokenizer with 5 small documents. The details of the ﬁt\\nTokenizer are printed. Then the 5 documents are encoded using a word count. Each document\\nis encoded as a 9-element vector with one position for each word and the chosen encoding\\nscheme value for each word position. In this case, a simple word count mode is used.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 75}, page_content=\"7.6. Further Reading 59\\nOrderedDict([( 'well ', 1), ( 'done ', 1), ( 'good ', 1), ( 'work ', 2), ( 'great ', 1), ( 'effort ',\\n1), ( 'nice ', 1), ( 'excellent ', 1)])\\n5\\n{ 'work ': 1, 'effort ': 6, 'done ': 3, 'great ': 5, 'good ': 4, 'excellent ': 8, 'well ': 2,\\n'nice ': 7}\\n{ 'work ': 2, 'effort ': 1, 'done ': 1, 'well ': 1, 'good ': 1, 'great ': 1, 'excellent ': 1,\\n'nice ': 1}\\n[[ 0. 0. 1. 1. 0. 0. 0. 0. 0.]\\n[ 0. 1. 0. 0. 1. 0. 0. 0. 0.]\\n[ 0. 0. 0. 0. 0. 1. 1. 0. 0.]\\n[ 0. 1. 0. 0. 0. 0. 0. 1. 0.]\\n[ 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\\nListing 7.11: Example output from ﬁtting and encoding with the Tokenizer .\\nThe Tokenizer will be the key way we will prepare text for word embeddings throughout\\nthis book.\\n7.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Text Preprocessing Keras API.\\nhttps://keras.io/preprocessing/text/\\n\\x88text toword sequence Keras API.\\nhttps://keras.io/preprocessing/text/#text_to_word_sequence\\n\\x88onehotKeras API.\\nhttps://keras.io/preprocessing/text/#one_hot\\n\\x88hashing trick Keras API.\\nhttps://keras.io/preprocessing/text/#hashing_trick\\n\\x88Tokenizer Keras API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n7.7 Summary\\nIn this tutorial, you discovered how you can use the Keras API to prepare your text data for\\ndeep learning. Speciﬁcally, you learned:\\n\\x88About the convenience methods that you can use to quickly prepare text data.\\n\\x88TheTokenizer API that can be ﬁt on training data and used to encode training, validation,\\nand test documents.\\n\\x88The range of 4 diﬀerent document encoding schemes oﬀered by the Tokenizer API.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 76}, page_content='7.7. Summary 60\\n7.7.1 Next\\nThis is the last chapter in the data preparation part. In the next part, you will discover how to\\ndevelop bag-of-words models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 77}, page_content='Part IV\\nBag-of-Words\\n61'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 78}, page_content='Chapter 8\\nThe Bag-of-Words Model\\nThe bag-of-words model is a way of representing text data when modeling text with machine\\nlearning algorithms. The bag-of-words model is simple to understand and implement and has\\nseen great success in problems such as language modeling and document classiﬁcation. In this\\ntutorial, you will discover the bag-of-words model for feature extraction in natural language\\nprocessing. After completing this tutorial, you will know:\\n\\x88What the bag-of-words model is and why it is needed to represent text.\\n\\x88How to develop a bag-of-words model for a collection of documents.\\n\\x88How to use diﬀerent techniques to prepare a vocabulary and score words.\\nLet’s get started.\\n8.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. The Problem with Text\\n2. What is a Bag-of-Words?\\n3. Example of the Bag-of-Words Model\\n4. Managing Vocabulary\\n5. Scoring Words\\n6. Limitations of Bag-of-Words\\n8.2 The Problem with Text\\nA problem with modeling text is that it is messy, and techniques like machine learning algorithms\\nprefer well deﬁned ﬁxed-length inputs and outputs. Machine learning algorithms cannot work\\nwith raw text directly; the text must be converted into numbers. Speciﬁcally, vectors of numbers.\\n62'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 79}, page_content='8.3. What is a Bag-of-Words? 63\\nIn language processing, the vectors x are derived from textual data, in order to\\nreﬂect various linguistic properties of the text.\\n— Page 65, Neural Network Methods in Natural Language Processing , 2017.\\nThis is called feature extraction or feature encoding. A popular and simple method of feature\\nextraction with text data is called the bag-of-words model of text.\\n8.3 What is a Bag-of-Words?\\nA bag-of-words model, or BoW for short, is a way of extracting features from text for use in\\nmodeling, such as with machine learning algorithms. The approach is very simple and ﬂexible,\\nand can be used in a myriad of ways for extracting features from documents. A bag-of-words is\\na representation of text that describes the occurrence of words within a document. It involves\\ntwo things:\\n\\x88A vocabulary of known words.\\n\\x88A measure of the presence of known words.\\nIt is called a bag-of-words , because any information about the order or structure of words\\nin the document is discarded. The model is only concerned with whether known words occur in\\nthe document, not where in the document.\\nA very common feature extraction procedures for sentences and documents is the\\nbag-of-words approach (BOW). In this approach, we look at the histogram of the\\nwords within the text, i.e. considering each word count as a feature.\\n— Page 69, Neural Network Methods in Natural Language Processing , 2017.\\nThe intuition is that documents are similar if they have similar content. Further, that from\\nthe content alone we can learn something about the meaning of the document. The bag-of-words\\ncan be as simple or complex as you like. The complexity comes both in deciding how to design\\nthe vocabulary of known words (or tokens) and how to score the presence of known words. We\\nwill take a closer look at both of these concerns.\\n8.4 Example of the Bag-of-Words Model\\nLet’s make the bag-of-words model concrete with a worked example.\\n8.4.1 Step 1: Collect Data\\nBelow is a snippet of the ﬁrst few lines of text from the book A Tale of Two Cities by Charles\\nDickens, taken from Project Gutenberg.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 80}, page_content='8.4. Example of the Bag-of-Words Model 64\\nIt was the best of times,\\nit was the worst of times,\\nit was the age of wisdom,\\nit was the age of foolishness,\\nListing 8.1: Sample of text from A Tale of Two Cities by Charles Dickens.\\nFor this small example, let’s treat each line as a separate document and the 4 lines as our\\nentire corpus of documents.\\n8.4.2 Step 2: Design the Vocabulary\\nNow we can make a list of all of the words in our model vocabulary. The unique words here\\n(ignoring case and punctuation) are:\\nit\\nwas\\nthe\\nbest\\nof\\ntimes\\nworst\\nage\\nwisdom\\nfoolishness\\nListing 8.2: List of unique words.\\nThat is a vocabulary of 10 words from a corpus containing 24 words.\\n8.4.3 Step 3: Create Document Vectors\\nThe next step is to score the words in each document. The objective is to turn each document of\\nfree text into a vector that we can use as input or output for a machine learning model. Because\\nwe know the vocabulary has 10 words, we can use a ﬁxed-length document representation of 10,\\nwith one position in the vector to score each word. The simplest scoring method is to mark the\\npresence of words as a boolean value, 0 for absent, 1 for present. Using the arbitrary ordering of\\nwords listed above in our vocabulary, we can step through the ﬁrst document ( It was the best of\\ntimes ) and convert it into a binary vector. The scoring of the document would look as follows:\\nit = 1\\nwas = 1\\nthe = 1\\nbest = 1\\nof = 1\\ntimes = 1\\nworst = 0\\nage = 0\\nwisdom = 0\\nfoolishness = 0\\nListing 8.3: List of unique words and their occurrence in a document.\\nAs a binary vector, this would look as follows:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 81}, page_content='8.5. Managing Vocabulary 65\\n[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\\nListing 8.4: First line of text as a binary vector.\\nThe other three documents would look as follows:\\n\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\\n\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\\n\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\\nListing 8.5: Remaining three lines of text as binary vectors.\\nAll ordering of the words is nominally discarded and we have a consistent way of extracting\\nfeatures from any document in our corpus, ready for use in modeling. New documents that\\noverlap with the vocabulary of known words, but may contain words outside of the vocabulary,\\ncan still be encoded, where only the occurrence of known words are scored and unknown\\nwords are ignored. You can see how this might naturally scale to large vocabularies and larger\\ndocuments.\\n8.5 Managing Vocabulary\\nAs the vocabulary size increases, so does the vector representation of documents. In the previous\\nexample, the length of the document vector is equal to the number of known words. You can\\nimagine that for a very large corpus, such as thousands of books, that the length of the vector\\nmight be thousands or millions of positions. Further, each document may contain very few of\\nthe known words in the vocabulary.\\nThis results in a vector with lots of zero scores, called a sparse vector or sparse representation.\\nSparse vectors require more memory and computational resources when modeling and the\\nvast number of positions or dimensions can make the modeling process very challenging for\\ntraditional algorithms. As such, there is pressure to decrease the size of the vocabulary when\\nusing a bag-of-words model.\\nThere are simple text cleaning techniques that can be used as a ﬁrst step, such as:\\n\\x88Ignoring case.\\n\\x88Ignoring punctuation.\\n\\x88Ignoring frequent words that don’t contain much information, called stop words, like a,of,\\netc.\\n\\x88Fixing misspelled words.\\n\\x88Reducing words to their stem (e.g. play from playing ) using stemming algorithms.\\nA more sophisticated approach is to create a vocabulary of grouped words. This both\\nchanges the scope of the vocabulary and allows the bag-of-words to capture a little bit more\\nmeaning from the document. In this approach, each word or token is called a gram . Creating a\\nvocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that\\nappear in the corpus are modeled, not all possible bigrams.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 82}, page_content='8.6. Scoring Words 66\\nAn n-gram is an n-token sequence of words: a 2-gram (more commonly called a\\nbigram) is a two-word sequence of words like “please turn”, “turn your”, or “your\\nhomework”, and a 3-gram (more commonly called a trigram) is a three-word sequence\\nof words like “please turn your”, or “turn your homework”.\\n— Page 85, Speech and Language Processing , 2009.\\nFor example, the bigrams in the ﬁrst line of text in the previous section: It was the best of\\ntimes are as follows:\\nit was\\nwas the\\nthe best\\nbest of\\nof times\\nListing 8.6: List of bi-grams for a document.\\nA vocabulary that tracks triplets of words is called a trigram model and the general approach\\nis called the n-gram model, where nrefers to the number of grouped words. Often a simple\\nbigram approach is better than a 1-gram bag-of-words model for tasks like documentation\\nclassiﬁcation.\\na bag-of-bigrams representation is much more powerful than bag-of-words, and in\\nmany cases proves very hard to beat.\\n— Page 75, Neural Network Methods in Natural Language Processing , 2017.\\n8.6 Scoring Words\\nOnce a vocabulary has been chosen, the occurrence of words in example documents needs to be\\nscored. In the worked example, we have already seen one very simple approach to scoring: a\\nbinary scoring of the presence or absence of words. Some additional simple scoring methods\\ninclude:\\n\\x88Counts . Count the number of times each word appears in a document.\\n\\x88Frequencies . Calculate the frequency that each word appears in a document out of all\\nthe words in the document.\\n8.6.1 Word Hashing\\nYou may remember from computer science that a hash function is a bit of math that maps data\\nto a ﬁxed size set of numbers. For example, we use them in hash tables when programming where\\nperhaps names are converted to numbers for fast lookup. We can use a hash representation of\\nknown words in our vocabulary. This addresses the problem of having a very large vocabulary\\nfor a large text corpus because we can choose the size of the hash space, which is in turn the\\nsize of the vector representation of the document.\\nWords are hashed deterministically to the same integer index in the target hash space. A\\nbinary score or count can then be used to score the word. This is called the hash trick orfeature\\nhashing . The challenge is to choose a hash space to accommodate the chosen vocabulary size to\\nminimize the probability of collisions and trade-oﬀ sparsity.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 83}, page_content='8.7. Limitations of Bag-of-Words 67\\n8.6.2 TF-IDF\\nA problem with scoring word frequency is that highly frequent words start to dominate in the\\ndocument (e.g. larger score), but may not contain as much informational content to the model\\nas rarer but perhaps domain speciﬁc words. One approach is to rescale the frequency of words\\nby how often they appear in all documents, so that the scores for frequent words like thethat\\nare also frequent across all documents are penalized. This approach to scoring is called Term\\nFrequency - Inverse Document Frequency, or TF-IDF for short, where:\\n\\x88Term Frequency : is a scoring of the frequency of the word in the current document.\\n\\x88Inverse Document Frequency : is a scoring of how rare the word is across documents.\\nThe scores are a weighting where not all words are equally as important or interesting. The\\nscores have the eﬀect of highlighting words that are distinct (contain useful information) in a\\ngiven document.\\nThus the idf of a rare term is high, whereas the idf of a frequent term is likely to be\\nlow.\\n— Page 118, An Introduction to Information Retrieval , 2008.\\n8.7 Limitations of Bag-of-Words\\nThe bag-of-words model is very simple to understand and implement and oﬀers a lot of ﬂexibility\\nfor customization on your speciﬁc text data. It has been used with great success on prediction\\nproblems like language modeling and documentation classiﬁcation. Nevertheless, it suﬀers from\\nsome shortcomings, such as:\\n\\x88Vocabulary : The vocabulary requires careful design, most speciﬁcally in order to manage\\nthe size, which impacts the sparsity of the document representations.\\n\\x88Sparsity : Sparse representations are harder to model both for computational reasons\\n(space and time complexity) and also for information reasons, where the challenge is for\\nthe models to harness so little information in such a large representational space.\\n\\x88Meaning : Discarding word order ignores the context, and in turn meaning of words in\\nthe document (semantics). Context and meaning can oﬀer a lot to the model, that if\\nmodeled could tell the diﬀerence between the same words diﬀerently arranged ( this is\\ninteresting vsis this interesting ), synonyms ( old bike vsused bike ), and much more.\\n8.8 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 84}, page_content='8.9. Summary 68\\n8.8.1 Books\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2wycQKA\\n\\x88Speech and Language Processing , 2009.\\nhttp://amzn.to/2vaEb7T\\n\\x88An Introduction to Information Retrieval , 2008.\\nhttp://amzn.to/2vvnPHP\\n\\x88Foundations of Statistical Natural Language Processing , 1999.\\nhttp://amzn.to/2vvnPHP\\n8.8.2 Wikipedia\\n\\x88Bag-of-words model.\\nhttps://en.wikipedia.org/wiki/N-gram\\n\\x88n-gram.\\nhttps://en.wikipedia.org/wiki/N-gram\\n\\x88Feature hashing.\\nhttps://en.wikipedia.org/wiki/Feature_hashing\\n\\x88tf-idf.\\nhttps://en.wikipedia.org/wiki/Tf-idf\\n8.9 Summary\\nIn this tutorial, you discovered the bag-of-words model for feature extraction with text data.\\nSpeciﬁcally, you learned:\\n\\x88What the bag-of-words model is and why we need it.\\n\\x88How to work through the application of a bag-of-words model to a collection of documents.\\n\\x88What techniques can be used for preparing a vocabulary and scoring words.\\n8.9.1 Next\\nIn the next chapter, you will can prepare movie review data for the bag-of-words model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 85}, page_content='Chapter 9\\nHow to Prepare Movie Review Data\\nfor Sentiment Analysis\\nText data preparation is diﬀerent for each problem. Preparation starts with simple steps, like\\nloading data, but quickly gets diﬃcult with cleaning tasks that are very speciﬁc to the data you\\nare working with. You need help as to where to begin and what order to work through the steps\\nfrom raw data to data ready for modeling. In this tutorial, you will discover how to prepare\\nmovie review text data for sentiment analysis, step-by-step. After completing this tutorial, you\\nwill know:\\n\\x88How to load text data and clean it to remove punctuation and other non-words.\\n\\x88How to develop a vocabulary, tailor it, and save it to ﬁle.\\n\\x88How to prepare movie reviews using cleaning and a pre-deﬁned vocabulary and save them\\nto new ﬁles ready for modeling.\\nLet’s get started.\\n9.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Movie Review Dataset\\n2. Load Text Data\\n3. Clean Text Data\\n4. Develop Vocabulary\\n5. Save Prepared Data\\n69'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 86}, page_content='9.2. Movie Review Dataset 70\\n9.2 Movie Review Dataset\\nThe Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in\\nthe early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available\\nas part of their research on natural language processing. The reviews were originally released\\nin 2002, but an updated and cleaned up version was released in 2004, referred to as v2.0. The\\ndataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive\\nof the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as\\nthepolarity dataset .\\nOur data contains 1000 positive and 1000 negative reviews all written before 2002,\\nwith a cap of 20 reviews per author (312 authors total) per category. We refer to\\nthis corpus as the polarity dataset.\\n— A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on\\nMinimum Cuts, 2004.\\nThe data has been cleaned up somewhat, for example:\\n\\x88The dataset is comprised of only English reviews.\\n\\x88All text has been converted to lowercase.\\n\\x88There is white space around punctuation like periods, commas, and brackets.\\n\\x88Text has been split into one sentence per line.\\nThe data has been used for a few related natural language processing tasks. For classiﬁcation,\\nthe performance of classical models (such as Support Vector Machines) on the data is in the\\nrange of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see\\nresults as high as 86% with 10-fold cross-validation. This gives us a ballpark of low-to-mid 80s\\nif we were looking to use this dataset in experiments on modern methods.\\n... depending on choice of downstream polarity classiﬁer, we can achieve highly\\nstatistically signiﬁcant improvement (from 82.8% to 86.4%)\\n— A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on\\nMinimum Cuts, 2004.\\nYou can download the dataset from here:\\n\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention from cv000 tocv999 for each of negand pos. Next, let’s\\nlook at loading the text data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 87}, page_content=\"9.3. Load Text Data 71\\n9.3 Load Text Data\\nIn this section, we will look at loading individual text ﬁles, then processing the directories of\\nﬁles. We will assume that the review data is downloaded and available in the current working\\ndirectory in the folder txtsentoken . We can load an individual text ﬁle by opening it, reading\\nin the ASCII text, and closing the ﬁle. This is standard ﬁle handling stuﬀ. For example, we can\\nload the ﬁrst negative review ﬁle cv000 29416.txt as follows:\\n# load one file\\nfilename = 'txt_sentoken/neg/cv000_29416.txt '\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nListing 9.1: Example of loading a single movie review.\\nThis loads the document as ASCII and preserves any white space, like new lines. We can\\nturn this into a function called load doc() that takes a ﬁlename of the document to load and\\nreturns the text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 9.2: Function to load a document into memory.\\nWe have two directories each with 1,000 documents each. We can process each directory in\\nturn by ﬁrst getting a list of ﬁles in the directory using the listdir() function, then loading\\neach ﬁle in turn. For example, we can load each document in the negative directory using the\\nload doc() function to do the actual loading.\\nfrom os import listdir\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# specify directory to load\\ndirectory = 'txt_sentoken/neg '\\n# walk through all files in the folder\\nfor filename in listdir(directory):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 88}, page_content='9.3. Load Text Data 72\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load document\\ndoc = load_doc(path)\\nprint( \\'Loaded %s \\'% filename)\\nListing 9.3: Example of loading a all movie reviews.\\nRunning this example prints the ﬁlename of each review after it is loaded.\\n...\\nLoaded cv995_23113.txt\\nLoaded cv996_12447.txt\\nLoaded cv997_5152.txt\\nLoaded cv998_15691.txt\\nLoaded cv999_14636.txt\\nListing 9.4: Example output of loading all movie reviews.\\nWe can turn the processing of the documents into a function as well and use it as a template\\nlater for developing a function to clean all documents in a folder. For example, below we deﬁne\\naprocess docs() function to do the same thing.\\nfrom os import listdir\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, \\'r \\')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load all docs in a directory\\ndef process_docs(directory):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load document\\ndoc = load_doc(path)\\nprint( \\'Loaded %s \\'% filename)\\n# specify directory to load\\ndirectory = \\'txt_sentoken/neg \\'\\nprocess_docs(directory)\\nListing 9.5: Example of loading a all movie reviews with functions.\\nNow that we know how to load the movie review text data, let’s look at cleaning it.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 89}, page_content='9.4. Clean Text Data 73\\n9.4 Clean Text Data\\nIn this section, we will look at what data cleaning we might want to do to the movie review\\ndata. We will assume that we will be using a bag-of-words model or perhaps a word embedding\\nthat does not require too much preparation.\\n9.4.1 Split into Tokens\\nFirst, let’s load one document and look at the raw tokens split by white space. We will use the\\nload doc() function developed in the previous section. We can use the split() function to\\nsplit the loaded document into tokens separated by white space.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, \\'r \\')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load the document\\nfilename = \\'txt_sentoken/neg/cv000_29416.txt \\'\\ntext = load_doc(filename)\\n# split into tokens by white space\\ntokens = text.split()\\nprint(tokens)\\nListing 9.6: Load a movie review and split by white space.\\nRunning the example gives a nice long list of raw tokens from the document.\\n...\\n\\'years \\', \\'ago \\', \\'and \\', \\'has \\', \\'been \\', \\'sitting \\', \\'on \\', \\'the \\', \\'shelves \\', \\'ever \\', \\'since \\',\\n\\'. \\', \\'whatever \\', \\'. \\', \\'. \\', \\'. \\', \\'skip \\', \\'it \\', \\'! \\', \"where \\'s\", \\'joblo \\', \\'coming \\',\\n\\'from \\', \\'? \\', \\'a \\', \\'nightmare \\', \\'of \\', \\'elm \\', \\'street \\', \\'3 \\', \\'( \\', \\'7/10 \\', \\') \\', \\'- \\',\\n\\'blair \\', \\'witch \\', \\'2 \\', \\'( \\', \\'7/10 \\', \\') \\', \\'- \\', \\'the \\', \\'crow \\', \\'( \\', \\'9/10 \\', \\') \\', \\'- \\',\\n\\'the \\', \\'crow \\', \\': \\', \\'salvation \\', \\'( \\', \\'4/10 \\', \\') \\', \\'- \\', \\'lost \\', \\'highway \\', \\'( \\',\\n\\'10/10 \\', \\') \\', \\'- \\', \\'memento \\', \\'( \\', \\'10/10 \\', \\') \\', \\'- \\', \\'the \\', \\'others \\', \\'( \\', \\'9/10 \\',\\n\\') \\', \\'- \\', \\'stir \\', \\'of \\', \\'echoes \\', \\'( \\', \\'8/10 \\', \\') \\']\\nListing 9.7: Example output of spitting a review by white space.\\nJust looking at the raw tokens can give us a lot of ideas of things to try, such as:\\n\\x88Remove punctuation from words (e.g. ‘what’s’).\\n\\x88Removing tokens that are just punctuation (e.g. ‘-’).\\n\\x88Removing tokens that contain numbers (e.g. ‘10/10’).\\n\\x88Remove tokens that have one character (e.g. ‘a’).\\n\\x88Remove tokens that don’t have much meaning (e.g. ‘and’).'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 90}, page_content=\"9.4. Clean Text Data 74\\nSome ideas:\\n\\x88We can ﬁlter out punctuation from tokens using regular expressions.\\n\\x88We can remove tokens that are just punctuation or contain numbers by using an isalpha()\\ncheck on each token.\\n\\x88We can remove English stop words using the list loaded using NLTK.\\n\\x88We can ﬁlter out short tokens by checking their length.\\nBelow is an updated version of cleaning this review.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load the document\\nfilename = 'txt_sentoken/neg/cv000_29416.txt '\\ntext = load_doc(filename)\\n# split into tokens by white space\\ntokens = text.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nprint(tokens)\\nListing 9.8: Load and clean one movie review.\\nRunning the example gives a much cleaner looking list of tokens.\\n...\\n'explanation ', 'craziness ', 'came ', 'oh ', 'way ', 'horror ', 'teen ', 'slasher ', 'flick ',\\n'packaged ', 'look ', 'way ', 'someone ', 'apparently ', 'assuming ', 'genre ', 'still ',\\n'hot ', 'kids ', 'also ', 'wrapped ', 'production ', 'two ', 'years ', 'ago ', 'sitting ',\\n'shelves ', 'ever ', 'since ', 'whatever ', 'skip ', 'wheres ', 'joblo ', 'coming ',\\n'nightmare ', 'elm ', 'street ', 'blair ', 'witch ', 'crow ', 'crow ', 'salvation ', 'lost ',\\n'highway ', 'memento ', 'others ', 'stir ', 'echoes ']\\nListing 9.9: Example output of cleaning one movie review.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 91}, page_content=\"9.4. Clean Text Data 75\\nWe can put this into a function called clean doc() and test it on another review, this time\\na positive review.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 9.10: Function to clean movie reviews.\\nAgain, the cleaning procedure seems to produce a good set of tokens, at least as a ﬁrst cut.\\n...\\n'comic ', 'oscar ', 'winner ', 'martin ', 'childs ', 'shakespeare ', 'love ', 'production ',\\n'design ', 'turns ', 'original ', 'prague ', 'surroundings ', 'one ', 'creepy ', 'place ',\\n'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ', 'typically ', 'strong ',\\n'performance ', 'deftly ', 'handling ', 'british ', 'accent ', 'ians ', 'holm ', 'joe ',\\n'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ', 'supporting ', 'roles ',\\n'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ', 'opened ', 'mouth ',\\n'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ', 'half ', 'bad ', 'film ',\\n'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ', 'language ', 'drug ', 'content ']\\nListing 9.11: Example output of a function to clean movie reviews.\\nThere are many more cleaning steps we could take and I leave them to your imagination.\\nNext, let’s look at how we can manage a preferred vocabulary of tokens.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 92}, page_content='9.5. Develop Vocabulary 76\\n9.5 Develop Vocabulary\\nWhen working with predictive models of text, like a bag-of-words model, there is a pressure to\\nreduce the size of the vocabulary. The larger the vocabulary, the more sparse the representation\\nof each word or document. A part of preparing text for sentiment analysis involves deﬁning and\\ntailoring the vocabulary of words supported by the model. We can do this by loading all of the\\ndocuments in the dataset and building a set of words. We may decide to support all of these\\nwords, or perhaps discard some. The ﬁnal chosen vocabulary can then be saved to ﬁle for later\\nuse, such as ﬁltering words in new documents in the future.\\nWe can keep track of the vocabulary in a Counter , which is a dictionary of words and their\\ncount with some additional convenience functions. We need to develop a new function to process\\na document and add it to the vocabulary. The function needs to load a document by calling the\\npreviously developed load doc() function. It needs to clean the loaded document using the\\npreviously developed clean doc() function, then it needs to add all the tokens to the Counter ,\\nand update counts. We can do this last step by calling the update() function on the counter\\nobject. Below is a function called adddoctovocab() that takes as arguments a document\\nﬁlename and a Counter vocabulary.\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\nListing 9.12: Function add a movie review to a vocabulary.\\nFinally, we can use our template above for processing all documents in a directory called\\nprocess docs() and update it to call adddoctovocab() .\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\nListing 9.13: Updated process documents function.\\nWe can put all of this together and develop a full vocabulary from all documents in the\\ndataset.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 93}, page_content='9.5. Develop Vocabulary 77\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, \\'r \\')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( \\'[%s] \\'% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( \\'\\', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( \\'english \\'))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( \\'txt_sentoken/neg \\', vocab)\\nprocess_docs( \\'txt_sentoken/pos \\', vocab)\\n# print the size of the vocab\\nprint(len(vocab))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 94}, page_content=\"9.5. Develop Vocabulary 78\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\\nListing 9.14: Example of cleaning all reviews and building a vocabulary.\\nRunning the example creates a vocabulary with all documents in the dataset, including\\npositive and negative reviews. We can see that there are a little over 46,000 unique words across\\nall reviews and the top 3 words are ﬁlm,one, and movie .\\n46557\\n[( 'film ', 8860), ( 'one ', 5521), ( 'movie ', 5440), ( 'like ', 3553), ( 'even ', 2555), ( 'good ',\\n2320), ( 'time ', 2283), ( 'story ', 2118), ( 'films ', 2102), ( 'would ', 2042), ( 'much ',\\n2024), ( 'also ', 1965), ( 'characters ', 1947), ( 'get ', 1921), ( 'character ', 1906),\\n( 'two ', 1825), ( 'first ', 1768), ( 'see ', 1730), ( 'well ', 1694), ( 'way ', 1668), ( 'make ',\\n1590), ( 'really ', 1563), ( 'little ', 1491), ( 'life ', 1472), ( 'plot ', 1451), ( 'people ',\\n1420), ( 'movies ', 1416), ( 'could ', 1395), ( 'bad ', 1374), ( 'scene ', 1373), ( 'never ',\\n1364), ( 'best ', 1301), ( 'new ', 1277), ( 'many ', 1268), ( 'doesnt ', 1267), ( 'man ', 1266),\\n( 'scenes ', 1265), ( 'dont ', 1210), ( 'know ', 1207), ( 'hes ', 1150), ( 'great ', 1141),\\n( 'another ', 1111), ( 'love ', 1089), ( 'action ', 1078), ( 'go ', 1075), ( 'us ', 1065),\\n( 'director ', 1056), ( 'something ', 1048), ( 'end ', 1047), ( 'still ', 1038)]\\nListing 9.15: Example output of building a vocabulary.\\nPerhaps the least common words, those that only appear once across all reviews, are not\\npredictive. Perhaps some of the most common words are not useful too. These are good\\nquestions and really should be tested with a speciﬁc predictive model. Generally, words that\\nonly appear once or a few times across 2,000 reviews are probably not predictive and can be\\nremoved from the vocabulary, greatly cutting down on the tokens we need to model. We can do\\nthis by stepping through words and their counts and only keeping those with a count above a\\nchosen threshold. Here we will use 5 occurrences.\\n# keep tokens with > 5 occurrence\\nmin_occurane = 5\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\nListing 9.16: Example of ﬁltering the vocabulary by an occurrence count.\\nThis reduces the vocabulary from 46,557 to 14,803 words, a huge drop. Perhaps a minimum\\nof 5 occurrences is too aggressive; you can experiment with diﬀerent values. We can then save\\nthe chosen vocabulary of words to a new ﬁle. I like to save the vocabulary as ASCII with one\\nword per line. Below deﬁnes a function called save list() to save a list of items, in this case,\\ntokens to ﬁle, one per line.\\ndef save_list(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nListing 9.17: Function to save the vocabulary to ﬁle.\\nThe complete example for deﬁning and saving the vocabulary is listed below.\\nimport string\\nimport re\\nfrom os import listdir\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 95}, page_content='9.5. Develop Vocabulary 79\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, \\'r \\')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( \\'[%s] \\'% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( \\'\\', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( \\'english \\'))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# save list to file\\ndef save_list(lines, filename):\\ndata = \\'\\\\n \\'.join(lines)\\nfile = open(filename, \\'w \\')\\nfile.write(data)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 96}, page_content=\"9.6. Save Prepared Data 80\\nfile.close()\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\\n# keep tokens with > 5 occurrence\\nmin_occurane = 5\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 9.18: Example building and saving a ﬁnal vocabulary.\\nRunning this ﬁnal snippet after creating the vocabulary will save the chosen words to ﬁle. It\\nis a good idea to take a look at, and even study, your chosen vocabulary in order to get ideas\\nfor better preparing this data, or text data in the future.\\nhasnt\\nupdating\\nfiguratively\\nsymphony\\ncivilians\\nmight\\nfisherman\\nhokum\\nwitch\\nbuffoons\\n...\\nListing 9.19: Sample of the saved vocabulary ﬁle.\\nNext, we can look at using the vocabulary to create a prepared version of the movie review\\ndataset.\\n9.6 Save Prepared Data\\nWe can use the data cleaning and chosen vocabulary to prepare each movie review and save the\\nprepared versions of the reviews ready for modeling. This is a good practice as it decouples\\nthe data preparation from modeling, allowing you to focus on modeling and circle back to data\\nprep if you have new ideas. We can start oﬀ by loading the vocabulary from vocab.txt .\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 97}, page_content='9.6. Save Prepared Data 81\\nfile.close()\\nreturn text\\n# load vocabulary\\nvocab_filename = \\'vocab.txt \\'\\nvocab = load_doc(vocab_filename)\\nvocab = vocab.split()\\nvocab = set(vocab)\\nListing 9.20: Load the saved vocabulary.\\nNext, we can clean the reviews, use the loaded vocab to ﬁlter out unwanted tokens, and\\nsave the clean reviews in a new ﬁle. One approach could be to save all the positive reviews\\nin one ﬁle and all the negative reviews in another ﬁle, with the ﬁltered tokens separated by\\nwhite space for each review on separate lines. First, we can deﬁne a function to process a\\ndocument, clean it, ﬁlter it, and return it as a single line that could be saved in a ﬁle. Below\\ndeﬁnes the doctoline() function to do just that, taking a ﬁlename and vocabulary (as a set)\\nas arguments. It calls the previously deﬁned load doc() function to load the document and\\nclean doc() to tokenize the document.\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn \\' \\'.join(tokens)\\nListing 9.21: Function to ﬁlter a review by the vocabulary\\nNext, we can deﬁne a new version of process docs() to step through all reviews in a folder\\nand convert them to lines by calling doctoline() for each document. A list of lines is then\\nreturned.\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\nListing 9.22: Updated process docs function to ﬁlter all documents by the vocabulary.\\nWe can then call process docs() for both the directories of positive and negative reviews,\\nthen call save list() from the previous section to save each list of processed reviews to a ﬁle.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 98}, page_content=\"9.6. Save Prepared Data 82\\nThe complete code listing is provided below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# save list to file\\ndef save_list(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 99}, page_content='9.7. Further Reading 83\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load vocabulary\\nvocab_filename = \\'vocab.txt \\'\\nvocab = load_doc(vocab_filename)\\nvocab = vocab.split()\\nvocab = set(vocab)\\n# prepare negative reviews\\nnegative_lines = process_docs( \\'txt_sentoken/neg \\', vocab)\\nsave_list(negative_lines, \\'negative.txt \\')\\n# prepare positive reviews\\npositive_lines = process_docs( \\'txt_sentoken/pos \\', vocab)\\nsave_list(positive_lines, \\'positive.txt \\')\\nListing 9.23: Example of cleaning and ﬁltering all reviews by the vocab and saving the results\\nto ﬁle.\\nRunning the example saves two new ﬁles, negative.txt andpositive.txt , that contain the\\nprepared negative and positive reviews respectively. The data is ready for use in a bag-of-words\\nor even word embedding model.\\n9.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n9.7.1 Dataset\\n\\x88Movie Review Data.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\n\\x88A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based\\non Minimum Cuts , 2004.\\nhttp://xxx.lanl.gov/abs/cs/0409058\\n\\x88Movie Review Polarity Dataset.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\n\\x88Dataset Readme v2.0 and v1.1.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/poldata.README.2.0.\\ntxt\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/README.1.1'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 100}, page_content='9.8. Summary 84\\n9.7.2 APIs\\n\\x88nltk.tokenize package API.\\nhttp://www.nltk.org/api/nltk.tokenize.html\\n\\x88Chapter 2, Accessing Text Corpora and Lexical Resources .\\nhttp://www.nltk.org/book/ch02.html\\n\\x88osAPI Miscellaneous operating system interfaces.\\nhttps://docs.python.org/3/library/os.html\\n\\x88collections API - Container datatypes.\\nhttps://docs.python.org/3/library/collections.html\\n9.8 Summary\\nIn this tutorial, you discovered how to prepare movie review text data for sentiment analysis,\\nstep-by-step. Speciﬁcally, you learned:\\n\\x88How to load text data and clean it to remove punctuation and other non-words.\\n\\x88How to develop a vocabulary, tailor it, and save it to ﬁle.\\n\\x88How to prepare movie reviews using cleaning and a predeﬁned vocabulary and save them\\nto new ﬁles ready for modeling.\\n9.8.1 Next\\nIn the next chapter, you will discover how you can develop a neural bag-of-words model for\\nmovie review sentiment analysis.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 101}, page_content='Chapter 10\\nProject: Develop a Neural\\nBag-of-Words Model for Sentiment\\nAnalysis\\nMovie reviews can be classiﬁed as either favorable or not. The evaluation of movie review text\\nis a classiﬁcation problem often called sentiment analysis. A popular technique for developing\\nsentiment analysis models is to use a bag-of-words model that transforms documents into vectors\\nwhere each word in the document is assigned a score. In this tutorial, you will discover how you\\ncan develop a deep learning predictive model using the bag-of-words representation for movie\\nreview sentiment classiﬁcation. After completing this tutorial, you will know:\\n\\x88How to prepare the review text data for modeling with a restricted vocabulary.\\n\\x88How to use the bag-of-words model to prepare train and test data.\\n\\x88How to develop a Multilayer Perceptron bag-of-words model and use it to make predictions\\non new review text data.\\nLet’s get started.\\n10.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Movie Review Dataset\\n2. Data Preparation\\n3. Bag-of-Words Representation\\n4. Sentiment Analysis Models\\n5. Comparing Word Scoring Methods\\n6. Predicting Sentiment for New Reviews\\n85'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 102}, page_content='10.2. Movie Review Dataset 86\\n10.2 Movie Review Dataset\\nIn this tutorial, we will use the Movie Review Dataset. This dataset designed for sentiment\\nanalysis was described previously in Chapter 9. You can download the dataset from here:\\n\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention cv000 tocv999 for each of negand pos. Next, let’s look\\nat loading the text data.\\n10.3 Data Preparation\\nNote : The preparation of the movie review dataset was ﬁrst described in Chapter 9. In this\\nsection, we will look at 3 things:\\n1. Separation of data into training and test sets.\\n2. Loading and cleaning the data to remove punctuation and numbers.\\n3. Deﬁning a vocabulary of preferred words.\\n10.3.1 Split into Train and Test Sets\\nWe are pretending that we are developing a system that can predict the sentiment of a textual\\nmovie review as either positive or negative. This means that after the model is developed, we\\nwill need to make predictions on new textual reviews. This will require all of the same data\\npreparation to be performed on those new reviews as is performed on the training data for the\\nmodel.\\nWe will ensure that this constraint is built into the evaluation of our models by splitting the\\ntraining and test datasets prior to any data preparation. This means that any knowledge in the\\ntest set that could help us better prepare the data (e.g. the words used) is unavailable during\\nthe preparation of data and the training of the model. That being said, we will use the last 100\\npositive reviews and the last 100 negative reviews as a test set (100 reviews) and the remaining\\n1,800 reviews as the training dataset. This is a 90% train, 10% split of the data. The split can\\nbe imposed easily by using the ﬁlenames of the reviews where reviews named 000 to 899 are for\\ntraining data and reviews named 900 onwards are for testing the model.\\n10.3.2 Loading and Cleaning Reviews\\nThe text data is already pretty clean, so not much preparation is required. Without getting too\\nmuch into the details, we will prepare the data using the following method:\\n\\x88Split tokens on white space.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 103}, page_content=\"10.3. Data Preparation 87\\n\\x88Remove all punctuation from words.\\n\\x88Remove all words that are not purely comprised of alphabetical characters.\\n\\x88Remove all words that are known stop words.\\n\\x88Remove all words that have a length ≤1 character.\\nWe can put all of these steps into a function called clean doc() that takes as an argument\\nthe raw text loaded from a ﬁle and returns a list of cleaned tokens. We can also deﬁne a function\\nload doc() that loads a document from ﬁle ready for use with the clean doc() function. An\\nexample of cleaning the ﬁrst positive review is listed below.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 10.1: Example of cleaning a movie review.\\nRunning the example prints a long list of clean tokens. There are many more cleaning steps\\nwe may want to explore, and I leave them as further exercises. I’d love to see what you can\\ncome up with.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 104}, page_content=\"10.3. Data Preparation 88\\n...\\n'creepy ', 'place ', 'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ',\\n'typically ', 'strong ', 'performance ', 'deftly ', 'handling ', 'british ', 'accent ',\\n'ians ', 'holm ', 'joe ', 'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ',\\n'supporting ', 'roles ', 'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ',\\n'opened ', 'mouth ', 'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ',\\n'half ', 'bad ', 'film ', 'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ',\\n'language ', 'drug ', 'content ']\\nListing 10.2: Example output of cleaning a movie review.\\n10.3.3 Deﬁne a Vocabulary\\nIt is important to deﬁne a vocabulary of known words when using a bag-of-words model. The\\nmore words, the larger the representation of documents, therefore it is important to constrain\\nthe words to only those believed to be predictive. This is diﬃcult to know beforehand and often\\nit is important to test diﬀerent hypotheses about how to construct a useful vocabulary. We\\nhave already seen how we can remove punctuation and numbers from the vocabulary in the\\nprevious section. We can repeat this for all documents and build a set of all known words.\\nWe can develop a vocabulary as a Counter , which is a dictionary mapping of words and\\ntheir count that allows us to easily update and query. Each document can be added to the\\ncounter (a new function called adddoctovocab() ) and we can step over all of the reviews in\\nthe negative directory and then the positive directory (a new function called process docs() ).\\nThe complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 105}, page_content=\"10.3. Data Preparation 89\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\\nListing 10.3: Example of selecting a vocabulary for the dataset.\\nRunning the example shows that we have a vocabulary of 44,276 words. We also can see\\na sample of the top 50 most used words in the movie reviews. Note that this vocabulary was\\nconstructed based on only those reviews in the training dataset.\\n44276\\n[( 'film ', 7983), ( 'one ', 4946), ( 'movie ', 4826), ( 'like ', 3201), ( 'even ', 2262), ( 'good ',\\n2080), ( 'time ', 2041), ( 'story ', 1907), ( 'films ', 1873), ( 'would ', 1844), ( 'much ',\\n1824), ( 'also ', 1757), ( 'characters ', 1735), ( 'get ', 1724), ( 'character ', 1703),\\n( 'two ', 1643), ( 'first ', 1588), ( 'see ', 1557), ( 'way ', 1515), ( 'well ', 1511), ( 'make ',\\n1418), ( 'really ', 1407), ( 'little ', 1351), ( 'life ', 1334), ( 'plot ', 1288), ( 'people ',\\n1269), ( 'could ', 1248), ( 'bad ', 1248), ( 'scene ', 1241), ( 'movies ', 1238), ( 'never ',\\n1201), ( 'best ', 1179), ( 'new ', 1140), ( 'scenes ', 1135), ( 'man ', 1131), ( 'many ', 1130),\\n( 'doesnt ', 1118), ( 'know ', 1092), ( 'dont ', 1086), ( 'hes ', 1024), ( 'great ', 1014),\\n( 'another ', 992), ( 'action ', 985), ( 'love ', 977), ( 'us ', 967), ( 'go ', 952),\\n( 'director ', 948), ( 'end ', 946), ( 'something ', 945), ( 'still ', 936)]\\nListing 10.4: Example output of selecting a vocabulary for the dataset.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 106}, page_content=\"10.3. Data Preparation 90\\nWe can step through the vocabulary and remove all words that have a low occurrence, such\\nas only being used once or twice in all reviews. For example, the following snippet will retrieve\\nonly the tokens that appear 2 or more times in all reviews.\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\nListing 10.5: Example of ﬁltering the vocabulary by occurrence.\\nFinally, the vocabulary can be saved to a new ﬁle called vocab.txt that we can later load\\nand use to ﬁlter movie reviews prior to encoding them for modeling. We deﬁne a new function\\ncalled save list() that saves the vocabulary to ﬁle, with one word per ﬁle. For example:\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 10.6: Example of saving the ﬁltered vocabulary.\\nPulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 107}, page_content=\"10.3. Data Preparation 91\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 10.7: Example of ﬁltering the vocabulary for the dataset.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 108}, page_content='10.4. Bag-of-Words Representation 92\\nRunning the above example with this addition shows that the vocabulary size drops by a\\nlittle more than half its size, from about 44,000 to about 25,000 words.\\n25767\\nListing 10.8: Example output of ﬁltering the vocabulary by min occurrence.\\nRunning the min occurrence ﬁlter on the vocabulary and saving it to ﬁle, you should now\\nhave a new ﬁle called vocab.txt with only the words we are interested in.\\nThe order of words in your ﬁle will diﬀer, but should look something like the following:\\naberdeen\\ndupe\\nburt\\nlibido\\nhamlet\\narlene\\navailable\\ncorners\\nweb\\ncolumbia\\n...\\nListing 10.9: Sample of the vocabulary ﬁle vocab.txt .\\nWe are now ready to look at extracting features from the reviews ready for modeling.\\n10.4 Bag-of-Words Representation\\nIn this section, we will look at how we can convert each review into a representation that we\\ncan provide to a Multilayer Perceptron model. A bag-of-words model is a way of extracting\\nfeatures from text so the text input can be used with machine learning algorithms like neural\\nnetworks. Each document, in this case a review, is converted into a vector representation.\\nThe number of items in the vector representing a document corresponds to the number of\\nwords in the vocabulary. The larger the vocabulary, the longer the vector representation, hence\\nthe preference for smaller vocabularies in the previous section. The bag-of-words model was\\nintroduced previously in Chapter 8.\\nWords in a document are scored and the scores are placed in the corresponding location in\\nthe representation. We will look at diﬀerent word scoring methods in the next section. In this\\nsection, we are concerned with converting reviews into vectors ready for training a ﬁrst neural\\nnetwork model. This section is divided into 2 steps:\\n1. Converting reviews to lines of tokens.\\n2. Encoding reviews with a bag-of-words model representation.\\n10.4.1 Reviews to Lines of Tokens\\nBefore we can convert reviews to vectors for modeling, we must ﬁrst clean them up. This\\ninvolves loading them, performing the cleaning operation developed above, ﬁltering out words\\nnot in the chosen vocabulary, and converting the remaining tokens into a single string or line'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 109}, page_content=\"10.4. Bag-of-Words Representation 93\\nready for encoding. First, we need a function to prepare one document. Below lists the function\\ndoctoline() that will load a document, clean it, ﬁlter out tokens not in the vocabulary, then\\nreturn the document as a string of white space separated tokens.\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\nListing 10.10: Function to ﬁlter a review by pre-deﬁned vocabulary.\\nNext, we need a function to work through all documents in a directory (such as posand\\nneg) to convert the documents into lines. Below lists the process docs() function that does\\njust this, expecting a directory name and a vocabulary set as input arguments and returning a\\nlist of processed documents.\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\nListing 10.11: Function to ﬁlter all movie reviews by vocabulary.\\nWe can call the process docs() consistently for positive and negative reviews to construct\\na dataset of review text and their associated output labels, 0 for negative and 1 for positive.\\nThe load clean dataset() function below implements this behavior.\\n# load and clean a dataset\\ndef load_clean_dataset(vocab):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab)\\npos = process_docs( 'txt_sentoken/pos ', vocab)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\nListing 10.12: Function to load movie reviews and prepare output labels.\\nFinally, we need to load the vocabulary and turn it into a set for use in cleaning reviews.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 110}, page_content=\"10.4. Bag-of-Words Representation 94\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\nListing 10.13: Load the pre-deﬁned vocabulary of words.\\nWe can put all of this together, reusing the loading and cleaning functions developed in\\nprevious sections. The complete example is listed below, demonstrating how to prepare the\\npositive and negative reviews from the training dataset.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 111}, page_content=\"10.4. Bag-of-Words Representation 95\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab)\\npos = process_docs( 'txt_sentoken/pos ', vocab)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = vocab.split()\\nvocab = set(vocab)\\n# load all training reviews\\ndocs, labels = load_clean_dataset(vocab)\\n# summarize what we have\\nprint(len(docs), len(labels))\\nListing 10.14: Filter all movie reviews by the pre-deﬁned vocabulary.\\nRunning this example loads and cleans the review text and returns the labels.\\n1800 1800\\nListing 10.15: Example output from loading, cleaning and ﬁltering movie review data by a\\nconstrained vocabulary.\\n10.4.2 Movie Reviews to Bag-of-Words Vectors\\nWe will use the Keras API to convert reviews to encoded document vectors. Keras provides\\ntheTokenizer class that can do some of the cleaning and vocab deﬁnition tasks that we took\\ncare of in the previous section. It is better to do this ourselves to know exactly what was done\\nand why. Nevertheless, the Tokenizer class is convenient and will easily transform documents\\ninto encoded vectors. First, the Tokenizer must be created, then ﬁt on the text documents\\nin the training dataset. In this case, these are the aggregation of the positive lines and\\nnegative lines arrays developed in the previous section.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 112}, page_content=\"10.4. Bag-of-Words Representation 96\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 10.16: Function to ﬁt a Tokenizer on the clean and ﬁltered movie reviews.\\nThis process determines a consistent way to convert the vocabulary to a ﬁxed-length vector\\nwith 25,768 elements, which is the total number of words in the vocabulary ﬁle vocab.txt .\\nNext, documents can then be encoded using the Tokenizer by calling texts tomatrix() . The\\nfunction takes both a list of documents to encode and an encoding mode, which is the method\\nused to score words in the document. Here we specify freq to score words based on their\\nfrequency in the document. This can be used to encode the loaded training and test data, for\\nexample:\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'freq ')\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'freq ')\\nListing 10.17: Encode training data.\\nThis encodes all of the positive and negative reviews in the training dataset.\\nNext, the process docs() function from the previous section needs to be modiﬁed to\\nselectively process reviews in the test or train dataset. We support the loading of both the\\ntraining and test datasets by adding an istrain argument and using that to decide what\\nreview ﬁle names to skip.\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\nListing 10.18: Updated process documents function to load all reviews.\\nSimilarly, the load clean dataset() dataset must be updated to load either train or test\\ndata.\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 113}, page_content=\"10.4. Bag-of-Words Representation 97\\nreturn docs, labels\\nListing 10.19: Function to load text data and labels.\\nWe can put all of this together in a single example.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 114}, page_content=\"10.5. Sentiment Analysis Models 98\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'freq ')\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'freq ')\\nprint(Xtrain.shape, Xtest.shape)\\nListing 10.20: Complete example of preparing train and test data.\\nRunning the example prints both the shape of the encoded training dataset and test dataset\\nwith 1,800 and 200 documents respectively, each with the same sized encoding vocabulary\\n(vector length).\\n(1800, 25768) (200, 25768)\\nListing 10.21: Example output of loading and preparing the datasets.\\n10.5 Sentiment Analysis Models\\nIn this section, we will develop Multilayer Perceptron (MLP) models to classify encoded\\ndocuments as either positive or negative. The models will be simple feedforward network models\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 115}, page_content=\"10.5. Sentiment Analysis Models 99\\nwith fully connected layers called Dense in the Keras deep learning library. This section is\\ndivided into 3 sections:\\n1. First sentiment analysis model\\n2. Comparing word scoring modes\\n3. Making a prediction for new reviews\\n10.5.1 First Sentiment Analysis Model\\nWe can develop a simple MLP model to predict the sentiment of encoded reviews. The model\\nwill have an input layer that equals the number of words in the vocabulary, and in turn the\\nlength of the input documents. We can store this in a new variable called nwords , as follows:\\nn_words = Xtest.shape[1]\\nListing 10.22: Example of calculating the number of words.\\nWe can now deﬁne the network. All model conﬁguration was found with very little trial and\\nerror and should not be considered tuned for this problem. We will use a single hidden layer\\nwith 50 neurons and a rectiﬁed linear activation function. The output layer is a single neuron\\nwith a sigmoid activation function for predicting 0 for negative and 1 for positive reviews. The\\nnetwork will be trained using the eﬃcient Adam implementation of gradient descent and the\\nbinary cross entropy loss function, suited to binary classiﬁcation problems. We will keep track\\nof accuracy when training and evaluating the model.\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 10.23: Example of deﬁning an MLP for the bag-of-words model.\\nNext, we can ﬁt the model on the training data; in this case, the model is small and is easily\\nﬁt in 10 epochs.\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\nListing 10.24: Example of ﬁtting the deﬁned model.\\nFinally, once the model is trained, we can evaluate its performance by making predictions in\\nthe test dataset and printing the accuracy.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 116}, page_content=\"10.5. Sentiment Analysis Models 100\\n# evaluate\\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %f '% (acc*100))\\nListing 10.25: Example of evaluating the ﬁt model.\\nThe complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 117}, page_content=\"10.5. Sentiment Analysis Models 101\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'freq ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 118}, page_content=\"10.5. Sentiment Analysis Models 102\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'freq ')\\n# define the model\\nn_words = Xtest.shape[1]\\nmodel = define_model(n_words)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# evaluate\\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %f '% (acc*100))\\nListing 10.26: Complete example of training and evaluating an MLP bag-of-words model.\\nRunning the example ﬁrst prints a summary of the deﬁned model.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ndense_1 (Dense) (None, 50) 1288450\\n_________________________________________________________________\\ndense_2 (Dense) (None, 1) 51\\n=================================================================\\nTotal params: 1,288,501\\nTrainable params: 1,288,501\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 10.27: Summary of the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\\nFigure 10.1: Plot of the deﬁned bag-of-words model.\\nWe can see that the model easily ﬁts the training data within the 10 epochs, achieving close\\nto 100% accuracy. Evaluating the model on the test dataset, we can see that model does well,\\nachieving an accuracy of above 87%, well within the ballpark of low-to-mid 80s seen in the\\noriginal paper. Although, it is important to note that this is not an apples-to-apples comparison,\\nas the original paper used 10-fold cross-validation to estimate model skill instead of a single\\ntrain/test split.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 119}, page_content='10.6. Comparing Word Scoring Methods 103\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n...\\nEpoch 6/10\\n0s - loss: 0.5319 - acc: 0.9428\\nEpoch 7/10\\n0s - loss: 0.4839 - acc: 0.9506\\nEpoch 8/10\\n0s - loss: 0.4368 - acc: 0.9567\\nEpoch 9/10\\n0s - loss: 0.3927 - acc: 0.9611\\nEpoch 10/10\\n0s - loss: 0.3516 - acc: 0.9689\\nTest Accuracy: 87.000000\\nListing 10.28: Example output of training and evaluating the MLP model.\\nNext, let’s look at testing diﬀerent word scoring methods for the bag-of-words model.\\n10.6 Comparing Word Scoring Methods\\nThe texts tomatrix() function for the Tokenizer in the Keras API provides 4 diﬀerent\\nmethods for scoring words; they are:\\n\\x88binary Where words are marked as present (1) or absent (0).\\n\\x88count Where the occurrence count for each word is marked as an integer.\\n\\x88tfidf Where each word is scored based on their frequency, where words that are common\\nacross all documents are penalized.\\n\\x88freq Where words are scored based on their frequency of occurrence within the document.\\nWe can evaluate the skill of the model developed in the previous section ﬁt using each of the\\n4 supported word scoring modes. This ﬁrst involves the development of a function to create an\\nencoding of the loaded documents based on a chosen scoring model. The function creates the\\ntokenizer, ﬁts it on the training documents, then creates the train and test encodings using the\\nchosen model. The function prepare data() implements this behavior given lists of train and\\ntest documents.\\n# prepare bag-of-words encoding of docs\\ndef prepare_data(train_docs, test_docs, mode):\\n# create the tokenizer\\ntokenizer = Tokenizer()\\n# fit the tokenizer on the documents\\ntokenizer.fit_on_texts(train_docs)\\n# encode training data set\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\\n# encode training data set\\nXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 120}, page_content=\"10.6. Comparing Word Scoring Methods 104\\nreturn Xtrain, Xtest\\nListing 10.29: Updated data preparation to take encoding mode as a parameter.\\nWe also need a function to evaluate the MLP given a speciﬁc encoding of the data. Because\\nneural networks are stochastic, they can produce diﬀerent results when the same model is ﬁt on\\nthe same data. This is mainly because of the random initial weights and the shuﬄing of patterns\\nduring mini-batch gradient descent. This means that any one scoring of a model is unreliable\\nand we should estimate model skill based on an average of multiple runs. The function below,\\nnamed evaluate mode() , takes encoded documents and evaluates the MLP by training it on\\nthe train set and estimating skill on the test set 10 times and returns a list of the accuracy\\nscores across all of these runs.\\n# evaluate a neural network model\\ndef evaluate_mode(Xtrain, ytrain, Xtest, ytest):\\nscores = list()\\nn_repeats = 30\\nn_words = Xtest.shape[1]\\nfor i in range(n_repeats):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# evaluate\\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\\nscores.append(acc)\\nprint( '%d accuracy: %s '% ((i+1), acc))\\nreturn scores\\nListing 10.30: Function to create, ﬁt and evaluate a model multiple times.\\nWe are now ready to evaluate the performance of the 4 diﬀerent word scoring methods.\\nPulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom pandas import DataFrame\\nfrom matplotlib import pyplot\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 121}, page_content=\"10.6. Comparing Word Scoring Methods 105\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 122}, page_content=\"10.6. Comparing Word Scoring Methods 106\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\nreturn model\\n# evaluate a neural network model\\ndef evaluate_mode(Xtrain, ytrain, Xtest, ytest):\\nscores = list()\\nn_repeats = 10\\nn_words = Xtest.shape[1]\\nfor i in range(n_repeats):\\n# define network\\nmodel = define_model(n_words)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=0)\\n# evaluate\\n_, acc = model.evaluate(Xtest, ytest, verbose=0)\\nscores.append(acc)\\nprint( '%d accuracy: %s '% ((i+1), acc))\\nreturn scores\\n# prepare bag of words encoding of docs\\ndef prepare_data(train_docs, test_docs, mode):\\n# create the tokenizer\\ntokenizer = Tokenizer()\\n# fit the tokenizer on the documents\\ntokenizer.fit_on_texts(train_docs)\\n# encode training data set\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\\n# encode training data set\\nXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\\nreturn Xtrain, Xtest\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# run experiment\\nmodes = [ 'binary ', 'count ', 'tfidf ', 'freq ']\\nresults = DataFrame()\\nfor mode in modes:\\n# prepare data for mode\\nXtrain, Xtest = prepare_data(train_docs, test_docs, mode)\\n# evaluate model on data for mode\\nresults[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\\n# summarize results\\nprint(results.describe())\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 123}, page_content='10.6. Comparing Word Scoring Methods 107\\n# plot results\\nresults.boxplot()\\npyplot.show()\\nListing 10.31: Complete example of comparing document encoding schemes.\\nAt the end of the run, summary statistics for each word scoring method are provided,\\nsummarizing the distribution of model skill scores across each of the 10 runs per mode. We can\\nsee that the mean score of both the count andbinary methods appear to be better than freq\\nandtfidf .\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nbinary count tfidf freq\\ncount 10.000000 10.000000 10.000000 10.000000\\nmean 0.927000 0.903500 0.876500 0.871000\\nstd 0.011595 0.009144 0.017958 0.005164\\nmin 0.910000 0.885000 0.855000 0.865000\\n25% 0.921250 0.900000 0.861250 0.866250\\n50% 0.927500 0.905000 0.875000 0.870000\\n75% 0.933750 0.908750 0.888750 0.875000\\nmax 0.945000 0.915000 0.910000 0.880000\\nListing 10.32: Example output of comparing document encoding schemes.\\nA box and whisker plot of the results is also presented, summarizing the accuracy distributions\\nper conﬁguration. We can see that binary achieved the best results with a modest spread and\\nmight be the preferred approach for this dataset.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 124}, page_content='10.7. Predicting Sentiment for New Reviews 108\\nFigure 10.2: Box and Whisker Plot for Model Accuracy with Diﬀerent Word Scoring Methods.\\n10.7 Predicting Sentiment for New Reviews\\nFinally, we can develop and use a ﬁnal model to make predictions for new textual reviews. This\\nis why we wanted the model in the ﬁrst place. First we will train a ﬁnal model on all of the\\navailable data. We will use the binary mode for scoring the bag-of-words model that was shown\\nto give the best results in the previous section.\\nPredicting the sentiment of new reviews involves following the same steps used to prepare\\nthe test data. Speciﬁcally, loading the text, cleaning the document, ﬁltering tokens by the\\nchosen vocabulary, converting the remaining tokens to a line, encoding it using the Tokenizer ,\\nand making a prediction. We can make a prediction of a class value directly with the ﬁt model\\nby calling predict() that will return an integer of 0 for a negative review and 1 for a positive\\nreview. All of these steps can be put into a new function called predict sentiment() that\\nrequires the review text, the vocabulary, the tokenizer, and the ﬁt model and returns the\\npredicted sentiment and an associated percentage or conﬁdence-like output.\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, model):\\n# clean\\ntokens = clean_doc(review)\\n# filter by vocab'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 125}, page_content=\"10.7. Predicting Sentiment for New Reviews 109\\ntokens = [w for w in tokens if w in vocab]\\n# convert to line\\nline = ' '.join(tokens)\\n# encode\\nencoded = tokenizer.texts_to_matrix([line], mode= 'binary ')\\n# predict sentiment\\nyhat = model.predict(encoded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\nListing 10.33: Function for making predictions for new reviews.\\nWe can now make predictions for new review texts. Below is an example with both a clearly\\npositive and a clearly negative review using the simple MLP developed above with the frequency\\nword scoring mode.\\n# test positive text\\ntext = 'Best movie ever! It was great, I recommend it. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\n# test negative text\\ntext = 'This is a bad movie. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\nListing 10.34: Exampling of making predictions for new reviews.\\nPulling this all together, the complete example for making predictions for new reviews is\\nlisted below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 126}, page_content=\"10.7. Predicting Sentiment for New Reviews 110\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab)\\npos = process_docs( 'txt_sentoken/pos ', vocab)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 127}, page_content=\"10.7. Predicting Sentiment for New Reviews 111\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, model):\\n# clean\\ntokens = clean_doc(review)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\n# convert to line\\nline = ' '.join(tokens)\\n# encode\\nencoded = tokenizer.texts_to_matrix([line], mode= 'binary ')\\n# predict sentiment\\nyhat = model.predict(encoded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab)\\ntest_docs, ytest = load_clean_dataset(vocab)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'binary ')\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'binary ')\\n# define network\\nn_words = Xtrain.shape[1]\\nmodel = define_model(n_words)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# test positive text\\ntext = 'Best movie ever! It was great, I recommend it. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\n# test negative text\\ntext = 'This is a bad movie. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\nListing 10.35: Complete example of making predictions for new review data.\\nRunning the example correctly classiﬁes these reviews.\\nReview: [Best movie ever! It was great, I recommend it.]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 128}, page_content='10.8. Extensions 112\\nSentiment: POSITIVE (57.124%)\\nReview: [This is a bad movie.]\\nSentiment: NEGATIVE (64.404%)\\nListing 10.36: Example output of making predictions for new reviews.\\nIdeally, we would ﬁt the model on all available data (train and test) to create a ﬁnal model\\nand save the model and tokenizer to ﬁle so that they can be loaded and used in new software.\\n10.8 Extensions\\nThis section lists some extensions if you are looking to get more out of this tutorial.\\n\\x88Manage Vocabulary . Explore using a larger or smaller vocabulary. Perhaps you can\\nget better performance with a smaller set of words.\\n\\x88Tune the Network Topology . Explore alternate network topologies such as deeper or\\nwider networks. Perhaps you can get better performance with a more suited network.\\n\\x88Use Regularization . Explore the use of regularization techniques, such as dropout.\\nPerhaps you can delay the convergence of the model and achieve better test set performance.\\n\\x88More Data Cleaning . Explore more or less cleaning of the review text and see how it\\nimpacts the model skill.\\n\\x88Training Diagnostics . Use the test dataset as a validation dataset during training and\\ncreate plots of train and test loss. Use these diagnostics to tune the batch size and number\\nof training epochs.\\n\\x88Trigger Words . Explore whether there are speciﬁc words in reviews that are highly\\npredictive of the sentiment.\\n\\x88Use Bigrams . Prepare the model to score bigrams of words and evaluate the performance\\nunder diﬀerent scoring schemes.\\n\\x88Truncated Reviews . Explore how using a truncated version of the movie reviews results\\nimpacts model skill, try truncating the start, end and middle of reviews.\\n\\x88Ensemble Models . Create models with diﬀerent word scoring schemes and see if using\\nensembles of the models results in improves to model skill.\\n\\x88Real Reviews . Train a ﬁnal model on all data and evaluate the model on real movie\\nreviews taken from the internet.\\nIf you explore any of these extensions, I’d love to know.\\n10.9 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 129}, page_content='10.10. Summary 113\\n10.9.1 Dataset\\n\\x88Movie Review Data.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\n\\x88A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based\\non Minimum Cuts , 2004.\\nhttp://xxx.lanl.gov/abs/cs/0409058\\n\\x88Movie Review Polarity Dataset.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\n10.9.2 APIs\\n\\x88nltk.tokenize package API.\\nhttp://www.nltk.org/api/nltk.tokenize.html\\n\\x88Chapter 2, Accessing Text Corpora and Lexical Resources .\\nhttp://www.nltk.org/book/ch02.html\\n\\x88osAPI Miscellaneous operating system interfaces.\\nhttps://docs.python.org/3/library/os.html\\n\\x88collections API - Container datatypes.\\nhttps://docs.python.org/3/library/collections.html\\n\\x88Tokenizer Keras API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n10.10 Summary\\nIn this tutorial, you discovered how to develop a bag-of-words model for predicting the sentiment\\nof movie reviews. Speciﬁcally, you learned:\\n\\x88How to prepare the review text data for modeling with a restricted vocabulary.\\n\\x88How to use the bag-of-words model to prepare train and test data.\\n\\x88How to develop a Multilayer Perceptron bag-of-words model and use it to make predictions\\non new review text data.\\n10.10.1 Next\\nThis is the ﬁnal chapter in the bag-of-words part. In the next part, you will discover how to\\ndevelop word embedding models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 130}, page_content='Part V\\nWord Embeddings\\n114'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 131}, page_content='Chapter 11\\nThe Word Embedding Model\\nWord embeddings are a type of word representation that allows words with similar meaning to\\nhave a similar representation. They are a distributed representation for text that is perhaps one\\nof the key breakthroughs for the impressive performance of deep learning methods on challenging\\nnatural language processing problems. In this chapter, you will discover the word embedding\\napproach for representing text data. After completing this chapter, you will know:\\n\\x88What the word embedding approach for representing text is and how it diﬀers from other\\nfeature extraction methods.\\n\\x88That there are 3 main algorithms for learning a word embedding from text data.\\n\\x88That you can either train a new embedding or use a pre-trained embedding on your natural\\nlanguage processing task.\\nLet’s get started.\\n11.1 Overview\\nThis tutorial is divided into the following parts:\\n1. What Are Word Embeddings?\\n2. Word Embedding Algorithms\\n3. Using Word Embeddings\\n11.2 What Are Word Embeddings?\\nA word embedding is a learned representation for text where words that have the same meaning\\nhave a similar representation. It is this approach to representing words and documents that may\\nbe considered one of the key breakthroughs of deep learning on challenging natural language\\nprocessing problems.\\n115'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 132}, page_content='11.3. Word Embedding Algorithms 116\\nOne of the beneﬁts of using dense and low-dimensional vectors is computational:\\nthe majority of neural network toolkits do not play well with very high-dimensional,\\nsparse vectors. ... The main beneﬁt of the dense representations is generalization\\npower: if we believe some features may provide similar clues, it is worthwhile to\\nprovide a representation that is able to capture these similarities.\\n— Page 92, Neural Network Methods in Natural Language Processing , 2017.\\nWord embeddings are in fact a class of techniques where individual words are represented as\\nreal-valued vectors in a predeﬁned vector space. Each word is mapped to one vector and the\\nvector values are learned in a way that resembles a neural network, and hence the technique is\\noften lumped into the ﬁeld of deep learning.\\nKey to the approach is the idea of using a dense distributed representation for each word.\\nEach word is represented by a real-valued vector, often tens or hundreds of dimensions. This is\\ncontrasted to the thousands or millions of dimensions required for sparse word representations,\\nsuch as a one hot encoding.\\nassociate with each word in the vocabulary a distributed word feature vector ... The\\nfeature vector represents diﬀerent aspects of the word: each word is associated with\\na point in a vector space. The number of features ... is much smaller than the size\\nof the vocabulary\\n—A Neural Probabilistic Language Model , 2003.\\nThe distributed representation is learned based on the usage of words. This allows words\\nthat are used in similar ways to result in having similar representations, naturally capturing\\ntheir meaning. This can be contrasted with the crisp but fragile representation in a bag-of-words\\nmodel where, unless explicitly managed, diﬀerent words have diﬀerent representations, regardless\\nof how they are used.\\nThere is deeper linguistic theory behind the approach, namely the distributional hypothesis\\nby Zellig Harris that could be summarized as: words that have similar context will have similar\\nmeanings. For more depth see Harris’ 1956 paper Distributional structure . This notion of letting\\nthe usage of the word deﬁne its meaning can be summarized by an oft repeated quip by John\\nFirth:\\nYou shall know a word by the company it keeps!\\n— Page 11, A synopsis of linguistic theory 1930-1955 , in Studies in Linguistic Analysis\\n1930-1955, 1962.\\n11.3 Word Embedding Algorithms\\nWord embedding methods learn a real-valued vector representation for a predeﬁned ﬁxed sized\\nvocabulary from a corpus of text. The learning process is either joint with the neural network\\nmodel on some task, such as document classiﬁcation, or is an unsupervised process, using\\ndocument statistics. This section reviews three techniques that can be used to learn a word\\nembedding from text data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 133}, page_content='11.3. Word Embedding Algorithms 117\\n11.3.1 Embedding Layer\\nAn embedding layer, for lack of a better name, is a word embedding that is learned jointly\\nwith a neural network model on a speciﬁc natural language processing task, such as language\\nmodeling or document classiﬁcation. It requires that document text be cleaned and prepared\\nsuch that each word is one hot encoded. The size of the vector space is speciﬁed as part of\\nthe model, such as 50, 100, or 300 dimensions. The vectors are initialized with small random\\nnumbers. The embedding layer is used on the front end of a neural network and is ﬁt in a\\nsupervised way using the Backpropagation algorithm.\\n... when the input to a neural network contains symbolic categorical features\\n(e.g. features that take one of kdistinct symbols, such as words from a closed\\nvocabulary), it is common to associate each possible feature value (i.e., each word\\nin the vocabulary) with a d-dimensional vector for some d. These vectors are\\nthen considered parameters of the model, and are trained jointly with the other\\nparameters.\\n— Page 49, Neural Network Methods in Natural Language Processing , 2017.\\nThe one hot encoded words are mapped to the word vectors. If a Multilayer Perceptron\\nmodel is used, then the word vectors are concatenated before being fed as input to the model.\\nIf a recurrent neural network is used, then each word may be taken as one input in a sequence.\\nThis approach of learning an embedding layer requires a lot of training data and can be slow,\\nbut will learn an embedding both targeted to the speciﬁc text data and the NLP task.\\n11.3.2 Word2Vec\\nWord2Vec is a statistical method for eﬃciently learning a standalone word embedding from a\\ntext corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make\\nthe neural-network-based training of the embedding more eﬃcient and since then has become\\nthe de facto standard for developing pre-trained word embedding.\\nAdditionally, the work involved analysis of the learned vectors and the exploration of vector\\nmath on the representations of words. For example, that subtracting the man-ness from King\\nand adding women-ness results in the word Queen , capturing the analogy king is to queen as\\nman is to woman .\\nWe ﬁnd that these representations are surprisingly good at capturing syntactic and\\nsemantic regularities in language, and that each relationship is characterized by a\\nrelation-speciﬁc vector oﬀset. This allows vector-oriented reasoning based on the\\noﬀsets between words. For example, the male/female relationship is automatically\\nlearned, and with the induced vector representations, King - Man + Woman results\\nin a vector very close to Queen .\\n—Linguistic Regularities in Continuous Space Word Representations , 2013.\\nTwo diﬀerent learning models were introduced that can be used as part of the Word2Vec\\napproach to learn the word embedding; they are:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 134}, page_content='11.3. Word Embedding Algorithms 118\\n\\x88Continuous Bag-of-Words, or CBOW model.\\n\\x88Continuous Skip-Gram Model.\\nThe CBOW model learns the embedding by predicting the current word based on its context.\\nThe continuous skip-gram model learns by predicting the surrounding words given a current\\nword.\\nFigure 11.1: Word2Vec Training Models. Taken from Eﬃcient Estimation of Word Representa-\\ntions in Vector Space , 2013\\nBoth models are focused on learning about words given their local usage context, where the\\ncontext is deﬁned by a window of neighboring words. This window is a conﬁgurable parameter\\nof the model.\\nThe size of the sliding window has a strong eﬀect on the resulting vector similarities.\\nLarge windows tend to produce more topical similarities [...], while smaller windows\\ntend to produce more functional and syntactic similarities.\\n— Page 128, Neural Network Methods in Natural Language Processing , 2017.\\nThe key beneﬁt of the approach is that high-quality word embeddings can be learned\\neﬃciently (low space and time complexity), allowing larger embeddings to be learned (more\\ndimensions) from much larger corpora of text (billions of words).\\n11.3.3 GloVe\\nThe Global Vectors for Word Representation, or GloVe, algorithm is an extension to the\\nWord2Vec method for eﬃciently learning word vectors, developed by Pennington, et al. at\\nStanford. Classical vector space model representations of words were developed using matrix'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 135}, page_content='11.4. Using Word Embeddings 119\\nfactorization techniques such as Latent Semantic Analysis (LSA) that do a good job of using\\nglobal text statistics but are not as good as the learned methods like Word2Vec at capturing\\nmeaning and demonstrating it on tasks like calculating analogies (e.g. the King and Queen\\nexample above).\\nGloVe is an approach to marry both the global statistics of matrix factorization techniques\\nlike LSA with the local context-based learning in Word2Vec. Rather than using a window to\\ndeﬁne local context, GloVe constructs an explicit word-context or word co-occurrence matrix\\nusing statistics across the whole text corpus. The result is a learning model that may result in\\ngenerally better word embeddings.\\nGloVe, is a new global log-bilinear regression model for the unsupervised learning of\\nword representations that outperforms other models on word analogy, word similarity,\\nand named entity recognition tasks.\\n—GloVe: Global Vectors for Word Representation , 2014.\\n11.4 Using Word Embeddings\\nYou have some options when it comes time to using word embeddings on your natural language\\nprocessing project. This section outlines those options.\\n11.4.1 Learn an Embedding\\nYou may choose to learn a word embedding for your problem. This will require a large amount\\nof text data to ensure that useful embeddings are learned, such as millions or billions of words.\\nYou have two main options when training your word embedding:\\n\\x88Learn it Standalone , where a model is trained to learn the embedding, which is saved\\nand used as a part of another model for your task later. This is a good approach if you\\nwould like to use the same embedding in multiple models.\\n\\x88Learn Jointly , where the embedding is learned as part of a large task-speciﬁc model.\\nThis is a good approach if you only intend to use the embedding on one task.\\n11.4.2 Reuse an Embedding\\nIt is common for researchers to make pre-trained word embeddings available for free, often under\\na permissive license so that you can use them on your own academic or commercial projects. For\\nexample, both Word2Vec and GloVe word embeddings are available for free download. These\\ncan be used on your project instead of training your own embeddings from scratch. You have\\ntwo main options when it comes to using pre-trained embeddings:\\n\\x88Static , where the embedding is kept static and is used as a component of your model.\\nThis is a suitable approach if the embedding is a good ﬁt for your problem and gives good\\nresults.\\n\\x88Updated , where the pre-trained embedding is used to seed the model, but the embedding\\nis updated jointly during the training of the model. This may be a good option if you are\\nlooking to get the most out of the model and embedding on your task.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 136}, page_content='11.5. Further Reading 120\\n11.4.3 Which Option Should You Use?\\nExplore the diﬀerent options, and if possible, test to see which gives the best results on your\\nproblem. Perhaps start with fast methods, like using a pre-trained embedding, and only use a\\nnew embedding if it results in better performance on your problem.\\n11.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n11.5.1 Books\\n\\x88Neural Network Methods in Natural Language Processing, 2017.\\nhttp://amzn.to/2wycQKA\\n11.5.2 Articles\\n\\x88Word embedding on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word_embedding\\n\\x88Word2Vec on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word2vec\\n\\x88GloVe on Wikipedia\\nhttps://en.wikipedia.org/wiki/GloVe_(machine_learning)\\n\\x88An overview of word embeddings and their connection to distributional semantic models ,\\n2016.\\nhttp://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/\\n\\x88Deep Learning, NLP, and Representations , 2014.\\nhttp://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\\n11.5.3 Papers\\n\\x88Distributional structure , 1956.\\nhttp://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520\\n\\x88A Neural Probabilistic Language Model , 2003.\\nhttp://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\\n\\x88A Uniﬁed Architecture for Natural Language Processing: Deep Neural Networks with\\nMultitask Learning , 2008.\\nhttps://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf\\n\\x88Continuous space language models , 2007.\\nhttps://pdfs.semanticscholar.org/0fcc/184b3b90405ec3ceafd6a4007c749df7c363.\\npdf'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 137}, page_content='11.6. Summary 121\\n\\x88Eﬃcient Estimation of Word Representations in Vector Space , 2013.\\nhttps://arxiv.org/pdf/1301.3781.pdf\\n\\x88Distributed Representations of Words and Phrases and their Compositionality , 2013.\\nhttps://arxiv.org/pdf/1310.4546.pdf\\n\\x88GloVe: Global Vectors for Word Representation , 2014.\\nhttps://nlp.stanford.edu/pubs/glove.pdf\\n11.5.4 Projects\\n\\x88Word2Vec on Google Code.\\nhttps://code.google.com/archive/p/word2vec/\\n\\x88GloVe: Global Vectors for Word Representation.\\nhttps://nlp.stanford.edu/projects/glove/\\n11.6 Summary\\nIn this chapter, you discovered Word Embeddings as a representation method for text in deep\\nlearning applications. Speciﬁcally, you learned:\\n\\x88What the word embedding approach for representation text is and how it diﬀers from\\nother feature extraction methods.\\n\\x88That there are 3 main algorithms for learning a word embedding from text data.\\n\\x88That you can either train a new embedding or use a pre-trained embedding on your natural\\nlanguage processing task.\\n11.6.1 Next\\nIn the next chapter, you will discover how you can train and manipulate word embeddings using\\nthe Gensim Python library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 138}, page_content='Chapter 12\\nHow to Develop Word Embeddings\\nwith Gensim\\nWord embeddings are a modern approach for representing text in natural language processing.\\nEmbedding algorithms like Word2Vec and GloVe are key to the state-of-the-art results achieved\\nby neural network models on natural language processing problems like machine translation.\\nIn this tutorial, you will discover how to train and load word embedding models for natural\\nlanguage processing applications in Python using Gensim. After completing this tutorial, you\\nwill know:\\n\\x88How to train your own Word2Vec word embedding model on text data.\\n\\x88How to visualize a trained word embedding model using Principal Component Analysis.\\n\\x88How to load pre-trained Word2Vec and GloVe word embedding models from Google and\\nStanford.\\nLet’s get started.\\n12.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Word Embeddings\\n2. Gensim Library\\n3. Develop Word2Vec Embedding\\n4. Visualize Word Embedding\\n5. Load Google’s Word2Vec Embedding\\n6. Load Stanford’s GloVe Embedding\\n122'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 139}, page_content='12.2. Word Embeddings 123\\n12.2 Word Embeddings\\nA word embedding is an approach to provide a dense vector representation of words that capture\\nsomething about their meaning. Word embeddings are an improvement over simpler bag-of-word\\nmodel word encoding schemes like word counts and frequencies that result in large and sparse\\nvectors (mostly 0 values) that describe documents but not the meaning of the words.\\nWord embeddings work by using an algorithm to train a set of ﬁxed-length dense and\\ncontinuous-valued vectors based on a large corpus of text. Each word is represented by a\\npoint in the embedding space and these points are learned and moved around based on the\\nwords that surround the target word. It is deﬁning a word by the company that it keeps that\\nallows the word embedding to learn something about the meaning of words. The vector space\\nrepresentation of the words provides a projection where words with similar meanings are locally\\nclustered within the space.\\nThe use of word embeddings over other text representations is one of the key methods that\\nhas led to breakthrough performance with deep neural networks on problems like machine\\ntranslation. In this tutorial, we are going to look at how to use two diﬀerent word embedding\\nmethods called Word2Vec by researchers at Google and GloVe by researchers at Stanford.\\n12.3 Gensim Python Library\\nGensim is an open source Python library for natural language processing, with a focus on topic\\nmodeling. It is billed as “ topic modeling for humans ”. Gensim was developed and is maintained\\nby the Czech natural language processing researcher Radim Rehurek and his company RaRe\\nTechnologies. It is not an everything-including-the-kitchen-sink NLP research library (like\\nNLTK); instead, Gensim is a mature, focused, and eﬃcient suite of NLP tools for topic modeling.\\nMost notably for this tutorial, it supports an implementation of the Word2Vec word embedding\\nfor learning new word vectors from text.\\nIt also provides tools for loading pre-trained word embeddings in a few formats and for\\nmaking use and querying a loaded embedding. We will use the Gensim library in this tutorial.\\nGensim can be installed easily using piporeasy install . For example, you can install Gensim\\nwith pipby typing the following on your command line:\\nsudo pip install -U gensim\\nListing 12.1: Install the Gensim library with pip.\\nIf you need help installing Gensim on your system, you can see the Gensim Installation\\nInstructions (linked at the end of the chapter).\\n12.4 Develop Word2Vec Embedding\\nWord2Vec is one algorithm for learning a word embedding from a text corpus. There are two\\nmain training algorithms that can be used to learn the embedding from text; they are Continuous\\nBag-of-Words (CBOW) and skip grams. We will not get into the algorithms other than to say\\nthat they generally look at a window of words for each target word to provide context and in\\nturn meaning for words. The approach was developed by Tomas Mikolov, formerly at Google\\nand currently at Facebook.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 140}, page_content=\"12.4. Develop Word2Vec Embedding 124\\nWord2Vec models require a lot of text, e.g. the entire Wikipedia corpus. Nevertheless, we\\nwill demonstrate the principles using a small in-memory example of text. Gensim provides the\\nWord2Vec class for working with a Word2Vec model. Learning a word embedding from text\\ninvolves loading and organizing the text into sentences and providing them to the constructor\\nof a new Word2Vec() instance. For example:\\nsentences = ...\\nmodel = Word2Vec(sentences)\\nListing 12.2: Example of creating a Word2Vec model.\\nSpeciﬁcally, each sentence must be tokenized, meaning divided into words and prepared (e.g.\\nperhaps pre-ﬁltered and perhaps converted to a preferred case). The sentences could be text\\nloaded into memory, or an iterator that progressively loads text, required for very large text\\ncorpora. There are many parameters on this constructor; a few noteworthy arguments you may\\nwish to conﬁgure are:\\n\\x88size : (default 100) The number of dimensions of the embedding, e.g. the length of the\\ndense vector to represent each token (word).\\n\\x88window : (default 5) The maximum distance between a target word and words around the\\ntarget word.\\n\\x88mincount : (default 5) The minimum count of words to consider when training the model;\\nwords with an occurrence less than this count will be ignored.\\n\\x88workers : (default 3) The number of threads to use while training.\\n\\x88sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\\nThe defaults are often good enough when just getting started. If you have a lot of cores, as\\nmost modern computers do, I strongly encourage you to increase workers to match the number\\nof cores (e.g. 8). After the model is trained, it is accessible via the wvattribute. This is the\\nactual word vector model in which queries can be made. For example, you can print the learned\\nvocabulary of tokens (words) as follows:\\nwords = list(model.wv.vocab)\\nprint(words)\\nListing 12.3: Example of summarizing the words in the model vocabulary.\\nYou can review the embedded vector for a speciﬁc token as follows:\\nprint(model[ 'word '])\\nListing 12.4: Example of printing the embedding for a speciﬁc word.\\nFinally, a trained model can then be saved to ﬁle by calling the save word2vec format()\\nfunction on the word vector model. By default, the model is saved in a binary format to save\\nspace. For example:\\nmodel.wv.save_word2vec_format( 'model.bin ')\\nListing 12.5: Example of saving a word embedding.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 141}, page_content=\"12.4. Develop Word2Vec Embedding 125\\nWhen getting started, you can save the learned model in ASCII format and review the\\ncontents. You can do this by setting binary=False when calling the save word2vec format()\\nfunction, for example:\\nmodel.wv.save_word2vec_format( 'model.txt ', binary=False)\\nListing 12.6: Example of saving a word embedding in ASCII format.\\nThe saved model can then be loaded again by calling the Word2Vec.load() function. For\\nexample:\\nmodel = Word2Vec.load( 'model.bin ')\\nListing 12.7: Example of loading a saved word embedding.\\nWe can tie all of this together with a worked example. Rather than loading a large text\\ndocument or corpus from ﬁle, we will work with a small, in-memory list of pre-tokenized\\nsentences. The model is trained and the minimum count for words is set to 1 so that no words\\nare ignored. After the model is learned, we summarize, print the vocabulary, then print a single\\nvector for the word “ sentence ”. Finally, the model is saved to a ﬁle in binary format, loaded,\\nand then summarized.\\nfrom gensim.models import Word2Vec\\n# define training data\\nsentences = [[ 'this ', 'is ', 'the ', 'first ', 'sentence ', 'for ', 'word2vec '],\\n[ 'this ', 'is ', 'the ', 'second ', 'sentence '],\\n[ 'yet ', 'another ', 'sentence '],\\n[ 'one ', 'more ', 'sentence '],\\n[ 'and ', 'the ', 'final ', 'sentence ']]\\n# train model\\nmodel = Word2Vec(sentences, min_count=1)\\n# summarize the loaded model\\nprint(model)\\n# summarize vocabulary\\nwords = list(model.wv.vocab)\\nprint(words)\\n# access vector for one word\\nprint(model[ 'sentence '])\\n# save model\\nmodel.save( 'model.bin ')\\n# load model\\nnew_model = Word2Vec.load( 'model.bin ')\\nprint(new_model)\\nListing 12.8: Example demonstrating the Word2Vec model in Gensim.\\nRunning the example prints the following output.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nWord2Vec(vocab=14, size=100, alpha=0.025)\\n[ 'second ', 'sentence ', 'and ', 'this ', 'final ', 'word2vec ', 'for ', 'another ', 'one ',\\n'first ', 'more ', 'the ', 'yet ', 'is ']\\n[ -4.61881841e-03 -4.88735968e-03 -3.19508743e-03 4.08568839e-03\\n-3.38211656e-03 1.93076557e-03 3.90265253e-03 -1.04349572e-03\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 142}, page_content='12.5. Visualize Word Embedding 126\\n4.14286414e-03 1.55219622e-03 3.85653134e-03 2.22428422e-03\\n-3.52565176e-03 2.82056746e-03 -2.11121864e-03 -1.38054823e-03\\n-1.12888147e-03 -2.87318649e-03 -7.99703528e-04 3.67874932e-03\\n2.68940022e-03 6.31021452e-04 -4.36326629e-03 2.38655557e-04\\n-1.94210222e-03 4.87691024e-03 -4.04118607e-03 -3.17813386e-03\\n4.94802603e-03 3.43150692e-03 -1.44031656e-03 4.25637932e-03\\n-1.15106850e-04 -3.73274647e-03 2.50349124e-03 4.28692997e-03\\n-3.57313151e-03 -7.24728088e-05 -3.46099050e-03 -3.39612062e-03\\n3.54845310e-03 1.56780297e-03 4.58260969e-04 2.52689526e-04\\n3.06256465e-03 2.37558200e-03 4.06933809e-03 2.94650183e-03\\n-2.96231941e-03 -4.47433954e-03 2.89590308e-03 -2.16034567e-03\\n-2.58548348e-03 -2.06163677e-04 1.72605237e-03 -2.27384618e-04\\n-3.70194600e-03 2.11557443e-03 2.03793868e-03 3.09839356e-03\\n-4.71800892e-03 2.32995977e-03 -6.70911541e-05 1.39375112e-03\\n-3.84263694e-03 -1.03898917e-03 4.13251948e-03 1.06330717e-03\\n1.38514000e-03 -1.18144893e-03 -2.60811858e-03 1.54952740e-03\\n2.49916781e-03 -1.95435272e-03 8.86975031e-05 1.89820060e-03\\n-3.41996481e-03 -4.08187555e-03 5.88635216e-04 4.13103355e-03\\n-3.25899688e-03 1.02130906e-03 -3.61028523e-03 4.17646067e-03\\n4.65870230e-03 3.64110398e-04 4.95479070e-03 -1.29743712e-03\\n-5.03367570e-04 -2.52546836e-03 3.31060472e-03 -3.12870182e-03\\n-1.14580349e-03 -4.34387522e-03 -4.62882593e-03 3.19007039e-03\\n2.88707414e-03 1.62976081e-04 -6.05802808e-04 -1.06368808e-03]\\nWord2Vec(vocab=14, size=100, alpha=0.025)\\nListing 12.9: Example output of the Word2Vec model in Gensim.\\nYou can see that with a little work to prepare your text document, you can create your own\\nword embedding very easily with Gensim.\\n12.5 Visualize Word Embedding\\nAfter you learn word embedding for your text data, it can be nice to explore it with visualization.\\nYou can use classical projection methods to reduce the high-dimensional word vectors to two-\\ndimensional plots and plot them on a graph. The visualizations can provide a qualitative\\ndiagnostic for your learned model. We can retrieve all of the vectors from a trained model as\\nfollows:\\nX = model[model.wv.vocab]\\nListing 12.10: Access the model vocabulary.\\nWe can then train a projection method on the vectors, such as those methods oﬀered in\\nscikit-learn, then use Matplotlib to plot the projection as a scatter plot. Let’s look at an example\\nwith Principal Component Analysis or PCA.\\n12.5.1 Plot Word Vectors Using PCA\\nWe can create a 2-dimensional PCA model of the word vectors using the scikit-learn PCAclass\\nas follows.\\npca = PCA(n_components=2)\\nresult = pca.fit_transform(X)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 143}, page_content=\"12.5. Visualize Word Embedding 127\\nListing 12.11: Example of ﬁtting a 2D PCA model to the word vectors.\\nThe resulting projection can be plotted using Matplotlib as follows, pulling out the two\\ndimensions as xandycoordinates.\\npyplot.scatter(result[:, 0], result[:, 1])\\nListing 12.12: Example of plotting a scatter plot of the PCA vectors.\\nWe can go one step further and annotate the points on the graph with the words themselves.\\nA crude version without any nice oﬀsets looks as follows.\\nwords = list(model.wv.vocab)\\nfor i, word in enumerate(words):\\npyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\\nListing 12.13: Example of plotting words on the scatter plot.\\nPutting this all together with the model from the previous section, the complete example is\\nlisted below.\\nfrom gensim.models import Word2Vec\\nfrom sklearn.decomposition import PCA\\nfrom matplotlib import pyplot\\n# define training data\\nsentences = [[ 'this ', 'is ', 'the ', 'first ', 'sentence ', 'for ', 'word2vec '],\\n[ 'this ', 'is ', 'the ', 'second ', 'sentence '],\\n[ 'yet ', 'another ', 'sentence '],\\n[ 'one ', 'more ', 'sentence '],\\n[ 'and ', 'the ', 'final ', 'sentence ']]\\n# train model\\nmodel = Word2Vec(sentences, min_count=1)\\n# fit a 2d PCA model to the vectors\\nX = model[model.wv.vocab]\\npca = PCA(n_components=2)\\nresult = pca.fit_transform(X)\\n# create a scatter plot of the projection\\npyplot.scatter(result[:, 0], result[:, 1])\\nwords = list(model.wv.vocab)\\nfor i, word in enumerate(words):\\npyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\\npyplot.show()\\nListing 12.14: Example demonstrating how to plot word vectors.\\nRunning the example creates a scatter plot with the dots annotated with the words. It is\\nhard to pull much meaning out of the graph given such a tiny corpus was used to ﬁt the model.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 144}, page_content='12.6. Load Google’s Word2Vec Embedding 128\\nFigure 12.1: Scatter Plot of PCA Projection of Word2Vec Model\\n12.6 Load Google’s Word2Vec Embedding\\nTraining your own word vectors may be the best approach for a given NLP problem. But it\\ncan take a long time, a fast computer with a lot of RAM and disk space, and perhaps some\\nexpertise in ﬁnessing the input data and training algorithm. An alternative is to simply use an\\nexisting pre-trained word embedding. Along with the paper and code for Word2Vec, Google\\nalso published a pre-trained Word2Vec model on the Word2Vec Google Code Project.\\nA pre-trained model is nothing more than a ﬁle containing tokens and their associated word\\nvectors. The pre-trained Google Word2Vec model was trained on Google news data (about 100\\nbillion words); it contains 3 million words and phrases and was ﬁt using 300-dimensional word\\nvectors. It is a 1.53 Gigabyte ﬁle. You can download it from here:\\n\\x88GoogleNews-vectors-negative300.bin.gz .\\nhttps://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\\nUnzipped, the binary ﬁle ( GoogleNews-vectors-negative300.bin ) is 3.4 Gigabytes. The\\nGensim library provides tools to load this ﬁle. Speciﬁcally, you can call the\\nKeyedVectors.load word2vec format() function to load this model into memory, for example:\\nfrom gensim.models import KeyedVectors'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 145}, page_content=\"12.7. Load Stanford’s GloVe Embedding 129\\nfilename = 'GoogleNews-vectors-negative300.bin '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)\\nListing 12.15: Example of loading the Google word vectors in Gensim.\\nNote, this example may require a workstation with 8 or more Gigabytes of RAM to execute.\\nOn my modern workstation, it takes about 43 seconds to load. Another interesting thing that\\nyou can do is do a little linear algebra arithmetic with words. For example, a popular example\\ndescribed in lectures and introduction papers is:\\nqueen = (king - man) + woman\\nListing 12.16: Example of arithmetic of word vectors.\\nThat is the word queen is the closest word given the subtraction of the notion of man from\\nking and adding the word woman. The man-ness in king is replaced with woman-ness to give\\nus queen. A very cool concept. Gensim provides an interface for performing these types of\\noperations in the most similar() function on the trained or loaded model. For example:\\nresult = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.17: Example of arithmetic of word vectors in Gensim.\\nWe can put all of this together as follows.\\nfrom gensim.models import KeyedVectors\\n# load the google word2vec model\\nfilename = 'GoogleNews-vectors-negative300.bin '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)\\n# calculate: (king - man) + woman = ?\\nresult = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.18: Example demonstrating arithmetic with Google word vectors.\\nRunning the example loads the Google pre-trained Word2Vec model and then calculates the\\n(king - man) + woman = ? operation on the word vectors for those words. The answer, as we\\nwould expect, is queen .\\n[( 'queen ', 0.7118192315101624)]\\nListing 12.19: Output of arithmetic with Google word vectors.\\nSee some of the articles in the further reading section for more interesting arithmetic examples\\nthat you can explore.\\n12.7 Load Stanford’s GloVe Embedding\\nStanford researchers also have their own word embedding algorithm like Word2Vec called Global\\nVectors for Word Representation, or GloVe for short. I won’t get into the details of the diﬀerences\\nbetween Word2Vec and GloVe here, but generally, NLP practitioners seem to prefer GloVe at\\nthe moment based on results.\\nLike Word2Vec, the GloVe researchers also provide pre-trained word vectors, in this case, a\\ngreat selection to choose from. You can download the GloVe pre-trained word vectors and load\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 146}, page_content=\"12.7. Load Stanford’s GloVe Embedding 130\\nthem easily with Gensim. The ﬁrst step is to convert the GloVe ﬁle format to the Word2Vec ﬁle\\nformat. The only diﬀerence is the addition of a small header line. This can be done by calling\\ntheglove2word2vec() function. For example (note, this example is just a demonstration with\\na mock input ﬁlename):\\nfrom gensim.scripts.glove2word2vec import glove2word2vec\\nglove_input_file = 'glove.txt '\\nword2vec_output_file = 'word2vec.txt '\\nglove2word2vec(glove_input_file, word2vec_output_file)\\nListing 12.20: Example of converting a ﬁle from GloVe to Word2Vec format.\\nOnce converted, the ﬁle can be loaded just like Word2Vec ﬁle above. Let’s make this concrete\\nwith an example. You can download the smallest GloVe pre-trained model from the GloVe\\nwebsite. It an 822 Megabyte zip ﬁle with 4 diﬀerent models (50, 100, 200 and 300-dimensional\\nvectors) trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary. The\\ndirect download link is here:\\n\\x88glove.6B.zip .\\nhttp://nlp.stanford.edu/data/glove.6B.zip\\nWorking with the 100-dimensional version of the model, we can convert the ﬁle to Word2Vec\\nformat as follows:\\nfrom gensim.scripts.glove2word2vec import glove2word2vec\\nglove_input_file = 'glove.6B.100d.txt '\\nword2vec_output_file = 'glove.6B.100d.txt.word2vec '\\nglove2word2vec(glove_input_file, word2vec_output_file)\\nListing 12.21: Example of converting a speciﬁc GloVe ﬁle to Word2Vec format.\\nYou now have a copy of the GloVe model in Word2Vec format with the ﬁlename\\nglove.6B.100d.txt.word2vec . Now we can load it and perform the same (king - man) +\\nwoman = ? test as in the previous section. The complete code listing is provided below. Note\\nthat the converted ﬁle is ASCII format, not binary, so we set binary=False when loading.\\nfrom gensim.models import KeyedVectors\\n# load the Stanford GloVe model\\nfilename = 'glove.6B.100d.txt.word2vec '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=False)\\n# calculate: (king - man) + woman = ?\\nresult = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.22: Example of arithmetic with converted GloVe word vectors.\\nPulling all of this together, the complete example is listed below.\\nfrom gensim.models import KeyedVectors\\nfrom gensim.scripts.glove2word2vec import glove2word2vec\\n# convert glove to word2vec format\\nglove_input_file = 'glove.6B.100d.txt '\\nword2vec_output_file = 'glove.6B.100d.txt.word2vec '\\nglove2word2vec(glove_input_file, word2vec_output_file)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 147}, page_content=\"12.8. Further Reading 131\\n# load the converted model\\nfilename = 'glove.6B.100d.txt.word2vec '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=False)\\n# calculate: (king - man) + woman = ?\\nresult = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.23: Example demonstrating how to load and use GloVe word embeddings.\\nRunning the example prints the same result of queen .\\n[( 'queen ', 0.7698540687561035)]\\nListing 12.24: Example output of arithmetic with converted GloVe word vectors.\\n12.8 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n12.8.1 Word Embeddings\\n\\x88Word Embedding on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word2vec\\n\\x88Word2Vec on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word2vec\\n\\x88Google Word2Vec project.\\nhttps://code.google.com/archive/p/word2vec/\\n\\x88Stanford GloVe project.\\nhttps://nlp.stanford.edu/projects/glove/\\n12.8.2 Gensim\\n\\x88Gensim Python Library.\\nhttps://radimrehurek.com/gensim/index.html\\n\\x88Gensim Installation Instructions.\\nhttps://radimrehurek.com/gensim/install.html\\n\\x88models.word2vec Gensim API.\\nhttps://radimrehurek.com/gensim/models/keyedvectors.html\\n\\x88models.keyedvectors Gensim API.\\nhttps://radimrehurek.com/gensim/models/keyedvectors.html\\n\\x88scripts.glove2word2vec Gensim API.\\nhttps://radimrehurek.com/gensim/scripts/glove2word2vec.html\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 148}, page_content='12.9. Summary 132\\n12.8.3 Articles\\n\\x88Messing Around With Word2Vec , 2016.\\nhttps://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/\\n\\x88Vector Space Models for the Digital Humanities , 2015.\\nhttp://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html\\n\\x88Gensim Word2Vec Tutorial , 2014.\\nhttps://rare-technologies.com/word2vec-tutorial/\\n12.9 Summary\\nIn this tutorial, you discovered how to develop and load word embedding layers in Python using\\nGensim. Speciﬁcally, you learned:\\n\\x88How to train your own Word2Vec word embedding model on text data.\\n\\x88How to visualize a trained word embedding model using Principal Component Analysis.\\n\\x88How to load pre-trained Word2Vec and GloVe word embedding models from Google and\\nStanford.\\n12.9.1 Next\\nIn the next chapter, you will discover how you can develop a neural network model with a\\nlearned word embedding.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 149}, page_content='Chapter 13\\nHow to Learn and Load Word\\nEmbeddings in Keras\\nWord embeddings provide a dense representation of words and their relative meanings. They are\\nan improvement over sparse representations used in simpler bag of word model representations.\\nWord embeddings can be learned from text data and reused among projects. They can also be\\nlearned as part of ﬁtting a neural network on text data. In this tutorial, you will discover how\\nto use word embeddings for deep learning in Python with Keras. After completing this tutorial,\\nyou will know:\\n\\x88About word embeddings and that Keras supports word embeddings via the Embedding\\nlayer.\\n\\x88How to learn a word embedding while ﬁtting a neural network.\\n\\x88How to use a pre-trained word embedding in a neural network.\\nLet’s get started.\\n13.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Word Embedding\\n2. Keras Embedding Layer\\n3. Example of Learning an Embedding\\n4. Example of Using Pre-Trained GloVe Embedding\\n5. Tips for Cleaning Text for Word Embedding\\n133'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 150}, page_content='13.2. Word Embedding 134\\n13.2 Word Embedding\\nA word embedding is a class of approaches for representing words and documents using a\\ndense vector representation. It is an improvement over more the traditional bag-of-word model\\nencoding schemes where large sparse vectors were used to represent each word or to score each\\nword within a vector to represent an entire vocabulary. These representations were sparse\\nbecause the vocabularies were vast and a given word or document would be represented by a\\nlarge vector comprised mostly of zero values.\\nInstead, in an embedding, words are represented by dense vectors where a vector represents\\nthe projection of the word into a continuous vector space. The position of a word within the\\nvector space is learned from text and is based on the words that surround the word when it is\\nused. The position of a word in the learned vector space is referred to as its embedding. Two\\npopular examples of methods of learning word embeddings from text include:\\n\\x88Word2Vec.\\n\\x88GloVe.\\nIn addition to these carefully designed methods, a word embedding can be learned as part\\nof a deep learning model. This can be a slower approach, but tailors the model to a speciﬁc\\ntraining dataset.\\n13.3 Keras Embedding Layer\\nKeras oﬀers an Embedding layer that can be used for neural networks on text data. It requires\\nthat the input data be integer encoded, so that each word is represented by a unique integer.\\nThis data preparation step can be performed using the Tokenizer API also provided with\\nKeras.\\nThe Embedding layer is initialized with random weights and will learn an embedding for all\\nof the words in the training dataset. It is a ﬂexible layer that can be used in a variety of ways,\\nsuch as:\\n\\x88It can be used alone to learn a word embedding that can be saved and used in another\\nmodel later.\\n\\x88It can be used as part of a deep learning model where the embedding is learned along\\nwith the model itself.\\n\\x88It can be used to load a pre-trained word embedding model, a type of transfer learning.\\nThe Embedding layer is deﬁned as the ﬁrst hidden layer of a network. It must specify 3\\narguments:\\n\\x88input dim: This is the size of the vocabulary in the text data. For example, if your data\\nis integer encoded to values between 0-10, then the size of the vocabulary would be 11\\nwords.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 151}, page_content=\"13.4. Example of Learning an Embedding 135\\n\\x88output dim: This is the size of the vector space in which words will be embedded. It\\ndeﬁnes the size of the output vectors from this layer for each word. For example, it could\\nbe 32 or 100 or even larger. Test diﬀerent values for your problem.\\n\\x88input length : This is the length of input sequences, as you would deﬁne for any input\\nlayer of a Keras model. For example, if all of your input documents are comprised of 1000\\nwords, this would be 1000.\\nFor example, below we deﬁne an Embedding layer with a vocabulary of 200 (e.g. integer\\nencoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be\\nembedded, and input documents that have 50 words each.\\ne = Embedding(200, 32, input_length=50)\\nListing 13.1: Example of creating a word embedding layer.\\nThe Embedding layer has weights that are learned. If you save your model to ﬁle, this will\\ninclude weights for the Embedding layer. The output of the Embedding layer is a 2D vector with\\none embedding for each word in the input sequence of words (input document). If you wish\\nto connect a Dense layer directly to an Embedding layer, you must ﬁrst ﬂatten the 2D output\\nmatrix to a 1D vector using the Flatten layer. Now, let’s see how we can use an Embedding\\nlayer in practice.\\n13.4 Example of Learning an Embedding\\nIn this section, we will look at how we can learn a word embedding while ﬁtting a neural\\nnetwork on a text classiﬁcation problem. We will deﬁne a small problem where we have 10\\ntext documents, each with a comment about a piece of work a student submitted. Each text\\ndocument is classiﬁed as positive 1or negative 0. This is a simple sentiment analysis problem.\\nFirst, we will deﬁne the documents and their class labels.\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\nListing 13.2: Example of a small contrived classiﬁcation problem.\\nNext, we can integer encode each document. This means that as input the Embedding layer\\nwill have sequences of integers. We could experiment with other more sophisticated bag of word\\nmodel encoding like counts or TF-IDF. Keras provides the onehot() function that creates a\\nhash of each word as an eﬃcient integer encoding. We will estimate the vocabulary size of 50,\\nwhich is much larger than needed to reduce the probability of collisions from the hash function.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 152}, page_content=\"13.4. Example of Learning an Embedding 136\\n# integer encode the documents\\nvocab_size = 50\\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\\nprint(encoded_docs)\\nListing 13.3: Integer encode the text.\\nThe sequences have diﬀerent lengths and Keras prefers inputs to be vectorized and all inputs\\nto have the same length. We will pad all input sequences to have the length of 4. Again, we can\\ndo this with a built in Keras function, in this case the padsequences() function.\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\nListing 13.4: Pad the encoded text.\\nWe are now ready to deﬁne our Embedding layer as part of our neural network model.\\nThe Embedding layer has a vocabulary of 50 and an input length of 4. We will choose a\\nsmall embedding space of 8 dimensions. The model is a simple binary classiﬁcation model.\\nImportantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one\\nfor each word. We ﬂatten this to a one 32-element vector to pass on to the Dense output layer.\\n# define the model\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile the model\\nmodel.compile(optimizer= 'adam ', loss= 'binary_crossentropy ', metrics=[ 'acc '])\\n# summarize the model\\nmodel.summary()\\nListing 13.5: Deﬁne a simple model with an Embedding input.\\nFinally, we can ﬁt and evaluate the classiﬁcation model.\\n# fit the model\\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\\n# evaluate the model\\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\\nprint( 'Accuracy: %f '% (accuracy*100))\\nListing 13.6: Fit the deﬁned model and print model accuracy.\\nThe complete code listing is provided below.\\nfrom keras.preprocessing.text import one_hot\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers.embeddings import Embedding\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 153}, page_content=\"13.4. Example of Learning an Embedding 137\\n'Great effort ',\\n'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\n# integer encode the documents\\nvocab_size = 50\\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\\nprint(encoded_docs)\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\n# define the model\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile the model\\nmodel.compile(optimizer= 'adam ', loss= 'binary_crossentropy ', metrics=[ 'acc '])\\n# summarize the model\\nmodel.summary()\\n# fit the model\\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\\n# evaluate the model\\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\\nprint( 'Accuracy: %f '% (accuracy*100))\\nListing 13.7: Example of ﬁtting and evaluating a Keras model with an Embedding input layer.\\nRunning the example ﬁrst prints the integer encoded documents.\\n[[6, 16], [42, 24], [2, 17], [42, 24], [18], [17], [22, 17], [27, 42], [22, 24], [49, 46,\\n16, 34]]\\nListing 13.8: Example output of the encoded documents.\\nThen the padded versions of each document are printed, making them all uniform length.\\n[[ 6 16 0 0]\\n[42 24 0 0]\\n[ 2 17 0 0]\\n[42 24 0 0]\\n[18 0 0 0]\\n[17 0 0 0]\\n[22 17 0 0]\\n[27 42 0 0]\\n[22 24 0 0]\\n[49 46 16 34]]\\nListing 13.9: Example output of the padded encoded documents.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 154}, page_content='13.5. Example of Using Pre-Trained GloVe Embedding 138\\nAfter the network is deﬁned, a summary of the layers is printed. We can see that as expected,\\nthe output of the Embedding layer is a 4 x 8 matrix and this is squashed to a 32-element vector\\nby the Flatten layer.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 4, 8) 400\\n_________________________________________________________________\\nflatten_1 (Flatten) (None, 32) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 1) 33\\n=================================================================\\nTotal params: 433\\nTrainable params: 433\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 13.10: Example output of the model summary.\\nFinally, the accuracy of the trained model is printed, showing that it learned the training\\ndataset perfectly (which is not surprising).\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nAccuracy: 100.000000\\nListing 13.11: Example output of the model accuracy.\\nYou could save the learned weights from the Embedding layer to ﬁle for later use in other\\nmodels. You could also use this model generally to classify other documents that have the\\nsame kind vocabulary seen in the test dataset. Next, let’s look at loading a pre-trained word\\nembedding in Keras.\\n13.5 Example of Using Pre-Trained GloVe Embedding\\nThe Keras Embedding layer can also use a word embedding learned elsewhere. It is common\\nin the ﬁeld of Natural Language Processing to learn, save, and make freely available word\\nembeddings. For example, the researchers behind GloVe method provide a suite of pre-trained\\nword embeddings on their website released under a public domain license.\\nThe smallest package of embeddings is 822 Megabytes, called glove.6B.zip . It was trained\\non a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. There\\nare a few diﬀerent embedding vector sizes, including 50, 100, 200 and 300 dimensions. You\\ncan download this collection of embeddings and we can seed the Keras Embedding layer with\\nweights from the pre-trained embedding for the words in your training dataset.\\nThis example is inspired by an example in the Keras project: pretrained word embeddings.py .\\nAfter downloading and unzipping, you will see a few ﬁles, one of which is glove.6B.100d.txt ,\\nwhich contains a 100-dimensional version of the embedding. If you peek inside the ﬁle, you will\\nsee a token (word) followed by the weights (100 numbers) on each line. For example, below are\\nthe ﬁrst line of the embedding ASCII text ﬁle showing the embedding for the.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 155}, page_content=\"13.5. Example of Using Pre-Trained GloVe Embedding 139\\nthe -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459\\n0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336\\n0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107\\n-0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378\\n-0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857\\n-0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201\\n-0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044\\n0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624\\n0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217\\n0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\\nListing 13.12: Example GloVe word vector for the word ’the’.\\nAs in the previous section, the ﬁrst step is to deﬁne the examples, encode them as integers,\\nthen pad the sequences to be the same length. In this case, we need to be able to map words to\\nintegers as well as integers to words. Keras provides a Tokenizer class that can be ﬁt on the\\ntraining data, can convert text to sequences consistently by calling the texts tosequences()\\nmethod on the Tokenizer class, and provides access to the dictionary mapping of words to\\nintegers in a word index attribute.\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\n# prepare tokenizer\\nt = Tokenizer()\\nt.fit_on_texts(docs)\\nvocab_size = len(t.word_index) + 1\\n# integer encode the documents\\nencoded_docs = t.texts_to_sequences(docs)\\nprint(encoded_docs)\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\nListing 13.13: Deﬁne encode and pad sample documents.\\nNext, we need to load the entire GloVe word embedding ﬁle into memory as a dictionary of\\nword to embedding array.\\n# load the whole embedding into memory\\nembeddings_index = dict()\\nf = open( 'glove.6B.100d.txt ')\\nfor line in f:\\nvalues = line.split()\\nword = values[0]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 156}, page_content=\"13.5. Example of Using Pre-Trained GloVe Embedding 140\\ncoefs = asarray(values[1:], dtype= 'float32 ')\\nembeddings_index[word] = coefs\\nf.close()\\nprint( 'Loaded %s word vectors. '% len(embeddings_index))\\nListing 13.14: Load the GloVe word embedding into memory.\\nThis is pretty slow. It might be better to ﬁlter the embedding for the unique words in your\\ntraining data. Next, we need to create a matrix of one embedding for each word in the training\\ndataset. We can do that by enumerating all unique words in the Tokenizer.word index and\\nlocating the embedding weight vector from the loaded GloVe embedding. The result is a matrix\\nof weights only for words we will see during training.\\n# create a weight matrix for words in training docs\\nembedding_matrix = zeros((vocab_size, 100))\\nfor word, i in t.word_index.items():\\nembedding_vector = embeddings_index.get(word)\\nif embedding_vector is not None:\\nembedding_matrix[i] = embedding_vector\\nListing 13.15: Covert the word embedding into a weight matrix.\\nNow we can deﬁne our model, ﬁt, and evaluate it as before. The key diﬀerence is that\\ntheEmbedding layer can be seeded with the GloVe word embedding weights. We chose the\\n100-dimensional version, therefore the Embedding layer must be deﬁned with output dimset to\\n100. Finally, we do not want to update the learned word weights in this model, therefore we\\nwill set the trainable attribute for the model to be False .\\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\\nListing 13.16: Create an Embedding layer with the pre-loaded weights.\\nThe complete worked example is listed below.\\nfrom numpy import asarray\\nfrom numpy import zeros\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import Embedding\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\n# prepare tokenizer\\nt = Tokenizer()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 157}, page_content=\"13.5. Example of Using Pre-Trained GloVe Embedding 141\\nt.fit_on_texts(docs)\\nvocab_size = len(t.word_index) + 1\\n# integer encode the documents\\nencoded_docs = t.texts_to_sequences(docs)\\nprint(encoded_docs)\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\n# load the whole embedding into memory\\nembeddings_index = dict()\\nf = open( 'glove.6B.100d.txt ', mode= 'rt ', encoding= 'utf-8 ')\\nfor line in f:\\nvalues = line.split()\\nword = values[0]\\ncoefs = asarray(values[1:], dtype= 'float32 ')\\nembeddings_index[word] = coefs\\nf.close()\\nprint( 'Loaded %s word vectors. '% len(embeddings_index))\\n# create a weight matrix for words in training docs\\nembedding_matrix = zeros((vocab_size, 100))\\nfor word, i in t.word_index.items():\\nembedding_vector = embeddings_index.get(word)\\nif embedding_vector is not None:\\nembedding_matrix[i] = embedding_vector\\n# define model\\nmodel = Sequential()\\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\\nmodel.add(e)\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile the model\\nmodel.compile(optimizer= 'adam ', loss= 'binary_crossentropy ', metrics=[ 'acc '])\\n# summarize the model\\nmodel.summary()\\n# fit the model\\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\\n# evaluate the model\\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\\nprint( 'Accuracy: %f '% (accuracy*100))\\nListing 13.17: Example loading pre-trained GloVe weights into an Embedding input layer.\\nRunning the example may take a bit longer, but then demonstrates that it is just as capable\\nof ﬁtting this simple problem.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n...\\nAccuracy: 100.000000\\nListing 13.18: Example output of loading pre-trained GloVe weights into an Embedding input\\nlayer.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 158}, page_content='13.6. Tips for Cleaning Text for Word Embedding 142\\nIn practice, I would encourage you to experiment with learning a word embedding using\\na pre-trained embedding that is ﬁxed and trying to perform learning on top of a pre-trained\\nembedding. See what works best for your speciﬁc problem.\\n13.6 Tips for Cleaning Text for Word Embedding\\nRecently, the ﬁeld of natural language processing has been moving away from bag-of-word\\nmodels and word encoding toward word embeddings. The beneﬁt of word embeddings is that\\nthey encode each word into a dense vector that captures something about its relative meaning\\nwithin the training text. This means that variations of words like case, spelling, punctuation,\\nand so on will automatically be learned to be similar in the embedding space. In turn, this\\ncan mean that the amount of cleaning required from your text may be less and perhaps quite\\ndiﬀerent to classical text cleaning. For example, it may no-longer make sense to stem words or\\nremove punctuation for contractions.\\nTomas Mikolov is one of the developers of Word2Vec, a popular word embedding method.\\nHe suggests only very minimal text cleaning is required when learning a word embedding model.\\nBelow is his response when pressed with the question about how to best prepare text data for\\nWord2Vec.\\nThere is no universal answer. It all depends on what you plan to use the vectors\\nfor. In my experience, it is usually good to disconnect (or remove) punctuation from\\nwords, and sometimes also convert all characters to lowercase. One can also replace\\nall numbers (possibly greater than some constant) with some single token such as .\\nAll these pre-processing steps aim to reduce the vocabulary size without removing\\nany important content (which in some cases may not be true when you lowercase\\ncertain words, ie. ‘Bush’ is diﬀerent than ‘bush’, while ‘Another’ has usually the\\nsame sense as ‘another’). The smaller the vocabulary is, the lower is the memory\\ncomplexity, and the more robustly are the parameters for the words estimated. You\\nalso have to pre-process the test data in the same way.\\n[...]\\nIn short, you will understand all this much better if you will run experiments.\\n— Tomas Mikolov, word2vec-toolkit: google groups thread. , 2015.\\nhttps://goo.gl/KtDGst\\n13.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Word Embedding on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word_embedding\\n\\x88Keras Embedding Layer API.\\nhttps://keras.io/layers/embeddings/#embedding'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 159}, page_content='13.8. Summary 143\\n\\x88Using pre-trained word embeddings in a Keras model , 2016.\\nhttps://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\\n\\x88Example of using a pre-trained GloVe Embedding in Keras.\\nhttps://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.\\npy\\n\\x88GloVe Embedding.\\nhttps://nlp.stanford.edu/projects/glove/\\n\\x88An overview of word embeddings and their connection to distributional semantic models ,\\n2016.\\nhttp://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/\\n\\x88Deep Learning, NLP, and Representations , 2014.\\nhttp://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\\n13.8 Summary\\nIn this tutorial, you discovered how to use word embeddings for deep learning in Python with\\nKeras. Speciﬁcally, you learned:\\n\\x88About word embeddings and that Keras supports word embeddings via the Embedding\\nlayer.\\n\\x88How to learn a word embedding while ﬁtting a neural network.\\n\\x88How to use a pre-trained word embedding in a neural network.\\n13.8.1 Next\\nThis is the end of the part on word embeddings. In the next part you will discover neural text\\nclassiﬁcation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 160}, page_content='Part VI\\nText Classiﬁcation\\n144'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 161}, page_content='Chapter 14\\nNeural Models for Document\\nClassiﬁcation\\nText classiﬁcation describes a general class of problems such as predicting the sentiment of\\ntweets and movie reviews, as well as classifying email as spam or not. Deep learning methods are\\nproving very good at text classiﬁcation, achieving state-of-the-art results on a suite of standard\\nacademic benchmark problems. In this chapter, you will discover some best practices to consider\\nwhen developing deep learning models for text classiﬁcation. After reading this chapter, you\\nwill know:\\n\\x88The general combination of deep learning methods to consider when starting your text\\nclassiﬁcation problems.\\n\\x88The ﬁrst architecture to try with speciﬁc advice on how to conﬁgure hyperparameters.\\n\\x88That deeper networks may be the future of the ﬁeld in terms of ﬂexibility and capability.\\nLet’s get started.\\n14.1 Overview\\nThis tutorial is divided into the following parts:\\n1. Word Embeddings + CNN = Text Classiﬁcation\\n2. Use a Single Layer CNN Architecture\\n3. Dial in CNN Hyperparameters\\n4. Consider Character-Level CNNs\\n5. Consider Deeper CNNs for Classiﬁcation\\n145'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 162}, page_content='14.2. Word Embeddings + CNN = Text Classiﬁcation 146\\n14.2 Word Embeddings + CNN = Text Classiﬁcation\\nThe modus operandi for text classiﬁcation involves the use of a word embedding for representing\\nwords and a Convolutional Neural Network (CNN) for learning how to discriminate documents\\non classiﬁcation problems. Yoav Goldberg, in his primer on deep learning for natural language\\nprocessing, comments that neural networks in general oﬀer better performance than classical\\nlinear classiﬁers, especially when used with pre-trained word embeddings.\\nThe non-linearity of the network, as well as the ability to easily integrate pre-trained\\nword embeddings, often lead to superior classiﬁcation accuracy.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\nHe also comments that convolutional neural networks are eﬀective at document classiﬁcation,\\nnamely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in\\na way that is invariant to their position within the input sequences.\\nNetworks with convolutional and pooling layers are useful for classiﬁcation tasks in\\nwhich we expect to ﬁnd strong local clues regarding class membership, but these\\nclues can appear in diﬀerent places in the input. [...] We would like to learn that\\ncertain sequences of words are good indicators of the topic, and do not necessarily\\ncare where they appear in the document. Convolutional and pooling layers allow\\nthe model to learn to ﬁnd such local indicators, regardless of their position.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\nThe architecture is therefore comprised of three key pieces:\\n\\x88Word Embedding : A distributed representation of words where diﬀerent words that\\nhave a similar meaning (based on their usage) also have a similar representation.\\n\\x88Convolutional Model : A feature extraction model that learns to extract salient features\\nfrom documents represented using a word embedding.\\n\\x88Fully Connected Model : The interpretation of extracted features in terms of a predictive\\noutput.\\nYoav Goldberg highlights the CNNs role as a feature extractor model in his book:\\n... the CNN is in essence a feature-extracting architecture. It does not constitute a\\nstandalone, useful network on its own, but rather is meant to be integrated into a\\nlarger network, and to be trained to work in tandem with it in order to produce an\\nend result. The CNNs layer’s responsibility is to extract meaningful sub-structures\\nthat are useful for the overall prediction task at hand.\\n— Page 152, Neural Network Methods for Natural Language Processing , 2017.\\nThe tying together of these three elements is demonstrated in perhaps one of the most widely\\ncited examples of the combination, described in the next section.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 163}, page_content='14.3. Use a Single Layer CNN Architecture 147\\n14.3 Use a Single Layer CNN Architecture\\nYou can get good results for document classiﬁcation with a single layer CNN, perhaps with\\ndiﬀerently sized kernels across the ﬁlters to allow grouping of word representations at diﬀerent\\nscales. Yoon Kim in his study of the use of pre-trained word vectors for classiﬁcation tasks with\\nConvolutional Neural Networks found that using pre-trained static word vectors does very well.\\nHe suggests that pre-trained word embeddings that were trained on very large text corpora,\\nsuch as the freely available Word2Vec vectors trained on 100 billion tokens from Google news\\nmay oﬀer good universal features for use in natural language processing.\\nDespite little tuning of hyperparameters, a simple CNN with one layer of convolution\\nperforms remarkably well. Our results add to the well-established evidence that\\nunsupervised pre-training of word vectors is an important ingredient in deep learning\\nfor NLP\\n—Convolutional Neural Networks for Sentence Classiﬁcation , 2014.\\nHe also discovered that further task-speciﬁc tuning of the word vectors oﬀer a small additional\\nimprovement in performance. Kim describes the general approach of using CNN for natural\\nlanguage processing. Sentences are mapped to embedding vectors and are available as a matrix\\ninput to the model. Convolutions are performed across the input word-wise using diﬀerently\\nsized kernels, such as 2 or 3 words at a time. The resulting feature maps are then processed\\nusing a max pooling layer to condense or summarize the extracted features.\\nThe architecture is based on the approach used by Ronan Collobert, et al. in their paper\\nNatural Language Processing (almost) from Scratch , 2011. In it, they develop a single end-to-end\\nneural network model with convolutional and pooling layers for use across a range of fundamental\\nnatural language processing problems. Kim provides a diagram that helps to see the sampling\\nof the ﬁlters using diﬀerently sized kernels as diﬀerent colors (red and yellow).\\nFigure 14.1: An example of a CNN Filter and Polling Architecture for Natural Language\\nProcessing. Taken from Convolutional Neural Networks for Sentence Classiﬁcation .\\nUsefully, he reports his chosen model conﬁguration, discovered via grid search and used\\nacross a suite of 7 text classiﬁcation tasks, summarized as follows:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 164}, page_content='14.4. Dial in CNN Hyperparameters 148\\n\\x88Transfer function: rectiﬁed linear.\\n\\x88Kernel sizes: 2, 4, 5.\\n\\x88Number of ﬁlters: 100.\\n\\x88Dropout rate: 0.5.\\n\\x88Weight regularization (L2): 3.\\n\\x88Batch Size: 50.\\n\\x88Update Rule: Adadelta.\\nThese conﬁgurations could be used to inspire a starting point for your own experiments.\\n14.4 Dial in CNN Hyperparameters\\nSome hyperparameters matter more than others when tuning a convolutional neural network on\\nyour document classiﬁcation problem. Ye Zhang and Byron Wallace performed a sensitivity\\nanalysis into the hyperparameters needed to conﬁgure a single layer convolutional neural network\\nfor document classiﬁcation. The study is motivated by their claim that the models are sensitive\\nto their conﬁguration.\\nUnfortunately, a downside to CNN-based models - even simple ones - is that they\\nrequire practitioners to specify the exact model architecture to be used and to set\\nthe accompanying hyperparameters. To the uninitiated, making such decisions can\\nseem like something of a black art because there are many free parameters in the\\nmodel.\\n—A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for\\nSentence Classiﬁcation , 2015.\\nTheir aim was to provide general conﬁgurations that can be used for conﬁguring CNNs on\\nnew text classiﬁcation tasks. They provide a nice depiction of the model architecture and the\\ndecision points for conﬁguring the model, reproduced below.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 165}, page_content='14.4. Dial in CNN Hyperparameters 149\\nFigure 14.2: Convolutional Neural Network Architecture for Sentence Classiﬁcation. Taken\\nfrom A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for\\nSentence Classiﬁcation .\\nThe study makes a number of useful ﬁndings that could be used as a starting point for\\nconﬁguring shallow CNN models for text classiﬁcation. The general ﬁndings were as follows:\\n\\x88The choice of pre-trained Word2Vec and GloVe embeddings diﬀer from problem to problem,\\nand both performed better than using one hot encoded word vectors.\\n\\x88The size of the kernel is important and should be tuned for each problem.\\n\\x88The number of feature maps is also important and should be tuned.\\n\\x88The 1-max pooling generally outperformed other types of pooling.\\n\\x88Dropout has little eﬀect on the model performance.\\nThey go on to provide more speciﬁc heuristics, as follows:\\n\\x88Use Word2Vec or GloVe word embeddings as a starting point and tune them while ﬁtting\\nthe model.\\n\\x88Grid search across diﬀerent kernel sizes to ﬁnd the optimal conﬁguration for your problem,\\nin the range 1-10.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 166}, page_content='14.5. Consider Character-Level CNNs 150\\n\\x88Search the number of ﬁlters from 100-600 and explore a dropout of 0.0-0.5 as part of the\\nsame search.\\n\\x88Explore using tanh, relu, and linear activation functions.\\nThe key caveat is that the ﬁndings are based on empirical results on binary text classiﬁcation\\nproblems using single sentences as input.\\n14.5 Consider Character-Level CNNs\\nText documents can be modeled at the character level using convolutional neural networks\\nthat are capable of learning the relevant hierarchical structure of words, sentences, paragraphs,\\nand more. Xiang Zhang, et al. use a character-based representation of text as input for a\\nconvolutional neural network. The promise of the approach is that all of the labor-intensive\\neﬀort required to clean and prepare text could be overcome if a CNN can learn to abstract the\\nsalient details.\\n... deep ConvNets do not require the knowledge of words, in addition to the conclusion\\nfrom previous research that ConvNets do not require the knowledge about the\\nsyntactic or semantic structure of a language. This simpliﬁcation of engineering could\\nbe crucial for a single system that can work for diﬀerent languages, since characters\\nalways constitute a necessary construct regardless of whether segmentation into\\nwords is possible. Working on only characters also has the advantage that abnormal\\ncharacter combinations such as misspellings and emoticons may be naturally learnt.\\n—Character-level Convolutional Networks for Text Classiﬁcation , 2015.\\nThe model reads in one hot encoded characters in a ﬁxed-sized alphabet. Encoded characters\\nare read in blocks or sequences of 1,024 characters. A stack of 6 convolutional layers with\\npooling follows, with 3 fully connected layers at the output end of the network in order to make\\na prediction.\\nFigure 14.3: Character-based Convolutional Neural Network for Text Classiﬁcation. Taken from\\nCharacter-level Convolutional Networks for Text Classiﬁcation .\\nThe model achieves some success, performing better on problems that oﬀer a larger corpus\\nof text.\\n... analysis shows that character-level ConvNet is an eﬀective method. [...] how well\\nour model performs in comparisons depends on many factors, such as dataset size,\\nwhether the texts are curated and choice of alphabet.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 167}, page_content='14.6. Consider Deeper CNNs for Classiﬁcation 151\\n—Character-level Convolutional Networks for Text Classiﬁcation , 2015.\\nResults using an extended version of this approach were pushed to the state-of-the-art in a\\nfollow-up paper covered in the next section.\\n14.6 Consider Deeper CNNs for Classiﬁcation\\nBetter performance can be achieved with very deep convolutional neural networks, although\\nstandard and reusable architectures have not been adopted for classiﬁcation tasks, yet. Alexis\\nConneau, et al. comment on the relatively shallow networks used for natural language processing\\nand the success of much deeper networks used for computer vision applications. For example,\\nKim (above) restricted the model to a single convolutional layer.\\nOther architectures used for natural language reviewed in the paper are limited to 5 and 6\\nlayers. These are contrasted with successful architectures used in computer vision with 19 or\\neven up to 152 layers. They suggest and demonstrate that there are beneﬁts for hierarchical\\nfeature learning with very deep convolutional neural network model, called VDCNN.\\n... we propose to use deep architectures of many convolutional layers to approach\\nthis goal, using up to 29 layers. The design of our architecture is inspired by recent\\nprogress in computer vision [...] The proposed deep convolutional network shows\\nsigniﬁcantly better results than previous ConvNets approach.\\n—Very Deep Convolutional Networks for Text Classiﬁcation , 2016.\\nKey to their approach is an embedding of individual characters, rather than a word embed-\\nding.\\nWe present a new architecture (VDCNN) for text processing which operates directly\\nat the character level and uses only small convolutions and pooling operations.\\n—Very Deep Convolutional Networks for Text Classiﬁcation , 2016.\\nResults on a suite of 8 large text classiﬁcation tasks show better performance than more\\nshallow networks. Speciﬁcally, state-of-the-art results on all but two of the datasets tested,\\nat the time of writing. Generally, they make some key ﬁndings from exploring the deeper\\narchitectural approach:\\n\\x88The very deep architecture worked well on small and large datasets.\\n\\x88Deeper networks decrease classiﬁcation error.\\n\\x88Max-pooling achieves better results than other, more sophisticated types of pooling.\\n\\x88Generally going deeper degrades accuracy; the shortcut connections used in the architecture\\nare important.\\n... this is the ﬁrst time that the “beneﬁt of depths” was shown for convolutional\\nneural networks in NLP.\\n—Very Deep Convolutional Networks for Text Classiﬁcation , 2016.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 168}, page_content='14.7. Further Reading 152\\n14.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88A Primer on Neural Network Models for Natural Language Processing , 2015.\\nhttps://arxiv.org/abs/1510.00726\\n\\x88Convolutional Neural Networks for Sentence Classiﬁcation , 2014.\\nhttps://arxiv.org/abs/1103.0398\\n\\x88Natural Language Processing (almost) from Scratch , 2011.\\nhttps://arxiv.org/abs/1103.0398\\n\\x88Very Deep Convolutional Networks for Text Classiﬁcation , 2016.\\nhttps://arxiv.org/abs/1606.01781\\n\\x88Character-level Convolutional Networks for Text Classiﬁcation , 2015.\\nhttps://arxiv.org/abs/1509.01626\\n\\x88A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks\\nfor Sentence Classiﬁcation , 2015.\\nhttps://arxiv.org/abs/1510.03820\\n14.8 Summary\\nIn this chapter, you discovered some best practices for developing deep learning models for\\ndocument classiﬁcation. Speciﬁcally, you learned:\\n\\x88That a key approach is to use word embeddings and convolutional neural networks for\\ntext classiﬁcation.\\n\\x88That a single layer model can do well on moderate-sized problems, and ideas on how to\\nconﬁgure it.\\n\\x88That deeper models that operate directly on text may be the future of natural language\\nprocessing.\\n14.8.1 Next\\nIn the next chapter, you will discover how you can develop a neural text classiﬁcation model\\nwith word embeddings and a convolutional neural network.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 169}, page_content='Chapter 15\\nProject: Develop an Embedding +\\nCNN Model for Sentiment Analysis\\nWord embeddings are a technique for representing text where diﬀerent words with similar\\nmeaning have a similar real-valued vector representation. They are a key breakthrough that has\\nled to great performance of neural network models on a suite of challenging natural language\\nprocessing problems. In this tutorial, you will discover how to develop word embedding models\\nwith convolutional neural networks to classify movie reviews. After completing this tutorial,\\nyou will know:\\n\\x88How to prepare movie review text data for classiﬁcation with deep learning methods.\\n\\x88How to develop a neural classiﬁcation model with word embedding and convolutional\\nlayers.\\n\\x88How to evaluate the developed a neural classiﬁcation model.\\nLet’s get started.\\n15.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Movie Review Dataset\\n2. Data Preparation\\n3. Train CNN With Embedding Layer\\n4. Evaluate Model\\n15.2 Movie Review Dataset\\nIn this tutorial, we will use the Movie Review Dataset. This dataset designed for sentiment\\nanalysis was described previously in Chapter 9. You can download the dataset from here:\\n153'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 170}, page_content='15.3. Data Preparation 154\\n\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention cv000 tocv999 for each of negand pos.\\n15.3 Data Preparation\\nNote : The preparation of the movie review dataset was ﬁrst described in Chapter 9. In this\\nsection, we will look at 3 things:\\n1. Separation of data into training and test sets.\\n2. Loading and cleaning the data to remove punctuation and numbers.\\n3. Deﬁning a vocabulary of preferred words.\\n15.3.1 Split into Train and Test Sets\\nWe are pretending that we are developing a system that can predict the sentiment of a textual\\nmovie review as either positive or negative. This means that after the model is developed, we\\nwill need to make predictions on new textual reviews. This will require all of the same data\\npreparation to be performed on those new reviews as is performed on the training data for the\\nmodel. We will ensure that this constraint is built into the evaluation of our models by splitting\\nthe training and test datasets prior to any data preparation. This means that any knowledge in\\nthe data in the test set that could help us better prepare the data (e.g. the words used) are\\nunavailable in the preparation of data used for training the model.\\nThat being said, we will use the last 100 positive reviews and the last 100 negative reviews\\nas a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a\\n90% train, 10% split of the data. The split can be imposed easily by using the ﬁlenames of the\\nreviews where reviews named 000 to 899 are for training data and reviews named 900 onwards\\nare for test.\\n15.3.2 Loading and Cleaning Reviews\\nThe text data is already pretty clean; not much preparation is required. Without getting bogged\\ndown too much in the details, we will prepare the data using the following way:\\n\\x88Split tokens on white space.\\n\\x88Remove all punctuation from words.\\n\\x88Remove all words that are not purely comprised of alphabetical characters.\\n\\x88Remove all words that are known stop words.\\n\\x88Remove all words that have a length ≤1 character.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 171}, page_content=\"15.3. Data Preparation 155\\nWe can put all of these steps into a function called clean doc() that takes as an argument\\nthe raw text loaded from a ﬁle and returns a list of cleaned tokens. We can also deﬁne a function\\nload doc() that loads a document from ﬁle ready for use with the clean doc() function. An\\nexample of cleaning the ﬁrst positive review is listed below.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 15.1: Example of cleaning a movie review.\\nRunning the example prints a long list of clean tokens. There are many more cleaning steps\\nwe may want to explore and I leave them as further exercises.\\n...\\n'creepy ', 'place ', 'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ',\\n'typically ', 'strong ', 'performance ', 'deftly ', 'handling ', 'british ', 'accent ',\\n'ians ', 'holm ', 'joe ', 'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ',\\n'supporting ', 'roles ', 'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ',\\n'opened ', 'mouth ', 'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ',\\n'half ', 'bad ', 'film ', 'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ',\\n'language ', 'drug ', 'content ']\\nListing 15.2: Example output of cleaning a movie review.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 172}, page_content=\"15.3. Data Preparation 156\\n15.3.3 Deﬁne a Vocabulary\\nIt is important to deﬁne a vocabulary of known words when using a text model. The more\\nwords, the larger the representation of documents, therefore it is important to constrain the\\nwords to only those believed to be predictive. This is diﬃcult to know beforehand and often it\\nis important to test diﬀerent hypotheses about how to construct a useful vocabulary. We have\\nalready seen how we can remove punctuation and numbers from the vocabulary in the previous\\nsection. We can repeat this for all documents and build a set of all known words.\\nWe can develop a vocabulary as a Counter , which is a dictionary mapping of words and\\ntheir count that allows us to easily update and query. Each document can be added to the\\ncounter (a new function called adddoctovocab() ) and we can step over all of the reviews in\\nthe negative directory and then the positive directory (a new function called process docs() ).\\nThe complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 173}, page_content=\"15.3. Data Preparation 157\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\\nListing 15.3: Example of selecting a vocabulary for the dataset.\\nRunning the example shows that we have a vocabulary of 44,276 words. We also can see\\na sample of the top 50 most used words in the movie reviews. Note that this vocabulary was\\nconstructed based on only those reviews in the training dataset.\\n44276\\n[( 'film ', 7983), ( 'one ', 4946), ( 'movie ', 4826), ( 'like ', 3201), ( 'even ', 2262), ( 'good ',\\n2080), ( 'time ', 2041), ( 'story ', 1907), ( 'films ', 1873), ( 'would ', 1844), ( 'much ',\\n1824), ( 'also ', 1757), ( 'characters ', 1735), ( 'get ', 1724), ( 'character ', 1703),\\n( 'two ', 1643), ( 'first ', 1588), ( 'see ', 1557), ( 'way ', 1515), ( 'well ', 1511), ( 'make ',\\n1418), ( 'really ', 1407), ( 'little ', 1351), ( 'life ', 1334), ( 'plot ', 1288), ( 'people ',\\n1269), ( 'could ', 1248), ( 'bad ', 1248), ( 'scene ', 1241), ( 'movies ', 1238), ( 'never ',\\n1201), ( 'best ', 1179), ( 'new ', 1140), ( 'scenes ', 1135), ( 'man ', 1131), ( 'many ', 1130),\\n( 'doesnt ', 1118), ( 'know ', 1092), ( 'dont ', 1086), ( 'hes ', 1024), ( 'great ', 1014),\\n( 'another ', 992), ( 'action ', 985), ( 'love ', 977), ( 'us ', 967), ( 'go ', 952),\\n( 'director ', 948), ( 'end ', 946), ( 'something ', 945), ( 'still ', 936)]\\nListing 15.4: Example output of selecting a vocabulary for the dataset.\\nWe can step through the vocabulary and remove all words that have a low occurrence, such\\nas only being used once or twice in all reviews. For example, the following snippet will retrieve\\nonly the tokens that appear 2 or more times in all reviews.\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\nListing 15.5: Example of ﬁltering the vocabulary by occurrence.\\nFinally, the vocabulary can be saved to a new ﬁle called vocab.txt that we can later load\\nand use to ﬁlter movie reviews prior to encoding them for modeling. We deﬁne a new function\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 174}, page_content=\"15.3. Data Preparation 158\\ncalled save list() that saves the vocabulary to ﬁle, with one word per line. For example:\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 15.6: Example of saving the ﬁltered vocabulary.\\nPulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 175}, page_content=\"15.3. Data Preparation 159\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 15.7: Example of ﬁltering the vocabulary for the dataset.\\nRunning the above example with this addition shows that the vocabulary size drops by a\\nlittle more than half its size, from 44,276 to 25,767 words.\\n25767\\nListing 15.8: Example output of ﬁltering the vocabulary by min occurrence.\\nRunning the min occurrence ﬁlter on the vocabulary and saving it to ﬁle, you should now\\nhave a new ﬁle called vocab.txt with only the words we are interested in. The order of words\\nin your ﬁle will diﬀer, but should look something like the following:\\naberdeen\\ndupe\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 176}, page_content=\"15.4. Train CNN With Embedding Layer 160\\nburt\\nlibido\\nhamlet\\narlene\\navailable\\ncorners\\nweb\\ncolumbia\\n...\\nListing 15.9: Sample of the vocabulary ﬁle vocab.txt .\\nWe are now ready to look at extracting features from the reviews ready for modeling.\\n15.4 Train CNN With Embedding Layer\\nIn this section, we will learn a word embedding while training a convolutional neural network on\\nthe classiﬁcation problem. A word embedding is a way of representing text where each word in\\nthe vocabulary is represented by a real valued vector in a high-dimensional space. The vectors\\nare learned in such a way that words that have similar meanings will have similar representation\\nin the vector space (close in the vector space). This is a more expressive representation for text\\nthan more classical methods like bag-of-words, where relationships between words or tokens are\\nignored, or forced in bigram and trigram approaches.\\nThe real valued vector representation for words can be learned while training the neural\\nnetwork. We can do this in the Keras deep learning library using the Embedding layer. The\\nﬁrst step is to load the vocabulary. We will use it to ﬁlter out words from movie reviews that\\nwe are not interested in. If you have worked through the previous section, you should have a\\nlocal ﬁle called vocab.txt with one word per line. We can load that ﬁle and build a vocabulary\\nas a set for checking the validity of tokens.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\nListing 15.10: Load vocabulary.\\nNext, we need to load all of the training data movie reviews. For that we can adapt the\\nprocess docs() from the previous section to load the documents, clean them, and return them\\nas a list of strings, with one document per string. We want each document to be a string for\\neasy encoding as a sequence of integers later. Cleaning the document involves splitting each\\nreview based on white space, removing punctuation, and then ﬁltering out all tokens not in the\\nvocabulary. The updated clean doc() function is listed below.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 177}, page_content=\"15.4. Train CNN With Embedding Layer 161\\n# turn a doc into clean tokens\\ndef clean_doc(doc, vocab):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# filter out tokens not in vocab\\ntokens = [w for w in tokens if w in vocab]\\ntokens = ' '.join(tokens)\\nreturn tokens\\nListing 15.11: Function to load and ﬁlter a loaded review.\\nThe updated process docs() can then call the clean doc() for each document in a given\\ndirectory.\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc, vocab)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\nListing 15.12: Example to clean all movie reviews.\\nWe can call the process docs function for both the negandposdirectories and combine\\nthe reviews into a single train or test dataset. We also can deﬁne the class labels for the dataset.\\nThe load clean dataset() function below will load all reviews and prepare class labels for the\\ntraining or test dataset.\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\\nreturn docs, labels\\nListing 15.13: Function to load and clean all train or test movie reviews.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 178}, page_content=\"15.4. Train CNN With Embedding Layer 162\\nThe next step is to encode each document as a sequence of integers. The Keras Embedding\\nlayer requires integer inputs where each integer maps to a single token that has a speciﬁc\\nreal-valued vector representation within the embedding. These vectors are random at the\\nbeginning of training, but during training become meaningful to the network. We can encode\\nthe training documents as sequences of integers using the Tokenizer class in the Keras API.\\nFirst, we must construct an instance of the class then train it on all documents in the training\\ndataset. In this case, it develops a vocabulary of all tokens in the training dataset and develops\\na consistent mapping from words in the vocabulary to unique integers. We could just as easily\\ndevelop this mapping ourselves using our vocabulary ﬁle. The create tokenizer() function\\nbelow will prepare a Tokenizer from the training data.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 15.14: Function to create a Tokenizer from training.\\nNow that the mapping of words to integers has been prepared, we can use it to encode the\\nreviews in the training dataset. We can do that by calling the texts tosequences() function\\non the Tokenizer . We also need to ensure that all documents have the same length. This is a\\nrequirement of Keras for eﬃcient computation. We could truncate reviews to the smallest size\\nor zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid. In this case,\\nwe will pad all reviews to the length of the longest review in the training dataset. First, we can\\nﬁnd the longest review using the max() function on the training dataset and take its length.\\nWe can then call the Keras function padsequences() to pad the sequences to the maximum\\nlength by adding 0 values on the end.\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\\nListing 15.15: Calculate the maximum movie review length.\\nWe can then use the maximum length as a parameter to a function to integer encode and\\npad the sequences.\\n# integer encode and pad documents\\ndef encode_docs(tokenizer, max_length, docs):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(docs)\\n# pad sequences\\npadded = pad_sequences(encoded, maxlen=max_length, padding= 'post ')\\nreturn padded\\nListing 15.16: Function to integer encode and pad movie reviews.\\nWe are now ready to deﬁne our neural network model. The model will use an Embedding\\nlayer as the ﬁrst hidden layer. The Embedding layer requires the speciﬁcation of the vocabulary\\nsize, the size of the real-valued vector space, and the maximum length of input documents. The\\nvocabulary size is the total number of words in our vocabulary, plus one for unknown words.\\nThis could be the vocab set length or the size of the vocab within the tokenizer used to integer\\nencode the documents, for example:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 179}, page_content=\"15.4. Train CNN With Embedding Layer 163\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\nListing 15.17: Calculate the size of the vocabulary for the Embedding layer.\\nWe will use a 100-dimensional vector space, but you could try other values, such as 50 or\\n150. Finally, the maximum document length was calculated above in the maxlength variable\\nused during padding. The complete model deﬁnition is listed below including the Embedding\\nlayer. We use a Convolutional Neural Network (CNN) as they have proven to be successful\\nat document classiﬁcation problems. A conservative CNN conﬁguration is used with 32 ﬁlters\\n(parallel ﬁelds for processing words) and a kernel size of 8 with a rectiﬁed linear ( relu ) activation\\nfunction. This is followed by a pooling layer that reduces the output of the convolutional layer\\nby half.\\nNext, the 2D output from the CNN part of the model is ﬂattened to one long 2D vector to\\nrepresent the features extracted by the CNN. The back-end of the model is a standard Multilayer\\nPerceptron layers to interpret the CNN features. The output layer uses a sigmoid activation\\nfunction to output a value between 0 and 1 for the negative and positive sentiment in the review.\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 100, input_length=max_length))\\nmodel.add(Conv1D(filters=32, kernel_size=8, activation= 'relu '))\\nmodel.add(MaxPooling1D(pool_size=2))\\nmodel.add(Flatten())\\nmodel.add(Dense(10, activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 15.18: Deﬁne a CNN model with the Embedding Layer.\\nRunning just this piece provides a summary of the deﬁned network. We can see that the\\nEmbedding layer expects documents with a length of 1,317 words as input and encodes each\\nword in the document as a 100 element vector.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 1317, 100) 2576800\\n_________________________________________________________________\\nconv1d_1 (Conv1D) (None, 1310, 32) 25632\\n_________________________________________________________________\\nmax_pooling1d_1 (MaxPooling1 (None, 655, 32) 0\\n_________________________________________________________________\\nflatten_1 (Flatten) (None, 20960) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 10) 209610\\n_________________________________________________________________\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 180}, page_content=\"15.4. Train CNN With Embedding Layer 164\\ndense_2 (Dense) (None, 1) 11\\n=================================================================\\nTotal params: 2,812,053\\nTrainable params: 2,812,053\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 15.19: Summary of the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\\nFigure 15.1: Plot of the deﬁned CNN classiﬁcation model.\\nNext, we ﬁt the network on the training data. We use a binary cross entropy loss function\\nbecause the problem we are learning is a binary classiﬁcation problem. The eﬃcient Adam\\nimplementation of stochastic gradient descent is used and we keep track of accuracy in addition\\nto loss during training. The model is trained for 10 epochs, or 10 passes through the training\\ndata. The network conﬁguration and training schedule were found with a little trial and error,\\nbut are by no means optimal for this problem. If you can get better results with a diﬀerent\\nconﬁguration, let me know.\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\nListing 15.20: Train the deﬁned classiﬁcation model.\\nAfter the model is ﬁt, it is saved to a ﬁle named model.h5 for later evaluation.\\n# save the model\\nmodel.save( 'model.h5 ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 181}, page_content=\"15.4. Train CNN With Embedding Layer 165\\nListing 15.21: Save the ﬁt model to ﬁle.\\nWe can tie all of this together. The complete code listing is provided below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import Embedding\\nfrom keras.layers.convolutional import Conv1D\\nfrom keras.layers.convolutional import MaxPooling1D\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc, vocab):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# filter out tokens not in vocab\\ntokens = [w for w in tokens if w in vocab]\\ntokens = ' '.join(tokens)\\nreturn tokens\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load the doc\\ndoc = load_doc(path)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 182}, page_content=\"15.4. Train CNN With Embedding Layer 166\\n# clean doc\\ntokens = clean_doc(doc, vocab)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# integer encode and pad documents\\ndef encode_docs(tokenizer, max_length, docs):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(docs)\\n# pad sequences\\npadded = pad_sequences(encoded, maxlen=max_length, padding= 'post ')\\nreturn padded\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 100, input_length=max_length))\\nmodel.add(Conv1D(filters=32, kernel_size=8, activation= 'relu '))\\nmodel.add(MaxPooling1D(pool_size=2))\\nmodel.add(Flatten())\\nmodel.add(Dense(10, activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load training data\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 183}, page_content=\"15.5. Evaluate Model 167\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# calculate the maximum sequence length\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\\n# encode data\\nXtrain = encode_docs(tokenizer, max_length, train_docs)\\n# define model\\nmodel = define_model(vocab_size, max_length)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# save the model\\nmodel.save( 'model.h5 ')\\nListing 15.22: Complete example of ﬁtting a CNN model with an Embedding input layer.\\nRunning the example will ﬁrst provide a summary of the training dataset vocabulary (25,768)\\nand maximum input sequence length in words (1,317). The example should run in a few minutes\\nand the ﬁt model will be saved to ﬁle.\\n...\\nVocabulary size: 25768\\nMaximum length: 1317\\nEpoch 1/10\\n8s - loss: 0.6927 - acc: 0.4800\\nEpoch 2/10\\n7s - loss: 0.6610 - acc: 0.5922\\nEpoch 3/10\\n7s - loss: 0.3461 - acc: 0.8844\\nEpoch 4/10\\n7s - loss: 0.0441 - acc: 0.9889\\nEpoch 5/10\\n7s - loss: 0.0058 - acc: 1.0000\\nEpoch 6/10\\n7s - loss: 0.0024 - acc: 1.0000\\nEpoch 7/10\\n7s - loss: 0.0015 - acc: 1.0000\\nEpoch 8/10\\n7s - loss: 0.0011 - acc: 1.0000\\nEpoch 9/10\\n7s - loss: 8.0111e-04 - acc: 1.0000\\nEpoch 10/10\\n7s - loss: 5.4109e-04 - acc: 1.0000\\nListing 15.23: Example output from ﬁtting the model.\\n15.5 Evaluate Model\\nIn this section, we will evaluate the trained model and use it to make predictions on new data.\\nFirst, we can use the built-in evaluate() function to estimate the skill of the model on both\\nthe training and test dataset. This requires that we load and encode both the training and test\\ndatasets.\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 184}, page_content=\"15.5. Evaluate Model 168\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# calculate the maximum sequence length\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\\n# encode data\\nXtrain = encode_docs(tokenizer, max_length, train_docs)\\nXtest = encode_docs(tokenizer, max_length, test_docs)\\nListing 15.24: Load and encode both training and test datasets.\\nWe can then load the model and evaluate it on both datasets and print the accuracy.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# evaluate model on training dataset\\n_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\\nprint( 'Train Accuracy: %f '% (acc*100))\\n# evaluate model on test dataset\\n_, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %f '% (acc*100))\\nListing 15.25: Load and evaluate model on both train and test datasets.\\nNew data must then be prepared using the same text encoding and encoding schemes as was\\nused on the training dataset. Once prepared, a prediction can be made by calling the predict()\\nfunction on the model. The function below named predict sentiment() will encode and pad\\na given movie review text and return a prediction in terms of both the percentage and a label.\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, max_length, model):\\n# clean review\\nline = clean_doc(review, vocab)\\n# encode and pad review\\npadded = encode_docs(tokenizer, max_length, [line])\\n# predict sentiment\\nyhat = model.predict(padded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\nListing 15.26: Function to predict the sentiment for an ad hoc movie review.\\nWe can test out this model with two ad hoc movie reviews. The complete example is listed\\nbelow.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 185}, page_content=\"15.5. Evaluate Model 169\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc, vocab):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# filter out tokens not in vocab\\ntokens = [w for w in tokens if w in vocab]\\ntokens = ' '.join(tokens)\\nreturn tokens\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc, vocab)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\\nreturn docs, labels\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 186}, page_content=\"15.5. Evaluate Model 170\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# integer encode and pad documents\\ndef encode_docs(tokenizer, max_length, docs):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(docs)\\n# pad sequences\\npadded = pad_sequences(encoded, maxlen=max_length, padding= 'post ')\\nreturn padded\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, max_length, model):\\n# clean review\\nline = clean_doc(review, vocab)\\n# encode and pad review\\npadded = encode_docs(tokenizer, max_length, [line])\\n# predict sentiment\\nyhat = model.predict(padded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# calculate the maximum sequence length\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\\n# encode data\\nXtrain = encode_docs(tokenizer, max_length, train_docs)\\nXtest = encode_docs(tokenizer, max_length, test_docs)\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# evaluate model on training dataset\\n_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\\nprint( 'Train Accuracy: %.2f '% (acc*100))\\n# evaluate model on test dataset\\n_, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %.2f '% (acc*100))\\n# test positive text\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 187}, page_content=\"15.6. Extensions 171\\ntext = 'Everyone will enjoy this film. I love it, recommended! '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\n# test negative text\\ntext = 'This is a bad movie. Do not watch it. It sucks. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\nListing 15.27: Complete example of making a prediction on new text data.\\nRunning the example ﬁrst prints the skill of the model on the training and test dataset. We\\ncan see that the model achieves 100% accuracy on the training dataset and 87.5% on the test\\ndataset, an impressive score.\\nNext, we can see that the model makes the correct prediction on two contrived movie reviews.\\nWe can see that the percentage or conﬁdence of the prediction is close to 50% for both, this\\nmay be because the two contrived reviews are very short and the model is expecting sequences\\nof 1,000 or more words.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nTrain Accuracy: 100.00\\nTest Accuracy: 87.50\\nReview: [Everyone will enjoy this film. I love it, recommended!]\\nSentiment: POSITIVE (55.431%)\\nReview: [This is a bad movie. Do not watch it. It sucks.]\\nSentiment: NEGATIVE (54.746%)\\nListing 15.28: Example output from making a prediction on new reviews.\\n15.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Data Cleaning . Explore better data cleaning, perhaps leaving some punctuation in tact\\nor normalizing contractions.\\n\\x88Truncated Sequences . Padding all sequences to the length of the longest sequence\\nmight be extreme if the longest sequence is very diﬀerent to all other reviews. Study the\\ndistribution of review lengths and truncate reviews to a mean length.\\n\\x88Truncated Vocabulary . We removed infrequently occurring words, but still had a large\\nvocabulary of more than 25,000 words. Explore further reducing the size of the vocabulary\\nand the eﬀect on model skill.\\n\\x88Filters and Kernel Size . The number of ﬁlters and kernel size are important to model\\nskill and were not tuned. Explore tuning these two CNN parameters.\\n\\x88Epochs and Batch Size . The model appears to ﬁt the training dataset quickly. Explore\\nalternate conﬁgurations of the number of training epochs and batch size and use the test\\ndataset as a validation set to pick a better stopping point for training the model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 188}, page_content='15.7. Further Reading 172\\n\\x88Deeper Network . Explore whether a deeper network results in better skill, either in\\nterms of CNN layers, MLP layers and both.\\n\\x88Pre-Train an Embedding . Explore pre-training a Word2Vec word embedding in the\\nmodel and the impact on model skill with and without further ﬁne tuning during training.\\n\\x88Use GloVe Embedding . Explore loading the pre-trained GloVe embedding and the\\nimpact on model skill with and without further ﬁne tuning during training.\\n\\x88Longer Test Reviews . Explore whether the skill of model predictions is dependent on\\nthe length of movie reviews as suspected in the ﬁnal section on evaluating the model.\\n\\x88Train Final Model . Train a ﬁnal model on all available data and use it make predictions\\non real ad hoc movie reviews from the internet.\\nIf you explore any of these extensions, I’d love to know.\\n15.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n15.7.1 Dataset\\n\\x88Movie Review Data.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\n\\x88A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based\\non Minimum Cuts , 2004.\\nhttp://xxx.lanl.gov/abs/cs/0409058\\n\\x88Movie Review Polarity Dataset.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\n15.7.2 APIs\\n\\x88collections API - Container datatypes.\\nhttps://docs.python.org/3/library/collections.html\\n\\x88Tokenizer Keras API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n\\x88Embedding Keras API.\\nhttps://keras.io/layers/embeddings/'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 189}, page_content='15.8. Summary 173\\n15.8 Summary\\nIn this tutorial, you discovered how to develop word embeddings for the classiﬁcation of movie\\nreviews. Speciﬁcally, you learned:\\n\\x88How to prepare movie review text data for classiﬁcation with deep learning methods.\\n\\x88How to develop a neural classiﬁcation model with word embedding and convolutional\\nlayers.\\n\\x88How to evaluate the developed a neural classiﬁcation model.\\n15.8.1 Next\\nIn the next chapter, you will discover how you can develop an n-gram multichannel convolutional\\nneural network for text classiﬁcation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 190}, page_content='Chapter 16\\nProject: Develop an n-gram CNN\\nModel for Sentiment Analysis\\nA standard deep learning model for text classiﬁcation and sentiment analysis uses a word\\nembedding layer and one-dimensional convolutional neural network. The model can be expanded\\nby using multiple parallel convolutional neural networks that read the source document using\\ndiﬀerent kernel sizes. This, in eﬀect, creates a multichannel convolutional neural network for\\ntext that reads text with diﬀerent n-gram sizes (groups of words). In this tutorial, you will\\ndiscover how to develop a multichannel convolutional neural network for sentiment prediction\\non text movie review data. After completing this tutorial, you will know:\\n\\x88How to prepare movie review text data for modeling.\\n\\x88How to develop a multichannel convolutional neural network for text in Keras.\\n\\x88How to evaluate a ﬁt model on unseen movie review data.\\nLet’s get started.\\n16.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Movie Review Dataset.\\n2. Data Preparation.\\n3. Develop Multichannel Model.\\n4. Evaluate Model.\\n16.2 Movie Review Dataset\\nIn this tutorial, we will use the Movie Review Dataset. This dataset designed for sentiment\\nanalysis was described previously in Chapter 9. You can download the dataset from here:\\n174'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 191}, page_content='16.3. Data Preparation 175\\n\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention cv000 tocv999 for each of negand pos.\\n16.3 Data Preparation\\nNote : The preparation of the movie review dataset was ﬁrst described in Chapter 9. In this\\nsection, we will look at 3 things:\\n1. Separation of data into training and test sets.\\n2. Loading and cleaning the data to remove punctuation and numbers.\\n3. Clean All Reviews and Save.\\n16.3.1 Split into Train and Test Sets\\nWe are pretending that we are developing a system that can predict the sentiment of a textual\\nmovie review as either positive or negative. This means that after the model is developed, we\\nwill need to make predictions on new textual reviews. This will require all of the same data\\npreparation to be performed on those new reviews as is performed on the training data for the\\nmodel. We will ensure that this constraint is built into the evaluation of our models by splitting\\nthe training and test datasets prior to any data preparation. This means that any knowledge in\\nthe data in the test set that could help us better prepare the data (e.g. the words used) are\\nunavailable in the preparation of data used for training the model.\\nThat being said, we will use the last 100 positive reviews and the last 100 negative reviews\\nas a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a\\n90% train, 10% split of the data. The split can be imposed easily by using the ﬁlenames of the\\nreviews where reviews named 000 to 899 are for training data and reviews named 900 onwards\\nare for test.\\n16.3.2 Loading and Cleaning Reviews\\nThe text data is already pretty clean; not much preparation is required. Without getting bogged\\ndown too much in the details, we will prepare the data using the following way:\\n\\x88Split tokens on white space.\\n\\x88Remove all punctuation from words.\\n\\x88Remove all words that are not purely comprised of alphabetical characters.\\n\\x88Remove all words that are known stop words.\\n\\x88Remove all words that have a length ≤1 character.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 192}, page_content=\"16.3. Data Preparation 176\\nWe can put all of these steps into a function called clean doc() that takes as an argument\\nthe raw text loaded from a ﬁle and returns a list of cleaned tokens. We can also deﬁne a function\\nload doc() that loads a document from ﬁle ready for use with the clean doc() function. An\\nexample of cleaning the ﬁrst positive review is listed below.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 16.1: Example of cleaning a movie review.\\nRunning the example prints a long list of clean tokens. There are many more cleaning steps\\nwe may want to explore and I leave them as further exercises.\\n...\\n'creepy ', 'place ', 'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ',\\n'typically ', 'strong ', 'performance ', 'deftly ', 'handling ', 'british ', 'accent ',\\n'ians ', 'holm ', 'joe ', 'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ',\\n'supporting ', 'roles ', 'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ',\\n'opened ', 'mouth ', 'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ',\\n'half ', 'bad ', 'film ', 'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ',\\n'language ', 'drug ', 'content ']\\nListing 16.2: Example output of cleaning a movie review.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 193}, page_content=\"16.3. Data Preparation 177\\n16.3.3 Clean All Reviews and Save\\nWe can now use the function to clean reviews and apply it to all reviews. To do this, we will\\ndevelop a new function named process docs() below that will walk through all reviews in a\\ndirectory, clean them, and return them as a list. We will also add an argument to the function\\nto indicate whether the function is processing train or test reviews, that way the ﬁlenames can\\nbe ﬁltered (as described above) and only those train or test reviews requested will be cleaned\\nand returned. The full function is listed below.\\n# load all docs in a directory\\ndef process_docs(directory, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\nListing 16.3: Function for cleaning multiple review documents.\\nWe can call this function with negative training reviews. We also need labels for the train\\nand test documents. We know that we have 900 training documents and 100 test documents.\\nWe can use a Python list comprehension to create the labels for the negative (0) and positive\\n(1) reviews for both train and test sets. The function below named load clean dataset() will\\nload and clean the movie review text and also create the labels for the reviews.\\n# load and clean a dataset\\ndef load_clean_dataset(is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', is_train)\\npos = process_docs( 'txt_sentoken/pos ', is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\nListing 16.4: Function to prepare reviews and labels for a dataset.\\nFinally, we want to save the prepared train and test sets to ﬁle so that we can load them\\nlater for modeling and model evaluation. The function below-named save dataset() will save\\na given prepared dataset ( Xandyelements) to a ﬁle using the pickle API (this is the standard\\nAPI for saving objects in Python).\\n# save a dataset to file\\ndef save_dataset(dataset, filename):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 194}, page_content=\"16.3. Data Preparation 178\\ndump(dataset, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\nListing 16.5: Function for saving clean documents to ﬁle.\\n16.3.4 Complete Example\\nWe can tie all of these data preparation steps together. The complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom pickle import dump\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\ntokens = ' '.join(tokens)\\nreturn tokens\\n# load all docs in a directory\\ndef process_docs(directory, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 195}, page_content=\"16.4. Develop Multichannel Model 179\\n# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\n# load and clean a dataset\\ndef load_clean_dataset(is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', is_train)\\npos = process_docs( 'txt_sentoken/pos ', is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# save a dataset to file\\ndef save_dataset(dataset, filename):\\ndump(dataset, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\n# load and clean all reviews\\ntrain_docs, ytrain = load_clean_dataset(True)\\ntest_docs, ytest = load_clean_dataset(False)\\n# save training datasets\\nsave_dataset([train_docs, ytrain], 'train.pkl ')\\nsave_dataset([test_docs, ytest], 'test.pkl ')\\nListing 16.6: Complete example of cleaning and saving all movie reviews.\\nRunning the example cleans the text movie review documents, creates labels, and saves the\\nprepared data for both train and test datasets in train.pkl andtest.pkl respectively. Now\\nwe are ready to develop our model.\\n16.4 Develop Multichannel Model\\nIn this section, we will develop a multichannel convolutional neural network for the sentiment\\nanalysis prediction problem. This section is divided into 3 parts:\\n1. Encode Data\\n2. Deﬁne Model.\\n3. Complete Example.\\n16.4.1 Encode Data\\nThe ﬁrst step is to load the cleaned training dataset. The function below-named load dataset()\\ncan be called to load the pickled training dataset.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 196}, page_content=\"16.4. Develop Multichannel Model 180\\n# load a clean dataset\\ndef load_dataset(filename):\\nreturn load(open(filename, 'rb '))\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\nListing 16.7: Example of loading the cleaned and saved reviews.\\nNext, we must ﬁt a Keras Tokenizer on the training dataset. We will use this tokenizer to\\nboth deﬁne the vocabulary for the Embedding layer and encode the review documents as integers.\\nThe function create tokenizer() below will create a Tokenizer given a list of documents.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 16.8: Function for creating a Tokenizer .\\nWe also need to know the maximum length of input sequences as input for the model and\\nto pad all sequences to the ﬁxed length. The function maxlength() below will calculate the\\nmaximum length (number of words) for all reviews in the training dataset.\\n# calculate the maximum document length\\ndef max_length(lines):\\nreturn max([len(s.split()) for s in lines])\\nListing 16.9: Function to calculate the maximum movie review length.\\nWe also need to know the size of the vocabulary for the Embedding layer. This can be\\ncalculated from the prepared Tokenizer , as follows:\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nListing 16.10: Calculate the size of the vocabulary.\\nFinally, we can integer encode and pad the clean movie review text. The function below\\nnamed encode text() will both encode and pad text data to the maximum review length.\\n# encode a list of lines\\ndef encode_text(tokenizer, lines, length):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(lines)\\n# pad encoded sequences\\npadded = pad_sequences(encoded, maxlen=length, padding= 'post ')\\nreturn padded\\nListing 16.11: Function to encode and pad movie review text.\\n16.4.2 Deﬁne Model\\nA standard model for document classiﬁcation is to use an Embedding layer as input, followed by\\na one-dimensional convolutional neural network, pooling layer, and then a prediction output\\nlayer. The kernel size in the convolutional layer deﬁnes the number of words to consider as\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 197}, page_content=\"16.4. Develop Multichannel Model 181\\nthe convolution is passed across the input text document, providing a grouping parameter. A\\nmulti-channel convolutional neural network for document classiﬁcation involves using multiple\\nversions of the standard model with diﬀerent sized kernels. This allows the document to be\\nprocessed at diﬀerent resolutions or diﬀerent n-grams (groups of words) at a time, whilst the\\nmodel learns how to best integrate these interpretations.\\nThis approach was ﬁrst described by Yoon Kim in his 2014 paper titled Convolutional Neural\\nNetworks for Sentence Classiﬁcation . In the paper, Kim experimented with static and dynamic\\n(updated) embedding layers, we can simplify the approach and instead focus only on the use of\\ndiﬀerent kernel sizes. This approach is best understood with a diagram taken from Kim’s paper,\\nsee Chapter 14.\\nIn Keras, a multiple-input model can be deﬁned using the functional API. We will deﬁne a\\nmodel with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review\\ntext. Each channel is comprised of the following elements:\\n\\x88Input layer that deﬁnes the length of input sequences.\\n\\x88Embedding layer set to the size of the vocabulary and 100-dimensional real-valued repre-\\nsentations.\\n\\x88Conv1D layer with 32 ﬁlters and a kernel size set to the number of words to read at once.\\n\\x88MaxPooling1D layer to consolidate the output from the convolutional layer.\\n\\x88Flatten layer to reduce the three-dimensional output to two dimensional for concatenation.\\nThe output from the three channels are concatenated into a single vector and process by a\\nDense layer and an output layer. The function below deﬁnes and returns the model. As part of\\ndeﬁning the model, a summary of the deﬁned model is printed and a plot of the model graph is\\ncreated and saved to ﬁle.\\n# define the model\\ndef define_model(length, vocab_size):\\n# channel 1\\ninputs1 = Input(shape=(length,))\\nembedding1 = Embedding(vocab_size, 100)(inputs1)\\nconv1 = Conv1D(filters=32, kernel_size=4, activation= 'relu ')(embedding1)\\ndrop1 = Dropout(0.5)(conv1)\\npool1 = MaxPooling1D(pool_size=2)(drop1)\\nflat1 = Flatten()(pool1)\\n# channel 2\\ninputs2 = Input(shape=(length,))\\nembedding2 = Embedding(vocab_size, 100)(inputs2)\\nconv2 = Conv1D(filters=32, kernel_size=6, activation= 'relu ')(embedding2)\\ndrop2 = Dropout(0.5)(conv2)\\npool2 = MaxPooling1D(pool_size=2)(drop2)\\nflat2 = Flatten()(pool2)\\n# channel 3\\ninputs3 = Input(shape=(length,))\\nembedding3 = Embedding(vocab_size, 100)(inputs3)\\nconv3 = Conv1D(filters=32, kernel_size=8, activation= 'relu ')(embedding3)\\ndrop3 = Dropout(0.5)(conv3)\\npool3 = MaxPooling1D(pool_size=2)(drop3)\\nflat3 = Flatten()(pool3)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 198}, page_content=\"16.4. Develop Multichannel Model 182\\n# merge\\nmerged = concatenate([flat1, flat2, flat3])\\n# interpretation\\ndense1 = Dense(10, activation= 'relu ')(merged)\\noutputs = Dense(1, activation= 'sigmoid ')(dense1)\\nmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\\n# compile\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize\\nmodel.summary()\\nplot_model(model, show_shapes=True, to_file= 'multichannel.png ')\\nreturn model\\nListing 16.12: Function for deﬁning the classiﬁcation model.\\n16.4.3 Complete Example\\nPulling all of this together, the complete example is listed below.\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import Dropout\\nfrom keras.layers import Embedding\\nfrom keras.layers.convolutional import Conv1D\\nfrom keras.layers.convolutional import MaxPooling1D\\nfrom keras.layers.merge import concatenate\\n# load a clean dataset\\ndef load_dataset(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the maximum document length\\ndef max_length(lines):\\nreturn max([len(s.split()) for s in lines])\\n# encode a list of lines\\ndef encode_text(tokenizer, lines, length):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(lines)\\n# pad encoded sequences\\npadded = pad_sequences(encoded, maxlen=length, padding= 'post ')\\nreturn padded\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 199}, page_content=\"16.4. Develop Multichannel Model 183\\n# define the model\\ndef define_model(length, vocab_size):\\n# channel 1\\ninputs1 = Input(shape=(length,))\\nembedding1 = Embedding(vocab_size, 100)(inputs1)\\nconv1 = Conv1D(filters=32, kernel_size=4, activation= 'relu ')(embedding1)\\ndrop1 = Dropout(0.5)(conv1)\\npool1 = MaxPooling1D(pool_size=2)(drop1)\\nflat1 = Flatten()(pool1)\\n# channel 2\\ninputs2 = Input(shape=(length,))\\nembedding2 = Embedding(vocab_size, 100)(inputs2)\\nconv2 = Conv1D(filters=32, kernel_size=6, activation= 'relu ')(embedding2)\\ndrop2 = Dropout(0.5)(conv2)\\npool2 = MaxPooling1D(pool_size=2)(drop2)\\nflat2 = Flatten()(pool2)\\n# channel 3\\ninputs3 = Input(shape=(length,))\\nembedding3 = Embedding(vocab_size, 100)(inputs3)\\nconv3 = Conv1D(filters=32, kernel_size=8, activation= 'relu ')(embedding3)\\ndrop3 = Dropout(0.5)(conv3)\\npool3 = MaxPooling1D(pool_size=2)(drop3)\\nflat3 = Flatten()(pool3)\\n# merge\\nmerged = concatenate([flat1, flat2, flat3])\\n# interpretation\\ndense1 = Dense(10, activation= 'relu ')(merged)\\noutputs = Dense(1, activation= 'sigmoid ')(dense1)\\nmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\\n# compile\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize\\nmodel.summary()\\nplot_model(model, show_shapes=True, to_file= 'model.png ')\\nreturn model\\n# load training dataset\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\n# create tokenizer\\ntokenizer = create_tokenizer(trainLines)\\n# calculate max document length\\nlength = max_length(trainLines)\\nprint( 'Max document length: %d '% length)\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# encode data\\ntrainX = encode_text(tokenizer, trainLines, length)\\n# define model\\nmodel = define_model(length, vocab_size)\\n# fit model\\nmodel.fit([trainX,trainX,trainX], trainLabels, epochs=7, batch_size=16)\\n# save the model\\nmodel.save( 'model.h5 ')\\nListing 16.13: Complete example of ﬁtting the n-gram CNN model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 200}, page_content='16.4. Develop Multichannel Model 184\\nRunning the example ﬁrst prints a summary of the prepared training dataset.\\nMax document length: 1380\\nVocabulary size: 44277\\nListing 16.14: Example output from preparing the training data.\\nThe model is ﬁt relatively quickly and appears to show good skill on the training dataset.\\n...\\nEpoch 3/7\\n1800/1800 [==============================] - 29s - loss: 0.0460 - acc: 0.9894\\nEpoch 4/7\\n1800/1800 [==============================] - 30s - loss: 0.0041 - acc: 1.0000\\nEpoch 5/7\\n1800/1800 [==============================] - 31s - loss: 0.0010 - acc: 1.0000\\nEpoch 6/7\\n1800/1800 [==============================] - 30s - loss: 3.0271e-04 - acc: 1.0000\\nEpoch 7/7\\n1800/1800 [==============================] - 28s - loss: 1.3875e-04 - acc: 1.0000\\nListing 16.15: Example output from ﬁtting the model.\\nA plot of the deﬁned model is saved to ﬁle, clearly showing the three input channels for the\\nmodel.\\nFigure 16.1: Plot of the Multichannel Convolutional Neural Network For Text.\\nThe model is ﬁt for a number of epochs and saved to the ﬁle model.h5 for later evaluation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 201}, page_content=\"16.5. Evaluate Model 185\\n16.5 Evaluate Model\\nIn this section, we can evaluate the ﬁt model by predicting the sentiment on all reviews in the\\nunseen test dataset. Using the data loading functions developed in the previous section, we can\\nload and encode both the training and test datasets.\\n# load datasets\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\ntestLines, testLabels = load_dataset( 'test.pkl ')\\n# create tokenizer\\ntokenizer = create_tokenizer(trainLines)\\n# calculate max document length\\nlength = max_length(trainLines)\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Max document length: %d '% length)\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# encode data\\ntrainX = encode_text(tokenizer, trainLines, length)\\ntestX = encode_text(tokenizer, testLines, length)\\nprint(trainX.shape, testX.shape)\\nListing 16.16: Prepare train and test data for evaluating the model.\\nWe can load the saved model and evaluate it on both the training and test datasets. The\\ncomplete example is listed below.\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\\n# load a clean dataset\\ndef load_dataset(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the maximum document length\\ndef max_length(lines):\\nreturn max([len(s.split()) for s in lines])\\n# encode a list of lines\\ndef encode_text(tokenizer, lines, length):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(lines)\\n# pad encoded sequences\\npadded = pad_sequences(encoded, maxlen=length, padding= 'post ')\\nreturn padded\\n# load datasets\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 202}, page_content=\"16.6. Extensions 186\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\ntestLines, testLabels = load_dataset( 'test.pkl ')\\n# create tokenizer\\ntokenizer = create_tokenizer(trainLines)\\n# calculate max document length\\nlength = max_length(trainLines)\\nprint( 'Max document length: %d '% length)\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# encode data\\ntrainX = encode_text(tokenizer, trainLines, length)\\ntestX = encode_text(tokenizer, testLines, length)\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# evaluate model on training dataset\\n_, acc = model.evaluate([trainX,trainX,trainX], trainLabels, verbose=0)\\nprint( 'Train Accuracy: %.2f '% (acc*100))\\n# evaluate model on test dataset dataset\\n_, acc = model.evaluate([testX,testX,testX], testLabels, verbose=0)\\nprint( 'Test Accuracy: %.2f '% (acc*100))\\nListing 16.17: Complete example of evaluating the ﬁt model.\\nRunning the example prints the skill of the model on both the training and test datasets. We\\ncan see that, as expected, the skill on the training dataset is excellent, here at 100% accuracy.\\nWe can also see that the skill of the model on the unseen test dataset is also very impressive,\\nachieving 88.5%, which is above the skill of the model reported in the 2014 paper (although not\\na direct apples-to-apples comparison).\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nTrain Accuracy: 100.00\\nTest Accuracy: 88.50\\nListing 16.18: Example output from evaluating the ﬁt model.\\n16.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Diﬀerent n-grams . Explore the model by changing the kernel size (number of n-grams)\\nused by the channels in the model to see how it impacts model skill.\\n\\x88More or Fewer Channels . Explore using more or fewer channels in the model and see\\nhow it impacts model skill.\\n\\x88Shared Embedding . Explore conﬁgurations where each channel shares the same word\\nembedding and report on the impact on model skill.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 203}, page_content='16.7. Further Reading 187\\n\\x88Deeper Network . Convolutional neural networks perform better in computer vision\\nwhen they are deeper. Explore using deeper models here and see how it impacts model\\nskill.\\n\\x88Truncated Sequences . Padding all sequences to the length of the longest sequence\\nmight be extreme if the longest sequence is very diﬀerent to all other reviews. Study the\\ndistribution of review lengths and truncate reviews to a mean length.\\n\\x88Truncated Vocabulary . We removed infrequently occurring words, but still had a large\\nvocabulary of more than 25,000 words. Explore further reducing the size of the vocabulary\\nand the eﬀect on model skill.\\n\\x88Epochs and Batch Size . The model appears to ﬁt the training dataset quickly. Explore\\nalternate conﬁgurations of the number of training epochs and batch size and use the test\\ndataset as a validation set to pick a better stopping point for training the model.\\n\\x88Pre-Train an Embedding . Explore pre-training a Word2Vec word embedding in the\\nmodel and the impact on model skill with and without further ﬁne tuning during training.\\n\\x88Use GloVe Embedding . Explore loading the pre-trained GloVe embedding and the\\nimpact on model skill with and without further ﬁne tuning during training.\\n\\x88Train Final Model . Train a ﬁnal model on all available data and use it make predictions\\non real ad hoc movie reviews from the internet.\\nIf you explore any of these extensions, I’d love to know.\\n16.7 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Convolutional Neural Networks for Sentence Classiﬁcation , 2014.\\nhttps://arxiv.org/abs/1408.5882\\n\\x88Convolutional Neural Networks for Sentence Classiﬁcation (code).\\nhttps://github.com/yoonkim/CNN_sentence\\n\\x88Keras Functional API.\\nhttps://keras.io/getting-started/functional-api-guide/\\n16.8 Summary\\nIn this tutorial, you discovered how to develop a multichannel convolutional neural network for\\nsentiment prediction on text movie review data. Speciﬁcally, you learned:\\n\\x88How to prepare movie review text data for modeling.\\n\\x88How to develop a multichannel convolutional neural network for text in Keras.\\n\\x88How to evaluate a ﬁt model on unseen movie review data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 204}, page_content='16.8. Summary 188\\n16.8.1 Next\\nThis chapter is the last in the text classiﬁcation part. In the next part, you will discover how to\\ndevelop neural language models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 205}, page_content='Part VII\\nLanguage Modeling\\n189'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 206}, page_content='Chapter 17\\nNeural Language Modeling\\nLanguage modeling is central to many important natural language processing tasks. Recently,\\nneural-network-based language models have demonstrated better performance than classical\\nmethods both standalone and as part of more challenging natural language processing tasks. In\\nthis chapter, you will discover language modeling for natural language processing. After reading\\nthis chapter, you will know:\\n\\x88Why language modeling is critical to addressing tasks in natural language processing.\\n\\x88What a language model is and some examples of where they are used.\\n\\x88How neural networks can be used for language modeling.\\nLet’s get started.\\n17.1 Overview\\nThis tutorial is divided into the following parts:\\n1. Problem of Modeling Language\\n2. Statistical Language Modeling\\n3. Neural Language Models\\n17.2 Problem of Modeling Language\\nFormal languages, like programming languages, can be fully speciﬁed. All the reserved words\\ncan be deﬁned and the valid ways that they can be used can be precisely deﬁned. We cannot do\\nthis with natural language. Natural languages are not designed; they emerge, and therefore\\nthere is no formal speciﬁcation.\\nThere may be formal rules and heuristics for parts of the language, but as soon as rules\\nare deﬁned, you will devise or encounter counter examples that contradict the rules. Natural\\nlanguages involve vast numbers of terms that can be used in ways that introduce all kinds of\\nambiguities, yet can still be understood by other humans. Further, languages change, word\\n190'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 207}, page_content='17.3. Statistical Language Modeling 191\\nusages change: it is a moving target. Nevertheless, linguists try to specify the language with\\nformal grammars and structures. It can be done, but it is very diﬃcult and the results can\\nbe fragile. An alternative approach to specifying the model of the language is to learn it from\\nexamples.\\n17.3 Statistical Language Modeling\\nStatistical Language Modeling, or Language Modeling and LM for short, is the development of\\nprobabilistic models that are able to predict the next word in the sequence given the words that\\nprecede it.\\nLanguage modeling is the task of assigning a probability to sentences in a language.\\n[...] Besides assigning a probability to each sequence of words, the language models\\nalso assigns a probability for the likelihood of a given word (or a sequence of words)\\nto follow a sequence of words\\n— Page 105, Neural Network Methods in Natural Language Processing , 2017.\\nA language model learns the probability of word occurrence based on examples of text.\\nSimpler models may look at a context of a short sequence of words, whereas larger models may\\nwork at the level of sentences or paragraphs. Most commonly, language models operate at the\\nlevel of words.\\nThe notion of a language model is inherently probabilistic. A language model is a\\nfunction that puts a probability measure over strings drawn from some vocabulary.\\n— Page 238, An Introduction to Information Retrieval , 2008.\\nA language model can be developed and used standalone, such as to generate new sequences\\nof text that appear to have come from the corpus. Language modeling is a root problem for a\\nlarge range of natural language processing tasks. More practically, language models are used\\non the front-end or back-end of a more sophisticated model for a task that requires language\\nunderstanding.\\n... language modeling is a crucial component in real-world applications such as\\nmachine-translation and automatic speech recognition, [...] For these reasons, lan-\\nguage modeling plays a central role in natural-language processing, AI, and machine-\\nlearning research.\\n— Page 105, Neural Network Methods in Natural Language Processing , 2017.\\nA good example is speech recognition, where audio data is used as an input to the model\\nand the output requires a language model that interprets the input signal and recognizes each\\nnew word within the context of the words already recognized.\\nSpeech recognition is principally concerned with the problem of transcribing the\\nspeech signal as a sequence of words. [...] From this point of view, speech is assumed\\nto be a generated by a language model which provides estimates of Pr(w) for all word\\nstrings w independently of the observed signal [...] The goal of speech recognition is\\nto ﬁnd the most likely word sequence given the observed acoustic signal.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 208}, page_content='17.4. Neural Language Models 192\\n— Pages 205-206, The Oxford Handbook of Computational Linguistics , 2005\\nSimilarly, language models are used to generate text in many similar natural language\\nprocessing tasks, for example:\\n\\x88Optical Character Recognition\\n\\x88Handwriting Recognition.\\n\\x88Machine Translation.\\n\\x88Spelling Correction.\\n\\x88Image Captioning.\\n\\x88Text Summarization\\n\\x88And much more.\\nLanguage modeling is the art of determining the probability of a sequence of words.\\nThis is useful in a large variety of areas including speech recognition, optical character\\nrecognition, handwriting recognition, machine translation, and spelling correction\\n—A Bit of Progress in Language Modeling , 2001.\\nDeveloping better language models often results in models that perform better on their\\nintended natural language processing task. This is the motivation for developing better and\\nmore accurate language models.\\n[language models] have played a key role in traditional NLP tasks such as speech\\nrecognition, machine translation, or text summarization. Often (although not\\nalways), training better language models improves the underlying metrics of the\\ndownstream task (such as word error rate for speech recognition, or BLEU score for\\ntranslation), which makes the task of training better LMs valuable by itself.\\n—Exploring the Limits of Language Modeling , 2016.\\n17.4 Neural Language Models\\nRecently, the use of neural networks in the development of language models has become very\\npopular, to the point that it may now be the preferred approach. The use of neural networks in\\nlanguage modeling is often called Neural Language Modeling, or NLM for short. Neural network\\napproaches are achieving better results than classical methods both on standalone language\\nmodels and when models are incorporated into larger models on challenging tasks like speech\\nrecognition and machine translation. A key reason for the leaps in improved performance may\\nbe the method’s ability to generalize.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 209}, page_content='17.4. Neural Language Models 193\\nNonlinear neural network models solve some of the shortcomings of traditional\\nlanguage models: they allow conditioning on increasingly large context sizes with\\nonly a linear increase in the number of parameters, they alleviate the need for\\nmanually designing backoﬀ orders, and they support generalization across diﬀerent\\ncontexts.\\n— Page 109, Neural Network Methods in Natural Language Processing , 2017.\\nSpeciﬁcally, a word embedding is adopted that uses a real-valued vector to represent each\\nword in a projected vector space. This learned representation of words based on their usage\\nallows words with a similar meaning to have a similar representation.\\nNeural Language Models (NLM) address the n-gram data sparsity issue through\\nparameterization of words as vectors (word embeddings) and using them as inputs to\\na neural network. The parameters are learned as part of the training process. Word\\nembeddings obtained through NLMs exhibit the property whereby semantically close\\nwords are likewise close in the induced vector space.\\n—Character-Aware Neural Language Model , 2015.\\nThis generalization is something that the representation used in classical statistical language\\nmodels cannot easily achieve.\\n“True generalization” is diﬃcult to obtain in a discrete word indice space, since\\nthere is no obvious relation between the word indices.\\n—Connectionist language modeling for large vocabulary continuous speech recognition , 2002.\\nFurther, the distributed representation approach allows the embedding representation to scale\\nbetter with the size of the vocabulary. Classical methods that have one discrete representation\\nper word ﬁght the curse of dimensionality with larger and larger vocabularies of words that\\nresult in longer and more sparse representations. The neural network approach to language\\nmodeling can be described using the three following model properties, taken from A Neural\\nProbabilistic Language Model , 2003.\\n1. Associate each word in the vocabulary with a distributed word feature vector.\\n2.Express the joint probability function of word sequences in terms of the feature vectors of\\nthese words in the sequence.\\n3.Learn simultaneously the word feature vector and the parameters of the probability\\nfunction.\\nThis represents a relatively simple model where both the representation and probabilistic\\nmodel are learned together directly from raw text data. Recently, the neural based approaches\\nhave started to outperform the classical statistical approaches.\\nWe provide ample empirical evidence to suggest that connectionist language mod-\\nels are superior to standard n-gram techniques, except their high computational\\n(training) complexity.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 210}, page_content='17.5. Further Reading 194\\n—Recurrent neural network based language model , 2010.\\nInitially, feedforward neural network models were used to introduce the approach. More\\nrecently, recurrent neural networks and then networks with a long-term memory like the Long\\nShort-Term Memory network, or LSTM, allow the models to learn the relevant context over\\nmuch longer input sequences than the simpler feedforward networks.\\n[an RNN language model] provides further generalization: instead of considering\\njust several preceding words, neurons with input from recurrent connections are\\nassumed to represent short term memory. The model learns itself from the data how\\nto represent memory. While shallow feedforward neural networks (those with just\\none hidden layer) can only cluster similar words, recurrent neural network (which\\ncan be considered as a deep architecture) can perform clustering of similar histories.\\nThis allows for instance eﬃcient representation of patterns with variable length.\\n—Extensions of recurrent neural network language model , 2011.\\nRecently, researchers have been seeking the limits of these language models. In the paper\\nExploring the Limits of Language Modeling , evaluating language models over large datasets,\\nsuch as the corpus of one million words, the authors ﬁnd that LSTM-based neural language\\nmodels out-perform the classical methods.\\n... we have shown that RNN LMs can be trained on large amounts of data, and\\noutperform competing models including carefully tuned N-grams.\\n—Exploring the Limits of Language Modeling , 2016.\\nFurther, they propose some heuristics for developing high-performing neural language models\\nin general:\\n\\x88Size matters . The best models were the largest models, speciﬁcally number of memory\\nunits.\\n\\x88Regularization matters . Use of regularization like dropout on input connections\\nimproves results.\\n\\x88CNNs vs Embeddings . Character-level Convolutional Neural Network (CNN) models\\ncan be used on the front-end instead of word embeddings, achieving similar and sometimes\\nbetter results.\\n\\x88Ensembles matter . Combining the prediction from multiple models can oﬀer large\\nimprovements in model performance.\\n17.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 211}, page_content='17.5. Further Reading 195\\n17.5.1 Books\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2vStiIS\\n\\x88Natural Language Processing, Artiﬁcial Intelligence A Modern Approach , 2009.\\nhttp://amzn.to/2fDPfF3\\n\\x88Language models for information retrieval, An Introduction to Information Retrieval , 2008.\\nhttp://amzn.to/2vAavQd\\n17.5.2 Papers\\n\\x88A Neural Probabilistic Language Model , NIPS, 2001.\\nhttps://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf\\n\\x88A Neural Probabilistic Language Model , JMLR, 2003.\\nhttp://www.jmlr.org/papers/v3/bengio03a.html\\n\\x88Connectionist language modeling for large vocabulary continuous speech recognition , 2002.\\nhttps://pdfs.semanticscholar.org/b4db/83366f925e9a1e1528ee9f6b41d7cd666f41.\\npdf\\n\\x88Recurrent neural network based language model , 2010.\\nhttp://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_\\nIS100722.pdf\\n\\x88Extensions of recurrent neural network language model , 2011.\\nhttp://ieeexplore.ieee.org/abstract/document/5947611/\\n\\x88Character-Aware Neural Language Model , 2015.\\nhttps://arxiv.org/abs/1508.06615\\n\\x88LSTM Neural Networks for Language Modeling , 2012.\\nhttps://pdfs.semanticscholar.org/f9a1/b3850dfd837793743565a8af95973d395a4e.\\npdf\\n\\x88Exploring the Limits of Language Modeling , 2016.\\nhttps://arxiv.org/abs/1602.02410\\n17.5.3 Articles\\n\\x88Language Model, Wikipedia.\\nhttps://en.wikipedia.org/wiki/Language_model\\n\\x88Neural net language models, Scholarpedia.\\nhttp://www.scholarpedia.org/article/Neural_net_language_models'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 212}, page_content='17.6. Summary 196\\n17.6 Summary\\nIn this chapter, you discovered language modeling for natural language processing tasks. Specif-\\nically, you learned:\\n\\x88That natural language is not formally speciﬁed and requires the use of statistical models\\nto learn from examples.\\n\\x88That statistical language models are central to many challenging natural language pro-\\ncessing tasks.\\n\\x88That state-of-the-art results are achieved using neural language models, speciﬁcally those\\nwith word embeddings and recurrent neural network algorithms.\\n17.6.1 Next\\nIn the next chapter, you will discover how you can develop a character-based neural language\\nmodel.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 213}, page_content='Chapter 18\\nHow to Develop a Character-Based\\nNeural Language Model\\nA language model predicts the next word in the sequence based on the speciﬁc words that have\\ncome before it in the sequence. It is also possible to develop language models at the character\\nlevel using neural networks. The beneﬁt of character-based language models is their small\\nvocabulary and ﬂexibility in handling any words, punctuation, and other document structure.\\nThis comes at the cost of requiring larger models that are slower to train. Nevertheless, in the\\nﬁeld of neural language models, character-based models oﬀer a lot of promise for a general,\\nﬂexible and powerful approach to language modeling. In this tutorial, you will discover how to\\ndevelop a character-based neural language model. After completing this tutorial, you will know:\\n\\x88How to prepare text for character-based language modeling.\\n\\x88How to develop a character-based language model using LSTMs.\\n\\x88How to use a trained character-based language model to generate text.\\nLet’s get started.\\n18.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Sing a Song of Sixpence\\n2. Data Preparation\\n3. Train Language Model\\n4. Generate Text\\n197'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 214}, page_content=\"18.2. Sing a Song of Sixpence 198\\n18.2 Sing a Song of Sixpence\\nThe nursery rhyme Sing a Song of Sixpence is well known in the west. The ﬁrst verse is common,\\nbut there is also a 4 verse version that we will use to develop our character-based language\\nmodel. It is short, so ﬁtting the model will be fast, but not so short that we won’t see anything\\ninteresting. The complete 4 verse version we will use as source text is listed below.\\nSing a song of sixpence,\\nA pocket full of rye.\\nFour and twenty blackbirds,\\nBaked in a pie.\\nWhen the pie was opened\\nThe birds began to sing;\\nWasn 't that a dainty dish,\\nTo set before the king.\\nThe king was in his counting house,\\nCounting out his money;\\nThe queen was in the parlour,\\nEating bread and honey.\\nThe maid was in the garden,\\nHanging out the clothes,\\nWhen down came a blackbird\\nAnd pecked off her nose.\\nListing 18.1: Sing a Song of Sixpence nursery rhyme.\\nCopy the text and save it in a new ﬁle in your current working directory with the ﬁle name\\nrhyme.txt .\\n18.3 Data Preparation\\nThe ﬁrst step is to prepare the text data. We will start by deﬁning the type of language model.\\n18.3.1 Language Model Design\\nA language model must be trained on the text, and in the case of a character-based language\\nmodel, the input and output sequences must be characters. The number of characters used\\nas input will also deﬁne the number of characters that will need to be provided to the model\\nin order to elicit the ﬁrst predicted character. After the ﬁrst character has been generated, it\\ncan be appended to the input sequence and used as input for the model to generate the next\\ncharacter.\\nLonger sequences oﬀer more context for the model to learn what character to output next\\nbut take longer to train and impose more burden on seeding the model when generating text.\\nWe will use an arbitrary length of 10 characters for this model. There is not a lot of text, and\\n10 characters is a few words. We can now transform the raw text into a form that our model\\ncan learn; speciﬁcally, input and output sequences of characters.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 215}, page_content=\"18.3. Data Preparation 199\\n18.3.2 Load Text\\nWe must load the text into memory so that we can work with it. Below is a function named\\nload doc() that will load a text ﬁle given a ﬁlename and return the loaded text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 18.2: Function to load a document into memory.\\nWe can call this function with the ﬁlename of the nursery rhyme rhyme.txt to load the text\\ninto memory. The contents of the ﬁle are then printed to screen as a sanity check.\\n# load text\\nraw_text = load_doc( 'rhyme.txt ')\\nprint(raw_text)\\nListing 18.3: Load the document into memory.\\n18.3.3 Clean Text\\nNext, we need to clean the loaded text. We will not do much to it on this example. Speciﬁcally,\\nwe will strip all of the new line characters so that we have one long sequence of characters\\nseparated only by white space.\\n# clean\\ntokens = raw_text.split()\\nraw_text = ' '.join(tokens)\\nListing 18.4: Tokenize the loaded document.\\nYou may want to explore other methods for data cleaning, such as normalizing the case to\\nlowercase or removing punctuation in an eﬀort to reduce the ﬁnal vocabulary size and develop a\\nsmaller and leaner model.\\n18.3.4 Create Sequences\\nNow that we have a long list of characters, we can create our input-output sequences used to\\ntrain the model. Each input sequence will be 10 characters with one output character, making\\neach sequence 11 characters long. We can create the sequences by enumerating the characters\\nin the text, starting at the 11th character at index 10.\\n# organize into sequences of characters\\nlength = 10\\nsequences = list()\\nfor i in range(length, len(raw_text)):\\n# select sequence of tokens\\nseq = raw_text[i-length:i+1]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 216}, page_content=\"18.3. Data Preparation 200\\n# store\\nsequences.append(seq)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 18.5: Convert text into ﬁxed-length sequences.\\nRunning this snippet, we can see that we end up with just under 400 sequences of characters\\nfor training our language model.\\nTotal Sequences: 399\\nListing 18.6: Example output of converting text into ﬁxed-length sequences.\\n18.3.5 Save Sequences\\nFinally, we can save the prepared data to ﬁle so that we can load it later when we develop our\\nmodel. Below is a function save doc() that, given a list of strings and a ﬁlename, will save the\\nstrings to ﬁle, one per line.\\n# save tokens to file, one dialog per line\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nListing 18.7: Function to save sequences to ﬁle.\\nWe can call this function and save our prepared sequences to the ﬁlename char sequences.txt\\nin our current working directory.\\n# save sequences to file\\nout_filename = 'char_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 18.8: Call function to save sequences to ﬁle.\\n18.3.6 Complete Example\\nTying all of this together, the complete code listing is provided below.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# save tokens to file, one dialog per line\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 217}, page_content=\"18.4. Train Language Model 201\\nfile.write(data)\\nfile.close()\\n# load text\\nraw_text = load_doc( 'rhyme.txt ')\\nprint(raw_text)\\n# clean\\ntokens = raw_text.split()\\nraw_text = ' '.join(tokens)\\n# organize into sequences of characters\\nlength = 10\\nsequences = list()\\nfor i in range(length, len(raw_text)):\\n# select sequence of tokens\\nseq = raw_text[i-length:i+1]\\n# store\\nsequences.append(seq)\\nprint( 'Total Sequences: %d '% len(sequences))\\n# save sequences to file\\nout_filename = 'char_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 18.9: Complete example of preparing the text data.\\nRun the example to create the char sequences.txt ﬁle. Take a look inside you should see\\nsomething like the following:\\nSing a song\\ning a song\\nng a song o\\ng a song of\\na song of\\na song of s\\nsong of si\\nsong of six\\nong of sixp\\nng of sixpe\\n...\\nListing 18.10: Sample of the output ﬁle.\\nWe are now ready to train our character-based neural language model.\\n18.4 Train Language Model\\nIn this section, we will develop a neural language model for the prepared sequence data. The\\nmodel will read encoded characters and predict the next character in the sequence. A Long\\nShort-Term Memory recurrent neural network hidden layer will be used to learn the context\\nfrom the input sequence in order to make the predictions.\\n18.4.1 Load Data\\nThe ﬁrst step is to load the prepared character sequence data from char sequences.txt . We\\ncan use the same load doc() function developed in the previous section. Once loaded, we split\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 218}, page_content=\"18.4. Train Language Model 202\\nthe text by new line to give a list of sequences ready to be encoded.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load\\nin_filename = 'char_sequences.txt '\\nraw_text = load_doc(in_filename)\\nlines = raw_text.split( '\\\\n ')\\nListing 18.11: Load the prepared text data.\\n18.4.2 Encode Sequences\\nThe sequences of characters must be encoded as integers. This means that each unique character\\nwill be assigned a speciﬁc integer value and each sequence of characters will be encoded as a\\nsequence of integers. We can create the mapping given a sorted set of unique characters in the\\nraw input data. The mapping is a dictionary of character values to integer values.\\nchars = sorted(list(set(raw_text)))\\nmapping = dict((c, i) for i, c in enumerate(chars))\\nListing 18.12: Create a mapping between chars and integers.\\nNext, we can process each sequence of characters one at a time and use the dictionary\\nmapping to look up the integer value for each character.\\nsequences = list()\\nfor line in lines:\\n# integer encode line\\nencoded_seq = [mapping[char] for char in line]\\n# store\\nsequences.append(encoded_seq)\\nListing 18.13: Integer encode sequences of characters.\\nThe result is a list of integer lists. We need to know the size of the vocabulary later. We can\\nretrieve this as the size of the dictionary mapping.\\n# vocabulary size\\nvocab_size = len(mapping)\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 18.14: Summarize the size of the vocabulary.\\nRunning this piece, we can see that there are 38 unique characters in the input sequence\\ndata.\\nVocabulary Size: 38\\nListing 18.15: Example output from summarizing the size of the vocabulary.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 219}, page_content=\"18.4. Train Language Model 203\\n18.4.3 Split Inputs and Output\\nNow that the sequences have been integer encoded, we can separate the columns into input and\\noutput sequences of characters. We can do this using a simple array slice.\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\nListing 18.16: Split sequences into input and output elements.\\nNext, we need to one hot encode each character. That is, each character becomes a vector as\\nlong as the vocabulary (38 elements) with a 1 marked for the speciﬁc character. This provides\\na more precise input representation for the network. It also provides a clear objective for the\\nnetwork to predict, where a probability distribution over characters can be output by the model\\nand compared to the ideal case of all 0 values with a 1 for the actual next character. We can\\nuse the tocategorical() function in the Keras API to one hot encode the input and output\\nsequences.\\nsequences = [to_categorical(x, num_classes=vocab_size) for x in X]\\nX = array(sequences)\\ny = to_categorical(y, num_classes=vocab_size)\\nListing 18.17: Convert sequences into a format ready for training.\\nWe are now ready to ﬁt the model.\\n18.4.4 Fit Model\\nThe model is deﬁned with an input layer that takes sequences that have 10 time steps and 38\\nfeatures for the one hot encoded input sequences. Rather than specify these numbers, we use\\nthe second and third dimensions on the X input data. This is so that if we change the length of\\nthe sequences or size of the vocabulary, we do not need to change the model deﬁnition. The\\nmodel has a single LSTM hidden layer with 75 memory cells, chosen with a little trial and\\nerror. The model has a fully connected output layer that outputs one vector with a probability\\ndistribution across all characters in the vocabulary. A softmax activation function is used on\\nthe output layer to ensure the output has the properties of a probability distribution.\\n# define the model\\ndef define_model(X):\\nmodel = Sequential()\\nmodel.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 18.18: Deﬁne the language model.\\nThe model is learning a multiclass classiﬁcation problem, therefore we use the categorical log\\nloss intended for this type of problem. The eﬃcient Adam implementation of gradient descent\\nis used to optimize the model and accuracy is reported at the end of each batch update. The\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 220}, page_content=\"18.4. Train Language Model 204\\nmodel is ﬁt for 100 training epochs, again found with a little trial and error. Running this prints\\na summary of the deﬁned network as a sanity check.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nlstm_1 (LSTM) (None, 75) 34200\\n_________________________________________________________________\\ndense_1 (Dense) (None, 38) 2888\\n=================================================================\\nTotal params: 37,088\\nTrainable params: 37,088\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 18.19: Example output from summarizing the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\\nFigure 18.1: Plot of the deﬁned character-based language model.\\n18.4.5 Save Model\\nAfter the model is ﬁt, we save it to ﬁle for later use. The Keras model API provides the save()\\nfunction that we can use to save the model to a single ﬁle, including weights and topology\\ninformation.\\n# save the model to file\\nmodel.save( 'model.h5 ')\\nListing 18.20: Save the ﬁt model to ﬁle.\\nWe also save the mapping from characters to integers that we will need to encode any input\\nwhen using the model and decode any output from the model.\\n# save the mapping\\ndump(mapping, open( 'mapping.pkl ', 'wb '))\\nListing 18.21: Save the mapping of chars to integers to ﬁle.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 221}, page_content=\"18.4. Train Language Model 205\\n18.4.6 Complete Example\\nTying all of this together, the complete code listing for ﬁtting the character-based neural\\nlanguage model is listed below.\\nfrom numpy import array\\nfrom pickle import dump\\nfrom keras.utils import to_categorical\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# define the model\\ndef define_model(X):\\nmodel = Sequential()\\nmodel.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load\\nin_filename = 'char_sequences.txt '\\nraw_text = load_doc(in_filename)\\nlines = raw_text.split( '\\\\n ')\\n# integer encode sequences of characters\\nchars = sorted(list(set(raw_text)))\\nmapping = dict((c, i) for i, c in enumerate(chars))\\nsequences = list()\\nfor line in lines:\\n# integer encode line\\nencoded_seq = [mapping[char] for char in line]\\n# store\\nsequences.append(encoded_seq)\\n# vocabulary size\\nvocab_size = len(mapping)\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# separate into input and output\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\nsequences = [to_categorical(x, num_classes=vocab_size) for x in X]\\nX = array(sequences)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 222}, page_content=\"18.5. Generate Text 206\\ny = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(X)\\n# fit model\\nmodel.fit(X, y, epochs=100, verbose=2)\\n# save the model to file\\nmodel.save( 'model.h5 ')\\n# save the mapping\\ndump(mapping, open( 'mapping.pkl ', 'wb '))\\nListing 18.22: Complete example of training the language model.\\nRunning the example might take one minute. You will see that the model learns the problem\\nwell, perhaps too well for generating surprising sequences of characters.\\n...\\nEpoch 96/100\\n0s - loss: 0.2193 - acc: 0.9950\\nEpoch 97/100\\n0s - loss: 0.2124 - acc: 0.9950\\nEpoch 98/100\\n0s - loss: 0.2054 - acc: 0.9950\\nEpoch 99/100\\n0s - loss: 0.1982 - acc: 0.9950\\nEpoch 100/100\\n0s - loss: 0.1910 - acc: 0.9950\\nListing 18.23: Example output from training the language model.\\nAt the end of the run, you will have two ﬁles saved to the current working directory,\\nspeciﬁcally model.h5 andmapping.pkl . Next, we can look at using the learned model.\\n18.5 Generate Text\\nWe will use the learned language model to generate new sequences of text that have the same\\nstatistical properties.\\n18.5.1 Load Model\\nThe ﬁrst step is to load the model saved to the ﬁle model.h5 . We can use the load model()\\nfunction from the Keras API.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\nListing 18.24: Load the saved model.\\nWe also need to load the pickled dictionary for mapping characters to integers from the ﬁle\\nmapping.pkl . We will use the Pickle API to load the object.\\n# load the mapping\\nmapping = load(open( 'mapping.pkl ', 'rb '))\\nListing 18.25: Load the saved mapping from chars to integers.\\nWe are now ready to use the loaded model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 223}, page_content=\"18.5. Generate Text 207\\n18.5.2 Generate Characters\\nWe must provide sequences of 10 characters as input to the model in order to start the generation\\nprocess. We will pick these manually. A given input sequence will need to be prepared in the\\nsame way as preparing the training data for the model. First, the sequence of characters must\\nbe integer encoded using the loaded mapping.\\n# encode the characters as integers\\nencoded = [mapping[char] for char in in_text]\\nListing 18.26: Encode input text to integers.\\nNext, the integers need to be one hot encoded using the tocategorical() Keras function.\\nWe also need to reshape the sequence to be 3-dimensional, as we only have one sequence and\\nLSTMs require all input to be three dimensional (samples, time steps, features).\\n# one hot encode\\nencoded = to_categorical(encoded, num_classes=len(mapping))\\nencoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\\nListing 18.27: One hot encode the integer encoded text.\\nWe can then use the model to predict the next character in the sequence. We use\\npredict classes() instead of predict() to directly select the integer for the character with\\nthe highest probability instead of getting the full probability distribution across the entire set of\\ncharacters.\\n# predict character\\nyhat = model.predict_classes(encoded, verbose=0)\\nListing 18.28: Predict the next character in the sequence.\\nWe can then decode this integer by looking up the mapping to see the character to which it\\nmaps.\\nout_char = ''\\nfor char, index in mapping.items():\\nif index == yhat:\\nout_char = char\\nbreak\\nListing 18.29: Map the predicted integer back to a character.\\nThis character can then be added to the input sequence. We then need to make sure that the\\ninput sequence is 10 characters by truncating the ﬁrst character from the input sequence text.\\nWe can use the padsequences() function from the Keras API that can perform this truncation\\noperation. Putting all of this together, we can deﬁne a new function named generate seq()\\nfor using the loaded model to generate new sequences of text.\\n# generate a sequence of characters with a language model\\ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\\nin_text = seed_text\\n# generate a fixed number of characters\\nfor _ in range(n_chars):\\n# encode the characters as integers\\nencoded = [mapping[char] for char in in_text]\\n# truncate sequences to a fixed length\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 224}, page_content=\"18.5. Generate Text 208\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# one hot encode\\nencoded = to_categorical(encoded, num_classes=len(mapping))\\nencoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\\n# predict character\\nyhat = model.predict_classes(encoded, verbose=0)\\n# reverse map integer to character\\nout_char = ''\\nfor char, index in mapping.items():\\nif index == yhat:\\nout_char = char\\nbreak\\n# append to input\\nin_text += char\\nreturn in_text\\nListing 18.30: Function to predict a sequence of characters given seed text.\\n18.5.3 Complete Example\\nTying all of this together, the complete example for generating text using the ﬁt neural language\\nmodel is listed below.\\nfrom pickle import load\\nfrom keras.models import load_model\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.sequence import pad_sequences\\n# generate a sequence of characters with a language model\\ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\\nin_text = seed_text\\n# generate a fixed number of characters\\nfor _ in range(n_chars):\\n# encode the characters as integers\\nencoded = [mapping[char] for char in in_text]\\n# truncate sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# one hot encode\\nencoded = to_categorical(encoded, num_classes=len(mapping))\\nencoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\\n# predict character\\nyhat = model.predict_classes(encoded, verbose=0)\\n# reverse map integer to character\\nout_char = ''\\nfor char, index in mapping.items():\\nif index == yhat:\\nout_char = char\\nbreak\\n# append to input\\nin_text += out_char\\nreturn in_text\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# load the mapping\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 225}, page_content=\"18.6. Further Reading 209\\nmapping = load(open( 'mapping.pkl ', 'rb '))\\n# test start of rhyme\\nprint(generate_seq(model, mapping, 10, 'Sing a son ', 20))\\n# test mid-line\\nprint(generate_seq(model, mapping, 10, 'king was i ', 20))\\n# test not in original\\nprint(generate_seq(model, mapping, 10, 'hello worl ', 20))\\nListing 18.31: Complete example of generating characters with the ﬁt model.\\nRunning the example generates three sequences of text. The ﬁrst is a test to see how the\\nmodel does at starting from the beginning of the rhyme. The second is a test to see how well it\\ndoes at beginning in the middle of a line. The ﬁnal example is a test to see how well it does\\nwith a sequence of characters never seen before.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nSing a song of sixpence, A poc\\nking was in his counting house\\nhello worls e pake wofey. The\\nListing 18.32: Example output from generating sequences of characters.\\nWe can see that the model did very well with the ﬁrst two examples, as we would expect.\\nWe can also see that the model still generated something for the new text, but it is nonsense.\\n18.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Sing a Song of Sixpence on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Sing_a_Song_of_Sixpence\\n\\x88Keras Utils API.\\nhttps://keras.io/utils/\\n\\x88Keras Sequence Processing API.\\nhttps://keras.io/preprocessing/sequence/\\n18.7 Summary\\nIn this tutorial, you discovered how to develop a character-based neural language model.\\nSpeciﬁcally, you learned:\\n\\x88How to prepare text for character-based language modeling.\\n\\x88How to develop a character-based language model using LSTMs.\\n\\x88How to use a trained character-based language model to generate text.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 226}, page_content='18.7. Summary 210\\n18.7.1 Next\\nIn the next chapter, you will discover how you can develop a word-based neural language model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 227}, page_content='Chapter 19\\nHow to Develop a Word-Based Neural\\nLanguage Model\\nLanguage modeling involves predicting the next word in a sequence given the sequence of words\\nalready present. A language model is a key element in many natural language processing models\\nsuch as machine translation and speech recognition. The choice of how the language model is\\nframed must match how the language model is intended to be used. In this tutorial, you will\\ndiscover how the framing of a language model aﬀects the skill of the model when generating\\nshort sequences from a nursery rhyme. After completing this tutorial, you will know:\\n\\x88The challenge of developing a good framing of a word-based language model for a given\\napplication.\\n\\x88How to develop one-word, two-word, and line-based framings for word-based language\\nmodels.\\n\\x88How to generate sequences using a ﬁt language model.\\nLet’s get started.\\n19.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Framing Language Modeling\\n2. Jack and Jill Nursery Rhyme\\n3. Model 1: One-Word-In, One-Word-Out Sequences\\n4. Model 2: Line-by-Line Sequence\\n5. Model 3: Two-Words-In, One-Word-Out Sequence\\n211'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 228}, page_content='19.2. Framing Language Modeling 212\\n19.2 Framing Language Modeling\\nA statistical language model is learned from raw text and predicts the probability of the next\\nword in the sequence given the words already present in the sequence. Language models are\\na key component in larger models for challenging natural language processing problems, like\\nmachine translation and speech recognition. They can also be developed as standalone models\\nand used for generating new sequences that have the same statistical properties as the source\\ntext.\\nLanguage models both learn and predict one word at a time. The training of the network\\ninvolves providing sequences of words as input that are processed one at a time where a prediction\\ncan be made and learned for each input sequence. Similarly, when making predictions, the\\nprocess can be seeded with one or a few words, then predicted words can be gathered and\\npresented as input on subsequent predictions in order to build up a generated output sequence\\nTherefore, each model will involve splitting the source text into input and output sequences,\\nsuch that the model can learn to predict words. There are many ways to frame the sequences\\nfrom a source text for language modeling. In this tutorial, we will explore 3 diﬀerent ways of\\ndeveloping word-based language models in the Keras deep learning library. There is no single\\nbest approach, just diﬀerent framings that may suit diﬀerent applications.\\n19.3 Jack and Jill Nursery Rhyme\\nJack and Jill is a simple nursery rhyme. It is comprised of 4 lines, as follows:\\nJack and Jill went up the hill\\nTo fetch a pail of water\\nJack fell down and broke his crown\\nAnd Jill came tumbling after\\nListing 19.1: Jack and Jill nursery rhyme.\\nWe will use this as our source text for exploring diﬀerent framings of a word-based language\\nmodel. We can deﬁne this text in Python as follows:\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n\\nJack fell down and broke his crown\\\\n\\nAnd Jill came tumbling after\\\\n \"\"\"\\nListing 19.2: Sample text for this tutorial.\\n19.4 Model 1: One-Word-In, One-Word-Out Sequences\\nWe can start with a very simple model. Given one word as input, the model will learn to predict\\nthe next word in the sequence. For example:\\nX, y\\nJack, and\\nand, Jill\\nJill, went'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 229}, page_content=\"19.4. Model 1: One-Word-In, One-Word-Out Sequences 213\\n...\\nListing 19.3: Example of input and output pairs.\\nThe ﬁrst step is to encode the text as integers. Each lowercase word in the source text is\\nassigned a unique integer and we can convert the sequences of words to sequences of integers.\\nKeras provides the Tokenizer class that can be used to perform this encoding. First, the\\nTokenizer is ﬁt on the source text to develop the mapping from words to unique integers. Then\\nsequences of text can be converted to sequences of integers by calling the texts tosequences()\\nfunction.\\n# integer encode text\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\nencoded = tokenizer.texts_to_sequences([data])[0]\\nListing 19.4: Example of training a Tokenizer on the sample text.\\nWe will need to know the size of the vocabulary later for both deﬁning the word embedding\\nlayer in the model, and for encoding output words using a one hot encoding. The size of the\\nvocabulary can be retrieved from the trained Tokenizer by accessing the word index attribute.\\n# determine the vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 19.5: Summarize the size of the vocabulary.\\nRunning this example, we can see that the size of the vocabulary is 21 words. We add one,\\nbecause we will need to specify the integer for the largest encoded word as an array index, e.g.\\nwords encoded 1 to 21 with array indicies 0 to 21 or 22 positions. Next, we need to create\\nsequences of words to ﬁt the model with one word as input and one word as output.\\n# create word -> word sequences\\nsequences = list()\\nfor i in range(1, len(encoded)):\\nsequence = encoded[i-1:i+1]\\nsequences.append(sequence)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 19.6: Example of encoding the source text.\\nRunning this piece shows that we have a total of 24 input-output pairs to train the network.\\nTotal Sequences: 24\\nListing 19.7: Example of output of summarizing the encoded text.\\nWe can then split the sequences into input ( X) and output elements ( y). This is straightfor-\\nward as we only have two columns in the data.\\n# split into X and y elements\\nsequences = array(sequences)\\nX, y = sequences[:,0],sequences[:,1]\\nListing 19.8: Split the encoded text into input and output pairs.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 230}, page_content=\"19.4. Model 1: One-Word-In, One-Word-Out Sequences 214\\nWe will ﬁt our model to predict a probability distribution across all words in the vocabulary.\\nThat means that we need to turn the output element from a single integer into a one hot\\nencoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value.\\nThis gives the network a ground truth to aim for from which we can calculate error and update\\nthe model. Keras provides the tocategorical() function that we can use to convert the\\ninteger to a one hot encoding while specifying the number of classes as the vocabulary size.\\n# one hot encode outputs\\ny = to_categorical(y, num_classes=vocab_size)\\nListing 19.9: One hot encode the output words.\\nWe are now ready to deﬁne the neural network model. The model uses a learned word\\nembedding in the input layer. This has one real-valued vector for each word in the vocabulary,\\nwhere each word vector has a speciﬁed length. In this case we will use a 10-dimensional\\nprojection. The input sequence contains a single word, therefore the input length=1 . The\\nmodel has a single hidden LSTM layer with 50 units. This is far more than is needed. The\\noutput layer is comprised of one neuron for each word in the vocabulary and uses a softmax\\nactivation function to ensure the output is normalized to look like a probability.\\n# define the model\\ndef define_model(vocab_size):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 19.10: Deﬁne and compile the language model.\\nThe structure of the network can be summarized as follows:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 1, 10) 220\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 50) 12200\\n_________________________________________________________________\\ndense_1 (Dense) (None, 22) 1122\\n=================================================================\\nTotal params: 13,542\\nTrainable params: 13,542\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 19.11: Example output summarizing the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 231}, page_content=\"19.4. Model 1: One-Word-In, One-Word-Out Sequences 215\\nFigure 19.1: Plot of the deﬁned word-based language model.\\nWe will use this same general network structure for each example in this tutorial, with\\nminor changes to the learned embedding layer. We can compile and ﬁt the network on the\\nencoded text data. Technically, we are modeling a multiclass classiﬁcation problem (predict the\\nword in the vocabulary), therefore using the categorical cross entropy loss function. We use the\\neﬃcient Adam implementation of gradient descent and track accuracy at the end of each epoch.\\nThe model is ﬁt for 500 training epochs, again, perhaps more than is needed. The network\\nconﬁguration was not tuned for this and later experiments; an over-prescribed conﬁguration\\nwas chosen to ensure that we could focus on the framing of the language model.\\nAfter the model is ﬁt, we test it by passing it a given word from the vocabulary and\\nhaving the model predict the next word. Here we pass in ‘ Jack’ by encoding it and calling\\nmodel.predict classes() to get the integer output for the predicted word. This is then looked\\nup in the vocabulary mapping to give the associated word.\\n# evaluate\\nin_text = 'Jack '\\nprint(in_text)\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\nencoded = array(encoded)\\nyhat = model.predict_classes(encoded, verbose=0)\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nprint(word)\\nListing 19.12: Evaluate the ﬁt language model.\\nThis process could then be repeated a few times to build up a generated sequence of words.\\nTo make this easier, we wrap up the behavior in a function that we can call by passing in our\\nmodel and the seed word.\\n# generate a sequence from the model\\ndef generate_seq(model, tokenizer, seed_text, n_words):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 232}, page_content=\"19.4. Model 1: One-Word-In, One-Word-Out Sequences 216\\nin_text, result = seed_text, seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\nencoded = array(encoded)\\n# predict a word in the vocabulary\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text, result = out_word, result + ' ' + out_word\\nreturn result\\nListing 19.13: Function to generate output sequences given a ﬁt model.\\nWe can tie all of this together. The complete code listing is provided below.\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils import to_categorical\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\n# generate a sequence from the model\\ndef generate_seq(model, tokenizer, seed_text, n_words):\\nin_text, result = seed_text, seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\nencoded = array(encoded)\\n# predict a word in the vocabulary\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text, result = out_word, result + ' ' + out_word\\nreturn result\\n# define the model\\ndef define_model(vocab_size):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 233}, page_content='19.4. Model 1: One-Word-In, One-Word-Out Sequences 217\\n# compile network\\nmodel.compile(loss= \\'categorical_crossentropy \\', optimizer= \\'adam \\', metrics=[ \\'accuracy \\'])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= \\'model.png \\', show_shapes=True)\\nreturn model\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n\\nJack fell down and broke his crown\\\\n\\nAnd Jill came tumbling after\\\\n \"\"\"\\n# integer encode text\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\nencoded = tokenizer.texts_to_sequences([data])[0]\\n# determine the vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( \\'Vocabulary Size: %d \\'% vocab_size)\\n# create word -> word sequences\\nsequences = list()\\nfor i in range(1, len(encoded)):\\nsequence = encoded[i-1:i+1]\\nsequences.append(sequence)\\nprint( \\'Total Sequences: %d \\'% len(sequences))\\n# split into X and y elements\\nsequences = array(sequences)\\nX, y = sequences[:,0],sequences[:,1]\\n# one hot encode outputs\\ny = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(vocab_size)\\n# fit network\\nmodel.fit(X, y, epochs=500, verbose=2)\\n# evaluate\\nprint(generate_seq(model, tokenizer, \\'Jack \\', 6))\\nListing 19.14: Complete example of model1.\\nRunning the example prints the loss and accuracy each training epoch.\\n...\\nEpoch 496/500\\n0s - loss: 0.2358 - acc: 0.8750\\nEpoch 497/500\\n0s - loss: 0.2355 - acc: 0.8750\\nEpoch 498/500\\n0s - loss: 0.2352 - acc: 0.8750\\nEpoch 499/500\\n0s - loss: 0.2349 - acc: 0.8750\\nEpoch 500/500\\n0s - loss: 0.2346 - acc: 0.8750\\nListing 19.15: Example output of ﬁtting the language model.\\nWe can see that the model does not memorize the source sequences, likely because there is\\nsome ambiguity in the input sequences, for example:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 234}, page_content=\"19.5. Model 2: Line-by-Line Sequence 218\\njack => and\\njack => fell\\nListing 19.16: Example output of predicting the next word.\\nAnd so on. At the end of the run, Jack is passed in and a prediction or new sequence is\\ngenerated. We get a reasonable sequence as output that has some elements of the source.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nJack and jill came tumbling after down\\nListing 19.17: Example output of predicting a sequence of words.\\nThis is a good ﬁrst cut language model, but does not take full advantage of the LSTM’s\\nability to handle sequences of input and disambiguate some of the ambiguous pairwise sequences\\nby using a broader context.\\n19.5 Model 2: Line-by-Line Sequence\\nAnother approach is to split up the source text line-by-line, then break each line down into a\\nseries of words that build up. For example:\\nX, y\\n_, _, _, _, _, Jack, and\\n_, _, _, _, Jack, and, Jill\\n_, _, _, Jack, and, Jill, went\\n_, _, Jack, and, Jill, went, up\\n_, Jack, and, Jill, went, up, the\\nJack, and, Jill, went, up, the, hill\\nListing 19.18: Example framing of the problem as sequences of words.\\nThis approach may allow the model to use the context of each line to help the model in those\\ncases where a simple one-word-in-and-out model creates ambiguity. In this case, this comes at\\nthe cost of predicting words across lines, which might be ﬁne for now if we are only interested\\nin modeling and generating lines of text. Note that in this representation, we will require a\\npadding of sequences to ensure they meet a ﬁxed length input. This is a requirement when\\nusing Keras. First, we can create the sequences of integers, line-by-line by using the Tokenizer\\nalready ﬁt on the source text.\\n# create line-based sequences\\nsequences = list()\\nfor line in data.split( '\\\\n '):\\nencoded = tokenizer.texts_to_sequences([line])[0]\\nfor i in range(1, len(encoded)):\\nsequence = encoded[:i+1]\\nsequences.append(sequence)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 19.19: Example of preparing sequences of words.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 235}, page_content=\"19.5. Model 2: Line-by-Line Sequence 219\\nNext, we can pad the prepared sequences. We can do this using the padsequences()\\nfunction provided in Keras. This ﬁrst involves ﬁnding the longest sequence, then using that as\\nthe length by which to pad-out all other sequences.\\n# pad input sequences\\nmax_length = max([len(seq) for seq in sequences])\\nsequences = pad_sequences(sequences, maxlen=max_length, padding= 'pre ')\\nprint( 'Max Sequence Length: %d '% max_length)\\nListing 19.20: Example of padding sequences of words.\\nNext, we can split the sequences into input and output elements, much like before.\\n# split into input and output elements\\nsequences = array(sequences)\\nX, y = sequences[:,:-1],sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\nListing 19.21: Example of preparing the input and output sequences.\\nThe model can then be deﬁned as before, except the input sequences are now longer than a\\nsingle word. Speciﬁcally, they are maxlength-1 in length, -1 because when we calculated the\\nmaximum length of sequences, they included the input and output elements.\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=max_length-1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 19.22: Deﬁne and compile the language model.\\nWe can use the model to generate new sequences as before. The generate seq() function\\ncan be updated to build up an input sequence by adding predictions to the list of input words\\neach iteration.\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# pre-pad sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=max_length, padding= 'pre ')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 236}, page_content='19.5. Model 2: Line-by-Line Sequence 220\\nout_word = word\\nbreak\\n# append to input\\nin_text += \\' \\' + out_word\\nreturn in_text\\nListing 19.23: Function to generate sequences of words given input text.\\nTying all of this together, the complete code example is provided below.\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# pre-pad sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=max_length, padding= \\'pre \\')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = \\'\\'\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += \\' \\' + out_word\\nreturn in_text\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=max_length-1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= \\'softmax \\'))\\n# compile network\\nmodel.compile(loss= \\'categorical_crossentropy \\', optimizer= \\'adam \\', metrics=[ \\'accuracy \\'])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= \\'model.png \\', show_shapes=True)\\nreturn model\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 237}, page_content='19.5. Model 2: Line-by-Line Sequence 221\\nJack fell down and broke his crown\\\\n\\nAnd Jill came tumbling after\\\\n \"\"\"\\n# prepare the tokenizer on the source text\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\n# determine the vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( \\'Vocabulary Size: %d \\'% vocab_size)\\n# create line-based sequences\\nsequences = list()\\nfor line in data.split( \\'\\\\n \\'):\\nencoded = tokenizer.texts_to_sequences([line])[0]\\nfor i in range(1, len(encoded)):\\nsequence = encoded[:i+1]\\nsequences.append(sequence)\\nprint( \\'Total Sequences: %d \\'% len(sequences))\\n# pad input sequences\\nmax_length = max([len(seq) for seq in sequences])\\nsequences = pad_sequences(sequences, maxlen=max_length, padding= \\'pre \\')\\nprint( \\'Max Sequence Length: %d \\'% max_length)\\n# split into input and output elements\\nsequences = array(sequences)\\nX, y = sequences[:,:-1],sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(vocab_size, max_length)\\n# fit network\\nmodel.fit(X, y, epochs=500, verbose=2)\\n# evaluate model\\nprint(generate_seq(model, tokenizer, max_length-1, \\'Jack \\', 4))\\nprint(generate_seq(model, tokenizer, max_length-1, \\'Jill \\', 4))\\nListing 19.24: Complete example of model2.\\nRunning the example achieves a better ﬁt on the source data. The added context has allowed\\nthe model to disambiguate some of the examples. There are still two lines of text that start\\nwith “ Jack” that may still be a problem for the network.\\n...\\nEpoch 496/500\\n0s - loss: 0.1039 - acc: 0.9524\\nEpoch 497/500\\n0s - loss: 0.1037 - acc: 0.9524\\nEpoch 498/500\\n0s - loss: 0.1035 - acc: 0.9524\\nEpoch 499/500\\n0s - loss: 0.1033 - acc: 0.9524\\nEpoch 500/500\\n0s - loss: 0.1032 - acc: 0.9524\\nListing 19.25: Example output of ﬁtting the language model.\\nAt the end of the run, we generate two sequences with diﬀerent seed words: Jack and Jill.\\nThe ﬁrst generated line looks good, directly matching the source text. The second is a bit\\nstrange. This makes sense, because the network only ever saw Jillwithin an input sequence,\\nnot at the beginning of the sequence, so it has forced an output to use the word Jill, i.e. the'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 238}, page_content=\"19.6. Model 3: Two-Words-In, One-Word-Out Sequence 222\\nlast line of the rhyme.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nJack fell down and broke\\nJill jill came tumbling after\\nListing 19.26: Example output of generating sequences of words.\\nThis was a good example of how the framing may result in better new lines, but not good\\npartial lines of input.\\n19.6 Model 3: Two-Words-In, One-Word-Out Sequence\\nWe can use an intermediate between the one-word-in and the whole-sentence-in approaches\\nand pass in a sub-sequences of words as input. This will provide a trade-oﬀ between the two\\nframings allowing new lines to be generated and for generation to be picked up mid line. We will\\nuse 3 words as input to predict one word as output. The preparation of the sequences is much\\nlike the ﬁrst example, except with diﬀerent oﬀsets in the source sequence arrays, as follows:\\n# encode 2 words -> 1 word\\nsequences = list()\\nfor i in range(2, len(encoded)):\\nsequence = encoded[i-2:i+1]\\nsequences.append(sequence)\\nListing 19.27: Example of preparing constrained sequence data.\\nThe complete example is listed below\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# pre-pad sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=max_length, padding= 'pre ')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 239}, page_content='19.6. Model 3: Two-Words-In, One-Word-Out Sequence 223\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += \\' \\' + out_word\\nreturn in_text\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=max_length-1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= \\'softmax \\'))\\n# compile network\\nmodel.compile(loss= \\'categorical_crossentropy \\', optimizer= \\'adam \\', metrics=[ \\'accuracy \\'])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= \\'model.png \\', show_shapes=True)\\nreturn model\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n\\nJack fell down and broke his crown\\\\n\\nAnd Jill came tumbling after\\\\n \"\"\"\\n# integer encode sequences of words\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\nencoded = tokenizer.texts_to_sequences([data])[0]\\n# retrieve vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( \\'Vocabulary Size: %d \\'% vocab_size)\\n# encode 2 words -> 1 word\\nsequences = list()\\nfor i in range(2, len(encoded)):\\nsequence = encoded[i-2:i+1]\\nsequences.append(sequence)\\nprint( \\'Total Sequences: %d \\'% len(sequences))\\n# pad sequences\\nmax_length = max([len(seq) for seq in sequences])\\nsequences = pad_sequences(sequences, maxlen=max_length, padding= \\'pre \\')\\nprint( \\'Max Sequence Length: %d \\'% max_length)\\n# split into input and output elements\\nsequences = array(sequences)\\nX, y = sequences[:,:-1],sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(vocab_size, max_length)\\n# fit network\\nmodel.fit(X, y, epochs=500, verbose=2)\\n# evaluate model\\nprint(generate_seq(model, tokenizer, max_length-1, \\'Jack and \\', 5))\\nprint(generate_seq(model, tokenizer, max_length-1, \\'And Jill \\', 3))\\nprint(generate_seq(model, tokenizer, max_length-1, \\'fell down \\', 5))\\nprint(generate_seq(model, tokenizer, max_length-1, \\'pail of \\', 5))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 240}, page_content='19.7. Further Reading 224\\nListing 19.28: Complete example of model3.\\nRunning the example again gets a good ﬁt on the source text at around 95% accuracy.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n...\\nEpoch 496/500\\n0s - loss: 0.0685 - acc: 0.9565\\nEpoch 497/500\\n0s - loss: 0.0685 - acc: 0.9565\\nEpoch 498/500\\n0s - loss: 0.0684 - acc: 0.9565\\nEpoch 499/500\\n0s - loss: 0.0684 - acc: 0.9565\\nEpoch 500/500\\n0s - loss: 0.0684 - acc: 0.9565\\nListing 19.29: Example output of ﬁtting the language model.\\nWe look at 4 generation examples, two start of line cases and two starting mid line.\\nJack and jill went up the hill\\nAnd Jill went up the\\nfell down and broke his crown and\\npail of water jack fell down and\\nListing 19.30: Example output of generating sequences of words.\\nThe ﬁrst start of line case generated correctly, but the second did not. The second case was\\nan example from the 4th line, which is ambiguous with content from the ﬁrst line. Perhaps a\\nfurther expansion to 3 input words would be better. The two mid-line generation examples were\\ngenerated correctly, matching the source text.\\nWe can see that the choice of how the language model is framed and the requirements on\\nhow the model will be used must be compatible. That careful design is required when using\\nlanguage models in general, perhaps followed-up by spot testing with sequence generation to\\nconﬁrm model requirements have been met.\\n19.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Jack and Jill on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Jack_and_Jill_(nursery_rhyme)\\n\\x88Language Model on Wikpedia.\\nhttps://en.wikipedia.org/wiki/Language_model\\n\\x88Keras Embedding Layer API.\\nhttps://keras.io/layers/embeddings/#embedding'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 241}, page_content='19.8. Summary 225\\n\\x88Keras Text Processing API.\\nhttps://keras.io/preprocessing/text/\\n\\x88Keras Sequence Processing API.\\nhttps://keras.io/preprocessing/sequence/\\n\\x88Keras Utils API.\\nhttps://keras.io/utils/\\n19.8 Summary\\nIn this tutorial, you discovered how to develop diﬀerent word-based language models for a simple\\nnursery rhyme. Speciﬁcally, you learned:\\n\\x88The challenge of developing a good framing of a word-based language model for a given\\napplication.\\n\\x88How to develop one-word, two-word, and line-based framings for word-based language\\nmodels.\\n\\x88How to generate sequences using a ﬁt language model.\\n19.8.1 Next\\nIn the next chapter, you will discover how you can develop a word-based neural language model\\non a large corpus of text.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 242}, page_content='Chapter 20\\nProject: Develop a Neural Language\\nModel for Text Generation\\nA language model can predict the probability of the next word in the sequence, based on the\\nwords already observed in the sequence. Neural network models are a preferred method for\\ndeveloping statistical language models because they can use a distributed representation where\\ndiﬀerent words with similar meanings have similar representation and because they can use a\\nlarge context of recently observed words when making predictions. In this tutorial, you will\\ndiscover how to develop a statistical language model using deep learning in Python. After\\ncompleting this tutorial, you will know:\\n\\x88How to prepare text for developing a word-based language model.\\n\\x88How to design and ﬁt a neural language model with a learned embedding and an LSTM\\nhidden layer.\\n\\x88How to use the learned language model to generate new text with similar statistical\\nproperties as the source text.\\nLet’s get started.\\n20.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. The Republic by Plato\\n2. Data Preparation\\n3. Train Language Model\\n4. Use Language Model\\n226'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 243}, page_content='20.2. The Republic by Plato 227\\n20.2 The Republic by Plato\\nThe Republic is the classical Greek philosopher Plato’s most famous work. It is structured as a\\ndialog (e.g. conversation) on the topic of order and justice within a city state The entire text is\\navailable for free in the public domain. It is available on the Project Gutenberg website in a\\nnumber of formats. You can download the ASCII text version of the entire book (or books)\\nhere (you might need to open the URL twice):\\n\\x88Download The Republic by Plato.\\nhttp://www.gutenberg.org/cache/epub/1497/pg1497.txt\\nDownload the book text and place it in your current working directly with the ﬁlename\\nrepublic.txt . Open the ﬁle in a text editor and delete the front and back matter. This\\nincludes details about the book at the beginning, a long analysis, and license information at the\\nend. The text should begin with:\\nBOOK I.\\nI went down yesterday to the Piraeus with Glaucon the son of Ariston, ...\\nAnd end with:\\n... And it shall be well with us both in this life and in the pilgrimage of a thousand\\nyears which we have been describing.\\nSave the cleaned version as republic clean.txt in your current working directory. The\\nﬁle should be about 15,802 lines of text. Now we can develop a language model from this text.\\n20.3 Data Preparation\\nWe will start by preparing the data for modeling. The ﬁrst step is to look at the data.\\n20.3.1 Review the Text\\nOpen the text in an editor and just look at the text data. For example, here is the ﬁrst piece of\\ndialog:\\nBOOK I.\\nI went down yesterday to the Piraeus with Glaucon the son of Ariston, that I might\\noﬀer up my prayers to the goddess (Bendis, the Thracian Artemis.); and also because\\nI wanted to see in what manner they would celebrate the festival, which was a\\nnew thing. I was delighted with the procession of the inhabitants; but that of the\\nThracians was equally, if not more, beautiful. When we had ﬁnished our prayers\\nand viewed the spectacle, we turned in the direction of the city; and at that instant\\nPolemarchus the son of Cephalus chanced to catch sight of us from a distance as we\\nwere starting on our way home, and told his servant to run and bid us wait for him.\\nThe servant took hold of me by the cloak behind, and said: Polemarchus desires you\\nto wait.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 244}, page_content='20.3. Data Preparation 228\\nI turned round, and asked him where his master was.\\nThere he is, said the youth, coming after you, if you will only wait.\\nCertainly we will, said Glaucon; and in a few minutes Polemarchus appeared, and\\nwith him Adeimantus, Glaucon’s brother, Niceratus the son of Nicias, and several\\nothers who had been at the procession.\\nPolemarchus said to me: I perceive, Socrates, that you and your companion are\\nalready on your way to the city.\\nYou are not far wrong, I said.\\n...\\nWhat do you see that we will need to handle in preparing the data? Here’s what I see from\\na quick look:\\n\\x88Book/Chapter headings (e.g. BOOK I. ).\\n\\x88Lots of punctuation (e.g. -,;-,?-, and more).\\n\\x88Strange names (e.g. Polemarchus ).\\n\\x88Some long monologues that go on for hundreds of lines.\\n\\x88Some quoted dialog (e.g. ‘...’).\\nThese observations, and more, suggest at ways that we may wish to prepare the text data.\\nThe speciﬁc way we prepare the data really depends on how we intend to model it, which in\\nturn depends on how we intend to use it.\\n20.3.2 Language Model Design\\nIn this tutorial, we will develop a model of the text that we can then use to generate new\\nsequences of text. The language model will be statistical and will predict the probability of\\neach word given an input sequence of text. The predicted word will be fed in as input to in\\nturn generate the next word. A key design decision is how long the input sequences should be.\\nThey need to be long enough to allow the model to learn the context for the words to predict.\\nThis input length will also deﬁne the length of seed text used to generate new sequences when\\nwe use the model.\\nThere is no correct answer. With enough time and resources, we could explore the ability of\\nthe model to learn with diﬀerently sized input sequences. Instead, we will pick a length of 50\\nwords for the length of the input sequences, somewhat arbitrarily. We could process the data so\\nthat the model only ever deals with self-contained sentences and pad or truncate the text to\\nmeet this requirement for each input sequence. You could explore this as an extension to this\\ntutorial.\\nInstead, to keep the example brief, we will let all of the text ﬂow together and train the\\nmodel to predict the next word across sentences, paragraphs, and even books or chapters in the\\ntext. Now that we have a model design, we can look at transforming the raw text into sequences\\nof 100 input words to 1 output word, ready to ﬁt a model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 245}, page_content=\"20.3. Data Preparation 229\\n20.3.3 Load Text\\nThe ﬁrst step is to load the text into memory. We can develop a small function to load the\\nentire text ﬁle into memory and return it. The function is called load doc() and is listed below.\\nGiven a ﬁlename, it returns a sequence of loaded text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 20.1: Function to load text into memory.\\nUsing this function, we can load the cleaner version of the document in the ﬁle\\nrepublic clean.txt as follows:\\n# load document\\nin_filename = 'republic_clean.txt '\\ndoc = load_doc(in_filename)\\nprint(doc[:200])\\nListing 20.2: Example of loading the text into memory.\\nRunning this snippet loads the document and prints the ﬁrst 200 characters as a sanity\\ncheck.\\nBOOK I.\\nI went down yesterday to the Piraeus with Glaucon the son of Ariston,\\nthat I might offer up my prayers to the goddess (Bendis, the Thracian\\nArtemis.); and also because I wanted to see in what\\nListing 20.3: Example output of loading the text into memory.\\nSo far, so good. Next, let’s clean the text.\\n20.3.4 Clean Text\\nWe need to transform the raw text into a sequence of tokens or words that we can use as a\\nsource to train the model. Based on reviewing the raw text (above), below are some speciﬁc\\noperations we will perform to clean the text. You may want to explore more cleaning operations\\nyourself as an extension.\\n\\x88Replace ‘-’ with a white space so we can split words better.\\n\\x88Split words based on white space.\\n\\x88Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes\\n‘What’).\\n\\x88Remove all words that are not alphabetic to remove standalone punctuation tokens.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 246}, page_content=\"20.3. Data Preparation 230\\n\\x88Normalize all words to lowercase to reduce the vocabulary size.\\nVocabulary size is a big deal with language modeling. A smaller vocabulary results in a\\nsmaller model that trains faster. We can implement each of these cleaning operations in this\\norder in a function. Below is the function clean doc() that takes a loaded document as an\\nargument and returns an array of clean tokens.\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# replace '-- 'with a space ' '\\ndoc = doc.replace( '-- ', ' ')\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# make lower case\\ntokens = [word.lower() for word in tokens]\\nreturn tokens\\nListing 20.4: Function to clean text.\\nWe can run this cleaning operation on our loaded document and print out some of the tokens\\nand statistics as a sanity check.\\n# clean document\\ntokens = clean_doc(doc)\\nprint(tokens[:200])\\nprint( 'Total Tokens: %d '% len(tokens))\\nprint( 'Unique Tokens: %d '% len(set(tokens)))\\nListing 20.5: Example of cleaning text.\\nFirst, we can see a nice list of tokens that look cleaner than the raw text. We could remove\\nthe ’Book I’ chapter markers and more, but this is a good start.\\n[ 'book ', 'i ', 'i ', 'went ', 'down ', 'yesterday ', 'to ', 'the ', 'piraeus ', 'with ', 'glaucon ',\\n'the ', 'son ', 'of ', 'ariston ', 'that ', 'i ', 'might ', 'offer ', 'up ', 'my ', 'prayers ',\\n'to ', 'the ', 'goddess ', 'bendis ', 'the ', 'thracian ', 'artemis ', 'and ', 'also ',\\n'because ', 'i ', 'wanted ', 'to ', 'see ', 'in ', 'what ', 'manner ', 'they ', 'would ',\\n'celebrate ', 'the ', 'festival ', 'which ', 'was ', 'a ', 'new ', 'thing ', 'i ', 'was ',\\n'delighted ', 'with ', 'the ', 'procession ', 'of ', 'the ', 'inhabitants ', 'but ', 'that ',\\n'of ', 'the ', 'thracians ', 'was ', 'equally ', 'if ', 'not ', 'more ', 'beautiful ', 'when ',\\n'we ', 'had ', 'finished ', 'our ', 'prayers ', 'and ', 'viewed ', 'the ', 'spectacle ', 'we ',\\n'turned ', 'in ', 'the ', 'direction ', 'of ', 'the ', 'city ', 'and ', 'at ', 'that ',\\n'instant ', 'polemarchus ', 'the ', 'son ', 'of ', 'cephalus ', 'chanced ', 'to ', 'catch ',\\n'sight ', 'of ', 'us ', 'from ', 'a ', 'distance ', 'as ', 'we ', 'were ', 'starting ', 'on ',\\n'our ', 'way ', 'home ', 'and ', 'told ', 'his ', 'servant ', 'to ', 'run ', 'and ', 'bid ', 'us ',\\n'wait ', 'for ', 'him ', 'the ', 'servant ', 'took ', 'hold ', 'of ', 'me ', 'by ', 'the ',\\n'cloak ', 'behind ', 'and ', 'said ', 'polemarchus ', 'desires ', 'you ', 'to ', 'wait ', 'i ',\\n'turned ', 'round ', 'and ', 'asked ', 'him ', 'where ', 'his ', 'master ', 'was ', 'there ',\\n'he ', 'is ', 'said ', 'the ', 'youth ', 'coming ', 'after ', 'you ', 'if ', 'you ', 'will ',\\n'only ', 'wait ', 'certainly ', 'we ', 'will ', 'said ', 'glaucon ', 'and ', 'in ', 'a ', 'few ',\\n'minutes ', 'polemarchus ', 'appeared ', 'and ', 'with ', 'him ', 'adeimantus ', 'glaucons ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 247}, page_content=\"20.3. Data Preparation 231\\n'brother ', 'niceratus ', 'the ', 'son ', 'of ', 'nicias ', 'and ', 'several ', 'others ',\\n'who ', 'had ', 'been ', 'at ', 'the ', 'procession ', 'polemarchus ', 'said ']\\nListing 20.6: Example output of tokenized and clean text.\\nWe also get some statistics about the clean document. We can see that there are just under\\n120,000 words in the clean text and a vocabulary of just under 7,500 words. This is smallish\\nand models ﬁt on this data should be manageable on modest hardware.\\nTotal Tokens: 118684\\nUnique Tokens: 7409\\nListing 20.7: Example output summarizing properties of the clean text.\\nNext, we can look at shaping the tokens into sequences and saving them to ﬁle.\\n20.3.5 Save Clean Text\\nWe can organize the long list of tokens into sequences of 50 input words and 1 output word.\\nThat is, sequences of 51 words. We can do this by iterating over the list of tokens from token 51\\nonwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of\\nthe list of tokens. We will transform the tokens into space-separated strings for later storage\\nin a ﬁle. The code to split the list of clean tokens into sequences with a length of 51 tokens is\\nlisted below.\\n# organize into sequences of tokens\\nlength = 50 + 1\\nsequences = list()\\nfor i in range(length, len(tokens)):\\n# select sequence of tokens\\nseq = tokens[i-length:i]\\n# convert into a line\\nline = ' '.join(seq)\\n# store\\nsequences.append(line)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 20.8: Split document into sequences of text.\\nRunning this piece creates a long list of lines. Printing statistics on the list, we can see that\\nwe will have exactly 118,633 training patterns to ﬁt our model.\\nTotal Sequences: 118633\\nListing 20.9: Example output of splitting the document into sequences.\\nNext, we can save the sequences to a new ﬁle for later loading. We can deﬁne a new function\\nfor saving lines of text to a ﬁle. This new function is called save doc() and is listed below. It\\ntakes as input a list of lines and a ﬁlename. The lines are written, one per line, in ASCII format.\\n# save tokens to file, one dialog per line\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 248}, page_content=\"20.3. Data Preparation 232\\nListing 20.10: Function to save sequences of text to ﬁle.\\nWe can call this function and save our training sequences to the ﬁle republic sequences.txt .\\n# save sequences to file\\nout_filename = 'republic_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 20.11: Example of saving sequences to ﬁle.\\nTake a look at the ﬁle with your text editor. You will see that each line is shifted along one\\nword, with a new word at the end to be predicted; for example, here are the ﬁrst 3 lines in\\ntruncated form:\\nbook i i ... catch sight of\\ni i went ... sight of us\\ni went down ... of us from\\n...\\nListing 20.12: Example contents of sequences saved to ﬁle.\\n20.3.6 Complete Example\\nTying all of this together, the complete code listing is provided below.\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# replace '-- 'with a space ' '\\ndoc = doc.replace( '-- ', ' ')\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# make lower case\\ntokens = [word.lower() for word in tokens]\\nreturn tokens\\n# save tokens to file, one dialog per line\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 249}, page_content=\"20.4. Train Language Model 233\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\n# load document\\nin_filename = 'republic_clean.txt '\\ndoc = load_doc(in_filename)\\nprint(doc[:200])\\n# clean document\\ntokens = clean_doc(doc)\\nprint(tokens[:200])\\nprint( 'Total Tokens: %d '% len(tokens))\\nprint( 'Unique Tokens: %d '% len(set(tokens)))\\n# organize into sequences of tokens\\nlength = 50 + 1\\nsequences = list()\\nfor i in range(length, len(tokens)):\\n# select sequence of tokens\\nseq = tokens[i-length:i]\\n# convert into a line\\nline = ' '.join(seq)\\n# store\\nsequences.append(line)\\nprint( 'Total Sequences: %d '% len(sequences))\\n# save sequences to file\\nout_filename = 'republic_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 20.13: Complete example preparing text data for modeling.\\nYou should now have training data stored in the ﬁle republic sequences.txt in your\\ncurrent working directory. Next, let’s look at how to ﬁt a language model to this data.\\n20.4 Train Language Model\\nWe can now train a statistical language model from the prepared data. The model we will train\\nis a neural language model. It has a few unique characteristics:\\n\\x88It uses a distributed representation for words so that diﬀerent words with similar meanings\\nwill have a similar representation.\\n\\x88It learns the representation at the same time as learning the model.\\n\\x88It learns to predict the probability for the next word using the context of the last 100\\nwords.\\nSpeciﬁcally, we will use an Embedding Layer to learn the representation of words, and a\\nLong Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on\\ntheir context. Let’s start by loading our training data.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 250}, page_content=\"20.4. Train Language Model 234\\n20.4.1 Load Sequences\\nWe can load our training data using the load doc() function we developed in the previous\\nsection. Once loaded, we can split the data into separate training sequences by splitting based\\non new lines. The snippet below will load the republic sequences.txt data ﬁle from the\\ncurrent working directory.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\nListing 20.14: Load the clean sequences from ﬁle.\\nNext, we can encode the training data.\\n20.4.2 Encode Sequences\\nThe word embedding layer expects input sequences to be comprised of integers. We can map\\neach word in our vocabulary to a unique integer and encode our input sequences. Later, when\\nwe make predictions, we can convert the prediction to numbers and look up their associated\\nwords in the same mapping. To do this encoding, we will use the Tokenizer class in the Keras\\nAPI.\\nFirst, the Tokenizer must be trained on the entire training dataset, which means it ﬁnds\\nall of the unique words in the data and assigns each a unique integer. We can then use the ﬁt\\nTokenizer to encode all of the training sequences, converting each sequence from a list of words\\nto a list of integers.\\n# integer encode sequences of words\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nsequences = tokenizer.texts_to_sequences(lines)\\nListing 20.15: Train a tokenizer on the loaded sequences.\\nWe can access the mapping of words to integers as a dictionary attribute called word index\\non the Tokenizer object. We need to know the size of the vocabulary for deﬁning the embedding\\nlayer later. We can determine the vocabulary by calculating the size of the mapping dictionary.\\nWords are assigned values from 1 to the total number of words (e.g. 7,409). The Embedding\\nlayer needs to allocate a vector representation for each word in this vocabulary from index 1 to\\nthe largest index and because indexing of arrays is zero-oﬀset, the index of the word at the end\\nof the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length. Therefore,\\nwhen specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the\\nactual vocabulary.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 251}, page_content='20.4. Train Language Model 235\\n# vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nListing 20.16: Calculate the size of the vocabulary.\\n20.4.3 Sequence Inputs and Output\\nNow that we have encoded the input sequences, we need to separate them into input ( X)and\\noutput ( y) elements. We can do this with array slicing. After separating, we need to one hot\\nencode the output word. This means converting it from an integer to a vector of 0 values, one\\nfor each word in the vocabulary, with a 1 to indicate the speciﬁc word at the index of the words\\ninteger value.\\nThis is so that the model learns to predict the probability distribution for the next word and\\nthe ground truth from which to learn from is 0 for all words except the actual word that comes\\nnext. Keras provides the tocategorical() that can be used to one hot encode the output\\nwords for each input-output sequence pair.\\nFinally, we need to specify to the Embedding layer how long input sequences are. We know\\nthat there are 50 words because we designed the model, but a good generic way to specify that\\nis to use the second dimension (number of columns) of the input data’s shape. That way, if\\nyou change the length of sequences when preparing data, you do not need to change this data\\nloading code; it is generic.\\n# separate into input and output\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\nseq_length = X.shape[1]\\nListing 20.17: Split text data into input and output sequences.\\n20.4.4 Fit Model\\nWe can now deﬁne and ﬁt our language model on the training data. The learned embedding\\nneeds to know the size of the vocabulary and the length of input sequences as previously\\ndiscussed. It also has a parameter to specify how many dimensions will be used to represent\\neach word. That is, the size of the embedding vector space.\\nCommon values are 50, 100, and 300. We will use 50 here, but consider testing smaller or\\nlarger values. We will use a two LSTM hidden layers with 100 memory cells each. More memory\\ncells and a deeper network may achieve better results.\\nA dense fully connected layer with 100 neurons connects to the LSTM hidden layers to\\ninterpret the features extracted from the sequence. The output layer predicts the next word as\\na single vector the size of the vocabulary with a probability for each word in the vocabulary. A\\nsoftmax activation function is used to ensure the outputs have the characteristics of normalized\\nprobabilities.\\n# define the model\\ndef define_model(vocab_size, seq_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 50, input_length=seq_length))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 252}, page_content=\"20.4. Train Language Model 236\\nmodel.add(LSTM(100, return_sequences=True))\\nmodel.add(LSTM(100))\\nmodel.add(Dense(100, activation= 'relu '))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 20.18: Deﬁne the language model.\\nA summary of the deﬁned network is printed as a sanity check to ensure we have constructed\\nwhat we intended.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 50, 50) 370500\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 50, 100) 60400\\n_________________________________________________________________\\nlstm_2 (LSTM) (None, 100) 80400\\n_________________________________________________________________\\ndense_1 (Dense) (None, 100) 10100\\n_________________________________________________________________\\ndense_2 (Dense) (None, 7410) 748410\\n=================================================================\\nTotal params: 1,269,810\\nTrainable params: 1,269,810\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 20.19: Example output from summarizing the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 253}, page_content='20.4. Train Language Model 237\\nFigure 20.1: Plot of the deﬁned word-based language model.\\nThe model is compiled specifying the categorical cross entropy loss needed to ﬁt the model.\\nTechnically, the model is learning a multiclass classiﬁcation and this is the suitable loss function\\nfor this type of problem. The eﬃcient Adam implementation to mini-batch gradient descent\\nis used and accuracy is evaluated of the model. Finally, the model is ﬁt on the data for 100\\ntraining epochs with a modest batch size of 128 to speed things up. Training may take a few\\nhours on modern hardware without GPUs. You can speed it up with a larger batch size and/or\\nfewer training epochs.\\nDuring training, you will see a summary of performance, including the loss and accuracy\\nevaluated from the training data at the end of each batch update. You will get diﬀerent results,\\nbut perhaps an accuracy of just over 50% of predicting the next word in the sequence, which is\\nnot bad. We are not aiming for 100% accuracy (e.g. a model that memorized the text), but\\nrather a model that captures the essence of the text.\\n...\\nEpoch 96/100\\n118633/118633 [==============================] - 265s - loss: 2.0324 - acc: 0.5187\\nEpoch 97/100\\n118633/118633 [==============================] - 265s - loss: 2.0136 - acc: 0.5247\\nEpoch 98/100'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 254}, page_content=\"20.4. Train Language Model 238\\n118633/118633 [==============================] - 267s - loss: 1.9956 - acc: 0.5262\\nEpoch 99/100\\n118633/118633 [==============================] - 266s - loss: 1.9812 - acc: 0.5291\\nEpoch 100/100\\n118633/118633 [==============================] - 270s - loss: 1.9709 - acc: 0.5315\\nListing 20.20: Example output from training the language model.\\n20.4.5 Save Model\\nAt the end of the run, the trained model is saved to ﬁle. Here, we use the Keras model API to\\nsave the model to the ﬁle model.h5 in the current working directory. Later, when we load the\\nmodel to make predictions, we will also need the mapping of words to integers. This is in the\\nTokenizer object, and we can save that too using Pickle.\\n# save the model to file\\nmodel.save( 'model.h5 ')\\n# save the tokenizer\\ndump(tokenizer, open( 'tokenizer.pkl ', 'wb '))\\nListing 20.21: Save the ﬁt model and Tokenizer to ﬁle.\\n20.4.6 Complete Example\\nWe can put all of this together; the complete example for ﬁtting the language model is listed\\nbelow.\\nfrom numpy import array\\nfrom pickle import dump\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.utils import to_categorical\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# define the model\\ndef define_model(vocab_size, seq_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 50, input_length=seq_length))\\nmodel.add(LSTM(100, return_sequences=True))\\nmodel.add(LSTM(100))\\nmodel.add(Dense(100, activation= 'relu '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 255}, page_content=\"20.5. Use Language Model 239\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\n# integer encode sequences of words\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nsequences = tokenizer.texts_to_sequences(lines)\\n# vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\n# separate into input and output\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\nseq_length = X.shape[1]\\n# define model\\nmodel = define_model(vocab_size, seq_length)\\n# fit model\\nmodel.fit(X, y, batch_size=128, epochs=100)\\n# save the model to file\\nmodel.save( 'model.h5 ')\\n# save the tokenizer\\ndump(tokenizer, open( 'tokenizer.pkl ', 'wb '))\\nListing 20.22: Complete example training the language model.\\n20.5 Use Language Model\\nNow that we have a trained language model, we can use it. In this case, we can use it to generate\\nnew sequences of text that have the same statistical properties as the source text. This is not\\npractical, at least not for this example, but it gives a concrete example of what the language\\nmodel has learned. We will start by loading the training sequences again.\\n20.5.1 Load Data\\nWe can use the same code from the previous section to load the training data sequences of text.\\nSpeciﬁcally, the load doc() function.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 256}, page_content=\"20.5. Use Language Model 240\\nfile.close()\\nreturn text\\n# load cleaned text sequences\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\nListing 20.23: Load the clean sequences from ﬁle.\\nWe need the text so that we can choose a source sequence as input to the model for generating\\na new sequence of text. The model will require 50 words as input. Later, we will need to specify\\nthe expected length of input. We can determine this from the input sequences by calculating\\nthe length of one line of the loaded data and subtracting 1 for the expected output word that is\\nalso on the same line.\\nseq_length = len(lines[0].split()) - 1\\nListing 20.24: Calculate the expected input length.\\n20.5.2 Load Model\\nWe can now load the model from ﬁle. Keras provides the load model() function for loading\\nthe model, ready for use.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\nListing 20.25: Load the saved model from ﬁle.\\nWe can also load the tokenizer from ﬁle using the Pickle API.\\n# load the tokenizer\\ntokenizer = load(open( 'tokenizer.pkl ', 'rb '))\\nListing 20.26: Load the saved Tokenizer from ﬁle.\\nWe are ready to use the loaded model.\\n20.5.3 Generate Text\\nThe ﬁrst step in generating text is preparing a seed input. We will select a random line of text\\nfrom the input text for this purpose. Once selected, we will print it so that we have some idea\\nof what was used.\\n# select a seed text\\nseed_text = lines[randint(0,len(lines))]\\nprint(seed_text + '\\\\n ')\\nListing 20.27: Select random examples as seed text.\\nNext, we can generate new words, one at a time. First, the seed text must be encoded to\\nintegers using the same tokenizer that we used when training the model.\\nencoded = tokenizer.texts_to_sequences([seed_text])[0]\\nListing 20.28: Encode the selected seed text.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 257}, page_content=\"20.5. Use Language Model 241\\nThe model can predict the next word directly by calling model.predict classes() that\\nwill return the index of the word with the highest probability.\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\nListing 20.29: Predict the next word in the sequence.\\nWe can then look up the index in the Tokenizer ’s mapping to get the associated word.\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\nListing 20.30: Map the predicted integer to a word in the known vocabulary.\\nWe can then append this word to the seed text and repeat the process. Importantly, the\\ninput sequence is going to get too long. We can truncate it to the desired length after the input\\nsequence has been encoded to integers. Keras provides the padsequences() function that we\\ncan use to perform this truncation.\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\nListing 20.31: Pad the encoded sequence.\\nWe can wrap all of this into a function called generate seq() that takes as input the model,\\nthe tokenizer, input sequence length, the seed text, and the number of words to generate. It\\nthen returns a sequence of words generated by the model.\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, seq_length, seed_text, n_words):\\nresult = list()\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# truncate sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += ' ' + out_word\\nresult.append(out_word)\\nreturn ' '.join(result)\\nListing 20.32: Function to generate a sequence of words given the model and seed text.\\nWe are now ready to generate a sequence of new words given some seed text.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 258}, page_content=\"20.5. Use Language Model 242\\n# generate new text\\ngenerated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\\nprint(generated)\\nListing 20.33: Example of generating a sequence of text.\\nPutting this all together, the complete code listing for generating text from the learned-\\nlanguage model is listed below.\\nfrom random import randint\\nfrom pickle import load\\nfrom keras.models import load_model\\nfrom keras.preprocessing.sequence import pad_sequences\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, seq_length, seed_text, n_words):\\nresult = list()\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# truncate sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += ' ' + out_word\\nresult.append(out_word)\\nreturn ' '.join(result)\\n# load cleaned text sequences\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\nseq_length = len(lines[0].split()) - 1\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# load the tokenizer\\ntokenizer = load(open( 'tokenizer.pkl ', 'rb '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 259}, page_content=\"20.6. Extensions 243\\n# select a seed text\\nseed_text = lines[randint(0,len(lines))]\\nprint(seed_text + '\\\\n ')\\n# generate new text\\ngenerated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\\nprint(generated)\\nListing 20.34: Complete example of generating sequences of text.\\nRunning the example ﬁrst prints the seed text.\\nwhen he said that a man when he grows old may learn many things for he can no more learn\\nmuch than he can run much youth is the time for any extraordinary toil of course and\\ntherefore calculation and geometry and all the other elements of instruction which are a\\nListing 20.35: Example output from selecting seed text.\\nThen 50 words of generated text are printed.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\npreparation for dialectic should be presented to the name of idle spendthrifts of whom the\\nother is the manifold and the unjust and is the best and the other which delighted to\\nbe the opening of the soul of the soul and the embroiderer will have to be said at\\nListing 20.36: Example output of generated text.\\nYou can see that the text seems reasonable. In fact, the addition of concatenation would\\nhelp in interpreting the seed and the generated text. Nevertheless, the generated text gets the\\nright kind of words in the right kind of order. Try running the example a few times to see other\\nexamples of generated text.\\n20.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Contrived Seed Text . Hand craft or select seed text and evaluate how the seed text\\nimpacts the generated text, speciﬁcally the initial words or sentences generated.\\n\\x88Simplify Vocabulary . Explore a simpler vocabulary, perhaps with stemmed words or\\nstop words removed.\\n\\x88Data Cleaning . Consider using more or less cleaning of the text, perhaps leave in some\\npunctuation or perhaps replacing all fancy names with one or a handful. Evaluate how\\nthese changes to the size of the vocabulary impact the generated text.\\n\\x88Tune Model . Tune the model, such as the size of the embedding or number of memory\\ncells in the hidden layer, to see if you can develop a better model.\\n\\x88Deeper Model . Extend the model to have multiple LSTM hidden layers, perhaps with\\ndropout to see if you can develop a better model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 260}, page_content='20.7. Further Reading 244\\n\\x88Develop Pre-Trained Embedding . Extend the model to use pre-trained Word2Vec\\nvectors to see if it results in a better model.\\n\\x88Use GloVe Embedding . Use the GloVe word embedding vectors with and without ﬁne\\ntuning by the network and evaluate how it impacts training and the generated words.\\n\\x88Sequence Length . Explore training the model with diﬀerent length input sequences,\\nboth shorter and longer, and evaluate how it impacts the quality of the generated text.\\n\\x88Reduce Scope . Consider training the model on one book (chapter) or a subset of\\nthe original text and evaluate the impact on training, training speed and the resulting\\ngenerated text.\\n\\x88Sentence-Wise Model . Split the raw data based on sentences and pad each sentence\\nto a ﬁxed length (e.g. the longest sentence length).\\nIf you explore any of these extensions, I’d love to know.\\n20.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Project Gutenberg.\\nhttps://www.gutenberg.org/\\n\\x88The Republic by Plato on Project Gutenberg.\\nhttps://www.gutenberg.org/ebooks/1497\\n\\x88Republic (Plato) on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Republic_(Plato)\\n\\x88Language model on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Language_model\\n20.8 Summary\\nIn this tutorial, you discovered how to develop a word-based language model using a word\\nembedding and a recurrent neural network. Speciﬁcally, you learned:\\n\\x88How to prepare text for developing a word-based language model.\\n\\x88How to design and ﬁt a neural language model with a learned embedding and an LSTM\\nhidden layer.\\n\\x88How to use the learned language model to generate new text with similar statistical\\nproperties as the source text.\\n20.8.1 Next\\nThis is the ﬁnal chapter in the language modeling part. In the next part you will discover how\\nto develop automatic caption generation for photographs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 261}, page_content='Part VIII\\nImage Captioning\\n245'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 262}, page_content='Chapter 21\\nNeural Image Caption Generation\\nCaptioning an image involves generating a human readable textual description given an image,\\nsuch as a photograph. It is an easy problem for a human, but very challenging for a machine as\\nit involves both understanding the content of an image and how to translate this understanding\\ninto natural language. Recently, deep learning methods have displaced classical methods and\\nare achieving state-of-the-art results for the problem of automatically generating descriptions,\\ncalled captions , for images. In this chapter, you will discover how deep neural network models\\ncan be used to automatically generate descriptions for images, such as photographs. After\\ncompleting this chapter, you will know:\\n\\x88About the challenge of generating textual descriptions for images and the need to combine\\nbreakthroughs from computer vision and natural language processing.\\n\\x88About the elements that comprise a neural feature captioning model, namely the feature\\nextractor and language model.\\n\\x88How the elements of the model can be arranged into an Encoder-Decoder, possibly with\\nthe use of an attention mechanism.\\nLet’s get started.\\n21.1 Overview\\nThis tutorial is divided into the following parts:\\n1. Describing an Image with Text\\n2. Neural Captioning Model\\n3. Encoder-Decoder Architecture\\n21.2 Describing an Image with Text\\nDescribing an image is the problem of generating a human-readable textual description of an\\nimage, such as a photograph of an object or scene. The problem is sometimes called automatic\\nimage annotation orimage tagging . It is an easy problem for a human, but very challenging for\\na machine.\\n246'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 263}, page_content='21.3. Neural Captioning Model 247\\nA quick glance at an image is suﬃcient for a human to point out and describe an\\nimmense amount of details about the visual scene. However, this remarkable ability\\nhas proven to be an elusive task for our visual recognition models\\n—Deep Visual-Semantic Alignments for Generating Image Descriptions , 2015.\\nA solution requires both that the content of the image be understood and translated to\\nmeaning in the terms of words, and that the words must string together to be comprehensible.\\nIt combines both computer vision and natural language processing and marks a true challenging\\nproblem in broader artiﬁcial intelligence.\\nAutomatically describing the content of an image is a fundamental problem in\\nartiﬁcial intelligence that connects computer vision and natural language processing.\\n—Show and Tell: A Neural Image Caption Generator , 2015.\\nFurther, the problems can range in diﬃculty; let’s look at three diﬀerent variations on the\\nproblem with examples.\\n\\x88Classify Image . Assign an image a class label from one of many known classes.\\n\\x88Describe Image . Generate a textual description of the contents image.\\n\\x88Annotate Image . Generate textual descriptions for speciﬁc regions on the image.\\nThe general problem can also be extended to describe images over time in video. In this\\nchapter, we will focus our attention on describing images, which we will describe as image\\ncaptioning .\\n21.3 Neural Captioning Model\\nNeural network models have come to dominate the ﬁeld of automatic caption generation;\\nthis is primarily because the methods are demonstrating state-of-the-art results. The two\\ndominant methods prior to end-to-end neural network models for generating image captions\\nwere template-based methods and nearest-neighbor-based methods and modifying existing\\ncaptions.\\nPrior to the use of neural networks for generating captions, two main approaches were\\ndominant. The ﬁrst involved generating caption templates which were ﬁlled in based\\non the results of object detections and attribute discovery. The second approach\\nwas based on ﬁrst retrieving similar captioned images from a large database then\\nmodifying these retrieved captions to ﬁt the query. [...] Both of these approaches\\nhave since fallen out of favour to the now dominant neural network methods.\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nNeural network models for captioning involve two main elements:\\n1. Feature Extraction.\\n2. Language Model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 264}, page_content='21.3. Neural Captioning Model 248\\n21.3.1 Feature Extraction Model\\nThe feature extraction model is a neural network that given an image is able to extract the\\nsalient features, often in the form of a ﬁxed-length vector. The extracted features are an internal\\nrepresentation of the image, not something directly intelligible. A deep convolutional neural\\nnetwork, or CNN, is used as the feature extraction submodel. This network can be trained\\ndirectly on the images in the image captioning dataset. Alternately, a pre-trained model, such\\nas a state-of-the-art model used for image classiﬁcation, can be used, or some hybrid where a\\npre-trained model is used and ﬁne tuned on the problem. It is popular to use top performing\\nmodels in the ImageNet dataset developed for the ILSVRC challenge, such as the Oxford Vision\\nGeometry Group model, called VGG for short.\\n[...] we explored several techniques to deal with overﬁtting. The most obvious way\\nto not overﬁt is to initialize the weights of the CNN component of our system to a\\npretrained model (e.g., on ImageNet)\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nFigure 21.1: Feature Extractor\\n21.3.2 Language Model\\nGenerally, a language model predicts the probability of the next word in the sequence given\\nthe words already present in the sequence. For image captioning, the language model is a\\nneural network that given the extracted features from the network is capable of predicting the\\nsequence of words in the description and build up the description conditional on the words\\nthat have already been generated. It is popular to use a recurrent neural network, such as a\\nLong Short-Term Memory network, or LSTM, as the language model. Each output time step\\ngenerates a new word in the sequence. Each word that is generated is then encoded using a\\nword embedding (such as Word2Vec) and passed as input to the decoder for generating the\\nsubsequent word.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 265}, page_content='21.4. Encoder-Decoder Architecture 249\\nAn improvement to the model involves gathering the probability distribution of words\\nacross the vocabulary for the output sequence and searching it to generate multiple possible\\ndescriptions. These descriptions can be scored and ranked by likelihood. It is common to use a\\nBeam Search for this search. The language model can be trained standalone using pre-computed\\nfeatures extracted from the image dataset; it can be trained jointly with the feature extraction\\nnetwork, or some combination.\\nFigure 21.2: Language Model\\n21.4 Encoder-Decoder Architecture\\nA popular way to structure the sub-models is to use an Encoder-Decoder architecture where\\nboth models are trained jointly.\\n[the model] is based on a convolution neural network that encodes an image into\\na compact representation, followed by a recurrent neural network that generates\\na corresponding sentence. The model is trained to maximize the likelihood of the\\nsentence given the image.\\n—Show and Tell: A Neural Image Caption Generator , 2015.\\nThis is an architecture developed for machine translation where an input sequence, say in\\nFrench, is encoded as a ﬁxed-length vector by an encoder network. A separate decoder network\\nthen reads the encoding and generates an output sequence in the new language, say English.\\nA beneﬁt of this approach in addition to the impressive skill of the approach is that a single\\nend-to-end model can be trained on the problem. When adapted for image captioning, the\\nencoder network is a deep convolutional neural network, and the decoder network is a stack of\\nLSTM layers.\\n[in machine translation] An “encoder” RNN reads the source sentence and transforms\\nit into a rich ﬁxed-length vector representation, which in turn in used as the initial\\nhidden state of a “decoder” RNN that generates the target sentence. Here, we propose'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 266}, page_content='21.5. Further Reading 250\\nto follow this elegant recipe, replacing the encoder RNN by a deep convolution neural\\nnetwork (CNN).\\n—Show and Tell: A Neural Image Caption Generator , 2015.\\n21.4.1 Captioning Model with Attention\\nA limitation of the Encoder-Decoder architecture is that a single ﬁxed-length representation is\\nused to hold the extracted features. This was addressed in machine translation through the\\ndevelopment of attention across a richer encoding, allowing the decoder to learn where to place\\nattention as each word in the translation is generated. The approach of attention has also been\\nused to improve the performance of the Encoder-Decoder architecture for image captioning by\\nallowing the decoder to learn where to put attention in the image when generating each word in\\nthe description.\\nEncouraged by recent advances in caption generation and inspired by recent success\\nin employing attention in machine translation and object recognition we investigate\\nmodels that can attend to salient part of an image while generating its caption.\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nA beneﬁt of this approach is that it is possible to visualize exactly where attention is placed\\nwhile generating each word in a description.\\nWe also show through visualization how the model is able to automatically learn\\nto ﬁx its gaze on salient objects while generating the corresponding words in the\\noutput sequence.\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\n21.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n21.5.1 Papers\\n\\x88Show and Tell: A Neural Image Caption Generator , 2015.\\nhttps://arxiv.org/abs/1411.4555\\n\\x88Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nhttps://arxiv.org/abs/1502.03044\\n\\x88Long-term recurrent convolutional networks for visual recognition and description , 2015.\\nhttps://arxiv.org/abs/1411.4389\\n\\x88Deep Visual-Semantic Alignments for Generating Image Descriptions , 2015.\\nhttps://arxiv.org/abs/1412.2306'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 267}, page_content='21.6. Summary 251\\n21.5.2 Articles\\n\\x88Automatic image annotation on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Automatic_image_annotation\\n\\x88Show and Tell: image captioning open sourced in TensorFlow , 2016.\\nhttps://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.\\nhtml\\n\\x88Presentation: Automated Image Captioning with ConvNets and Recurrent Nets , Andrej\\nKarpathy and Fei-Fei Li.\\nhttps://www.youtube.com/watch?v=xKt21ucdBY0\\n21.5.3 Projects\\n\\x88Deep Visual-Semantic Alignments for Generating Image Descriptions , 2015.\\nhttp://cs.stanford.edu/people/karpathy/deepimagesent/\\n\\x88NeuralTalk2: Eﬃcient Image Captioning code in Torch, runs on GPU , Andrej Karpathy.\\nhttps://github.com/karpathy/neuraltalk2\\n21.6 Summary\\nIn this chapter, you discovered how deep neural network models can be used to automatically\\ngenerate descriptions for images, such as photographs. Speciﬁcally, you learned:\\n\\x88About the challenge of generating textual descriptions for images and the need to combine\\nbreakthroughs from computer vision and natural language processing.\\n\\x88About the elements that comprise a neural feature captioning model, namely the feature\\nextractor and language model.\\n\\x88How the elements of the model can be arranged into an Encoder-Decoder, possibly with\\nthe use of an attention mechanism.\\n21.6.1 Next\\nIn the next chapter, you will discover how you can develop neural caption models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 268}, page_content='Chapter 22\\nNeural Network Models for Caption\\nGeneration\\nCaption generation is a challenging artiﬁcial intelligence problem that draws on both computer\\nvision and natural language processing. The encoder-decoder recurrent neural network architec-\\nture has been shown to be eﬀective at this problem. The implementation of this architecture\\ncan be distilled into inject and merge based models, and both make diﬀerent assumptions about\\nthe role of the recurrent neural network in addressing the problem. In this chapter, you will\\ndiscover the inject and merge architectures for the encoder-decoder recurrent neural network\\nmodels on caption generation. After reading this chapter, you will know:\\n\\x88The challenge of caption generation and the use of the encoder-decoder architecture.\\n\\x88The inject model that combines the encoded image with each word to generate the next\\nword in the caption.\\n\\x88The merge model that separately encodes the image and description which are decoded in\\norder to generate the next word in the caption.\\nLet’s get started.\\n22.1 Image Caption Generation\\nThe problem of image caption generation involves outputting a readable and concise description\\nof the contents of a photograph. It is a challenging artiﬁcial intelligence problem as it requires\\nboth techniques from computer vision to interpret the contents of the photograph and techniques\\nfrom natural language processing to generate the textual description. Recently, deep learning\\nmethods have achieved state-of-the-art results on this challenging problem. The results are so\\nimpressive that this problem has become a standard demonstration problem for the capabilities\\nof deep learning.\\n22.1.1 Encoder-Decoder Architecture\\nA standard encoder-decoder recurrent neural network architecture is used to address the image\\ncaption generation problem. This involves two elements:\\n252'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 269}, page_content='22.2. Inject Model 253\\n\\x88Encoder : A network model that reads the photograph input and encodes the content\\ninto a ﬁxed-length vector using an internal representation.\\n\\x88Decoder : A network model that reads the encoded photograph and generates the textual\\ndescription output.\\nGenerally, a convolutional neural network is used to encode the images and a recurrent\\nneural network, such as a Long Short-Term Memory network, is used to either encode the\\ntext sequence generated so far, and/or generate the next word in the sequence. There are\\nmany ways to realize this architecture for the problem of caption generation. It is common\\nto use a pre-trained convolutional neural network model trained on a challenging photograph\\nclassiﬁcation problem to encode the photograph. The pre-trained model can be loaded, the\\noutput of the model removed, and the internal representation of the photograph used as the\\nencoding or internal representation of the input image.\\nIt is also common to frame the problem such that the model generates one word of the\\noutput textual description, given both the photograph and the description generated so far\\nas input. In this framing, the model is called recursively until the entire output sequence is\\ngenerated.\\nFigure 22.1: Recursive Framing of the Caption Generation Model. Taken from Where to put\\nthe Image in an Image Caption Generator .\\nThis framing can be implemented using one of two architectures, called by Marc Tanti, et al.\\nas the inject and the merge models.\\n22.2 Inject Model\\nThe inject model combines the encoded form of the image with each word from the text description\\ngenerated so-far. The approach uses the recurrent neural network as a text generation model\\nthat uses a sequence of both image and word information as input in order to generate the next\\nword in the sequence.\\nIn these ’inject’ architectures, the image vector (usually derived from the activation\\nvalues of a hidden layer in a convolutional neural network) is injected into the RNN,\\nfor example by treating the image vector on a par with a ’word’ and including it as\\npart of the caption preﬁx.\\n—Where to put the Image in an Image Caption Generator , 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 270}, page_content='22.3. Merge Model 254\\nFigure 22.2: Inject Architecture for Encoder-Decoder Model. Taken from What is the Role of\\nRecurrent Neural Networks (RNNs) in an Image Caption Generator? .\\nThis model combines the concerns of the image with each input word, requiring the encoder\\nto develop an encoding that incorporates both visual and linguistic information together.\\nIn an inject model, the RNN is trained to predict sequences based on histories\\nconsisting of both linguistic and perceptual features. Hence, in this model, the RNN\\nis primarily responsible for image-conditioned language generation.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\n22.3 Merge Model\\nThe merge model combines both the encoded form of the image input with the encoded form of\\nthe text description generated so far. The combination of these two encoded inputs is then used\\nby a very simple decoder model to generate the next word in the sequence. The approach uses\\nthe recurrent neural network only to encode the text generated so far.\\nIn the case of ‘merge’ architectures, the image is left out of the RNN subnetwork,\\nsuch that the RNN handles only the caption preﬁx, that is, handles only purely\\nlinguistic information. After the preﬁx has been vectorised, the image vector is then\\nmerged with the preﬁx vector in a separate ‘multimodal layer’ which comes after\\nthe RNN subnetwork\\n—Where to put the Image in an Image Caption Generator , 2017.\\nFigure 22.3: Merge Architecture for Encoder-Decoder Model. Taken from What is the Role of\\nRecurrent Neural Networks (RNNs) in an Image Caption Generator? .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 271}, page_content='22.4. More on the Merge Model 255\\nThis separates the concern of modeling the image input, the text input and the combining\\nand interpretation of the encoded inputs. As mentioned, it is common to use a pre-trained model\\nfor encoding the image, but similarly, this architecture also permits a pre-trained language\\nmodel to be used to encode the caption text input.\\n... in the merge architecture, RNNs in eﬀect encode linguistic representations,\\nwhich themselves constitute the input to a later prediction stage that comes after\\na multimodal layer. It is only at this late stage that image features are used to\\ncondition predictions\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nThere are multiple ways to combine the two encoded inputs, such as concatenation, multi-\\nplication, and addition, although experiments by Marc Tanti, et al. have shown addition to\\nwork better. Generally, Marc Tanti, et al. found the merge architecture to be more eﬀective\\ncompared to the inject approach.\\nOverall, the evidence suggests that delaying the merging of image features with\\nlinguistic encodings to a late stage in the architecture may be advantageous [...]\\nresults suggest that a merge architecture has a higher capacity than an inject\\narchitecture and can generate better quality captions with smaller layers.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\n22.4 More on the Merge Model\\nThe success of the merge model for the encoder-decoder architecture suggests that the role of\\nthe recurrent neural network is to encode input rather than generate output. This is a departure\\nfrom the common understanding where it is believed that the contribution of the recurrent\\nneural network is that of a generative model.\\nIf the RNN had the primary role of generating captions, then it would need to have\\naccess to the image in order to know what to generate. This does not seem to\\nbe the case as including the image into the RNN is not generally beneﬁcial to its\\nperformance as a caption generator.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nThe explicit comparison of the inject and merge models, and the success of merge over\\ninject for caption generation, raises the question of whether this approach translates to related\\nsequence-to-sequence generation problems. Instead of pre-trained models used to encode images,\\npre-trained language models could be used to encode source text in problems such as text\\nsummarization, question answering, and machine translation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 272}, page_content='22.5. Further Reading 256\\nWe would like to investigate whether similar changes in architecture would work in\\nsequence-to-sequence tasks such as machine translation, where instead of conditioning\\na language model on an image we are conditioning a target language model on\\nsentences in a source language.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\n22.5 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Marc Tanti’s Blog.\\nhttps://geekyisawesome.blogspot.com.au/\\n\\x88Where to put the Image in an Image Caption Generator , 2017.\\nhttps://arxiv.org/abs/1703.09137\\n\\x88What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nhttps://arxiv.org/abs/1708.02043\\n22.6 Summary\\nIn this chapter, you discovered the inject and merge architectures for the encoder-decoder\\nrecurrent neural network model on caption generation. Speciﬁcally, you learned:\\n\\x88The challenge of caption generation and the use of the encoder-decoder architecture.\\n\\x88The inject model that combines the encoded image with each word to generate the next\\nword in the caption.\\n\\x88The merge model that separately encodes the image and description which are decoded in\\norder to generate the next word in the caption.\\n22.6.1 Next\\nIn the next chapter, you will discover how you can load and re-use a pre-trained deep computer\\nvision model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 273}, page_content='Chapter 23\\nHow to Load and Use a Pre-Trained\\nObject Recognition Model\\nConvolutional neural networks are now capable of outperforming humans on some computer\\nvision tasks, such as classifying images. That is, given a photograph of an object, answer the\\nquestion as to which of 1,000 speciﬁc objects the photograph shows. A competition-winning\\nmodel for this task is the VGG model by researchers at Oxford. What is important about this\\nmodel, besides its capability of classifying objects in photographs, is that the model weights\\nare freely available and can be loaded and used in your own models and applications. In this\\ntutorial, you will discover the VGG convolutional neural network models for image classiﬁcation.\\nAfter completing this tutorial, you will know:\\n\\x88About the ImageNet dataset and competition and the VGG winning models.\\n\\x88How to load the VGG model in Keras and summarize its structure.\\n\\x88How to use the loaded VGG model to classifying objects in ad hoc photographs.\\nLet’s get started.\\n23.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. ImageNet\\n2. The Oxford VGG Models\\n3. Load the VGG Model in Keras\\n4. Develop a Simple Photo Classiﬁer\\nNote, Keras makes use of the Python Imaging Library or PIL library for manipulating\\nimages. Installation on your system may vary.\\n257'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 274}, page_content='23.2. ImageNet 258\\n23.2 ImageNet\\nImageNet is a research project to develop a large database of images with annotations, e.g.\\nimages and their descriptions. The images and their annotations have been the basis for an\\nimage classiﬁcation challenge called the ImageNet Large Scale Visual Recognition Challenge\\nor ILSVRC since 2010. The result is that research organizations battle it out on pre-deﬁned\\ndatasets to see who has the best model for classifying the objects in images.\\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object\\ncategory classiﬁcation and detection on hundreds of object categories and millions\\nof images. The challenge has been run annually from 2010 to present, attracting\\nparticipation from more than ﬁfty institutions.\\n—ImageNet Large Scale Visual Recognition Challenge , 2015.\\nFor the classiﬁcation task, images must be classiﬁed into one of 1,000 diﬀerent categories.\\nFor the last few years very deep convolutional neural network models have been used to win\\nthese challenges and results on the tasks have exceeded human performance.\\nFigure 23.1: Sample of Images from the ImageNet Dataset used in the ILSVRC Challenge.\\nTaken From ImageNet Large Scale Visual Recognition Challenge .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 275}, page_content='23.3. The Oxford VGG Models 259\\n23.3 The Oxford VGG Models\\nResearchers from the Oxford Visual Geometry Group, or VGG for short, participate in the\\nILSVRC challenge. In 2014, convolutional neural network models (CNN) developed by the\\nVGG won the image classiﬁcation tasks.\\nFigure 23.2: ILSVRC Results in 2014 for the Classiﬁcation task.\\nAfter the competition, the participants wrote up their ﬁndings in the paper Very Deep\\nConvolutional Networks for Large-Scale Image Recognition , 2014. They also made their models\\nand learned weights available online. This allowed other researchers and developers to use a\\nstate-of-the-art image classiﬁcation model in their own work and programs. This helped to fuel\\na rash of transfer learning work where pre-trained models are used with minor modiﬁcation\\non wholly new predictive modeling tasks, harnessing the state-of-the-art feature extraction\\ncapabilities of proven models.\\n... we come up with signiﬁcantly more accurate ConvNet architectures, which not\\nonly achieve the state-of-the-art accuracy on ILSVRC classiﬁcation and localisation\\ntasks, but are also applicable to other image recognition datasets, where they achieve\\nexcellent performance even when used as a part of a relatively simple pipelines (e.g.\\ndeep features classiﬁed by a linear SVM without ﬁne-tuning). We have released our\\ntwo best-performing models to facilitate further research.\\n—Very Deep Convolutional Networks for Large-Scale Image Recognition , 2014.\\nVGG released two diﬀerent CNN models, speciﬁcally a 16-layer model and a 19-layer model.\\nRefer to the paper for the full details of these models. The VGG models are not longer state-of-\\nthe-art by only a few percentage points. Nevertheless, they are very powerful models and useful\\nboth as image classiﬁers and as the basis for new models that use image inputs. In the next\\nsection, we will see how we can use the VGG model directly in Keras.\\n23.4 Load the VGG Model in Keras\\nThe VGG model can be loaded and used in the Keras deep learning library. Keras provides an\\nApplications interface for loading and using pre-trained models. Using this interface, you can'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 276}, page_content='23.4. Load the VGG Model in Keras 260\\ncreate a VGG model using the pre-trained weights provided by the Oxford group and use it as\\na starting point in your own model, or use it as a model directly for classifying images. In this\\ntutorial, we will focus on the use case of classifying new images using the VGG model. Keras\\nprovides both the 16-layer and 19-layer version via the VGG16 and VGG19 classes. Let’s focus\\non the VGG16 model. The model can be created as follows:\\nfrom keras.applications.vgg16 import VGG16\\nmodel = VGG16()\\nListing 23.1: Create the VGG16 model in Keras.\\nThat’s it. The ﬁrst time you run this example, Keras will download the weight ﬁles from\\nthe Internet and store them in the ∼/.keras/models directory. Note that the weights are\\nabout 528 megabytes, so the download may take a few minutes depending on the speed of your\\nInternet connection.\\nThe weights are only downloaded once. The next time you run the example, the weights are\\nloaded locally and the model should be ready to use in seconds. We can use the standard Keras\\ntools for inspecting the model structure. For example, you can print a summary of the network\\nlayers as follows:\\nfrom keras.applications.vgg16 import VGG16\\nmodel = VGG16()\\nmodel.summary()\\nListing 23.2: Create and summarize the VGG16 model.\\nYou can see that the model is huge. You can also see that, by default, the model expects\\nimages as input with the size 224 x 224 pixels with 3 channels (e.g. color).\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 224, 224, 3) 0\\n_________________________________________________________________\\nblock1_conv1 (Conv2D) (None, 224, 224, 64) 1792\\n_________________________________________________________________\\nblock1_conv2 (Conv2D) (None, 224, 224, 64) 36928\\n_________________________________________________________________\\nblock1_pool (MaxPooling2D) (None, 112, 112, 64) 0\\n_________________________________________________________________\\nblock2_conv1 (Conv2D) (None, 112, 112, 128) 73856\\n_________________________________________________________________\\nblock2_conv2 (Conv2D) (None, 112, 112, 128) 147584\\n_________________________________________________________________\\nblock2_pool (MaxPooling2D) (None, 56, 56, 128) 0\\n_________________________________________________________________\\nblock3_conv1 (Conv2D) (None, 56, 56, 256) 295168\\n_________________________________________________________________\\nblock3_conv2 (Conv2D) (None, 56, 56, 256) 590080\\n_________________________________________________________________\\nblock3_conv3 (Conv2D) (None, 56, 56, 256) 590080\\n_________________________________________________________________\\nblock3_pool (MaxPooling2D) (None, 28, 28, 256) 0\\n_________________________________________________________________\\nblock4_conv1 (Conv2D) (None, 28, 28, 512) 1180160\\n_________________________________________________________________'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 277}, page_content=\"23.4. Load the VGG Model in Keras 261\\nblock4_conv2 (Conv2D) (None, 28, 28, 512) 2359808\\n_________________________________________________________________\\nblock4_conv3 (Conv2D) (None, 28, 28, 512) 2359808\\n_________________________________________________________________\\nblock4_pool (MaxPooling2D) (None, 14, 14, 512) 0\\n_________________________________________________________________\\nblock5_conv1 (Conv2D) (None, 14, 14, 512) 2359808\\n_________________________________________________________________\\nblock5_conv2 (Conv2D) (None, 14, 14, 512) 2359808\\n_________________________________________________________________\\nblock5_conv3 (Conv2D) (None, 14, 14, 512) 2359808\\n_________________________________________________________________\\nblock5_pool (MaxPooling2D) (None, 7, 7, 512) 0\\n_________________________________________________________________\\nflatten (Flatten) (None, 25088) 0\\n_________________________________________________________________\\nfc1 (Dense) (None, 4096) 102764544\\n_________________________________________________________________\\nfc2 (Dense) (None, 4096) 16781312\\n_________________________________________________________________\\npredictions (Dense) (None, 1000) 4097000\\n=================================================================\\nTotal params: 138,357,544\\nTrainable params: 138,357,544\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 23.3: Output summary for the VGG16 model.\\nWe can also create a plot of the layers in the VGG model, as follows:\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.utils.vis_utils import plot_model\\nmodel = VGG16()\\nplot_model(model, to_file= 'vgg.png ')\\nListing 23.4: Create and plot the graph of the VGG16 model.\\nAgain, because the model is large, the plot is a little too large and perhaps unreadable.\\nNevertheless, it is provided below.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 278}, page_content='23.4. Load the VGG Model in Keras 262\\nFigure 23.3: Plot of Layers in the VGG Model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 279}, page_content='23.5. Develop a Simple Photo Classiﬁer 263\\nThe VGG() class takes a few arguments that may only interest you if you are looking to use\\nthe model in your own project, e.g. for transfer learning. For example:\\n\\x88include top(True): Whether or not to include the output layers for the model. You\\ndon’t need these if you are ﬁtting the model on your own problem.\\n\\x88weights (‘imagenet’ ): What weights to load. You can specify None to not load pre-\\ntrained weights if you are interested in training the model yourself from scratch.\\n\\x88input tensor (None ): A new input layer if you intend to ﬁt the model on new data of a\\ndiﬀerent size.\\n\\x88input shape (None ): The size of images that the model is expected to take if you change\\nthe input layer.\\n\\x88pooling (None): The type of pooling to use when you are training a new set of output\\nlayers.\\n\\x88classes (1000 ): The number of classes (e.g. size of output vector) for the model.\\nNext, let’s look at using the loaded VGG model to classify ad hoc photographs.\\n23.5 Develop a Simple Photo Classiﬁer\\nLet’s develop a simple image classiﬁcation script.\\n23.5.1 Get a Sample Image\\nFirst, we need an image we can classify. You can download a random photograph of a coﬀee\\nmug from Flickr.\\nFigure 23.4: Coﬀee Mug. Photo by jfanaian , some rights reserved.\\nDownload the image and save it to your current working directory with the ﬁlename mug.jpg .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 280}, page_content=\"23.5. Develop a Simple Photo Classiﬁer 264\\n23.5.2 Load the VGG Model\\nLoad the weights for the VGG-16 model, as we did in the previous section.\\nfrom keras.applications.vgg16 import VGG16\\n# load the model\\nmodel = VGG16()\\nListing 23.5: Create the VGG16 model.\\n23.5.3 Load and Prepare Image\\nNext, we can load the image as pixel data and prepare it to be presented to the network. Keras\\nprovides some tools to help with this step. First, we can use the load img() function to load\\nthe image and resize it to the required size of 224 x 224 pixels.\\nfrom keras.preprocessing.image import load_img\\n# load an image from file\\nimage = load_img( 'mug.jpg ', target_size=(224, 224))\\nListing 23.6: Load and resize the image.\\nNext, we can convert the pixels to a NumPy array so that we can work with it in Keras. We\\ncan use the imgtoarray() function for this.\\nfrom keras.preprocessing.image import img_to_array\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\\nListing 23.7: Convert the image pixels to a NumPy array.\\nThe network expects one or more images as input; that means the input array will need to\\nbe 4-dimensional: samples, rows, columns, and channels. We only have one sample (one image).\\nWe can reshape the array by calling reshape() and adding the extra dimension.\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\nListing 23.8: Reshape the NumPy array of pixels.\\nNext, the image pixels need to be prepared in the same way as the ImageNet training data\\nwas prepared. Speciﬁcally, from the paper:\\nThe only preprocessing we do is subtracting the mean RGB value, computed on the\\ntraining set, from each pixel.\\n—Very Deep Convolutional Networks for Large-Scale Image Recognition , 2014.\\nKeras provides a function called preprocess input() to prepare new input for the network.\\nfrom keras.applications.vgg16 import preprocess_input\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\nListing 23.9: Pre-process the pixel data for the model.\\nWe are now ready to make a prediction for our loaded and prepared image.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 281}, page_content=\"23.5. Develop a Simple Photo Classiﬁer 265\\n23.5.4 Make a Prediction\\nWe can call the predict() function on the model in order to get a prediction of the probability\\nof the image belonging to each of the 1,000 known object types.\\n# predict the probability across all output classes\\nyhat = model.predict(image)\\nListing 23.10: Classify the image with the VGG16 model.\\nNearly there, now we need to interpret the probabilities.\\n23.5.5 Interpret Prediction\\nKeras provides a function to interpret the probabilities called decode predictions() . It can\\nreturn a list of classes and their probabilities in case you would like to present the top 3 objects\\nthat may be in the photo. We will just report the ﬁrst most likely object.\\nfrom keras.applications.vgg16 import decode_predictions\\n# convert the probabilities to class labels\\nlabel = decode_predictions(yhat)\\n# retrieve the most likely result, e.g. highest probability\\nlabel = label[0][0]\\n# print the classification\\nprint( '%s (%.2f%%) '% (label[1], label[2]*100))\\nListing 23.11: Interpret the prediction probabilities.\\nAnd that’s it.\\n23.5.6 Complete Example\\nTying all of this together, the complete example is listed below:\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.applications.vgg16 import decode_predictions\\nfrom keras.applications.vgg16 import VGG16\\n# load the model\\nmodel = VGG16()\\n# load an image from file\\nimage = load_img( 'mug.jpg ', target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# predict the probability across all output classes\\nyhat = model.predict(image)\\n# convert the probabilities to class labels\\nlabel = decode_predictions(yhat)\\n# retrieve the most likely result, e.g. highest probability\\nlabel = label[0][0]\\n# print the classification\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 282}, page_content=\"23.6. Further Reading 266\\nprint( '%s (%.2f%%) '% (label[1], label[2]*100))\\nListing 23.12: Complete example for classifying an image with the VGG model.\\nRunning the example, we can see that the image is correctly classiﬁed as a coﬀee mug with\\na 75% likelihood.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\ncoffee_mug (75.27%)\\nListing 23.13: Sample output of making a prediction for the image.\\n23.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88ImageNet.\\nhttp://www.image-net.org/\\n\\x88ImageNet on Wikipedia.\\nhttps://en.wikipedia.org/wiki/ImageNet\\n\\x88Very Deep Convolutional Networks for Large-Scale Image Recognition , 2015.\\nhttps://arxiv.org/abs/1409.1556\\n\\x88Very Deep Convolutional Networks for Large-Scale Visual Recognition , at Oxford.\\nhttp://www.robots.ox.ac.uk/ ~vgg/research/very_deep/\\n\\x88Building powerful image classiﬁcation models using very little data , 2016.\\nhttps://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.\\nhtml\\n\\x88Keras Applications API.\\nhttps://keras.io/applications/\\n\\x88Keras weight ﬁles ﬁles.\\nhttps://github.com/fchollet/deep-learning-models/releases/\\n23.7 Summary\\nIn this tutorial, you discovered the VGG convolutional neural network models for image\\nclassiﬁcation. Speciﬁcally, you learned:\\n\\x88About the ImageNet dataset and competition and the VGG winning models.\\n\\x88How to load the VGG model in Keras and summarize its structure.\\n\\x88How to use the loaded VGG model to classifying objects in ad hoc photographs.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 283}, page_content='23.7. Summary 267\\n23.7.1 Next\\nIn the next chapter, you will discover how you can evaluate generated text against a ground\\ntruth.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 284}, page_content='Chapter 24\\nHow to Evaluate Generated Text With\\nthe BLEU Score\\nBLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation\\nof text to one or more reference translations. Although developed for translation, it can be used\\nto evaluate text generated for a suite of natural language processing tasks. In this tutorial, you\\nwill discover the BLEU score for evaluating and scoring candidate text using the NLTK library\\nin Python. After completing this tutorial, you will know:\\n\\x88A gentle introduction to the BLEU score and an intuition for what is being calculated.\\n\\x88How you can calculate BLEU scores in Python using the NLTK library for sentences and\\ndocuments.\\n\\x88How you can use a suite of small examples to develop an intuition for how diﬀerences\\nbetween a candidate and reference text impact the ﬁnal BLEU score.\\nLet’s get started.\\n24.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Bilingual Evaluation Understudy Score\\n2. Calculate BLEU Scores\\n3. Cumulative and Individual BLEU Scores\\n4. Worked Examples\\n24.2 Bilingual Evaluation Understudy Score\\nThe Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a\\ngenerated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a\\nperfect mismatch results in a score of 0.0. The score was developed for evaluating the predictions\\n268'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 285}, page_content='24.2. Bilingual Evaluation Understudy Score 269\\nmade by automatic machine translation systems. It is not perfect, but does oﬀer 5 compelling\\nbeneﬁts:\\n\\x88It is quick and inexpensive to calculate.\\n\\x88It is easy to understand.\\n\\x88It is language independent.\\n\\x88It correlates highly with human evaluation.\\n\\x88It has been widely adopted.\\nThe BLEU score was proposed by Kishore Papineni, et al. in their 2002 paper BLEU: a\\nMethod for Automatic Evaluation of Machine Translation . The approach works by counting\\nmatching n-grams in the candidate translation to n-grams in the reference text, where 1-gram\\nor unigram would be each token and a bigram comparison would be each word pair. The\\ncomparison is made regardless of word order.\\nThe primary programming task for a BLEU implementor is to compare n-grams of\\nthe candidate with the n-grams of the reference translation and count the number\\nof matches. These matches are position-independent. The more the matches, the\\nbetter the candidate translation is.\\n—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nThe counting of matching n-grams is modiﬁed to ensure that it takes the occurrence of the\\nwords in the reference text into account, not rewarding a candidate translation that generates\\nan abundance of reasonable words. This is referred to in the paper as modiﬁed n-gram precision.\\nUnfortunately, MT systems can overgenerate “reasonable” words, resulting in im-\\nprobable, but high-precision, translations [...] Intuitively the problem is clear: a\\nreference word should be considered exhausted after a matching candidate word is\\nidentiﬁed. We formalize this intuition as the modiﬁed unigram precision.\\n—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nThe score is for comparing sentences, but a modiﬁed version that normalizes n-grams by\\ntheir occurrence is also proposed for better scoring blocks of multiple sentences.\\nWe ﬁrst compute the n-gram matches sentence by sentence. Next, we add the clipped\\nn-gram counts for all the candidate sentences and divide by the number of candidate\\nn-grams in the test corpus to compute a modiﬁed precision score, pn, for the entire\\ntest corpus.\\n—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nA perfect score is not possible in practice as a translation would have to match the reference\\nexactly. This is not even possible by human translators. The number and quality of the\\nreferences used to calculate the BLEU score means that comparing scores across datasets can\\nbe troublesome.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 286}, page_content=\"24.3. Calculate BLEU Scores 270\\nThe BLEU metric ranges from 0 to 1. Few translations will attain a score of 1\\nunless they are identical to a reference translation. For this reason, even a human\\ntranslator will not necessarily score 1. [...] on a test corpus of about 500 sentences\\n(40 general news stories), a human translator scored 0.3468 against four references\\nand scored 0.2571 against two references.\\n—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nIn addition to translation, we can use the BLEU score for other language generation problems\\nwith deep learning methods such as:\\n\\x88Language generation.\\n\\x88Image caption generation.\\n\\x88Text summarization.\\n\\x88Speech recognition.\\n\\x88And much more.\\n24.3 Calculate BLEU Scores\\nThe Python Natural Language Toolkit library, or NLTK, provides an implementation of the\\nBLEU score that you can use to evaluate your generated text against a reference.\\n24.3.1 Sentence BLEU Score\\nNLTK provides the sentence bleu() function for evaluating a candidate sentence against one\\nor more reference sentences. The reference sentences must be provided as a list of sentences\\nwhere each reference is a list of tokens. The candidate sentence is provided as a list of tokens.\\nFor example:\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'a ', 'test '], [ 'this ', 'is ' 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.1: Example of calculating a sentence BLEU score.\\nRunning this example prints a perfect score as the candidate matches one of the references\\nexactly.\\n1.0\\nListing 24.2: Sample output of calculating the sentence BLEU score.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 287}, page_content=\"24.4. Cumulative and Individual BLEU Scores 271\\n24.3.2 Corpus BLEU Score\\nNLTK also provides a function called corpus bleu() for calculating the BLEU score for multiple\\nsentences such as a paragraph or a document. The references must be speciﬁed as a list of\\ndocuments where each document is a list of references and each alternative reference is a list of\\ntokens, e.g. a list of lists of lists of tokens. The candidate documents must be speciﬁed as a list\\nwhere each document is a list of tokens, e.g. a list of lists of tokens. This is a little confusing;\\nhere is an example of two references for one document.\\n# two references for one document\\nfrom nltk.translate.bleu_score import corpus_bleu\\nreferences = [[[ 'this ', 'is ', 'a ', 'test '], [ 'this ', 'is ' 'test ']]]\\ncandidates = [[ 'this ', 'is ', 'a ', 'test ']]\\nscore = corpus_bleu(references, candidates)\\nprint(score)\\nListing 24.3: Example of calculating a corpus BLEU score.\\nRunning the example prints a perfect score as before.\\n1.0\\nListing 24.4: Sample output of calculating the corpus BLEU score.\\n24.4 Cumulative and Individual BLEU Scores\\nThe BLEU score calculations in NLTK allow you to specify the weighting of diﬀerent n-grams\\nin the calculation of the BLEU score. This gives you the ﬂexibility to calculate diﬀerent types\\nof BLEU score, such as individual and cumulative n-gram scores. Let’s take a look.\\n24.4.1 Individual n-gram Scores\\nAn individual n-gram score is the evaluation of just matching grams of a speciﬁc order, such\\nas single words (1-gram) or word pairs (2-gram or bigram). The weights are speciﬁed as a\\ntuple where each index refers to the gram order. To calculate the BLEU score only for 1-gram\\nmatches, you can specify a weight of 1 for 1-gram and 0 for 2, 3 and 4 (1, 0, 0, 0). For example:\\n# 1-gram individual BLEU\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'small ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nscore = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\\nprint(score)\\nListing 24.5: Example of calculating an individual 1-gram BLEU score.\\nRunning this example prints a score of 0.5.\\n0.75\\nListing 24.6: Sample output of calculating an individual 1-gram BLEU score.\\nWe can repeat this example for individual n-grams from 1 to 4 as follows:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 288}, page_content=\"24.4. Cumulative and Individual BLEU Scores 272\\n# n-gram individual BLEU\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'a ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nprint( 'Individual 1-gram: %f '% sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\\nprint( 'Individual 2-gram: %f '% sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\\nprint( 'Individual 3-gram: %f '% sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\\nprint( 'Individual 4-gram: %f '% sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\\nListing 24.7: Example of calculating an individual n-gram BLEU scores.\\nRunning the example gives the following results.\\nIndividual 1-gram: 1.000000\\nIndividual 2-gram: 1.000000\\nIndividual 3-gram: 1.000000\\nIndividual 4-gram: 1.000000\\nListing 24.8: Sample output of calculating an individual n-gram BLEU scores.\\nAlthough we can calculate the individual BLEU scores, this is not how the method was\\nintended to be used and the scores do not carry a lot of meaning, or seem that interpretable.\\n24.4.2 Cumulative n-gram Scores\\nCumulative scores refer to the calculation of individual n-gram scores at all orders from 1 to nand\\nweighting them by calculating the weighted geometric mean. By default, the sentence bleu()\\nand corpus bleu() scores calculate the cumulative 4-gram BLEU score, also called BLEU-4.\\nThe weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and\\n4-gram scores. For example:\\n# 4-gram cumulative BLEU\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'small ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nscore = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\\nprint(score)\\nListing 24.9: Example of calculating a cumulative 4-gram BLEU score.\\nRunning this example prints the following score:\\n0.707106781187\\nListing 24.10: Sample output of calculating a cumulative 4-gram BLEU score.\\nThe cumulative and individual 1-gram BLEU use the same weights, e.g. (1, 0, 0, 0). The\\n2-gram weights assign a 50% to each of 1-gram and 2-gram and the 3-gram weights are 33%\\nfor each of the 1, 2 and 3-gram scores. Let’s make this concrete by calculating the cumulative\\nscores for BLEU-1, BLEU-2, BLEU-3 and BLEU-4:\\n# cumulative BLEU scores\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'small ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nprint( 'Cumulative 1-gram: %f '% sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 289}, page_content=\"24.5. Worked Examples 273\\nprint( 'Cumulative 2-gram: %f '% sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0,\\n0)))\\nprint( 'Cumulative 3-gram: %f '% sentence_bleu(reference, candidate, weights=(0.33, 0.33,\\n0.33, 0)))\\nprint( 'Cumulative 4-gram: %f '% sentence_bleu(reference, candidate, weights=(0.25, 0.25,\\n0.25, 0.25)))\\nListing 24.11: Example of calculating cumulative n-gram BLEU scores.\\nRunning the example prints the following scores. They are quite diﬀerent and more expressive\\nthan the They are quite diﬀerent and more expressive than the standalone individual n-gram\\nscores.\\nCumulative 1-gram: 0.750000\\nCumulative 2-gram: 0.500000\\nCumulative 3-gram: 0.632878\\nCumulative 4-gram: 0.707107\\nListing 24.12: Sample output of calculating cumulative n-gram BLEU scores.\\nIt is common to report the cumulative BLEU-1 to BLEU-4 scores when describing the skill\\nof a text generation system.\\n24.5 Worked Examples\\nIn this section, we try to develop further intuition for the BLEU score with some examples. We\\nwork at the sentence level with a single reference sentence of the following:\\nthe quick brown fox jumped over the lazy dog\\nListing 24.13: Sample text for the worked example of calculating BLEU scores.\\nFirst, let’s look at a perfect score.\\n# prefect match\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.14: Example of two matching cases.\\nRunning the example prints a perfect match.\\n1.0\\nListing 24.15: Sample output BLEU score for two matching cases.\\nNext, let’s change one word, ‘ quick ’ to ‘ fast’.\\n# one word different\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'fast ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.16: Example of making one word diﬀerent.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 290}, page_content=\"24.5. Worked Examples 274\\nThis result is a slight drop in score.\\n0.7506238537503395\\nListing 24.17: Sample output BLEU score when making one word diﬀerent.\\nTry changing two words, both ‘ quick ’ to ‘ fast’ and ‘ lazy’ to ‘ sleepy ’.\\n# two words different\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'fast ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'sleepy ', 'dog ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.18: Example of making two words diﬀerent.\\nRunning the example, we can see a linear drop in skill.\\n0.4854917717073234\\nListing 24.19: Sample output BLEU score when making two words diﬀerent.\\nWhat about if all words are diﬀerent in the candidate?\\n# all words different\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'a ', 'b ', 'c ', 'd ', 'e ', 'f ', 'g ', 'h ', 'i ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.20: Example of making all words diﬀerent.\\nWe get the worse possible score.\\n0.0\\nListing 24.21: Sample output BLEU score when making all words diﬀerent.\\nNow, let’s try a candidate that has fewer words than the reference (e.g. drop the last two\\nwords), but the words are all correct.\\n# shorter candidate\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.22: Example of making one case shorter.\\nThe score is much like the score when two words were wrong above.\\n0.7514772930752859\\nListing 24.23: Sample output BLEU score when making one case shorter.\\nHow about if we make the candidate two words longer than the reference?\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 291}, page_content=\"24.6. Further Reading 275\\n# longer candidate\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ',\\n'from ', 'space ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.24: Example of making one case longer.\\nAgain, we can see that our intuition holds and the score is something like two words wrong .\\n0.7860753021519787\\nListing 24.25: Sample output BLEU score when making one longer shorter.\\nFinally, let’s compare a candidate that is way too short: only two words in length.\\n# very short\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.26: Example of making one case too short.\\nRunning this example ﬁrst prints a warning message indicating that the 3-gram and above\\npart of the evaluation (up to 4-gram) cannot be performed. This is fair given we only have\\n2-grams to work with in the candidate.\\nUserWarning:\\nCorpus/Sentence contains 0 counts of 3-gram overlaps.\\nBLEU scores might be undesirable; use SmoothingFunction().\\nwarnings.warn(_msg)\\nListing 24.27: Sample warning message.\\nNext, we can a score that is very low indeed.\\n0.0301973834223185\\nListing 24.28: Sample output BLEU score when making one case too short.\\nI encourage you to continue to play with examples. The math is pretty simple and I would\\nalso encourage you to read the paper and explore calculating the sentence-level score yourself in\\na spreadsheet.\\n24.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88BLEU on Wikipedia.\\nhttps://en.wikipedia.org/wiki/BLEU\\n\\x88BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nhttp://www.aclweb.org/anthology/P02-1040.pdf\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 292}, page_content='24.7. Summary 276\\n\\x88Source code for nltk.translate.bleu score .\\nhttp://www.nltk.org/_modules/nltk/translate/bleu_score.html\\n\\x88nltk.translate package API Documentation.\\nhttp://www.nltk.org/api/nltk.translate.html\\n24.7 Summary\\nIn this tutorial, you discovered the BLEU score for evaluating and scoring candidate text to\\nreference text in machine translation and other language generation tasks. Speciﬁcally, you\\nlearned:\\n\\x88A gentle introduction to the BLEU score and an intuition for what is being calculated.\\n\\x88How you can calculate BLEU scores in Python using the NLTK library for sentences and\\ndocuments.\\n\\x88How to can use a suite of small examples to develop an intuition for how diﬀerences\\nbetween a candidate and reference text impact the ﬁnal BLEU score.\\n24.7.1 Next\\nIn the next chapter, you will discover how you can prepare data for training a caption generation\\nmodel.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 293}, page_content='Chapter 25\\nHow to Prepare a Photo Caption\\nDataset For Modeling\\nAutomatic photo captioning is a problem where a model must generate a human-readable textual\\ndescription given a photograph. It is a challenging problem in artiﬁcial intelligence that requires\\nboth image understanding from the ﬁeld of computer vision as well as language generation from\\nthe ﬁeld of natural language processing. It is now possible to develop your own image caption\\nmodels using deep learning and freely available datasets of photos and their descriptions. In this\\ntutorial, you will discover how to prepare photos and textual descriptions ready for developing\\na deep learning automatic photo caption generation model. After completing this tutorial, you\\nwill know:\\n\\x88About the Flickr8K dataset comprised of more than 8,000 photos and up to 5 captions for\\neach photo.\\n\\x88How to generally load and prepare photo and text data for modeling with deep learning.\\n\\x88How to speciﬁcally encode data for two diﬀerent types of deep learning models in Keras.\\nLet’s get started.\\n25.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Download the Flickr8K Dataset\\n2. How to Load Photographs\\n3. Pre-Calculate Photo Features\\n4. How to Load Descriptions\\n5. Prepare Description Text\\n6. Whole Description Sequence Model\\n7. Word-By-Word Model\\n8. Progressive Loading\\n277'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 294}, page_content='25.2. Download the Flickr8K Dataset 278\\n25.2 Download the Flickr8K Dataset\\nA good dataset to use when getting started with image captioning is the Flickr8K dataset. The\\nreason is that it is realistic and relatively small so that you can download it and build models on\\nyour workstation using a CPU. The deﬁnitive description of the dataset is in the paper Framing\\nImage Description as a Ranking Task: Data, Models and Evaluation Metrics from 2013. The\\nauthors describe the dataset as follows:\\nWe introduce a new benchmark collection for sentence-based image description and\\nsearch, consisting of 8,000 images that are each paired with ﬁve diﬀerent captions\\nwhich provide clear descriptions of the salient entities and events.\\n...\\nThe images were chosen from six diﬀerent Flickr groups, and tend not to contain\\nany well-known people or locations, but were manually selected to depict a variety\\nof scenes and situations.\\n—Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics , 2013.\\nThe dataset is available for free. You must complete a request form and the links to the\\ndataset will be emailed to you. I would love to link to them for you, but the email address\\nexpressly requests: Please do not redistribute the dataset . You can use the link below to request\\nthe dataset:\\n\\x88Dataset Request Form.\\nhttps://illinois.edu/fb/sec/1713398\\nWithin a short time, you will receive an email that contains links to two ﬁles:\\n\\x88Flickr8k Dataset.zip (1 Gigabyte) An archive of all photographs.\\n\\x88Flickr8k text.zip (2.2 Megabytes) An archive of all text descriptions for photographs.\\nDownload the datasets and unzip them into your current working directory. You will have\\ntwo directories:\\n\\x88Flicker8k Dataset : Contains more than 8000 photographs in JPEG format (yes the\\ndirectory name spells it ‘Flicker’ not ‘Flickr’).\\n\\x88Flickr8k text : Contains a number of ﬁles containing diﬀerent sources of descriptions for\\nthe photographs.\\nNext, let’s look at how to load the images.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 295}, page_content=\"25.3. How to Load Photographs 279\\n25.3 How to Load Photographs\\nIn this section, we will develop some code to load the photos for use with the Keras deep learning\\nlibrary in Python. The image ﬁle names are unique image identiﬁers. For example, here is a\\nsample of image ﬁle names:\\n990890291_afc72be141.jpg\\n99171998_7cc800ceef.jpg\\n99679241_adc853a5c0.jpg\\n997338199_7343367d7f.jpg\\n997722733_0cb5439472.jpg\\nListing 25.1: Example of photographs ﬁlenames.\\nKeras provides the load img() function that can be used to load the image ﬁles directly as\\nan array of pixels.\\nfrom keras.preprocessing.image import load_img\\nimage = load_img( '990890291_afc72be141.jpg ')\\nListing 25.2: Example of loading a single photograph\\nThe pixel data needs to be converted to a NumPy array for use in Keras. We can use the\\nimgtoarray() Keras function to convert the loaded data.\\nfrom keras.preprocessing.image import img_to_array\\nimage = img_to_array(image)\\nListing 25.3: Example of converting a photograph to a NumPy array\\nWe may want to use a pre-deﬁned feature extraction model, such as a state-of-the-art deep\\nimage classiﬁcation network trained on Image net. The Oxford Visual Geometry Group (VGG)\\nmodel is popular for this purpose and is available in Keras. If we decide to use this pre-trained\\nmodel as a feature extractor in our model, we can pre-process the pixel data for the model by\\nusing the preprocess input() function in Keras, for example:\\nfrom keras.applications.vgg16 import preprocess_input\\n# reshape data into a single sample of an image\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\nListing 25.4: Prepare image data for the VGG16 model\\nWe may also want to force the loading of the photo to have the same pixel dimensions as the\\nVGG model, which are 224 x 224 pixels. We can do that in the call to load img() , for example:\\nimage = load_img( '990890291_afc72be141.jpg ', target_size=(224, 224))\\nListing 25.5: Load an image in Keras to a speciﬁc size\\nWe may want to extract the unique image identiﬁer from the image ﬁlename. We can do\\nthat by splitting the ﬁlename string by the ‘.’ (period) character and retrieving the ﬁrst element\\nof the resulting array:\\nimage_id = filename.split( '. ')[0]\\nListing 25.6: Retrieve image identiﬁer from ﬁlename\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 296}, page_content=\"25.4. Pre-Calculate Photo Features 280\\nWe can tie all of this together and develop a function that, given the name of the directory\\ncontaining the photos, will load and pre-process all of the photos for the VGG model and return\\nthem in a dictionary keyed on their unique image identiﬁers.\\nfrom os import listdir\\nfrom os import path\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\ndef load_photos(directory):\\nimages = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get image id\\nimage_id = name.split( '. ')[0]\\nimages[image_id] = image\\nreturn images\\n# load images\\ndirectory = 'Flicker8k_Dataset '\\nimages = load_photos(directory)\\nprint( 'Loaded Images: %d '% len(images))\\nListing 25.7: Complete example of loading photos from ﬁle.\\nRunning this example prints the number of loaded images. It takes a few minutes to run.\\nLoaded Images: 8091\\nListing 25.8: Example output of loading photos from ﬁle.\\n25.4 Pre-Calculate Photo Features\\nIt is possible to use a pre-trained model to extract the features from photos in the dataset and\\nstore the features to ﬁle. This is an eﬃciency that means that the language part of the model\\nthat turns features extracted from the photo into textual descriptions can be trained standalone\\nfrom the feature extraction model. The beneﬁt is that the very large pre-trained models do not\\nneed to be loaded, held in memory, and used to process each photo while training the language\\nmodel.\\nLater, the feature extraction model and language model can be put back together for making\\npredictions on new photos. In this section, we will extend the photo loading behavior developed\\nin the previous section to load all photos, extract their features using a pre-trained VGG model,\\nand store the extracted features to a new ﬁle that can be loaded and used to train the language\\nmodel. The ﬁrst step is to load the VGG model. This model is provided directly in Keras and\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 297}, page_content=\"25.4. Pre-Calculate Photo Features 281\\ncan be loaded as follows. Note that this will download the 500-megabyte model weights to your\\ncomputer, which may take a few minutes.\\nfrom keras.applications.vgg16 import VGG16\\n# load the model\\nin_layer = Input(shape=(224, 224, 3))\\nmodel = VGG16(include_top=False, input_tensor=in_layer, pooling= 'avg ')\\nmodel.summary()\\nListing 25.9: Load the VGG mode.\\nThis will load the VGG 16-layer model. The two Dense output layers as well as the\\nclassiﬁcation output layer are removed from the model by setting include top=False . The\\noutput from the ﬁnal pooling layer is taken as the features extracted from the image. Next, we\\ncan walk over all images in the directory of images as in the previous section and call predict()\\nfunction on the model for each prepared image to get the extracted features. The features can\\nthen be stored in a dictionary keyed on the image id. The complete example is listed below.\\nfrom os import listdir\\nfrom os import path\\nfrom pickle import dump\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.layers import Input\\n# extract features from each photo in the directory\\ndef extract_features(directory):\\n# load the model\\nin_layer = Input(shape=(224, 224, 3))\\nmodel = VGG16(include_top=False, input_tensor=in_layer)\\nmodel.summary()\\n# extract features from each photo\\nfeatures = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\n# get image id\\nimage_id = name.split( '. ')[0]\\n# store feature\\nfeatures[image_id] = feature\\nprint( '>%s '% name)\\nreturn features\\n# extract features from all images\\ndirectory = 'Flicker8k_Dataset '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 298}, page_content=\"25.5. How to Load Descriptions 282\\nfeatures = extract_features(directory)\\nprint( 'Extracted Features: %d '% len(features))\\n# save to file\\ndump(features, open( 'features.pkl ', 'wb '))\\nListing 25.10: Complete example of pre-calculating VGG16 photo features.\\nThe example may take some time to complete, perhaps one hour. After all features are\\nextracted, the dictionary is stored in the ﬁle features.pkl in the current working directory.\\nThese features can then be loaded later and used as input for training a language model. You\\ncould experiment with other types of pre-trained models in Keras.\\n25.5 How to Load Descriptions\\nIt is important to take a moment to talk about the descriptions; there are a number available.\\nThe ﬁle Flickr8k.token.txt contains a list of image identiﬁers (used in the image ﬁlenames)\\nand tokenized descriptions. Each image has multiple descriptions. Below is a sample of the\\ndescriptions from the ﬁle showing 5 diﬀerent descriptions for a single image.\\n1305564994_00513f9a5b.jpg#0 A man in street racer armor be examine the tire of another\\nracer 's motorbike .\\n1305564994_00513f9a5b.jpg#1 Two racer drive a white bike down a road .\\n1305564994_00513f9a5b.jpg#2 Two motorist be ride along on their vehicle that be oddly\\ndesign and color .\\n1305564994_00513f9a5b.jpg#3 Two person be in a small race car drive by a green hill .\\n1305564994_00513f9a5b.jpg#4 Two person in race uniform in a street car .\\nListing 25.11: Sample of raw photo descriptions.\\nThe ﬁle ExpertAnnotations.txt indicates which of the descriptions for each image were\\nwritten by experts which were written by crowdsource workers asked to describe the image.\\nFinally, the ﬁle CrowdFlowerAnnotations.txt provides the frequency of crowd workers that\\nindicate whether captions suit each image. These frequencies can be interpreted probabilistically.\\nThe authors of the paper describe the annotations as follows:\\n... annotators were asked to write sentences that describe the depicted scenes,\\nsituations, events and entities (people, animals, other objects). We collected multiple\\ncaptions for each image because there is a considerable degree of variance in the way\\nmany images can be described.\\n—Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics , 2013.\\nThere are also lists of the photo identiﬁers to use in a train/test split so that you can compare\\nresults reported in the paper. The ﬁrst step is to decide which captions to use. The simplest\\napproach is to use the ﬁrst description for each photograph. First, we need a function to load\\nthe entire annotations ﬁle ( Flickr8k.token.txt ) into memory. Below is a function to do this\\ncalled load doc() that, given a ﬁlename, will return the document as a string.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 299}, page_content=\"25.5. How to Load Descriptions 283\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 25.12: Function for loading a ﬁle into memory.\\nWe can see from the sample of the ﬁle above that we need only split each line by white space\\nand take the ﬁrst element as the image identiﬁer and the rest as the image description. For\\nexample:\\n# split line by white space\\ntokens = line.split()\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\nListing 25.13: Example of splitting a line into an identiﬁer and a description.\\nWe can then clean up the image identiﬁer by removing the ﬁlename extension and the\\ndescription number.\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\nListing 25.14: Example of cleaning up the photo identiﬁer.\\nWe can also put the description tokens back together into a string for later processing.\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\nListing 25.15: Example of converting the description tokens into a string.\\nWe can put all of this together into a function. Below deﬁnes the load descriptions()\\nfunction that will take the loaded ﬁle, process it line-by-line, and return a dictionary of image\\nidentiﬁers to their ﬁrst description.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 300}, page_content=\"25.6. Prepare Description Text 284\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# store the first description for each image\\nif image_id not in mapping:\\nmapping[image_id] = image_desc\\nreturn mapping\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\ndoc = load_doc(filename)\\ndescriptions = load_descriptions(doc)\\nprint( 'Loaded: %d '% len(descriptions))\\nListing 25.16: Complete example of loading photo descriptions.\\nRunning the example prints the number of loaded image descriptions.\\nLoaded: 8092\\nListing 25.17: Example output of loading photo descriptions.\\nThere are other ways to load descriptions that may turn out to be more accurate for the\\ndata. Use the above example as a starting point and let me know what you come up with.\\n25.6 Prepare Description Text\\nThe descriptions are tokenized; this means that each token is comprised of words separated by\\nwhite space. It also means that punctuation are separated as their own tokens, such as periods\\n(‘.’) and apostrophes for word plurals (’s). It is a good idea to clean up the description text\\nbefore using it in a model. Some ideas of data cleaning we can form include:\\n\\x88Normalizing the case of all tokens to lowercase.\\n\\x88Remove all punctuation from tokens.\\n\\x88Removing all tokens that contain one or fewer characters (after punctuation is removed),\\ne.g. ‘a’ and hanging ‘s’ characters.\\nWe can implement these simple cleaning operations in a function that cleans each description\\nin the loaded dictionary from the previous section. Below deﬁnes the clean descriptions()\\nfunction that will clean each loaded description.\\n# clean description text\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor key, desc in descriptions.items():\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\\ndesc = [word.lower() for word in desc]\\n# remove punctuation from each word\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 301}, page_content=\"25.6. Prepare Description Text 285\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\\n# store as string\\ndescriptions[key] = ' '.join(desc)\\nListing 25.18: Function to clean photo descriptions.\\nWe can then save the clean text to ﬁle for later use by our model. Each line will contain the\\nimage identiﬁer followed by the clean description. Below deﬁnes the save doc() function for\\nsaving the cleaned descriptions to ﬁle.\\n# save descriptions to file, one per line\\ndef save_doc(descriptions, filename):\\nlines = list()\\nfor key, desc in mapping.items():\\nlines.append(key + ' ' + desc)\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nListing 25.19: Function to save clean descriptions.\\nPutting this all together with the loading of descriptions from the previous section, the\\ncomplete example is listed below.\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# store the first description for each image\\nif image_id not in mapping:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 302}, page_content=\"25.6. Prepare Description Text 286\\nmapping[image_id] = image_desc\\nreturn mapping\\n# clean description text\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor key, desc in descriptions.items():\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\\ndesc = [word.lower() for word in desc]\\n# remove punctuation from each word\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\\n# store as string\\ndescriptions[key] = ' '.join(desc)\\n# save descriptions to file, one per line\\ndef save_doc(descriptions, filename):\\nlines = list()\\nfor key, desc in descriptions.items():\\nlines.append(key + ' ' + desc)\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\n# load descriptions\\ndoc = load_doc(filename)\\n# parse descriptions\\ndescriptions = load_descriptions(doc)\\nprint( 'Loaded: %d '% len(descriptions))\\n# clean descriptions\\nclean_descriptions(descriptions)\\n# summarize vocabulary\\nall_tokens = ' '.join(descriptions.values()).split()\\nvocabulary = set(all_tokens)\\nprint( 'Vocabulary Size: %d '% len(vocabulary))\\n# save descriptions\\nsave_doc(descriptions, 'descriptions.txt ')\\nListing 25.20: Complete example of cleaning photo descriptions.\\nRunning the example ﬁrst loads 8,092 descriptions, cleans them, summarizes the vocabulary\\nof 4,484 unique words, then saves them to a new ﬁle called descriptions.txt .\\nLoaded: 8092\\nVocabulary Size: 4484\\nListing 25.21: Example output of cleaning photo descriptions.\\nOpen the new ﬁle descriptions.txt in a text editor and review the contents. You should\\nsee somewhat readable descriptions of photos ready for modeling.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 303}, page_content=\"25.7. Whole Description Sequence Model 287\\n...\\n3139118874_599b30b116 two girls pose for picture at christmastime\\n2065875490_a46b58c12b person is walking on sidewalk and skeleton is on the left inside of\\nfence\\n2682382530_f9f8fd1e89 man in black shorts is stretching out his leg\\n3484019369_354e0b88c0 hockey team in red and white on the side of the ice rink\\n505955292_026f1489f2 boy rides horse\\nListing 25.22: Sample of clean photo descriptions.\\nThe vocabulary is still relatively large. To make modeling easier, especially the ﬁrst time\\naround, I would recommend further reducing the vocabulary by removing words that only\\nappear once or twice across all descriptions.\\n25.7 Whole Description Sequence Model\\nThere are many ways to model the caption generation problem. One naive way is to create a\\nmodel that outputs the entire textual description in a one-shot manner. This is a naive model\\nbecause it puts a heavy burden on the model to both interpret the meaning of the photograph\\nand generate words, then arrange those words into the correct order.\\nThis is not unlike the language translation problem used in an Encoder-Decoder recurrent\\nneural network where the entire translated sentence is output one word at a time given an\\nencoding of the input sequence. Here we would use an encoding of the image to generate the\\noutput sentence instead. The image may be encoded using a pre-trained model used for image\\nclassiﬁcation, such as the VGG trained on the ImageNet model mentioned above.\\nThe output of the model would be a probability distribution over each word in the vocabulary.\\nThe sequence would be as long as the longest photo description. The descriptions would, therefore,\\nneed to be ﬁrst integer encoded where each word in the vocabulary is assigned a unique integer\\nand sequences of words would be replaced with sequences of integers. The integer sequences\\nwould then need to be one hot encoded to represent the idealized probability distribution\\nover the vocabulary for each word in the sequence. We can use tools in Keras to prepare the\\ndescriptions for this type of model. The ﬁrst step is to load the mapping of image identiﬁers to\\nclean descriptions stored in descriptions.txt .\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 304}, page_content=\"25.7. Whole Description Sequence Model 288\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\nmapping[image_id] = ' '.join(image_desc)\\nreturn descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\nListing 25.23: Load the cleaned descriptions.\\nRunning this piece loads the 8,092 photo descriptions into a dictionary keyed on image\\nidentiﬁers. These identiﬁers can then be used to load each photo ﬁle for the corresponding\\ninputs to the model.\\nLoaded 8092\\nListing 25.24: Example output from loading the clean descriptions.\\nNext, we need to extract all of the description text so we can encode it.\\n# extract all text\\ndesc_text = list(descriptions.values())\\nListing 25.25: Convert loaded description text to a list.\\nWe can use the Keras Tokenizer class to consistently map each word in the vocabulary to\\nan integer. First, the object is created, then is ﬁt on the description text. The ﬁt tokenizer can\\nlater be saved to ﬁle for consistent decoding of the predictions back to vocabulary words.\\nfrom keras.preprocessing.text import Tokenizer\\n# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 25.26: Fit a Tokenizer of the photo description text.\\nNext, we can use the ﬁt tokenizer to encode the photo descriptions into sequences of integers.\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\nListing 25.27: Example of integer encoding the description text.\\nThe model will require all output sequences to have the same length for training. We can\\nachieve this by padding all encoded sequences to have the same length as the longest encoded\\nsequence. We can pad the sequences with 0 values after the list of words. Keras provides the\\npadsequences() function to pad the sequences.\\nfrom keras.preprocessing.sequence import pad_sequences\\n# pad all sequences to a fixed length\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\npadded = pad_sequences(sequences, maxlen=max_length, padding= 'post ')\\nListing 25.28: Pad descriptions to a maximum length.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 305}, page_content=\"25.7. Whole Description Sequence Model 289\\nFinally, we can one hot encode the padded sequences to have one sparse vector for each word\\nin the sequence. Keras provides the tocategorical() function to perform this operation.\\nfrom keras.utils import to_categorical\\n# one hot encode\\ny = to_categorical(padded, num_classes=vocab_size)\\nListing 25.29: Example of one hot encoding output text.\\nOnce encoded, we can ensure that the sequence output data has the right shape for the\\nmodel.\\ny = y.reshape((len(descriptions), max_length, vocab_size))\\nprint(y.shape)\\nListing 25.30: Example of reshaping encoded text.\\nPutting all of this together, the complete example is listed below.\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\ndescriptions[image_id] = ' '.join(image_desc)\\nreturn descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\n# extract all text\\ndesc_text = list(descriptions.values())\\n# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\n# pad all sequences to a fixed length\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 306}, page_content=\"25.8. Word-By-Word Model 290\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\npadded = pad_sequences(sequences, maxlen=max_length, padding= 'post ')\\n# one hot encode\\ny = to_categorical(padded, num_classes=vocab_size)\\ny = y.reshape((len(descriptions), max_length, vocab_size))\\nprint(y.shape)\\nListing 25.31: Complete example of data preparation for a whole sequence model.\\nRunning the example ﬁrst prints the number of loaded image descriptions (8,092 photos),\\nthe dataset vocabulary size (4,485 words), the length of the longest description (28 words), then\\nﬁnally the shape of the data for ﬁtting a prediction model in the form [samples, sequence length,\\nfeatures].\\nLoaded 8092\\nVocabulary Size: 4485\\nDescription Length: 28\\n(8092, 28, 4485)\\nListing 25.32: Example output from preparing data for a whole sequence prediction model.\\nAs mentioned, outputting the entire sequence may be challenging for the model. We will\\nlook at a simpler model in the next section.\\n25.8 Word-By-Word Model\\nA simpler model for generating a caption for photographs is to generate one word given both\\nthe image as input and the last word generated. This model would then have to be called\\nrecursively to generate each word in the description with previous predictions as input. Using\\nthe word as input, give the model a forced context for predicting the next word in the sequence.\\nThis is the model used in prior research, such as: Show and Tell: A Neural Image Caption\\nGenerator , 2015. A word embedding layer can be used to represent the input words. Like the\\nfeature extraction model for the photos, this too can be pre-trained either on a large corpus or\\non the dataset of all descriptions.\\nThe model would take a full sequence of words as input; the length of the sequence would be\\nthe maximum length of descriptions in the dataset. The model must be started with something.\\nOne approach is to surround each photo description with special tags to signal the start and\\nend of the description, such as STARTDESC andENDDESC . For example, the description:\\nboy rides horse\\nListing 25.33: Example of a photo description.\\nWould become:\\nSTARTDESC boy rides horse ENDDESC\\nListing 25.34: Example of a wrapped photo description.\\nAnd would be fed to the model with the same image input to result in the following\\ninput-output word sequence pairs:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 307}, page_content=\"25.8. Word-By-Word Model 291\\nInput (X), Output (y)\\nSTARTDESC, boy\\nSTARTDESC, boy, rides\\nSTARTDESC, boy, rides, horse\\nSTARTDESC, boy, rides, horse ENDDESC\\nListing 25.35: Example input-output pairs for a wrapped description.\\nThe data preparation would begin much the same as was described in the previous section.\\nEach description must be integer encoded. After encoding, the sequences are split into multiple\\ninput and output pairs and only the output word ( y) is one hot encoded. This is because the\\nmodel is only required to predict the probability distribution of one word at a time. The code is\\nthe same up to the point where we calculate the maximum length of sequences.\\n...\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\n# extract all text\\ndesc_text = list(descriptions.values())\\n# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\n# determine the maximum sequence length\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\nListing 25.36: Example of loading and encoding photo descriptions.\\nNext, we split the each integer encoded sequence into input and output pairs. Let’s step\\nthrough a single sequence called seq at the i’th word in the sequence, where imore than or\\nequal to 1. First, we take the ﬁrst i-1words as the input sequence and the i’th word as the\\noutput word.\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\nListing 25.37: Example of splitting a description sequence.\\nNext, the input sequence is padded to the maximum length of the input sequences. Pre-\\npadding is used (the default) so that new words appear at the end of the sequence, instead of\\nthe input beginning.\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\nListing 25.38: Example of padding a split description sequence.\\nThe output word is one hot encoded, much like in the previous section.\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\nListing 25.39: Example of one hot encoding the output word.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 308}, page_content=\"25.8. Word-By-Word Model 292\\nWe can put all of this together into a complete example to prepare description data for the\\nword-by-word model.\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\ndescriptions[image_id] = ' '.join(image_desc)\\nreturn descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\n# extract all text\\ndesc_text = list(descriptions.values())\\n# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\n# determine the maximum sequence length\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\nX, y = list(), list()\\nfor img_no, seq in enumerate(sequences):\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 309}, page_content='25.9. Progressive Loading 293\\n# store\\nX.append(in_seq)\\ny.append(out_seq)\\n# convert to numpy arrays\\nX, y = array(X), array(y)\\nprint(X.shape)\\nprint(y.shape)\\nListing 25.40: Complete example of data preparation for a word-by-word model.\\nRunning the example prints the same statistics, but prints the size of the resulting encoded\\ninput and output sequences. Note that the input of images must follow the exact same ordering\\nwhere the same photo is shown for each example drawn from a single description. One way\\nto do this would be to load the photo and store it for each example prepared from a single\\ndescription.\\nLoaded 8092\\nVocabulary Size: 4485\\nDescription Length: 28\\n(66456, 28)\\n(66456, 4485)\\nListing 25.41: Example output of data preparation for a word-by-word prediction model.\\n25.9 Progressive Loading\\nThe Flicr8K dataset of photos and descriptions can ﬁt into RAM, if you have a lot of RAM\\n(e.g. 8 Gigabytes or more), and most modern systems do. This is ﬁne if you want to ﬁt a deep\\nlearning model using the CPU. Alternately, if you want to ﬁt a model using a GPU, then you\\nwill not be able to ﬁt the data into memory of an average GPU video card. One solution is to\\nprogressively load the photos and descriptions as-needed by the model.\\nKeras supports progressively loaded datasets by using the fitgenerator() function on the\\nmodel. A generator is the term used to describe a function used to return batches of samples\\nfor the model to train on. This can be as simple as a standalone function, the name of which is\\npassed to the fitgenerator() function when ﬁtting the model. As a reminder, a model is ﬁt\\nfor multiple epochs, where one epoch is one pass through the entire training dataset, such as all\\nphotos. One epoch is comprised of multiple batches of examples where the model weights are\\nupdated at the end of each batch.\\nA generator must create and yield one batch of examples. For example, the average sentence\\nlength in the dataset is 11 words; that means that each photo will result in 11 examples for\\nﬁtting the model and two photos will result in about 22 examples on average. A good default\\nbatch size for modern hardware may be 32 examples, so that is about 2-3 photos worth of\\nexamples.\\nWe can write a custom generator to load a few photos and return the samples as a single\\nbatch. Let’s assume we are working with a word-by-word model described in the previous\\nsection that expects a sequence of words and a prepared image as input and predicts a single\\nword. Let’s design a data generator that given a loaded dictionary of image identiﬁers to clean\\ndescriptions, a trained tokenizer, and a maximum sequence length will load one-image worth of\\nexamples for each batch.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 310}, page_content=\"25.9. Progressive Loading 294\\nA generator must loop forever and yield each batch of samples. We can loop forever with\\na while loop and within this, loop over each image in the image directory. For each image\\nﬁlename, we can load the image and create all of the input-output sequence pairs from the\\nimage’s description. Below is the data generator function.\\ndef data_generator(mapping, tokenizer, max_length):\\n# loop for ever over images\\ndirectory = 'Flicker8k_Dataset '\\nwhile 1:\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = directory + '/ '+ name\\nimage, image_id = load_image(filename)\\n# create word sequences\\ndesc = mapping[image_id]\\nin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc, image)\\nyield [[in_img, in_seq], out_word]\\nListing 25.42: Example of a generator for progressive loading.\\nYou could extend it to take the name of the dataset directory as a parameter. The generator\\nreturns an array containing the inputs ( X) and output ( y) for the model. The input is comprised\\nof an array with two items for the input images and encoded word sequences. The outputs\\nare one hot encoded words. You can see that it calls a function called load photo() to load a\\nsingle photo and return the pixels and image identiﬁer. This is a simpliﬁed version of the photo\\nloading function developed at the beginning of this tutorial.\\n# load a single photo intended as input for the VGG feature extractor model\\ndef load_photo(filename):\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)[0]\\n# get image id\\nimage_id = filename.split( '/ ')[-1].split( '. ')[0]\\nreturn image, image_id\\nListing 25.43: Example of a function for loading and preparing a photo.\\nAnother function named create sequences() is called to create sequences of images, input\\nsequences of words, and output words that we then yield to the caller. This is a function that\\nincludes everything discussed in the previous section, and also creates copies of the image pixels,\\none for each input-output pair created from the photo’s description.\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, descriptions, images):\\nXimages, XSeq, y = list(), list(),list()\\nvocab_size = len(tokenizer.word_index) + 1\\nfor j in range(len(descriptions)):\\nseq = descriptions[j]\\nimage = images[j]\\n# integer encode\\nseq = tokenizer.texts_to_sequences([seq])[0]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 311}, page_content=\"25.9. Progressive Loading 295\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# select\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nXimages.append(image)\\nXSeq.append(in_seq)\\ny.append(out_seq)\\nXimages, XSeq, y = array(Ximages), array(XSeq), array(y)\\nreturn Ximages, XSeq, y\\nListing 25.44: Example of a function for preparing description text.\\nPrior to preparing the model that uses the data generator, we must load the clean descriptions,\\nprepare the tokenizer, and calculate the maximum sequence length. All 3 of must be passed to\\nthedata generator() as parameters. We use the same load clean descriptions() function\\ndeveloped previously and a new create tokenizer() function that simpliﬁes the creation of\\nthe tokenizer. Tying all of this together, the complete data generator is listed below, ready for\\nuse to train a model.\\nfrom os import listdir\\nfrom os import path\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\ndescriptions[image_id] = ' '.join(image_desc)\\nreturn descriptions\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 312}, page_content=\"25.9. Progressive Loading 296\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = list(descriptions.values())\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# load a single photo intended as input for the VGG feature extractor model\\ndef load_photo(filename):\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)[0]\\n# get image id\\nimage_id = path.basename(filename).split( '. ')[0]\\nreturn image, image_id\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, desc, image):\\nXimages, XSeq, y = list(), list(),list()\\nvocab_size = len(tokenizer.word_index) + 1\\n# integer encode the description\\nseq = tokenizer.texts_to_sequences([desc])[0]\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# select\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nXimages.append(image)\\nXSeq.append(in_seq)\\ny.append(out_seq)\\nXimages, XSeq, y = array(Ximages), array(XSeq), array(y)\\nreturn [Ximages, XSeq, y]\\n# data generator, intended to be used in a call to model.fit_generator()\\ndef data_generator(descriptions, tokenizer, max_length):\\n# loop for ever over images\\ndirectory = 'Flicker8k_Dataset '\\nwhile 1:\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage, image_id = load_photo(filename)\\n# create word sequences\\ndesc = descriptions[image_id]\\nin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc, image)\\nyield [[in_img, in_seq], out_word]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 313}, page_content=\"25.10. Further Reading 297\\n# load mapping of ids to descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\n# integer encode sequences of words\\ntokenizer = create_tokenizer(descriptions)\\n# pad to fixed length\\nmax_length = max(len(s.split()) for s in list(descriptions.values()))\\nprint( 'Description Length: %d '% max_length)\\n# test the data generator\\ngenerator = data_generator(descriptions, tokenizer, max_length)\\ninputs, outputs = next(generator)\\nprint(inputs[0].shape)\\nprint(inputs[1].shape)\\nprint(outputs.shape)\\nListing 25.45: Complete example of progressive loading.\\nA data generator can be tested by calling the next() function. We can test the generator as\\nfollows.\\n# test the data generator\\ngenerator = data_generator(descriptions, tokenizer, max_length)\\ninputs, outputs = next(generator)\\nprint(inputs[0].shape)\\nprint(inputs[1].shape)\\nprint(outputs.shape)\\nListing 25.46: Example of testing the custom generator function.\\nRunning the example prints the shape of the input and output example for a single batch\\n(e.g. 13 input-output pairs):\\n(13, 224, 224, 3)\\n(13, 28)\\n(13, 4485)\\nListing 25.47: Example output from testing the generator function.\\nThe generator can be used to ﬁt a model by calling the fitgenerator() function on the\\nmodel (instead of fit() ) and passing in the generator. We must also specify the number of\\nsteps or batches per epoch. We could estimate this as (10 x training dataset size), perhaps\\n70,000 if 7,000 images are used for training.\\n# define model\\n# ...\\n# fit model\\nmodel.fit_generator(data_generator(descriptions, tokenizer, max_length),\\nsteps_per_epoch=70000, ...)\\nListing 25.48: Example of using the progressive loading data generator when ﬁtting a Keras\\nmodel.\\n25.10 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 314}, page_content='25.11. Summary 298\\n25.10.1 Flickr8K Dataset\\n\\x88Framing image description as a ranking task: data, models and evaluation metrics (Home-\\npage).\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.\\nhtml\\n\\x88Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics ,\\n013.\\nhttps://www.jair.org/media/3994/live-3994-7274-jair.pdf\\n\\x88Dataset Request Form.\\nhttps://illinois.edu/fb/sec/1713398\\n\\x88Old Flicrk8K Homepage.\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html\\n25.10.2 API\\n\\x88Python Generators.\\nhttps://wiki.python.org/moin/Generators\\n\\x88Keras Model API.\\nhttps://keras.io/models/model/\\n\\x88Keras padsequences() API.\\nhttps://keras.io/preprocessing/sequence/#pad_sequences\\n\\x88Keras Tokenizer API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n\\x88Keras VGG16 API.\\nhttps://keras.io/applications/#vgg16\\n25.11 Summary\\nIn this tutorial, you discovered how to prepare photos and textual descriptions ready for\\ndeveloping an automatic photo caption generation model. Speciﬁcally, you learned:\\n\\x88About the Flickr8K dataset comprised of more than 8,000 photos and up to 5 captions for\\neach photo.\\n\\x88How to generally load and prepare photo and text data for modeling with deep learning.\\n\\x88How to speciﬁcally encode data for two diﬀerent types of deep learning models in Keras.\\n25.11.1 Next\\nIn the next chapter, you will discover how you can develop a model for automatic caption\\ngeneration.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 315}, page_content='Chapter 26\\nProject: Develop a Neural Image\\nCaption Generation Model\\nCaption generation is a challenging artiﬁcial intelligence problem where a textual description\\nmust be generated for a given photograph. It requires both methods from computer vision to\\nunderstand the content of the image and a language model from the ﬁeld of natural language\\nprocessing to turn the understanding of the image into words in the right order. Recently, deep\\nlearning methods have achieved state-of-the-art results on examples of this problem.\\nDeep learning methods have demonstrated state-of-the-art results on caption generation\\nproblems. What is most impressive about these methods is a single end-to-end model can be\\ndeﬁned to predict a caption, given a photo, instead of requiring sophisticated data preparation\\nor a pipeline of speciﬁcally designed models. In this tutorial, you will discover how to develop\\na photo captioning deep learning model from scratch. After completing this tutorial, you will\\nknow:\\n\\x88How to prepare photo and text data for training a deep learning model.\\n\\x88How to design and train a deep learning caption generation model.\\n\\x88How to evaluate a train caption generation model and use it to caption entirely new\\nphotographs.\\nLet’s get started.\\n26.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Photo and Caption Dataset\\n2. Prepare Photo Data\\n3. Prepare Text Data\\n4. Develop Deep Learning Model\\n5. Evaluate Model\\n6. Generate New Captions\\n299'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 316}, page_content='26.2. Photo and Caption Dataset 300\\n26.2 Photo and Caption Dataset\\nIn this tutorial, we will use the Flickr8k dataset. This dataset was introduced previously in\\nChapter 25. The dataset is available for free. You must complete a request form and the links to\\nthe dataset will be emailed to you. I would love to link to them for you, but the email address\\nexpressly requests: Please do not redistribute the dataset . You can use the link below to request\\nthe dataset:\\n\\x88Dataset Request Form.\\nhttps://illinois.edu/fb/sec/1713398\\nWithin a short time, you will receive an email that contains links to two ﬁles:\\n\\x88Flickr8k Dataset.zip (1 Gigabyte) An archive of all photographs.\\n\\x88Flickr8k text.zip (2.2 Megabytes) An archive of all text descriptions for photographs.\\nDownload the datasets and unzip them into your current working directory. You will have\\ntwo directories:\\n\\x88Flicker8k Dataset : Contains 8092 photographs in JPEG format (yes the directory name\\nspells it ‘Flicker’ not ‘Flickr’).\\n\\x88Flickr8k text : Contains a number of ﬁles containing diﬀerent sources of descriptions for\\nthe photographs.\\nThe dataset has a pre-deﬁned training dataset (6,000 images), development dataset (1,000\\nimages), and test dataset (1,000 images). One measure that can be used to evaluate the skill of\\nthe model are BLEU scores. For reference, below are some ball-park BLEU scores for skillful\\nmodels when evaluated on the test dataset (taken from the 2017 paper Where to put the Image\\nin an Image Caption Generator ):\\n\\x88BLEU-1: 0.401 to 0.578.\\n\\x88BLEU-2: 0.176 to 0.390.\\n\\x88BLEU-3: 0.099 to 0.260.\\n\\x88BLEU-4: 0.059 to 0.170.\\nWe describe the BLEU metric more later when we work on evaluating our model. Next, let’s\\nlook at how to load the images.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 317}, page_content=\"26.3. Prepare Photo Data 301\\n26.3 Prepare Photo Data\\nWe will use a pre-trained model to interpret the content of the photos. There are many models\\nto choose from. In this case, we will use the Oxford Visual Geometry Group, or VGG, model\\nthat won the ImageNet competition in 2014. Keras provides this pre-trained model directly.\\nNote, the ﬁrst time you use this model, Keras will download the model weights from the Internet,\\nwhich are about 500 Megabytes. This may take a few minutes depending on your internet\\nconnection. Note the use of the VGG pre-trained model was introduced in Chapter 23.\\nWe could use this model as part of a broader image caption model. The problem is, it\\nis a large model and running each photo through the network every time we want to test a\\nnew language model conﬁguration (downstream) is redundant. Instead, we can pre-compute\\nthephoto features using the pre-trained model and save them to ﬁle. We can then load these\\nfeatures later and feed them into our model as the interpretation of a given photo in the dataset.\\nIt is no diﬀerent to running the photo through the full VGG model; it is just we will have done\\nit once in advance.\\nThis is an optimization that will make training our models faster and consume less memory.\\nWe can load the VGG model in Keras using the VGG class. We will remove the last layer from\\nthe loaded model, as this is the model used to predict a classiﬁcation for a photo. We are not\\ninterested in classifying images, but we are interested in the internal representation of the photo\\nright before a classiﬁcation is made. These are the features that the model has extracted from\\nthe photo.\\nKeras also provides tools for reshaping the loaded photo into the preferred size for the model\\n(e.g. 3 channel 224 x 224 pixel image). Below is a function named extract features() that,\\ngiven a directory name, will load each photo, prepare it for VGG, and collect the predicted\\nfeatures from the VGG model. The image features are a 1-dimensional 4,096 element vector.\\nThe function returns a dictionary of image identiﬁer to image features.\\n# extract features from each photo in the directory\\ndef extract_features(directory):\\n# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# summarize\\nmodel.summary()\\n# extract features from each photo\\nfeatures = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = directory + '/ '+ name\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\n# get image id\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 318}, page_content=\"26.3. Prepare Photo Data 302\\nimage_id = name.split( '. ')[0]\\n# store feature\\nfeatures[image_id] = feature\\nprint( '>%s '% name)\\nreturn features\\nListing 26.1: Function to extract photo features\\nWe can call this function to prepare the photo data for testing our models, then save the\\nresulting dictionary to a ﬁle named features.pkl . The complete example is listed below.\\nfrom os import listdir\\nfrom os import path\\nfrom pickle import dump\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.models import Model\\n# extract features from each photo in the directory\\ndef extract_features(directory):\\n# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# summarize\\nmodel.summary()\\n# extract features from each photo\\nfeatures = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\n# get image id\\nimage_id = name.split( '. ')[0]\\n# store feature\\nfeatures[image_id] = feature\\nprint( '>%s '% name)\\nreturn features\\n# extract features from all images\\ndirectory = 'Flicker8k_Dataset '\\nfeatures = extract_features(directory)\\nprint( 'Extracted Features: %d '% len(features))\\n# save to file\\ndump(features, open( 'features.pkl ', 'wb '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 319}, page_content=\"26.4. Prepare Text Data 303\\nListing 26.2: Complete example of extracting photo features.\\nRunning this data preparation step may take a while depending on your hardware, perhaps\\none hour on the CPU with a modern workstation. At the end of the run, you will have\\nthe extracted features stored in features.pkl for later use. This ﬁle will be a few hundred\\nMegabytes in size.\\n26.4 Prepare Text Data\\nThe dataset contains multiple descriptions for each photograph and the text of the descriptions\\nrequires some minimal cleaning. Note, a fuller investigation into how this text data can\\nbe prepared was described in Chapter 25. First, we will load the ﬁle containing all of the\\ndescriptions.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\n# load descriptions\\ndoc = load_doc(filename)\\nListing 26.3: Example of loading photo descriptions into memory\\nEach photo has a unique identiﬁer. This identiﬁer is used on the photo ﬁlename and in the\\ntext ﬁle of descriptions. Next, we will step through the list of photo descriptions. Below deﬁnes\\na function load descriptions() that, given the loaded document text, will return a dictionary\\nof photo identiﬁers to descriptions. Each photo identiﬁer maps to a list of one or more textual\\ndescriptions.\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# create the list if needed\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 320}, page_content=\"26.4. Prepare Text Data 304\\nif image_id not in mapping:\\nmapping[image_id] = list()\\n# store description\\nmapping[image_id].append(image_desc)\\nreturn mapping\\n# parse descriptions\\ndescriptions = load_descriptions(doc)\\nprint( 'Loaded: %d '% len(descriptions))\\nListing 26.4: Example of splitting descriptions from photo identiﬁers\\nNext, we need to clean the description text. The descriptions are already tokenized and easy\\nto work with. We will clean the text in the following ways in order to reduce the size of the\\nvocabulary of words we will need to work with:\\n\\x88Convert all words to lowercase.\\n\\x88Remove all punctuation.\\n\\x88Remove all words that are one character or less in length (e.g. ‘a’).\\n\\x88Remove all words with numbers in them.\\nBelow deﬁnes the clean descriptions() function that, given the dictionary of image\\nidentiﬁers to descriptions, steps through each description and cleans the text.\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor key, desc_list in descriptions.items():\\nfor i in range(len(desc_list)):\\ndesc = desc_list[i]\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\\ndesc = [word.lower() for word in desc]\\n# remove punctuation from each token\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\\n# remove tokens with numbers in them\\ndesc = [word for word in desc if word.isalpha()]\\n# store as string\\ndesc_list[i] = ' '.join(desc)\\n# clean descriptions\\nclean_descriptions(descriptions)\\nListing 26.5: Example of cleaning description text\\nOnce cleaned, we can summarize the size of the vocabulary. Ideally, we want a vocabulary\\nthat is both expressive and as small as possible. A smaller vocabulary will result in a smaller\\nmodel that will train faster. For reference, we can transform the clean descriptions into a set\\nand print its size to get an idea of the size of our dataset vocabulary.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 321}, page_content=\"26.4. Prepare Text Data 305\\n# convert the loaded descriptions into a vocabulary of words\\ndef to_vocabulary(descriptions):\\n# build a list of all description strings\\nall_desc = set()\\nfor key in descriptions.keys():\\n[all_desc.update(d.split()) for d in descriptions[key]]\\nreturn all_desc\\n# summarize vocabulary\\nvocabulary = to_vocabulary(descriptions)\\nprint( 'Vocabulary Size: %d '% len(vocabulary))\\nListing 26.6: Example of deﬁning the description text vocabulary\\nFinally, we can save the dictionary of image identiﬁers and descriptions to a new ﬁle\\nnamed descriptions.txt , with one image identiﬁer and description per line. Below deﬁnes the\\nsave doc() function that, given a dictionary containing the mapping of identiﬁers to descriptions\\nand a ﬁlename, saves the mapping to ﬁle.\\n# save descriptions to file, one per line\\ndef save_descriptions(descriptions, filename):\\nlines = list()\\nfor key, desc_list in descriptions.items():\\nfor desc in desc_list:\\nlines.append(key + ' ' + desc)\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\n# save descriptions\\nsave_doc(descriptions, 'descriptions.txt ')\\nListing 26.7: Example of saving clean descriptions to ﬁle\\nPutting this all together, the complete listing is provided below.\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 322}, page_content=\"26.4. Prepare Text Data 306\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# create the list if needed\\nif image_id not in mapping:\\nmapping[image_id] = list()\\n# store description\\nmapping[image_id].append(image_desc)\\nreturn mapping\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor _, desc_list in descriptions.items():\\nfor i in range(len(desc_list)):\\ndesc = desc_list[i]\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\\ndesc = [word.lower() for word in desc]\\n# remove punctuation from each token\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\\n# remove tokens with numbers in them\\ndesc = [word for word in desc if word.isalpha()]\\n# store as string\\ndesc_list[i] = ' '.join(desc)\\n# convert the loaded descriptions into a vocabulary of words\\ndef to_vocabulary(descriptions):\\n# build a list of all description strings\\nall_desc = set()\\nfor key in descriptions.keys():\\n[all_desc.update(d.split()) for d in descriptions[key]]\\nreturn all_desc\\n# save descriptions to file, one per line\\ndef save_descriptions(descriptions, filename):\\nlines = list()\\nfor key, desc_list in descriptions.items():\\nfor desc in desc_list:\\nlines.append(key + ' ' + desc)\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\n# load descriptions\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 323}, page_content=\"26.5. Develop Deep Learning Model 307\\ndoc = load_doc(filename)\\n# parse descriptions\\ndescriptions = load_descriptions(doc)\\nprint( 'Loaded: %d '% len(descriptions))\\n# clean descriptions\\nclean_descriptions(descriptions)\\n# summarize vocabulary\\nvocabulary = to_vocabulary(descriptions)\\nprint( 'Vocabulary Size: %d '% len(vocabulary))\\n# save to file\\nsave_descriptions(descriptions, 'descriptions.txt ')\\nListing 26.8: Complete example of text data preparation.\\nRunning the example ﬁrst prints the number of loaded photo descriptions (8,092) and the\\nsize of the clean vocabulary (8,763 words).\\nLoaded: 8,092\\nVocabulary Size: 8,763\\nListing 26.9: Example output from preparing the text data\\nFinally, the clean descriptions are written to descriptions.txt . Taking a look at the ﬁle,\\nwe can see that the descriptions are ready for modeling. The order of descriptions in your ﬁle\\nmay vary.\\n2252123185_487f21e336 bunch on people are seated in stadium\\n2252123185_487f21e336 crowded stadium is full of people watching an event\\n2252123185_487f21e336 crowd of people fill up packed stadium\\n2252123185_487f21e336 crowd sitting in an indoor stadium\\n2252123185_487f21e336 stadium full of people watch game\\n...\\nListing 26.10: Sample of text from the clean photo descriptions\\n26.5 Develop Deep Learning Model\\nIn this section, we will deﬁne the deep learning model and ﬁt it on the training dataset. This\\nsection is divided into the following parts:\\n1. Loading Data.\\n2. Deﬁning the Model.\\n3. Fitting the Model.\\n4. Complete Example.\\n26.5.1 Loading Data\\nFirst, we must load the prepared photo and text data so that we can use it to ﬁt the model.\\nWe are going to train the data on all of the photos and captions in the training dataset. While\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 324}, page_content=\"26.5. Develop Deep Learning Model 308\\ntraining, we are going to monitor the performance of the model on the development dataset and\\nuse that performance to decide when to save models to ﬁle.\\nThe train and development dataset have been predeﬁned in the Flickr 8k.trainImages.txt\\nand Flickr 8k.devImages.txt ﬁles respectively, that both contain lists of photo ﬁle names.\\nFrom these ﬁle names, we can extract the photo identiﬁers and use these identiﬁers to ﬁlter\\nphotos and descriptions for each set. The function load set() below will load a pre-deﬁned set\\nof identiﬁers given the train or development sets ﬁlename.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\nListing 26.11: Functions for loading the photo description text and identiﬁers\\nNow, we can load the photos and descriptions using the pre-deﬁned set of train or development\\nidentiﬁers. Below is the function load clean descriptions() that loads the cleaned text\\ndescriptions from descriptions.txt for a given set of identiﬁers and returns a dictionary of\\nidentiﬁers to lists of text descriptions.\\nThe model we will develop will generate a caption given a photo, and the caption will be\\ngenerated one word at a time. The sequence of previously generated words will be provided as\\ninput. Therefore, we will need a ﬁrst word to kick-oﬀ the generation process and a last word to\\nsignal the end of the caption. We will use the strings startseq and endseq for this purpose.\\nThese tokens are added to the loaded descriptions as they are loaded. It is important to do this\\nnow before we encode the text so that the tokens are also encoded correctly.\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 325}, page_content=\"26.5. Develop Deep Learning Model 309\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\nListing 26.12: Function for loading the clean photo descriptions\\nNext, we can load the photo features for a given dataset. Below deﬁnes a function named\\nload photo features() that loads the entire set of photo descriptions, then returns the subset\\nof interest for a given set of photo identiﬁers. This is not very eﬃcient; nevertheless, this will\\nget us up and running quickly.\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\\n# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\\nListing 26.13: Function for loading pre-calculated photo features\\nWe can pause here and test everything developed so far. The complete code example is\\nlisted below.\\nfrom pickle import load\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 326}, page_content=\"26.5. Develop Deep Learning Model 310\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\\n# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\\n# load training dataset (6K)\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\\ntrain = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# photo features\\ntrain_features = load_photo_features( 'features.pkl ', train)\\nprint( 'Photos: train=%d '% len(train_features))\\nListing 26.14: Complete example of loading the prepared data.\\nRunning this example ﬁrst loads the 6,000 photo identiﬁers in the test dataset. These\\nfeatures are then used to ﬁlter and load the cleaned description text and the pre-computed\\nphoto features. We are nearly there.\\nDataset: 6,000\\nDescriptions: train=6,000\\nPhotos: train=6,000\\nListing 26.15: Example output from preparing the text data\\nThe description text will need to be encoded to numbers before it can be presented to\\nthe model as in input or compared to the model’s predictions. The ﬁrst step in encoding the\\ndata is to create a consistent mapping from words to unique integer values. Keras provides\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 327}, page_content=\"26.5. Develop Deep Learning Model 311\\ntheTokenizer class that can learn this mapping from the loaded description data. Below\\ndeﬁnes the tolines() to convert the dictionary of descriptions into a list of strings and the\\ncreate tokenizer() function that will ﬁt a Tokenizer given the loaded photo description text.\\n# convert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 26.16: Example of preparing the Tokenizer\\nWe can now encode the text. Each description will be split into words. The model will be\\nprovided one word and the photo and generate the next word. Then the ﬁrst two words of the\\ndescription will be provided to the model as input with the image to generate the next word.\\nThis is how the model will be trained. For example, the input sequence “ little girl running in\\nﬁeld” would be split into 6 input-output pairs to train the model:\\nX1, X2 (text sequence), y (word)\\nphoto startseq, little\\nphoto startseq, little, girl\\nphoto startseq, little, girl, running\\nphoto startseq, little, girl, running, in\\nphoto startseq, little, girl, running, in, field\\nphoto startseq, little, girl, running, in, field, endseq\\nListing 26.17: Example of how a photo description is transformed into input and output\\nsequences\\nLater, when the model is used to generate descriptions, the generated words will be con-\\ncatenated and recursively provided as input to generate a caption for an image. The function\\nbelow named create sequences() , given the tokenizer, a maximum sequence length, and the\\ndictionary of all descriptions and photos, will transform the data into input-output pairs of data\\nfor training the model. There are two input arrays to the model: one for photo features and\\none for the encoded text. There is one output for the model which is the encoded next word in\\nthe text sequence.\\nThe input text is encoded as integers, which will be fed to a word embedding layer. The\\nphoto features will be fed directly to another part of the model. The model will output a\\nprediction, which will be a probability distribution over all words in the vocabulary. The\\noutput data will therefore be a one hot encoded version of each word, representing an idealized\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 328}, page_content='26.5. Develop Deep Learning Model 312\\nprobability distribution with 0 values at all word positions except the actual word position,\\nwhich has a value of 1.\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, descriptions, photos):\\nX1, X2, y = list(), list(), list()\\n# walk through each image identifier\\nfor key, desc_list in descriptions.items():\\n# walk through each description for the image\\nfor desc in desc_list:\\n# encode the sequence\\nseq = tokenizer.texts_to_sequences([desc])[0]\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nX1.append(photos[key][0])\\nX2.append(in_seq)\\ny.append(out_seq)\\nreturn array(X1), array(X2), array(y)\\nListing 26.18: Function for creating input and output sequences\\nWe will need to calculate the maximum number of words in the longest description. A short\\nhelper function named maxlength() is deﬁned below.\\n# calculate the length of the description with the most words\\ndef max_length(descriptions):\\nlines = to_lines(descriptions)\\nreturn max(len(d.split()) for d in lines)\\nListing 26.19: Function for calculating the maximum sequence length.\\nWe now have enough to load the data for the training and development datasets and\\ntransform the loaded data into input-output pairs for ﬁtting a deep learning model.\\n26.5.2 Deﬁning the Model\\nWe will deﬁne a deep learning based on the merge-model described by Marc Tanti, et al. in\\ntheir 2017 papers. Note, the merge model for image captioning was introduced in Chapter 22.\\nWe will describe the model in three parts:\\n\\x88Photo Feature Extractor . This is a 16-layer VGG model pre-trained on the ImageNet\\ndataset. We have pre-processed the photos with the VGG model (without the output\\nlayer) and will use the extracted features predicted by this model as input.\\n\\x88Sequence Processor . This is a word embedding layer for handling the text input,\\nfollowed by a Long Short-Term Memory (LSTM) recurrent neural network layer.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 329}, page_content=\"26.5. Develop Deep Learning Model 313\\n\\x88Decoder (for lack of a better name). Both the feature extractor and sequence processor\\noutput a ﬁxed-length vector. These are merged together and processed by a Dense layer\\nto make a ﬁnal prediction.\\nThe Photo Feature Extractor model expects input photo features to be a vector of 4,096\\nelements. These are processed by a Dense layer to produce a 256 element representation of the\\nphoto. The Sequence Processor model expects input sequences with a pre-deﬁned length (34\\nwords) which are fed into an Embedding layer that uses a mask to ignore padded values. This is\\nfollowed by an LSTM layer with 256 memory units.\\nBoth the input models produce a 256 element vector. Further, both input models use\\nregularization in the form of 50% dropout. This is to reduce overﬁtting the training dataset, as\\nthis model conﬁguration learns very fast. The Decoder model merges the vectors from both\\ninput models using an addition operation. This is then fed to a Dense 256 neuron layer and then\\nto a ﬁnal output Dense layer that makes a softmax prediction over the entire output vocabulary\\nfor the next word in the sequence. The function below named define model() deﬁnes and\\nreturns the model ready to be ﬁt.\\n# define the captioning model\\ndef define_model(vocab_size, max_length):\\n# feature extractor model\\ninputs1 = Input(shape=(4096,))\\nfe1 = Dropout(0.5)(inputs1)\\nfe2 = Dense(256, activation= 'relu ')(fe1)\\n# sequence model\\ninputs2 = Input(shape=(max_length,))\\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\\nse2 = Dropout(0.5)(se1)\\nse3 = LSTM(256)(se2)\\n# decoder model\\ndecoder1 = add([fe2, se3])\\ndecoder2 = Dense(256, activation= 'relu ')(decoder1)\\noutputs = Dense(vocab_size, activation= 'softmax ')(decoder2)\\n# tie it together [image, seq] [word]\\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\\n# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ')\\n# summarize model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 26.20: Function to deﬁne the caption generation model.\\nA plot of the model is created and helps to better understand the structure of the network\\nand the two streams of input.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 330}, page_content=\"26.5. Develop Deep Learning Model 314\\nFigure 26.1: Plot of the deﬁned caption generation model.\\n26.5.3 Fitting the Model\\nNow that we know how to deﬁne the model, we can ﬁt it on the training dataset. The model\\nlearns fast and quickly overﬁts the training dataset. For this reason, we will monitor the skill\\nof the trained model on the holdout development dataset. When the skill of the model on the\\ndevelopment dataset improves at the end of an epoch, we will save the whole model to ﬁle.\\nAt the end of the run, we can then use the saved model with the best skill on the training\\ndataset as our ﬁnal model. We can do this by deﬁning a ModelCheckpoint in Keras and\\nspecifying it to monitor the minimum loss on the validation dataset and save the model to a ﬁle\\nthat has both the training and validation loss in the ﬁlename.\\n# define checkpoint callback\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\\nListing 26.21: Example of checkpoint conﬁguration.\\nWe can then specify the checkpoint in the call to fit() via the callbacks argument. We\\nmust also specify the development dataset in fit() via the validation data argument. We\\nwill only ﬁt the model for 20 epochs, but given the amount of training data, each epoch may\\ntake 30 minutes on modern hardware.\\n# fit model\\nmodel.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint],\\nvalidation_data=([X1test, X2test], ytest))\\nListing 26.22: Example of ﬁtting the caption generation model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 331}, page_content=\"26.5. Develop Deep Learning Model 315\\n26.5.4 Complete Example\\nThe complete example for ﬁtting the model on the training data is listed below. Note, running\\nthis example may require a machine with 8 or more Gigabytes of RAM. See the appendix for\\nusing AWS, if needed.\\nfrom numpy import array\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\nfrom keras.layers import Dropout\\nfrom keras.layers.merge import add\\nfrom keras.callbacks import ModelCheckpoint\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 332}, page_content=\"26.5. Develop Deep Learning Model 316\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\\n# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\\n# covert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the length of the description with the most words\\ndef max_length(descriptions):\\nlines = to_lines(descriptions)\\nreturn max(len(d.split()) for d in lines)\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, descriptions, photos):\\nX1, X2, y = list(), list(), list()\\n# walk through each image identifier\\nfor key, desc_list in descriptions.items():\\n# walk through each description for the image\\nfor desc in desc_list:\\n# encode the sequence\\nseq = tokenizer.texts_to_sequences([desc])[0]\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nX1.append(photos[key][0])\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 333}, page_content=\"26.5. Develop Deep Learning Model 317\\nX2.append(in_seq)\\ny.append(out_seq)\\nreturn array(X1), array(X2), array(y)\\n# define the captioning model\\ndef define_model(vocab_size, max_length):\\n# feature extractor model\\ninputs1 = Input(shape=(4096,))\\nfe1 = Dropout(0.5)(inputs1)\\nfe2 = Dense(256, activation= 'relu ')(fe1)\\n# sequence model\\ninputs2 = Input(shape=(max_length,))\\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\\nse2 = Dropout(0.5)(se1)\\nse3 = LSTM(256)(se2)\\n# decoder model\\ndecoder1 = add([fe2, se3])\\ndecoder2 = Dense(256, activation= 'relu ')(decoder1)\\noutputs = Dense(vocab_size, activation= 'softmax ')(decoder2)\\n# tie it together [image, seq] [word]\\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\\n# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ')\\n# summarize model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load training dataset (6K)\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\\ntrain = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# photo features\\ntrain_features = load_photo_features( 'features.pkl ', train)\\nprint( 'Photos: train=%d '% len(train_features))\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# determine the maximum sequence length\\nmax_length = max_length(train_descriptions)\\nprint( 'Description Length: %d '% max_length)\\n# prepare sequences\\nX1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions,\\ntrain_features)\\n# load test set\\nfilename = 'Flickr8k_text/Flickr_8k.devImages.txt '\\ntest = load_set(filename)\\nprint( 'Dataset: %d '% len(test))\\n# descriptions\\ntest_descriptions = load_clean_descriptions( 'descriptions.txt ', test)\\nprint( 'Descriptions: test=%d '% len(test_descriptions))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 334}, page_content=\"26.6. Evaluate Model 318\\n# photo features\\ntest_features = load_photo_features( 'features.pkl ', test)\\nprint( 'Photos: test=%d '% len(test_features))\\n# prepare sequences\\nX1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions,\\ntest_features)\\n# define the model\\nmodel = define_model(vocab_size, max_length)\\n# define checkpoint callback\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\\n# fit model\\nmodel.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint],\\nvalidation_data=([X1test, X2test], ytest))\\nListing 26.23: Complete example of training the caption generation model.\\nRunning the example ﬁrst prints a summary of the loaded training and development datasets.\\nDataset: 6,000\\nDescriptions: train=6,000\\nPhotos: train=6,000\\nVocabulary Size: 7,579\\nDescription Length: 34\\nDataset: 1,000\\nDescriptions: test=1,000\\nPhotos: test=1,000\\nTrain on 306,404 samples, validate on 50,903 samples\\nListing 26.24: Sample output from ﬁtting the caption generation model\\nAfter the summary of the model, we can get an idea of the total number of training and\\nvalidation (development) input-output pairs. The model then runs, saving the best model to\\n.h5ﬁles along the way. Note, that even on a modern CPU, each epoch may take 20 minutes.\\nYou may want to consider running the example on a GPU, such as on AWS. See the appendix\\nfor details on how to set this up. When I ran the example, the best model was saved at the end\\nof epoch 2 with a loss of 3.245 on the training dataset and a loss of 3.612 on the development\\ndataset.\\n26.6 Evaluate Model\\nOnce the model is ﬁt, we can evaluate the skill of its predictions on the holdout test dataset.\\nWe will evaluate a model by generating descriptions for all photos in the test dataset and\\nevaluating those predictions with a standard cost function. First, we need to be able to generate\\na description for a photo using a trained model.\\nThis involves passing in the start description token startseq , generating one word, then\\ncalling the model recursively with generated words as input until the end of sequence token\\nis reached endseq or the maximum description length is reached. The function below named\\ngenerate desc() implements this behavior and generates a textual description given a trained\\nmodel, and a given prepared photo as input. It calls the function word forid() in order to\\nmap an integer prediction back to a word.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 335}, page_content=\"26.6. Evaluate Model 319\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# generate a description for an image\\ndef generate_desc(model, tokenizer, photo, max_length):\\n# seed the generation process\\nin_text = 'startseq '\\n# iterate over the whole length of the sequence\\nfor i in range(max_length):\\n# integer encode input sequence\\nsequence = tokenizer.texts_to_sequences([in_text])[0]\\n# pad input\\nsequence = pad_sequences([sequence], maxlen=max_length)\\n# predict next word\\nyhat = model.predict([photo,sequence], verbose=0)\\n# convert probability to integer\\nyhat = argmax(yhat)\\n# map integer to word\\nword = word_for_id(yhat, tokenizer)\\n# stop if we cannot map the word\\nif word is None:\\nbreak\\n# append as input for generating the next word\\nin_text += ' ' + word\\n# stop if we predict the end of the sequence\\nif word == 'endseq ':\\nbreak\\nreturn in_text\\nListing 26.25: Functions for generating a description for a photo.\\nWhen generating and comparing photo descriptions, we will need to strip oﬀ the special\\nstart and end of sequence words. The function below named cleanup summary() will perform\\nthis operation.\\n# remove start/end sequence tokens from a summary\\ndef cleanup_summary(summary):\\n# remove start of sequence token\\nindex = summary.find( 'startseq ')\\nif index > -1:\\nsummary = summary[len( 'startseq '):]\\n# remove end of sequence token\\nindex = summary.find( 'endseq ')\\nif index > -1:\\nsummary = summary[:index]\\nreturn summary\\nListing 26.26: Functions to remove start and end of sequence words.\\nWe will generate predictions for all photos in the test dataset. The function below named\\nevaluate model() will evaluate a trained model against a given dataset of photo descriptions\\nand photo features. The actual and predicted descriptions are collected and evaluated collectively\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 336}, page_content=\"26.6. Evaluate Model 320\\nusing the corpus BLEU score that summarizes how close the generated text is to the expected\\ntext.\\n# evaluate the skill of the model\\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\\nactual, predicted = list(), list()\\n# step over the whole set\\nfor key, desc_list in descriptions.items():\\n# generate description\\nyhat = generate_desc(model, tokenizer, photos[key], max_length)\\n# clean up prediction\\nyhat = cleanup_summary(yhat)\\n# store actual and predicted\\nreferences = [cleanup_summary(d).split() for d in desc_list]\\nactual.append(references)\\npredicted.append(yhat.split())\\n# calculate BLEU score\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\\nprint( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\\nListing 26.27: Functions for evaluating a caption generation model.\\nBLEU scores are used in text translation for evaluating translated text against one or\\nmore reference translations. Here, we compare each generated description against all of the\\nreference descriptions for the photograph. We then calculate BLEU scores for 1, 2, 3 and 4\\ncumulative n-grams. The NLTK Python library implements the BLEU score calculation in the\\ncorpus bleu() function. A higher score close to 1.0 is better, a score closer to zero is worse.\\nNote that the BLEU score and NLTK API were introduced in Chapter 24.\\nWe can put all of this together with the functions from the previous section for loading the\\ndata. We ﬁrst need to load the training dataset in order to prepare a Tokenizer so that we\\ncan encode generated words as input sequences for the model. It is critical that we encode the\\ngenerated words using exactly the same encoding scheme as was used when training the model.\\nWe then use these functions for loading the test dataset. The complete example is listed below.\\nfrom numpy import argmax\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\\nfrom nltk.translate.bleu_score import corpus_bleu\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 337}, page_content=\"26.6. Evaluate Model 321\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\\n# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\\n# covert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the length of the description with the most words\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 338}, page_content=\"26.6. Evaluate Model 322\\ndef max_length(descriptions):\\nlines = to_lines(descriptions)\\nreturn max(len(d.split()) for d in lines)\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# generate a description for an image\\ndef generate_desc(model, tokenizer, photo, max_length):\\n# seed the generation process\\nin_text = 'startseq '\\n# iterate over the whole length of the sequence\\nfor _ in range(max_length):\\n# integer encode input sequence\\nsequence = tokenizer.texts_to_sequences([in_text])[0]\\n# pad input\\nsequence = pad_sequences([sequence], maxlen=max_length)\\n# predict next word\\nyhat = model.predict([photo,sequence], verbose=0)\\n# convert probability to integer\\nyhat = argmax(yhat)\\n# map integer to word\\nword = word_for_id(yhat, tokenizer)\\n# stop if we cannot map the word\\nif word is None:\\nbreak\\n# append as input for generating the next word\\nin_text += ' ' + word\\n# stop if we predict the end of the sequence\\nif word == 'endseq ':\\nbreak\\nreturn in_text\\n# remove start/end sequence tokens from a summary\\ndef cleanup_summary(summary):\\n# remove start of sequence token\\nindex = summary.find( 'startseq ')\\nif index > -1:\\nsummary = summary[len( 'startseq '):]\\n# remove end of sequence token\\nindex = summary.find( 'endseq ')\\nif index > -1:\\nsummary = summary[:index]\\nreturn summary\\n# evaluate the skill of the model\\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\\nactual, predicted = list(), list()\\n# step over the whole set\\nfor key, desc_list in descriptions.items():\\n# generate description\\nyhat = generate_desc(model, tokenizer, photos[key], max_length)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 339}, page_content=\"26.6. Evaluate Model 323\\n# clean up prediction\\nyhat = cleanup_summary(yhat)\\n# store actual and predicted\\nreferences = [cleanup_summary(d).split() for d in desc_list]\\nactual.append(references)\\npredicted.append(yhat.split())\\n# calculate BLEU score\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\\nprint( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\\n# load training dataset (6K)\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\\ntrain = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# determine the maximum sequence length\\nmax_length = max_length(train_descriptions)\\nprint( 'Description Length: %d '% max_length)\\n# load test set\\nfilename = 'Flickr8k_text/Flickr_8k.testImages.txt '\\ntest = load_set(filename)\\nprint( 'Dataset: %d '% len(test))\\n# descriptions\\ntest_descriptions = load_clean_descriptions( 'descriptions.txt ', test)\\nprint( 'Descriptions: test=%d '% len(test_descriptions))\\n# photo features\\ntest_features = load_photo_features( 'features.pkl ', test)\\nprint( 'Photos: test=%d '% len(test_features))\\n# load the model\\nfilename = 'model.h5 '\\nmodel = load_model(filename)\\n# evaluate model\\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\\nListing 26.28: Complete example of evaluating the caption generation model.\\nRunning the example prints the BLEU scores. We can see that the scores ﬁt within the\\nexpected range of a skillful model on the problem. The chosen model conﬁguration is by no\\nmeans optimized.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nBLEU-1: 0.438805\\nBLEU-2: 0.230646\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 340}, page_content=\"26.7. Generate New Captions 324\\nBLEU-3: 0.150245\\nBLEU-4: 0.062847\\nListing 26.29: Sample output from evaluating the caption generation model\\n26.7 Generate New Captions\\nNow that we know how to develop and evaluate a caption generation model, how can we use it?\\nAlmost everything we need to generate captions for entirely new photographs is in the model\\nﬁle. We also need the Tokenizer for encoding generated words for the model while generating\\na sequence, and the maximum length of input sequences, used when we deﬁned the model (e.g.\\n34).\\nWe can hard code the maximum sequence length. With the encoding of text, we can create\\nthe tokenizer and save it to a ﬁle so that we can load it quickly whenever we need it without\\nneeding the entire Flickr8K dataset. An alternative would be to use our own vocabulary ﬁle\\nand mapping to integers function during training. We can create the Tokenizer as before and\\nsave it as a pickle ﬁle tokenizer.pkl . The complete example is listed below.\\nfrom keras.preprocessing.text import Tokenizer\\nfrom pickle import dump\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 341}, page_content=\"26.7. Generate New Captions 325\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# covert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# load training dataset\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\\ntrain = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\n# save the tokenizer\\ndump(tokenizer, open( 'tokenizer.pkl ', 'wb '))\\nListing 26.30: Complete example of preparing and saving the Tokenizer .\\nWe can now load the tokenizer whenever we need it without having to load the entire training\\ndataset of annotations. Now, let’s generate a description for a new photograph. Below is a new\\nphotograph that I chose randomly on Flickr (available under a permissive license)1.\\n1https://www.flickr.com/photos/bambe1964/7837618434/\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 342}, page_content=\"26.7. Generate New Captions 326\\nFigure 26.2: Photo of a dog at the beach. Photo by bambe1964 , some rights reserved.\\nWe will generate a description for it using our model. Download the photograph and save\\nit to your local directory with the ﬁlename example.jpg . First, we must load the Tokenizer\\nfrom tokenizer.pkl and deﬁne the maximum length of the sequence to generate, needed for\\npadding inputs.\\n# load the tokenizer\\ntokenizer = load(open( 'tokenizer.pkl ', 'rb '))\\n# pre-define the max sequence length (from training)\\nmax_length = 34\\nListing 26.31: Example of loading the saved Tokenizer\\nThen we must load the model, as before.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\nListing 26.32: Example of loading the saved model\\nNext, we must load the photo we wish to describe and extract the features. We could do this\\nby re-deﬁning the model and adding the VGG-16 model to it, or we can use the VGG model to\\npredict the features and use them as inputs to our existing model. We will do the latter and\\nuse a modiﬁed version of the extract features() function used during data preparation, but\\nadapted to work on a single photo.\\n# extract features from each photo in the directory\\ndef extract_features(filename):\\n# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# load the photo\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 343}, page_content=\"26.7. Generate New Captions 327\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\nreturn feature\\n# load and prepare the photograph\\nphoto = extract_features( 'example.jpg ')\\nListing 26.33: Example of extracting features for the provided photo.\\nWe can then generate a description using the generate desc() function deﬁned when\\nevaluating the model. The complete example for generating a description for an entirely new\\nstandalone photograph is listed below.\\nfrom pickle import load\\nfrom numpy import argmax\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.models import Model\\nfrom keras.models import load_model\\n# extract features from each photo in the directory\\ndef extract_features(filename):\\n# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# load the photo\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\nreturn feature\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# remove start/end sequence tokens from a summary\\ndef cleanup_summary(summary):\\n# remove start of sequence token\\nindex = summary.find( 'startseq ')\\nif index > -1:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 344}, page_content=\"26.7. Generate New Captions 328\\nsummary = summary[len( 'startseq '):]\\n# remove end of sequence token\\nindex = summary.find( 'endseq ')\\nif index > -1:\\nsummary = summary[:index]\\nreturn summary\\n# generate a description for an image\\ndef generate_desc(model, tokenizer, photo, max_length):\\n# seed the generation process\\nin_text = 'startseq '\\n# iterate over the whole length of the sequence\\nfor _ in range(max_length):\\n# integer encode input sequence\\nsequence = tokenizer.texts_to_sequences([in_text])[0]\\n# pad input\\nsequence = pad_sequences([sequence], maxlen=max_length)\\n# predict next word\\nyhat = model.predict([photo,sequence], verbose=0)\\n# convert probability to integer\\nyhat = argmax(yhat)\\n# map integer to word\\nword = word_for_id(yhat, tokenizer)\\n# stop if we cannot map the word\\nif word is None:\\nbreak\\n# append as input for generating the next word\\nin_text += ' ' + word\\n# stop if we predict the end of the sequence\\nif word == 'endseq ':\\nbreak\\nreturn in_text\\n# load the tokenizer\\ntokenizer = load(open( 'tokenizer.pkl ', 'rb '))\\n# pre-define the max sequence length (from training)\\nmax_length = 34\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# load and prepare the photograph\\nphoto = extract_features( 'example.jpg ')\\n# generate description\\ndescription = generate_desc(model, tokenizer, photo, max_length)\\ndescription = cleanup_summary(description)\\nprint(description)\\nListing 26.34: Complete example of generating a description for a new photo.\\nIn this case, the description generated was as follows:\\ndog is running across the beach\\nListing 26.35: Sample output from generating a caption for the new photograph\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 345}, page_content='26.8. Extensions 329\\n26.8 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Alternate Pre-Trained Image Models . A small 16-layer VGG model was used for\\nfeature extraction. Consider exploring larger models that oﬀer better performance on the\\nImageNet dataset, such as Inception.\\n\\x88Smaller Vocabulary . A larger vocabulary of nearly eight thousand words was used in\\nthe development of the model. Many of the words supported may be misspellings or only\\nused once in the entire dataset. Reﬁne the vocabulary and reduce the size, perhaps by\\nhalf.\\n\\x88Pre-trained Word Vectors . The model learned the word vectors as part of ﬁtting the\\nmodel. Better performance may be achieved by using word vectors either pre-trained on\\nthe training dataset or trained on a much larger corpus of text, such as news articles or\\nWikipedia.\\n\\x88Train Word2Vec Vectors . Pre-train word vectors using Word2Vec on the description\\ndata and explore models that allow and don’t allow ﬁne tuning of the vectors during\\ntraining, then compare skill.\\n\\x88Tune Model . The conﬁguration of the model was not tuned on the problem. Explore\\nalternate conﬁgurations and see if you can achieve better performance.\\n\\x88Inject Architecture . Explore the inject architecture for caption generation and compare\\nperformance to the merge architecture used in this tutorial.\\n\\x88Alternate Framings . Explore alternate framings of the problems such as generating the\\nentire sequence from the photo alone.\\n\\x88Pre-Train Language Model . Pre-train a language model for generating description\\ntext, then use it in the caption generation model and evaluate the impact on model\\ntraining time and skill.\\n\\x88Truncate Descriptions . Only train the model on description at or below a speciﬁc\\nnumber of words and explore truncating long descriptions to a preferred length. Evaluate\\nthe impact on training time and model skill.\\n\\x88Alternate Measure . Explore alternate performance measures beside BLEU such as\\nROGUE. Compare scores for the same descriptions to develop an intuition for how the\\nmeasures diﬀer in practice.\\nIf you explore any of these extensions, I’d love to know.\\n26.9 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 346}, page_content='26.9. Further Reading 330\\n26.9.1 Caption Generation Papers\\n\\x88Show and Tell: A Neural Image Caption Generator , 2015.\\nhttps://arxiv.org/abs/1411.4555\\n\\x88Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nhttps://arxiv.org/abs/1502.03044\\n\\x88Where to put the Image in an Image Caption Generator , 2017.\\nhttps://arxiv.org/abs/1703.09137\\n\\x88What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nhttps://arxiv.org/abs/1708.02043\\n\\x88Automatic Description Generation from Images: A Survey of Models, Datasets, and\\nEvaluation Measures , 2016.\\nhttps://arxiv.org/abs/1601.03896\\n26.9.2 Flickr8K Dataset\\n\\x88Framing image description as a ranking task: data, models and evaluation metrics (Home-\\npage).\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.\\nhtml\\n\\x88Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics ,\\n2013.\\nhttps://www.jair.org/media/3994/live-3994-7274-jair.pdf\\n\\x88Dataset Request Form.\\nhttps://illinois.edu/fb/sec/1713398\\n\\x88Old Flicrk8K Homepage.\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html\\n26.9.3 API\\n\\x88Keras Model API.\\nhttps://keras.io/models/model/\\n\\x88Keras padsequences() API.\\nhttps://keras.io/preprocessing/sequence/#pad_sequences\\n\\x88Keras Tokenizer API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n\\x88Keras VGG16 API.\\nhttps://keras.io/applications/#vgg16'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 347}, page_content='26.10. Summary 331\\n\\x88Gensim Word2Vec API.\\nhttps://radimrehurek.com/gensim/models/word2vec.html\\n\\x88nltk.translate package API Documentation.\\nhttp://www.nltk.org/api/nltk.translate.html\\n26.10 Summary\\nIn this tutorial, you discovered how to develop a photo captioning deep learning model from\\nscratch. Speciﬁcally, you learned:\\n\\x88How to prepare photo and text data ready for training a deep learning model.\\n\\x88How to design and train a deep learning caption generation model.\\n\\x88How to evaluate a train caption generation model and use it to caption entirely new\\nphotographs.\\n26.10.1 Next\\nThis is the last chapter in the image captioning part. In the next part you will discover how to\\ndevelop neural machine translation models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 348}, page_content='Part IX\\nMachine Translation\\n332'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 349}, page_content='Chapter 27\\nNeural Machine Translation\\nOne of the earliest goals for computers was the automatic translation of text from one language\\nto another. Automatic or machine translation is perhaps one of the most challenging artiﬁcial\\nintelligence tasks given the ﬂuidity of human language. Classically, rule-based systems were\\nused for this task, which were replaced in the 1990s with statistical methods. More recently,\\ndeep neural network models achieve state-of-the-art results in a ﬁeld that is aptly named neural\\nmachine translation. In this chapter, you will discover the challenge of machine translation\\nand the eﬀectiveness of neural machine translation models. After reading this chapter, you will\\nknow:\\n\\x88Machine translation is challenging given the inherent ambiguity and ﬂexibility of human\\nlanguage.\\n\\x88Statistical machine translation replaces classical rule-based systems with models that learn\\nto translate from examples.\\n\\x88Neural machine translation models ﬁt a single model rather than a pipeline of ﬁne-tuned\\nmodels and currently achieve state-of-the-art results.\\nLet’s get started.\\n27.1 What is Machine Translation?\\nMachine translation is the task of automatically converting source text in one language to text\\nin another language.\\nIn a machine translation task, the input already consists of a sequence of symbols\\nin some language, and the computer program must convert this into a sequence of\\nsymbols in another language.\\n— Page 98, Deep Learning , 2016.\\nGiven a sequence of text in a source language, there is no one single best translation of that\\ntext to another language. This is because of the natural ambiguity and ﬂexibility of human\\nlanguage. This makes the challenge of automatic machine translation diﬃcult, perhaps one of\\nthe most diﬃcult in artiﬁcial intelligence:\\n333'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 350}, page_content='27.2. What is Statistical Machine Translation? 334\\nThe fact is that accurate translation requires background knowledge in order to\\nresolve ambiguity and establish the content of the sentence.\\n— Page 21, Artiﬁcial Intelligence, A Modern Approach , 3rd Edition, 2009.\\nClassical machine translation methods often involve rules for converting text in the source\\nlanguage to the target language. The rules are often developed by linguists and may operate at\\nthe lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study:\\nRule-based Machine Translation, or RBMT.\\nRBMT is characterized with the explicit use and manual creation of linguistically\\ninformed rules and representations.\\n— Page 133, Handbook of Natural Language Processing and Machine Translation , 2011.\\nThe key limitations of the classical machine translation approaches are both the expertise\\nrequired to develop the rules, and the vast number of rules and exceptions required.\\n27.2 What is Statistical Machine Translation?\\nStatistical machine translation, or SMT for short, is the use of statistical models that learn to\\ntranslate text from a source language to a target language given a large corpus of examples.\\nThis task of using a statistical model can be stated formally as follows:\\nGiven a sentence T in the target language, we seek the sentence S from which the\\ntranslator produced T. We know that our chance of error is minimized by choosing\\nthat sentence S that is most probable given T. Thus, we wish to choose S so as to\\nmaximize Pr(S|T).\\n—A Statistical Approach to Machine Translation , 1990.\\nThis formal speciﬁcation makes the maximizing of the probability of the output sequence\\ngiven the input sequence of text explicit. It also makes the notion of there being a suite of\\ncandidate translations explicit and the need for a search process or decoder to select the one\\nmost likely translation from the model’s output probability distribution.\\nGiven a text in the source language, what is the most probable translation in the\\ntarget language? [...] how should one construct a statistical model that assigns high\\nprobabilities to “good” translations and low probabilities to “bad” translations?\\n— Page xiii, Syntax-based Statistical Machine Translation , 2017.\\nThe approach is data-driven, requiring only a corpus of examples with both source and target\\nlanguage text. This means linguists are not longer required to specify the rules of translation.\\nThis approach does not need a complex ontology of interlingua concepts, nor does it\\nneed handcrafted grammars of the source and target languages, nor a hand-labeled\\ntreebank. All it needs is data-sample translations from which a translation model\\ncan be learned.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 351}, page_content='27.3. What is Neural Machine Translation? 335\\n— Page 909, Artiﬁcial Intelligence, A Modern Approach , 3rd Edition, 2009.\\nQuickly, the statistical approach to machine translation outperformed the classical rule-based\\nmethods to become the de-facto standard set of techniques.\\nSince the inception of the ﬁeld at the end of the 1980s, the most popular models for\\nstatistical machine translation [...] have been sequence-based. In these models, the\\nbasic units of translation are words or sequences of words [...] These kinds of models\\nare simple and eﬀective, and they work well for man language pairs\\n—Syntax-based Statistical Machine Translation , 2017.\\nThe most widely used techniques were phrase-based and focus on translating sub-sequences\\nof the source text piecewise.\\nStatistical Machine Translation (SMT) has been the dominant translation paradigm\\nfor decades. Practical implementations of SMT are generally phrase-based systems\\n(PBMT) which translate sequences of words or phrases where the lengths may diﬀer\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nAlthough eﬀective, statistical machine translation methods suﬀered from a narrow focus on\\nthe phrases being translated, losing the broader nature of the target text. The hard focus on\\ndata-driven approaches also meant that methods may have ignored important syntax distinctions\\nknown by linguists. Finally, the statistical approaches required careful tuning of each module in\\nthe translation pipeline.\\n27.3 What is Neural Machine Translation?\\nNeural machine translation, or NMT for short, is the use of neural network models to learn\\na statistical model for machine translation. The key beneﬁt to the approach is that a single\\nsystem can be trained directly on source and target text, no longer requiring the pipeline of\\nspecialized systems used in statistical machine learning.\\nUnlike the traditional phrase-based translation system which consists of many small\\nsub-components that are tuned separately, neural machine translation attempts to\\nbuild and train a single, large neural network that reads a sentence and outputs a\\ncorrect translation.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nAs such, neural machine translation systems are said to be end-to-end systems as only one\\nmodel is required for the translation.\\nThe strength of NMT lies in its ability to learn directly, in an end-to-end fashion,\\nthe mapping from input text to associated output text.\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 352}, page_content='27.3. What is Neural Machine Translation? 336\\n27.3.1 Encoder-Decoder Model\\nMultilayer Perceptron neural network models can be used for machine translation, although the\\nmodels are limited by a ﬁxed-length input sequence where the output must be the same length.\\nThese early models have been greatly improved upon recently through the use of recurrent\\nneural networks organized into an encoder-decoder architecture that allow for variable length\\ninput and output sequences.\\nAn encoder neural network reads and encodes a source sentence into a ﬁxed-length\\nvector. A decoder then outputs a translation from the encoded vector. The whole\\nencoder-decoder system, which consists of the encoder and the decoder for a language\\npair, is jointly trained to maximize the probability of a correct translation given a\\nsource sentence.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nKey to the encoder-decoder architecture is the ability of the model to encode the source\\ntext into an internal ﬁxed-length representation called the context vector. Interestingly, once\\nencoded, diﬀerent decoding systems could be used, in principle, to translate the context into\\ndiﬀerent languages.\\n... one model ﬁrst reads the input sequence and emits a data structure that\\nsummarizes the input sequence. We call this summary the “context” C. [...] A\\nsecond mode, usually an RNN, then reads the context C and generates a sentence in\\nthe target language.\\n— Page 461, Deep Learning , 2016.\\n27.3.2 Encoder-Decoders with Attention\\nAlthough eﬀective, the Encoder-Decoder architecture has problems with long sequences of text\\nto be translated. The problem stems from the ﬁxed-length internal representation that must\\nbe used to decode each word in the output sequence. The solution is the use of an attention\\nmechanism that allows the model to learn where to place attention on the input sequence as\\neach word of the output sequence is decoded.\\nUsing a ﬁxed-sized representation to capture all the semantic details of a very long\\nsentence [...] is very diﬃcult. [...] A more eﬃcient approach, however, is to read\\nthe whole sentence or paragraph [...], then to produce the translated words one at\\na time, each time focusing on a diﬀerent part of the input sentence to gather the\\nsemantic details required to produce the next output word.\\n— Page 462, Deep Learning , 2016.\\nThe encoder-decoder recurrent neural network architecture with attention is currently the\\nstate-of-the-art on some benchmark problems for machine translation. And this architecture is\\nused in the heart of the Google Neural Machine Translation system, or GNMT, used in their\\nGoogle Translate service.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 353}, page_content='27.4. Further Reading 337\\n... current state-of-the-art machine translation systems are powered by models that\\nemploy attention.\\n— Page 209, Neural Network Methods in Natural Language Processing , 2017.\\nAlthough eﬀective, the neural machine translation systems still suﬀer some issues, such as\\nscaling to larger vocabularies of words and the slow speed of training the models. There are\\nthe current areas of focus for large production neural translation systems, such as the Google\\nsystem.\\nThree inherent weaknesses of Neural Machine Translation [...]: its slower training\\nand inference speed, ineﬀectiveness in dealing with rare words, and sometimes failure\\nto translate all words in the source sentence.\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\n27.4 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n27.4.1 Books\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2wPrW37\\n\\x88Syntax-based Statistical Machine Translation , 2017.\\nhttp://amzn.to/2xCrl3p\\n\\x88Deep Learning , 2016.\\nhttp://amzn.to/2xBEsBJ\\n\\x88Statistical Machine Translation , 2010.\\nhttp://amzn.to/2xCe1vP\\n\\x88Handbook of Natural Language Processing and Machine Translation , 2011.\\nhttp://amzn.to/2jYUFfy\\n\\x88Artiﬁcial Intelligence, A Modern Approach , 3rd Edition, 2009.\\nhttp://amzn.to/2wUZesr\\n27.4.2 Papers\\n\\x88A Statistical Approach to Machine Translation , 1990.\\nhttps://dl.acm.org/citation.cfm?id=92860\\n\\x88Review Article: Example-based Machine Translation , 1999.\\nhttps://link.springer.com/article/10.1023/A:1008109312730'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 354}, page_content='27.5. Summary 338\\n\\x88Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\nhttps://arxiv.org/abs/1406.1078\\n\\x88Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nhttps://arxiv.org/abs/1409.0473\\n\\x88Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nhttps://arxiv.org/abs/1609.08144\\n\\x88Sequence to sequence learning with neural networks , 2014.\\nhttps://arxiv.org/abs/1409.3215\\n\\x88Recurrent Continuous Translation Models , 2013.\\nhttp://www.aclweb.org/anthology/D13-1176\\n\\x88Continuous space translation models for phrase-based statistical machine translation , 2013.\\nhttps://aclweb.org/anthology/C/C12/C12-2104.pdf\\n27.4.3 Additional\\n\\x88Machine Translation Archive.\\nhttp://www.mt-archive.info/\\n\\x88Neural machine translation on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Neural_machine_translation\\n\\x88Neural Machine Translation, Statistical Machine Translation , 2017.\\nhttps://arxiv.org/abs/1709.07809\\n27.5 Summary\\nIn this chapter, you discovered the challenge of machine translation and the eﬀectiveness of\\nneural machine translation models. Speciﬁcally, you learned:\\n\\x88Machine translation is challenging given the inherent ambiguity and ﬂexibility of human\\nlanguage.\\n\\x88Statistical machine translation replaces classical rule-based systems with models that learn\\nto translate from examples.\\n\\x88Neural machine translation models ﬁt a single model rather than a pipeline of ﬁne tuned\\nmodels and currently achieve state-of-the-art results.\\n27.5.1 Next\\nIn the next chapter, you will discover how you can design neural machine translation models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 355}, page_content='Chapter 28\\nWhat are Encoder-Decoder Models for\\nNeural Machine Translation\\nThe encoder-decoder architecture for recurrent neural networks is the standard neural machine\\ntranslation method that rivals and in some cases outperforms classical statistical machine\\ntranslation methods. This architecture is very new, having only been pioneered in 2014,\\nalthough, has been adopted as the core technology inside Google’s translate service. In this\\nchapter, you will discover the two seminal examples of the encoder-decoder model for neural\\nmachine translation. After reading this chapter, you will know:\\n\\x88The encoder-decoder recurrent neural network architecture is the core technology inside\\nGoogle’s translate service.\\n\\x88The so-called Sutskever model for direct end-to-end machine translation.\\n\\x88The so-called Cho model that extends the architecture with GRU units and an attention\\nmechanism.\\nLet’s get started.\\n28.1 Encoder-Decoder Architecture for NMT\\nThe Encoder-Decoder architecture with recurrent neural networks has become an eﬀective\\nand standard approach for both neural machine translation (NMT) and sequence-to-sequence\\n(seq2seq) prediction in general. The key beneﬁts of the approach are the ability to train a single\\nend-to-end model directly on source and target sentences and the ability to handle variable\\nlength input and output sequences of text. As evidence of the success of the method, the\\narchitecture is the core of the Google translation service.\\nOur model follows the common sequence-to-sequence learning framework with\\nattention. It has three components: an encoder network, a decoder network, and an\\nattention network.\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016\\n339'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 356}, page_content='28.2. Sutskever NMT Model 340\\nIn this chapter, we will take a closer look at two diﬀerent research projects that developed\\nthe same Encoder-Decoder architecture at the same time in 2014 and achieved results that put\\nthe spotlight on the approach. They are:\\n\\x88Sutskever NMT Model\\n\\x88Cho NMT Model\\n28.2 Sutskever NMT Model\\nIn this section, we will look at the neural machine translation model developed by Ilya Sutskever,\\net al. as described in their 2014 paper Sequence to Sequence Learning with Neural Networks .\\nWe will refer to it as the Sutskever NMT Model , for lack of a better name. This is an important\\npaper as it was one of the ﬁrst to introduce the Encoder-Decoder model for machine translation\\nand more generally sequence-to-sequence learning. It is an important model in the ﬁeld of\\nmachine translation as it was one of the ﬁrst neural machine translation systems to outperform\\na baseline statistical machine learning model on a large translation task.\\n28.2.1 Problem\\nThe model was applied to English to French translation, speciﬁcally the WMT 2014 translation\\ntask. The translation task was processed one sentence at a time, and an end-of-sequence ( <EOS> )\\ntoken was added to the end of output sequences during training to signify the end of the\\ntranslated sequence. This allowed the model to be capable of predicting variable length output\\nsequences.\\nNote that we require that each sentence ends with a special end-of-sentence symbol\\n<EOS> , which enables the model to deﬁne a distribution over sequences of all possible\\nlengths.\\n—Sequence to Sequence Learning with Neural Networks , 2014.\\nThe model was trained on a subset of the 12 Million sentences in the dataset, comprised of\\n348 Million French words and 304 Million English words. This set was chosen because it was\\npre-tokenized. The source vocabulary was reduced to the 160,000 most frequent source English\\nwords and 80,000 of the most frequent target French words. All out-of-vocabulary words were\\nreplaced with the UNKtoken.\\n28.2.2 Model\\nAn Encoder-Decoder architecture was developed where an input sequence was read in entirety\\nand encoded to a ﬁxed-length internal representation. A decoder network then used this internal\\nrepresentation to output words until the end of sequence token was reached. LSTM networks\\nwere used for both the encoder and decoder.\\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to\\nobtain large ﬁxed-dimensional vector representation, and then to use another LSTM\\nto extract the output sequence from that vector'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 357}, page_content='28.2. Sutskever NMT Model 341\\n—Sequence to Sequence Learning with Neural Networks , 2014.\\nThe ﬁnal model was an ensemble of 5 deep learning models. A left-to-right beam search was\\nused during the inference of the translations.\\nFigure 28.1: Depiction of Sutskever Encoder-Decoder Model for Text Translation. Taken from\\nSequence to Sequence Learning with Neural Networks .\\n28.2.3 Model Conﬁguration\\nThe following provides a summary of the model conﬁguration taken from the paper:\\n\\x88Input sequences were reversed.\\n\\x88A 1000-dimensional word embedding layer was used to represent the input words.\\n\\x88Softmax was used on the output layer.\\n\\x88The input and output models had 4 layers with 1,000 units per layer.\\n\\x88The model was ﬁt for 7.5 epochs where some learning rate decay was performed.\\n\\x88A batch-size of 128 sequences was used during training.\\n\\x88Gradient clipping was used during training to mitigate the chance of gradient explosions.\\n\\x88Batches were comprised of sentences with roughly the same length to speed-up computation.\\nThe model was ﬁt on an 8-GPU machine where each layer was run on a diﬀerent GPU.\\nTraining took 10 days.\\nThe resulting implementation achieved a speed of 6,300 (both English and French)\\nwords per second with a minibatch size of 128. Training took about ten days with\\nthis implementation.\\n—Sequence to Sequence Learning with Neural Networks , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 358}, page_content='28.3. Cho NMT Model 342\\n28.2.4 Result\\nThe system achieved a BLEU score of 34.81, which is a good score compared to the baseline\\nscore developed with a statistical machine translation system of 33.30. Importantly, this is\\nthe ﬁrst example of a neural machine translation system that outperformed a phrase-based\\nstatistical machine translation baseline on a large scale problem.\\n... we obtained a BLEU score of 34.81 [...] This is by far the best result achieved by\\ndirect translation with large neural networks. For comparison, the BLEU score of\\nan SMT baseline on this dataset is 33.30\\n—Sequence to Sequence Learning with Neural Networks , 2014.\\nThe ﬁnal model was used to re-score the list of best translations and improved the score to\\n36.5 which brings it close to the best result at the time of 37.0.\\n28.3 Cho NMT Model\\nIn this section, we will look at the neural machine translation system described by Kyunghyun\\nCho, et al. in their 2014 paper titled Learning Phrase Representations using RNN Encoder-\\nDecoder for Statistical Machine Translation . We will refer to it as the Cho NMT Model model for\\nlack of a better name. Importantly, the Cho Model is used only to score candidate translations\\nand is not used directly for translation like the Sutskever model above. Although extensions to\\nthe work to better diagnose and improve the model do use it directly and alone for translation.\\n28.3.1 Problem\\nAs above, the problem is the English to French translation task from the WMT 2014 workshop.\\nThe source and target vocabulary were limited to the most frequent 15,000 French and English\\nwords which covers 93% of the dataset, and out of vocabulary words were replaced with UNK.\\n... called RNN Encoder-Decoder that consists of two recurrent neural networks\\n(RNN). One RNN encodes a sequence of symbols into a ﬁxed-length vector rep-\\nresentation, and the other decodes the representation into another sequence of\\nsymbols.\\n—Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 359}, page_content='28.3. Cho NMT Model 343\\nFigure 28.2: Depiction of the Encoder-Decoder architecture. Taken from Learning Phrase\\nRepresentations using RNN Encoder-Decoder for Statistical Machine Translation .\\nThe implementation does not use LSTM units; instead, a simpler recurrent neural network\\nunit is developed called the gated recurrent unit or GRU.\\n... we also propose a new type of hidden unit that has been motivated by the LSTM\\nunit but is much simpler to compute and implement.\\n—Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\n28.3.2 Model Conﬁguration\\nThe following provides a summary of the model conﬁguration taken from the paper:\\n\\x88A 100-dimensional word embedding was used to represent the input words.\\n\\x88The encoder and decoder were conﬁgured with 1 layer of 1000 GRU units.\\n\\x88500 Maxout units pooling 2 inputs were used after the decoder.\\n\\x88A batch size of 64 sentences was used during training.\\nThe model was trained for approximately 2 days.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 360}, page_content='28.3. Cho NMT Model 344\\n28.3.3 Extensions\\nIn the paper On the Properties of Neural Machine Translation: Encoder-Decoder Approaches ,\\nCho, et al. investigate the limitations of their model. They discover that performance degrades\\nquickly with the increase in the length of input sentences and with the number of words outside\\nof the vocabulary.\\nOur analysis revealed that the performance of the neural machine translation suﬀers\\nsigniﬁcantly from the length of sentences.\\n—On the Properties of Neural Machine Translation: Encoder-Decoder Approaches , 2014.\\nThey provide a useful graph of the performance of the model as the length of the sentence is\\nincreased that captures the graceful loss in skill with increased diﬃculty.\\nFigure 28.3: Loss in model skill with increased sentence length. Taken from On the Properties\\nof Neural Machine Translation: Encoder-Decoder Approaches .\\nTo address the problem of unknown words, they suggest dramatically increasing the vocabu-\\nlary of known words during training. They address the problem of sentence length in a follow-up\\npaper titled Neural Machine Translation by Jointly Learning to Align and Translate in which\\nthey propose the use of an attention mechanism. Instead of encoding the input sentence to a\\nﬁxed length vector, a fuller representation of the encoded input is kept and the model learns to\\nuse to pay attention to diﬀerent parts of the input for each word output by the decoder.\\nEach time the proposed model generates a word in a translation, it (soft-)searches\\nfor a set of positions in a source sentence where the most relevant information is\\nconcentrated. The model then predicts a target word based on the context vectors\\nassociated with these source positions and all the previous generated target words.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2015.\\nA wealth of technical details are provided in the paper; for example:\\n\\x88A similarly conﬁgured model is used, although with bidirectional layers.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 361}, page_content='28.4. Further Reading 345\\n\\x88The data is prepared such that 30,000 of the most common words are kept in the vocabulary.\\n\\x88The model is ﬁrst trained with sentences with a length up to 20 words, then with sentences\\nwith a length up to 50 words.\\n\\x88A batch size of 80 sentences is used and the model was ﬁt for 4-6 epochs.\\n\\x88A beam search was used during the inference to ﬁnd the most likely sequence of words for\\neach translation.\\nThis time the model takes approximately 5 days to train. The code for this follow-up work\\nis also made available. As with the Sutskever, the model achieved results within the reach of\\nclassical phrase-based statistical approaches.\\nPerhaps more importantly, the proposed approach achieved a translation performance\\ncomparable to the existing phrase-based statistical machine translation. It is a\\nstriking result, considering that the proposed architecture, or the whole family of\\nneural machine translation, has only been proposed as recently as this year. We\\nbelieve the architecture proposed here is a promising step toward better machine\\ntranslation and a better understanding of natural languages in general.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2015.\\nKyunghyun Cho is also the author of a 2015 series of posts on the Nvidia developer blog on\\nthe topic of the encoder-decoder architecture for neural machine translation titled Introduction\\nto Neural Machine Translation with GPUs. The series provides a good introduction to the topic\\nand the model.\\n28.4 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nhttps://arxiv.org/abs/1609.08144\\n\\x88Sequence to Sequence Learning with Neural Networks , 2014.\\nhttps://arxiv.org/abs/1409.3215\\n\\x88Presentation for Sequence to Sequence Learning with Neural Networks , 2016.\\nhttps://www.youtube.com/watch?v=-uyXE7dY5H0\\n\\x88Ilya Sutskever Homepage.\\nhttp://www.cs.toronto.edu/ ~ilya/\\n\\x88Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\nhttps://arxiv.org/abs/1406.1078'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 362}, page_content='28.5. Summary 346\\n\\x88Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nhttps://arxiv.org/abs/1409.0473\\n\\x88On the Properties of Neural Machine Translation: Encoder-Decoder Approaches , 2014.\\nhttps://arxiv.org/abs/1409.1259\\n\\x88Kyunghyun Cho Homepage.\\nhttp://www.kyunghyuncho.me/\\n\\x88Introduction to Neural Machine Translation with GPUs , 2015.\\nhttps://goo.gl/GmWjvX\\n28.5 Summary\\nIn this chapter, you discovered two examples of the encoder-decoder model for neural machine\\ntranslation. Speciﬁcally, you learned:\\n\\x88The encoder-decoder recurrent neural network architecture is the core technology inside\\nGoogle’s translate service.\\n\\x88The so-called Sutskever model for direct end-to-end machine translation.\\n\\x88The so-called Cho model that extends the architecture with GRU units and an attention\\nmechanism.\\n28.5.1 Next\\nIn the next chapter, you will discover how you can conﬁgure neural machine translation models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 363}, page_content='Chapter 29\\nHow to Conﬁgure Encoder-Decoder\\nModels for Machine Translation\\nThe encoder-decoder architecture for recurrent neural networks is achieving state-of-the-art\\nresults on standard machine translation benchmarks and is being used in the heart of industrial\\ntranslation services. The model is simple, but given the large amount of data required to train it,\\ntuning the myriad of design decisions in the model in order get top performance on your problem\\ncan be practically intractable. Thankfully, research scientists have used Google-scale hardware\\nto do this work for us and provide a set of heuristics for how to conﬁgure the encoder-decoder\\nmodel for neural machine translation and for sequence prediction generally.\\nIn this chapter, you will discover the details of how to best conﬁgure an encoder-decoder\\nrecurrent neural network for neural machine translation and other natural language processing\\ntasks. After reading this chapter, you will know:\\n\\x88The Google study that investigated each model design decision in the encoder-decoder\\nmodel to isolate their eﬀects.\\n\\x88The results and recommendations for design decisions like word embeddings, encoder and\\ndecoder depth, and attention mechanisms.\\n\\x88A set of base model design decisions that can be used as a starting point on your own\\nsequence-to-sequence projects.\\nLet’s get started.\\n29.1 Encoder-Decoder Model for Neural Machine Trans-\\nlation\\nThe Encoder-Decoder architecture for recurrent neural networks is displacing classical phrase-\\nbased statistical machine translation systems for state-of-the-art results. As evidence, by their\\n2016 paper Google’s Neural Machine Translation System: Bridging the Gap between Human\\nand Machine Translation , Google now uses the approach in their core of their Google Translate\\nservice.\\nA problem with this architecture is that the models are large, in turn requiring very large\\ndatasets on which to train. This has the eﬀect of model training taking days or weeks and\\n347'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 364}, page_content='29.2. Baseline Model 348\\nrequiring computational resources that are generally very expensive. As such, little work has\\nbeen done on the impact of diﬀerent design choices on the model and their impact on model\\nskill. This problem is addressed explicitly by Denny Britz, et al. in their 2017 paper Massive\\nExploration of Neural Machine Translation Architectures . In the paper, they design a baseline\\nmodel for a standard English-to-German translation task and enumerate a suite of diﬀerent\\nmodel design choices and describe their impact on the skill of the model. They claim that\\nthe complete set of experiments consumed more than 250,000 GPU compute hours, which is\\nimpressive, to say the least.\\nWe report empirical results and variance numbers for several hundred experimental\\nruns, corresponding to over 250,000 GPU hours on the standard WMT English\\nto German translation task. Our experiments lead to novel insights and practical\\nadvice for building and extending NMT architectures.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nIn this chapter, we will look at some of the ﬁndings from this paper that we can use to tune\\nour own neural machine translation models, as well as sequence-to-sequence models in general.\\n29.2 Baseline Model\\nWe can start-oﬀ by describing the baseline model used as the starting point for all experiments.\\nA baseline model conﬁguration was chosen such that the model would perform reasonably well\\non the translation task.\\n\\x88Embedding: 512-dimensions.\\n\\x88RNN Cell: Gated Recurrent Unit or GRU.\\n\\x88Encoder: Bidirectional.\\n\\x88Encoder Depth: 2-layers (1 layer in each direction).\\n\\x88Decoder Depth: 2-layers.\\n\\x88Attention: Bahdanau-style.\\n\\x88Optimizer: Adam.\\n\\x88Dropout: 20% on input.\\nEach experiment started with the baseline model and varied one element in an attempt to\\nisolate the impact of the design decision on the model skill, in this case, BLEU scores.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 365}, page_content='29.3. Word Embedding Size 349\\nFigure 29.1: Encoder-Decoder Architecture for Neural Machine Translation. Taken from Massive\\nExploration of Neural Machine Translation Architectures .\\n29.3 Word Embedding Size\\nA word-embedding is used to represent words input to the encoder. This is a distributed\\nrepresentation where each word is mapped to a ﬁxed-sized vector of continuous values. The\\nbeneﬁt of this approach is that diﬀerent words with similar meaning will have a similar\\nrepresentation. This distributed representation is often learned while ﬁtting the model on the\\ntraining data. The embedding size deﬁnes the length of the vectors used to represent words. It\\nis generally believed that a larger dimensionality will result in a more expressive representation,\\nand in turn, better skill. Interestingly, the results show that the largest size tested did achieve\\nthe best results, but the beneﬁt of increasing the size was minor overall.\\n[results show] that 2048-dimensional embeddings yielded the overall best result, they\\nonly did so by a small margin. Even small 128-dimensional embeddings performed\\nsurprisingly well, while converging almost twice as quickly.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Start with a small embedding, such as 128, perhaps increase the size\\nlater for a minor lift in skill.\\n29.4 RNN Cell Type\\nThere are generally three types of recurrent neural network cells that are commonly used:\\n\\x88Simple RNN.\\n\\x88Long Short-Term Memory or LSTM.\\n\\x88Gated Recurrent Unit or GRU.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 366}, page_content='29.5. Encoder-Decoder Depth 350\\nThe LSTM was developed to address the vanishing gradient problem of the Simple RNN\\nthat limited the training of deep RNNs. The GRU was developed in an attempt to simplify the\\nLSTM. Results showed that both the GRU and LSTM were signiﬁcantly better than the Simple\\nRNN, but the LSTM was generally better overall.\\nIn our experiments, LSTM cells consistently outperformed GRU cells\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use LSTM RNN units in your model.\\n29.5 Encoder-Decoder Depth\\nGenerally, deeper networks are believed to achieve better performance than shallow networks.\\nThe key is to ﬁnd a balance between network depth, model skill, and training time. This is\\nbecause we generally do not have inﬁnite resources to train very deep networks if the beneﬁt\\nto skill is minor. The authors explore the depth of both the encoder and decoder models and\\nthe impact on model skill. When it comes to encoders, it was found that depth did not have\\na dramatic impact on skill and more surprisingly, a 1-layer bidirectional model performs only\\nslightly better than a 4-layer bidirectional conﬁguration. A two-layer bidirectional encoder\\nperformed slightly better than other conﬁgurations tested.\\nWe found no clear evidence that encoder depth beyond two layers is necessary.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use a 1-layer bidirectional encoder and extend to 2 bidirectional layers\\nfor a small lift in skill.\\nA similar story was seen when it came to decoders. The skill between decoders with 1, 2,\\nand 4 layers was diﬀerent by a small amount where a 4-layer decoder was slightly better. An\\n8-layer decoder did not converge under the test conditions.\\nOn the decoder side, deeper models outperformed shallower ones by a small margin.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use a 1-layer decoder as a starting point and use a 4-layer decoder for\\nbetter results.\\n29.6 Direction of Encoder Input\\nThe order of the sequence of source text can be provided to the encoder a number of ways:\\n\\x88Forward or as-normal.\\n\\x88Reversed.\\n\\x88Both forward and reversed at the same time.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 367}, page_content='29.7. Attention Mechanism 351\\nThe authors explored the impact of the order of the input sequence on model skill comparing\\nvarious unidirectional and bidirectional conﬁgurations. Generally, they conﬁrmed previous\\nﬁndings that a reversed sequence is better than a forward sequence and that bidirectional is\\nslightly better than a reversed sequence.\\n... bidirectional encoders generally outperform unidirectional encoders, but not by\\na large margin. The encoders with reversed source consistently outperform their\\nnon-reversed counterparts.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use a reversed order input sequence or move to bidirectional for a\\nsmall lift in model skill.\\n29.7 Attention Mechanism\\nA problem with the naive Encoder-Decoder model is that the encoder maps the input to a\\nﬁxed-length internal representation from which the decoder must produce the entire output\\nsequence. Attention is an improvement to the model that allows the decoder to pay attention\\nto diﬀerent words in the input sequence as it outputs each word in the output sequence. The\\nauthors look at a few variations on simple attention mechanisms. The results show that having\\nattention results in dramatically better performance than not having attention.\\nWhile we did expect the attention-based models to signiﬁcantly outperform those\\nwithout an attention mechanism, we were surprised by just how poorly the [no\\nattention] models fared.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nThe simple weighted average style attention described by Bahdanau, et al. in their 2015\\npaper Neural machine translation by jointly learning to align and translate was found to perform\\nthe best.\\nRecommendation : Use attention and prefer the Bahdanau-style weighted average style\\nattention.\\n29.8 Inference\\nIt is common in neural machine translation systems to use a beam-search to sample the\\nprobabilities for the words in the sequence output by the model. The wider the beam width, the\\nmore exhaustive the search, and, it is believed, the better the results. The results showed that\\na modest beam-width of 3-5 performed the best, which could be improved only very slightly\\nthrough the use of length penalties. The authors generally recommend tuning the beam width\\non each speciﬁc problem.\\nWe found that a well-tuned beam search is crucial to achieving good results, and\\nthat it leads to consistent gains of more than one BLEU point\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Start with a greedy search (beam=1) and tune based on your problem.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 368}, page_content='29.9. Final Model 352\\n29.9 Final Model\\nThe authors pull together their ﬁndings into a single best model and compare the results of this\\nmodel to other well-performing models and state-of-the-art results. The speciﬁc conﬁgurations\\nof this model are summarized in the table below, taken from the paper. These parameters may\\nbe taken as a good or best starting point when developing your own encoder-decoder model for\\nan NLP application.\\nFigure 29.2: Summary of Model Conﬁguration for the Final NMT Model. Taken from Massive\\nExploration of Neural Machine Translation Architectures .\\nThe results of the system were shown to be impressive and achieve skill close to state-of-the-art\\nwith a simpler model, which was not the goal of the paper.\\n... we do show that through careful hyperparameter tuning and good initialization,\\nit is possible to achieve state-of-the-art performance on standard WMT benchmarks\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nImportantly, the authors provide all of their code as an open source project called tf-seq2seq.\\nBecause two of the authors were members of the Google Brain residency program, their work\\nwas announced on the Google Research blog with the title Introducing tf-seq2seq: An Open\\nSource Sequence-to-Sequence Framework in TensorFlow , 2017.\\n29.10 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Massive Exploration of Neural Machine Translation Architectures , 2017.\\nhttps://arxiv.org/abs/1703.03906'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 369}, page_content='29.11. Summary 353\\n\\x88Denny Britz Homepage.\\nhttp://blog.dennybritz.com/\\n\\x88WildML Blog.\\nhttp://www.wildml.com/\\n\\x88Introducing tf-seq2seq: An Open Source Sequence-to-Sequence Framework in TensorFlow ,\\n2017.\\nhttps://research.googleblog.com/2017/04/introducing-tf-seq2seq-open-source.\\nhtml\\n\\x88tf-seq2seq: A general-purpose encoder-decoder framework for TensorFlow.\\nhttps://github.com/google/seq2seq\\n\\x88tf-seq2seq Project Documentation.\\nhttps://google.github.io/seq2seq/\\n\\x88tf-seq2seq Tutorial: Neural Machine Translation Background.\\nhttps://google.github.io/seq2seq/nmt/\\n\\x88Neural machine translation by jointly learning to align and translate , 2015.\\nhttps://arxiv.org/abs/1409.0473\\n29.11 Summary\\nIn this chapter, you discovered how to best conﬁgure an encoder-decoder recurrent neural\\nnetwork for neural machine translation and other natural language processing tasks. Speciﬁcally,\\nyou learned:\\n\\x88The Google study that investigated each model design decision in the encoder-decoder\\nmodel to isolate their eﬀects.\\n\\x88The results and recommendations for design decisions like word embeddings, encoder and\\ndecoder depth, and attention mechanisms.\\n\\x88A set of base model design decisioning that can be used as a starting point on your own\\nsequence to sequence projects.\\n29.11.1 Next\\nIn the next chapter, you will discover how you can develop a neural machine translation model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 370}, page_content='Chapter 30\\nProject: Develop a Neural Machine\\nTranslation Model\\nMachine translation is a challenging task that traditionally involves large statistical models\\ndeveloped using highly sophisticated linguistic knowledge. Neural machine translation is the\\nuse of deep neural networks for the problem of machine translation. In this tutorial, you will\\ndiscover how to develop a neural machine translation system for translating German phrases to\\nEnglish. After completing this tutorial, you will know:\\n\\x88How to clean and prepare data ready to train a neural machine translation system.\\n\\x88How to develop an encoder-decoder model for machine translation.\\n\\x88How to use a trained model for inference on new input phrases and evaluate the model\\nskill.\\nLet’s get started.\\n30.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. German to English Translation Dataset\\n2. Preparing the Text Data\\n3. Train Neural Translation Model\\n4. Evaluate Neural Translation Model\\n30.2 German to English Translation Dataset\\nIn this tutorial, we will use a dataset of German to English terms used as the basis for ﬂashcards\\nfor language learning. The dataset is available from the ManyThings.org website, with examples\\ndrawn from the Tatoeba Project. The dataset is comprised of German phrases and their English\\ncounterparts and is intended to be used with the Anki ﬂashcard software.\\n354'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 371}, page_content='30.3. Preparing the Text Data 355\\n\\x88Download the English-German pairs dataset.\\nhttp://www.manythings.org/anki/deu-eng.zip\\nDownload the dataset to your current working directory and decompress it; for example:\\nunzip deu-eng.zip\\nListing 30.1: Unzip the dataset\\nYou will have a ﬁle called deu.txt that contains 152,820 pairs of English to German phases,\\none pair per line with a tab separating the language. For example, the ﬁrst 5 lines of the ﬁle\\nlook as follows:\\nHi. Hallo!\\nHi. GruB Gott!\\nRun! Lauf!\\nWow! Potzdonner!\\nWow! Donnerwetter!\\nListing 30.2: Sample of the raw dataset (with Unicode characters normalized).\\nWe will frame the prediction problem as given a sequence of words in German as input,\\ntranslate or predict the sequence of words in English. The model we will develop will be suitable\\nfor some beginner German phrases.\\n30.3 Preparing the Text Data\\nThe next step is to prepare the text data ready for modeling. Take a look at the raw data and\\nnote what you see that we might need to handle in a data cleaning operation. For example,\\nhere are some observations I note from reviewing the raw data:\\n\\x88There is punctuation.\\n\\x88The text contains uppercase and lowercase.\\n\\x88There are special characters in the German.\\n\\x88There are duplicate phrases in English with diﬀerent translations in German.\\n\\x88The ﬁle is ordered by sentence length with very long sentences toward the end of the ﬁle.\\nA good text cleaning procedure may handle some or all of these observations. Data\\npreparation is divided into two subsections:\\n1. Clean Text\\n2. Split Text'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 372}, page_content=\"30.3. Preparing the Text Data 356\\n30.3.1 Clean Text\\nFirst, we must load the data in a way that preserves the Unicode German characters. The\\nfunction below called load doc() will load the ﬁle as a blob of text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, mode= 'rt ', encoding= 'utf-8 ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 30.3: Function to load a ﬁle into memory\\nEach line contains a single pair of phrases, ﬁrst English and then German, separated by a tab\\ncharacter. We must split the loaded text by line and then by phrase. The function topairs()\\nbelow will split the loaded text.\\n# split a loaded document into sentences\\ndef to_pairs(doc):\\nlines = doc.strip().split( '\\\\n ')\\npairs = [line.split( '\\\\t ') for line in lines]\\nreturn pairs\\nListing 30.4: Function to split lines into pairs\\nWe are now ready to clean each sentence. The speciﬁc cleaning operations we will perform\\nare as follows:\\n\\x88Remove all non-printable characters.\\n\\x88Remove all punctuation characters.\\n\\x88Normalize all Unicode characters to ASCII (e.g. Latin characters).\\n\\x88Normalize the case to lowercase.\\n\\x88Remove any remaining tokens that are not alphabetic.\\nWe will perform these operations on each phrase for each pair in the loaded dataset. The\\nclean pairs() function below implements these operations.\\n# clean a list of lines\\ndef clean_pairs(lines):\\ncleaned = list()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nre_print = re.compile( '[^%s] '% re.escape(string.printable))\\nfor pair in lines:\\nclean_pair = list()\\nfor line in pair:\\n# normalize unicode characters\\nline = normalize( 'NFD ', line).encode( 'ascii ', 'ignore ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 373}, page_content=\"30.3. Preparing the Text Data 357\\nline = line.decode( 'UTF-8 ')\\n# tokenize on white space\\nline = line.split()\\n# convert to lowercase\\nline = [word.lower() for word in line]\\n# remove punctuation from each token\\nline = [re_punc.sub( '', w) for w in line]\\n# remove non-printable chars form each token\\nline = [re_print.sub( '', w) for w in line]\\n# remove tokens with numbers in them\\nline = [word for word in line if word.isalpha()]\\n# store as string\\nclean_pair.append( ' '.join(line))\\ncleaned.append(clean_pair)\\nreturn array(cleaned)\\nListing 30.5: Function to clean text\\nFinally, now that the data has been cleaned, we can save the list of phrase pairs to a ﬁle\\nready for use. The function save clean data() uses the pickle API to save the list of clean\\ntext to ﬁle. Pulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom pickle import dump\\nfrom unicodedata import normalize\\nfrom numpy import array\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, mode= 'rt ', encoding= 'utf-8 ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# split a loaded document into sentences\\ndef to_pairs(doc):\\nlines = doc.strip().split( '\\\\n ')\\npairs = [line.split( '\\\\t ') for line in lines]\\nreturn pairs\\n# clean a list of lines\\ndef clean_pairs(lines):\\ncleaned = list()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nre_print = re.compile( '[^%s] '% re.escape(string.printable))\\nfor pair in lines:\\nclean_pair = list()\\nfor line in pair:\\n# normalize unicode characters\\nline = normalize( 'NFD ', line).encode( 'ascii ', 'ignore ')\\nline = line.decode( 'UTF-8 ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 374}, page_content=\"30.3. Preparing the Text Data 358\\n# tokenize on white space\\nline = line.split()\\n# convert to lowercase\\nline = [word.lower() for word in line]\\n# remove punctuation from each token\\nline = [re_punc.sub( '', w) for w in line]\\n# remove non-printable chars form each token\\nline = [re_print.sub( '', w) for w in line]\\n# remove tokens with numbers in them\\nline = [word for word in line if word.isalpha()]\\n# store as string\\nclean_pair.append( ' '.join(line))\\ncleaned.append(clean_pair)\\nreturn array(cleaned)\\n# save a list of clean sentences to file\\ndef save_clean_data(sentences, filename):\\ndump(sentences, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\n# load dataset\\nfilename = 'deu.txt '\\ndoc = load_doc(filename)\\n# split into english-german pairs\\npairs = to_pairs(doc)\\n# clean sentences\\nclean_pairs = clean_pairs(pairs)\\n# save clean pairs to file\\nsave_clean_data(clean_pairs, 'english-german.pkl ')\\n# spot check\\nfor i in range(100):\\nprint( '[%s] => [%s] '% (clean_pairs[i,0], clean_pairs[i,1]))\\nListing 30.6: Complete example of text data preparation.\\nRunning the example creates a new ﬁle in the current working directory with the cleaned\\ntext called english-german.pkl . Some examples of the clean text are printed for us to evaluate\\nat the end of the run to conﬁrm that the clean operations were performed as expected.\\n30.3.2 Split Text\\nThe clean data contains a little over 150,000 phrase pairs and some of the pairs toward the end\\nof the ﬁle are very long. This is a good number of examples for developing a small translation\\nmodel. The complexity of the model increases with the number of examples, length of phrases,\\nand size of the vocabulary. Although we have a good dataset for modeling translation, we will\\nsimplify the problem slightly to dramatically reduce the size of the model required, and in turn\\nthe training time required to ﬁt the model.\\nYou can explore developing a model on the fuller dataset as an extension; I would love to\\nhear how you do. We will simplify the problem by reducing the dataset to the ﬁrst 10,000\\nexamples in the ﬁle; these will be the shortest phrases in the dataset. Further, we will then\\nstake the ﬁrst 9,000 of those as examples for training and the remaining 1,000 examples to test\\nthe ﬁt model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 375}, page_content=\"30.4. Train Neural Translation Model 359\\nBelow is the complete example of loading the clean data, splitting it, and saving the split\\nportions of data to new ﬁles.\\nfrom pickle import load\\nfrom pickle import dump\\nfrom numpy.random import shuffle\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# save a list of clean sentences to file\\ndef save_clean_data(sentences, filename):\\ndump(sentences, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\n# load dataset\\nraw_dataset = load_clean_sentences( 'english-german.pkl ')\\n# reduce dataset size\\nn_sentences = 10000\\ndataset = raw_dataset[:n_sentences, :]\\n# random shuffle\\nshuffle(dataset)\\n# split into train/test\\ntrain, test = dataset[:9000], dataset[9000:]\\n# save\\nsave_clean_data(dataset, 'english-german-both.pkl ')\\nsave_clean_data(train, 'english-german-train.pkl ')\\nsave_clean_data(test, 'english-german-test.pkl ')\\nListing 30.7: Complete example of splitting text data.\\nRunning the example creates three new ﬁles: the english-german-both.pkl that contains\\nall of the train and test examples that we can use to deﬁne the parameters of the problem,\\nsuch as max phrase lengths and the vocabulary, and the english-german-train.pkl and\\nenglish-german-test.pkl ﬁles for the train and test dataset. We are now ready to start\\ndeveloping our translation model.\\n30.4 Train Neural Translation Model\\nIn this section, we will develop the translation model. This involves both loading and preparing\\nthe clean text data ready for modeling and deﬁning and training the model on the prepared\\ndata. Let’s start oﬀ by loading the datasets so that we can prepare the data. The function\\nbelow named load clean sentences() can be used to load the train, test, and both datasets\\nin turn.\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\\ntrain = load_clean_sentences( 'english-german-train.pkl ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 376}, page_content=\"30.4. Train Neural Translation Model 360\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\nListing 30.8: Load cleaned data from ﬁle.\\nWe will use the both or combination of the train and test datasets to deﬁne the maximum\\nlength and vocabulary of the problem. This is for simplicity. Alternately, we could deﬁne these\\nproperties from the training dataset alone and truncate examples in the test set that are too\\nlong or have words that are out of the vocabulary. We can use the Keras Tokenize class to\\nmap words to integers, as needed for modeling. We will use separate tokenizer for the English\\nsequences and the German sequences. The function below-named create tokenizer() will\\ntrain a tokenizer on a list of phrases.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 30.9: Fit a tokenizer on the clean text data.\\nSimilarly, the function named maxlength() below will ﬁnd the length of the longest sequence\\nin a list of phrases.\\n# max sentence length\\ndef max_length(lines):\\nreturn max(len(line.split()) for line in lines)\\nListing 30.10: Calculate the maximum sequence length.\\nWe can call these functions with the combined dataset to prepare tokenizers, vocabulary\\nsizes, and maximum lengths for both the English and German phrases.\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\nprint( 'English Vocabulary Size: %d '% eng_vocab_size)\\nprint( 'English Max Length: %d '% (eng_length))\\n# prepare german tokenizer\\nger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\nprint( 'German Vocabulary Size: %d '% ger_vocab_size)\\nprint( 'German Max Length: %d '% (ger_length))\\nListing 30.11: Prepare Tokenizers for source and target sequences.\\nWe are now ready to prepare the training dataset. Each input and output sequence must be\\nencoded to integers and padded to the maximum phrase length. This is because we will use a\\nword embedding for the input sequences and one hot encode the output sequences The function\\nbelow named encode sequences() will perform these operations and return the result.\\n# encode and pad sequences\\ndef encode_sequences(tokenizer, length, lines):\\n# integer encode sequences\\nX = tokenizer.texts_to_sequences(lines)\\n# pad sequences with 0 values\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 377}, page_content=\"30.4. Train Neural Translation Model 361\\nX = pad_sequences(X, maxlen=length, padding= 'post ')\\nreturn X\\nListing 30.12: Function to encode and pad sequences.\\nThe output sequence needs to be one hot encoded. This is because the model will predict\\nthe probability of each word in the vocabulary as output. The function encode output() below\\nwill one hot encode English output sequences.\\n# one hot encode target sequence\\ndef encode_output(sequences, vocab_size):\\nylist = list()\\nfor sequence in sequences:\\nencoded = to_categorical(sequence, num_classes=vocab_size)\\nylist.append(encoded)\\ny = array(ylist)\\ny = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\\nreturn y\\nListing 30.13: One hot encode output sequences.\\nWe can make use of these two functions and prepare both the train and test dataset ready\\nfor training the model.\\n# prepare training data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\\ntrainY = encode_output(trainY, eng_vocab_size)\\n# prepare validation data\\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\\ntestY = encode_output(testY, eng_vocab_size)\\nListing 30.14: Prepare training and test data for modeling.\\nWe are now ready to deﬁne the model. We will use an encoder-decoder LSTM model on\\nthis problem. In this architecture, the input sequence is encoded by a front-end model called\\nthe encoder then decoded word by word by a backend model called the decoder. The function\\ndefine model() below deﬁnes the model and takes a number of arguments used to conﬁgure\\nthe model, such as the size of the input and output vocabularies, the maximum length of input\\nand output phrases, and the number of memory units used to conﬁgure the model.\\nThe model is trained using the eﬃcient Adam approach to stochastic gradient descent and\\nminimizes the categorical loss function because we have framed the prediction problem as\\nmulticlass classiﬁcation. The model conﬁguration was not optimized for this problem, meaning\\nthat there is plenty of opportunity for you to tune it and lift the skill of the translations. I\\nwould love to see what you can come up with.\\n# define NMT model\\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\\nmodel = Sequential()\\nmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\\nmodel.add(LSTM(n_units))\\nmodel.add(RepeatVector(tar_timesteps))\\nmodel.add(LSTM(n_units, return_sequences=True))\\nmodel.add(TimeDistributed(Dense(tar_vocab, activation= 'softmax ')))\\n# compile model\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 378}, page_content=\"30.4. Train Neural Translation Model 362\\nmodel.compile(optimizer= 'adam ', loss= 'categorical_crossentropy ')\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 30.15: Deﬁne and summarize the model.\\nFinally, we can train the model. We train the model for 30 epochs and a batch size of\\n64 examples. We use checkpointing to ensure that each time the model skill on the test set\\nimproves, the model is saved to ﬁle.\\n# fit model\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\\nmodel.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),\\ncallbacks=[checkpoint], verbose=2)\\nListing 30.16: Fit the deﬁned model and save models using checkpointing.\\nWe can tie all of this together and ﬁt the neural translation model. The complete working\\nexample is listed below.\\nfrom pickle import load\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM\\nfrom keras.layers import Dense\\nfrom keras.layers import Embedding\\nfrom keras.layers import RepeatVector\\nfrom keras.layers import TimeDistributed\\nfrom keras.callbacks import ModelCheckpoint\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# max sentence length\\ndef max_length(lines):\\nreturn max(len(line.split()) for line in lines)\\n# encode and pad sequences\\ndef encode_sequences(tokenizer, length, lines):\\n# integer encode sequences\\nX = tokenizer.texts_to_sequences(lines)\\n# pad sequences with 0 values\\nX = pad_sequences(X, maxlen=length, padding= 'post ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 379}, page_content=\"30.4. Train Neural Translation Model 363\\nreturn X\\n# one hot encode target sequence\\ndef encode_output(sequences, vocab_size):\\nylist = list()\\nfor sequence in sequences:\\nencoded = to_categorical(sequence, num_classes=vocab_size)\\nylist.append(encoded)\\ny = array(ylist)\\ny = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\\nreturn y\\n# define NMT model\\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\\nmodel = Sequential()\\nmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\\nmodel.add(LSTM(n_units))\\nmodel.add(RepeatVector(tar_timesteps))\\nmodel.add(LSTM(n_units, return_sequences=True))\\nmodel.add(TimeDistributed(Dense(tar_vocab, activation= 'softmax ')))\\n# compile model\\nmodel.compile(optimizer= 'adam ', loss= 'categorical_crossentropy ')\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\\ntrain = load_clean_sentences( 'english-german-train.pkl ')\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\nprint( 'English Vocabulary Size: %d '% eng_vocab_size)\\nprint( 'English Max Length: %d '% (eng_length))\\n# prepare german tokenizer\\nger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\nprint( 'German Vocabulary Size: %d '% ger_vocab_size)\\nprint( 'German Max Length: %d '% (ger_length))\\n# prepare training data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\\ntrainY = encode_output(trainY, eng_vocab_size)\\n# prepare validation data\\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\\ntestY = encode_output(testY, eng_vocab_size)\\n# define model\\nmodel = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\\n# fit model\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 380}, page_content='30.4. Train Neural Translation Model 364\\nmodel.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),\\ncallbacks=[checkpoint], verbose=2)\\nListing 30.17: Complete example of training the neural machine translation model.\\nRunning the example ﬁrst prints a summary of the parameters of the dataset such as\\nvocabulary size and maximum phrase lengths.\\nEnglish Vocabulary Size: 2404\\nEnglish Max Length: 5\\nGerman Vocabulary Size: 3856\\nGerman Max Length: 10\\nListing 30.18: Summary of the loaded data\\nNext, a summary of the deﬁned model is printed, allowing us to conﬁrm the model conﬁgu-\\nration.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 10, 256) 987136\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 256) 525312\\n_________________________________________________________________\\nrepeat_vector_1 (RepeatVecto (None, 5, 256) 0\\n_________________________________________________________________\\nlstm_2 (LSTM) (None, 5, 256) 525312\\n_________________________________________________________________\\ntime_distributed_1 (TimeDist (None, 5, 2404) 617828\\n=================================================================\\nTotal params: 2,655,588\\nTrainable params: 2,655,588\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 30.19: Summary of the deﬁned model\\nA plot of the model is also created providing another perspective on the model conﬁguration.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 381}, page_content='30.4. Train Neural Translation Model 365\\nFigure 30.1: Plot of the deﬁned neural machine translation model\\nNext, the model is trained. Each epoch takes about 30 seconds on modern CPU hardware;\\nno GPU is required. During the run, the model will be saved to the ﬁle model.h5 , ready for\\ninference in the next step.\\n...\\nEpoch 26/30\\nEpoch 00025: val_loss improved from 2.20048 to 2.19976, saving model to model.h5\\n17s - loss: 0.7114 - val_loss: 2.1998\\nEpoch 27/30\\nEpoch 00026: val_loss improved from 2.19976 to 2.18255, saving model to model.h5\\n17s - loss: 0.6532 - val_loss: 2.1826\\nEpoch 28/30\\nEpoch 00027: val_loss did not improve\\n17s - loss: 0.5970 - val_loss: 2.1970\\nEpoch 29/30\\nEpoch 00028: val_loss improved from 2.18255 to 2.17872, saving model to model.h5\\n17s - loss: 0.5474 - val_loss: 2.1787\\nEpoch 30/30\\nEpoch 00029: val_loss did not improve\\n17s - loss: 0.5023 - val_loss: 2.1823'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 382}, page_content=\"30.5. Evaluate Neural Translation Model 366\\nListing 30.20: Summary output from training the neural machine translation model.\\n30.5 Evaluate Neural Translation Model\\nWe will evaluate the model on the train and the test dataset. The model should perform very\\nwell on the train dataset and ideally have been generalized to perform well on the test dataset.\\nIdeally, we would use a separate validation dataset to help with model selection during training\\ninstead of the test set. You can try this as an extension. The clean datasets must be loaded\\nand prepared as before.\\n...\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\\ntrain = load_clean_sentences( 'english-german-train.pkl ')\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\n# prepare german tokenizer\\nger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\n# prepare data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\\nListing 30.21: Load and prepare data.\\nNext, the best model saved during training must be loaded.\\n# load model\\nmodel = load_model( 'model.h5 ')\\nListing 30.22: Load and the saved model.\\nEvaluation involves two steps: ﬁrst generating a translated output sequence, and then\\nrepeating this process for many input examples and summarizing the skill of the model across\\nmultiple cases. Starting with inference, the model can predict the entire output sequence in a\\none-shot manner.\\ntranslation = model.predict(source, verbose=0)\\nListing 30.23: Predict the target sequence given the source sequence.\\nThis will be a sequence of integers that we can enumerate and lookup in the tokenizer to map\\nback to words. The function below, named word forid() , will perform this reverse mapping.\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 383}, page_content=\"30.5. Evaluate Neural Translation Model 367\\nreturn None\\nListing 30.24: Map a predicted word index to the word in the vocabulary.\\nWe can perform this mapping for each integer in the translation and return the result as a\\nstring of words. The function predict sequence() below performs this operation for a single\\nencoded source phrase.\\n# generate target given source sequence\\ndef predict_sequence(model, tokenizer, source):\\nprediction = model.predict(source, verbose=0)[0]\\nintegers = [argmax(vector) for vector in prediction]\\ntarget = list()\\nfor i in integers:\\nword = word_for_id(i, tokenizer)\\nif word is None:\\nbreak\\ntarget.append(word)\\nreturn ' '.join(target)\\nListing 30.25: Predict and interpret the target sequence.\\nNext, we can repeat this for each source phrase in a dataset and compare the predicted result\\nto the expected target phrase in English. We can print some of these comparisons to screen to\\nget an idea of how the model performs in practice. We will also calculate the BLEU scores to\\nget a quantitative idea of how well the model has performed. The evaluate model() function\\nbelow implements this, calling the above predict sequence() function for each phrase in a\\nprovided dataset.\\n# evaluate the skill of the model\\ndef evaluate_model(model, tokenizer, sources, raw_dataset):\\nactual, predicted = list(), list()\\nfor i, source in enumerate(sources):\\n# translate encoded source text\\nsource = source.reshape((1, source.shape[0]))\\ntranslation = predict_sequence(model, eng_tokenizer, source)\\nraw_target, raw_src = raw_dataset[i]\\nif i < 10:\\nprint( 'src=[%s], target=[%s], predicted=[%s] '% (raw_src, raw_target, translation))\\nactual.append(raw_target.split())\\npredicted.append(translation.split())\\n# calculate BLEU score\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\\nprint( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\\nListing 30.26: Function to evaluate a ﬁt model.\\nWe can tie all of this together and evaluate the loaded model on both the training and test\\ndatasets. The complete code listing is provided below.\\nfrom pickle import load\\nfrom numpy import argmax\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 384}, page_content=\"30.5. Evaluate Neural Translation Model 368\\nfrom nltk.translate.bleu_score import corpus_bleu\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# max sentence length\\ndef max_length(lines):\\nreturn max(len(line.split()) for line in lines)\\n# encode and pad sequences\\ndef encode_sequences(tokenizer, length, lines):\\n# integer encode sequences\\nX = tokenizer.texts_to_sequences(lines)\\n# pad sequences with 0 values\\nX = pad_sequences(X, maxlen=length, padding= 'post ')\\nreturn X\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# generate target given source sequence\\ndef predict_sequence(model, tokenizer, source):\\nprediction = model.predict(source, verbose=0)[0]\\nintegers = [argmax(vector) for vector in prediction]\\ntarget = list()\\nfor i in integers:\\nword = word_for_id(i, tokenizer)\\nif word is None:\\nbreak\\ntarget.append(word)\\nreturn ' '.join(target)\\n# evaluate the skill of the model\\ndef evaluate_model(model, sources, raw_dataset):\\nactual, predicted = list(), list()\\nfor i, source in enumerate(sources):\\n# translate encoded source text\\nsource = source.reshape((1, source.shape[0]))\\ntranslation = predict_sequence(model, eng_tokenizer, source)\\nraw_target, raw_src = raw_dataset[i]\\nif i < 10:\\nprint( 'src=[%s], target=[%s], predicted=[%s] '% (raw_src, raw_target, translation))\\nactual.append(raw_target.split())\\npredicted.append(translation.split())\\n# calculate BLEU score\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 385}, page_content=\"30.5. Evaluate Neural Translation Model 369\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\\nprint( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\\ntrain = load_clean_sentences( 'english-german-train.pkl ')\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\n# prepare german tokenizer\\nger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\n# prepare data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\\n# load model\\nmodel = load_model( 'model.h5 ')\\n# test on some training sequences\\nprint( 'train ')\\nevaluate_model(model, trainX, train)\\n# test on some test sequences\\nprint( 'test ')\\nevaluate_model(model, testX, test)\\nListing 30.27: Complete example of translating text with a ﬁt neural machine translation model.\\nRunning the example ﬁrst prints examples of source text, expected and predicted translations,\\nas well as scores for the training dataset, followed by the test dataset. Your speciﬁc results will\\ndiﬀer given the random shuﬄing of the dataset and the stochastic nature of neural networks.\\nLooking at the results for the test dataset ﬁrst, we can see that the translations are readable\\nand mostly correct. For example: ‘ ich liebe dich ’ was correctly translated to ‘ i love you ’.\\nWe can also see that the translations were not perfect, with ‘ ich konnte nicht gehen ’ translated\\ntoi cant go instead of the expected ‘ i couldnt walk ’. We can also see the BLEU-4 score of 0.51,\\nwhich provides an upper bound on what we might expect from this model.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nsrc=[ich liebe dich], target=[i love you], predicted=[i love you]\\nsrc=[ich sagte du sollst den mund halten], target=[i said shut up], predicted=[i said stop\\nup]\\nsrc=[wie geht es eurem vater], target=[hows your dad], predicted=[hows your dad]\\nsrc=[das gefallt mir], target=[i like that], predicted=[i like that]\\nsrc=[ich gehe immer zu fu], target=[i always walk], predicted=[i will to]\\nsrc=[ich konnte nicht gehen], target=[i couldnt walk], predicted=[i cant go]\\nsrc=[er ist sehr jung], target=[he is very young], predicted=[he is very young]\\nsrc=[versucht es doch einfach], target=[just try it], predicted=[just try it]\\nsrc=[sie sind jung], target=[youre young], predicted=[youre young]\\nsrc=[er ging surfen], target=[he went surfing], predicted=[he went surfing]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 386}, page_content='30.6. Extensions 370\\nBLEU-1: 0.085682\\nBLEU-2: 0.284191\\nBLEU-3: 0.459090\\nBLEU-4: 0.517571\\nListing 30.28: Sample output translation on the training dataset.\\nLooking at the results on the test set, do see readable translations, which is not an easy task.\\nFor example, we see ‘ ich mag dich nicht ’ correctly translated to ‘ i dont like you ’. We also see\\nsome poor translations and a good case that the model could support from further tuning, such\\nas ‘ich bin etwas beschwipst ’ translated as ‘ i a bit bit ’ instead of the expected im a bit tipsy A\\nBLEU-4 score of 0.076238 was achieved, providing a baseline skill to improve upon with further\\nimprovements to the model.\\nsrc=[tom erblasste], target=[tom turned pale], predicted=[tom went pale]\\nsrc=[bring mich nach hause], target=[take me home], predicted=[let us at]\\nsrc=[ich bin etwas beschwipst], target=[im a bit tipsy], predicted=[i a bit bit]\\nsrc=[das ist eine frucht], target=[its a fruit], predicted=[thats a a]\\nsrc=[ich bin pazifist], target=[im a pacifist], predicted=[im am]\\nsrc=[unser plan ist aufgegangen], target=[our plan worked], predicted=[who is a man]\\nsrc=[hallo tom], target=[hi tom], predicted=[hello tom]\\nsrc=[sei nicht nervos], target=[dont be nervous], predicted=[dont be crazy]\\nsrc=[ich mag dich nicht], target=[i dont like you], predicted=[i dont like you]\\nsrc=[tom stellte eine falle], target=[tom set a trap], predicted=[tom has a cough]\\nBLEU-1: 0.082088\\nBLEU-2: 0.006182\\nBLEU-3: 0.046129\\nBLEU-4: 0.076238\\nListing 30.29: Sample output translation on the test dataset.\\n30.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Data Cleaning . Diﬀerent data cleaning operations could be performed on the data, such\\nas not removing punctuation or normalizing case, or perhaps removing duplicate English\\nphrases.\\n\\x88Vocabulary . The vocabulary could be reﬁned, perhaps removing words used less than 5\\nor 10 times in the dataset and replaced with unk.\\n\\x88More Data . The dataset used to ﬁt the model could be expanded to 50,000, 100,000\\nphrases, or more.\\n\\x88Input Order . The order of input phrases could be reversed, which has been reported to\\nlift skill, or a Bidirectional input layer could be used.\\n\\x88Layers . The encoder and/or the decoder models could be expanded with additional layers\\nand trained for more epochs, providing more representational capacity for the model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 387}, page_content='30.7. Further Reading 371\\n\\x88Units . The number of memory units in the encoder and decoder could be increased,\\nproviding more representational capacity for the model.\\n\\x88Regularization . The model could use regularization, such as weight or activation\\nregularization, or the use of dropout on the LSTM layers.\\n\\x88Pre-Trained Word Vectors . Pre-trained word vectors could be used in the model.\\n\\x88Alternate Measure . Explore alternate performance measures beside BLEU such as\\nROGUE. Compare scores for the same translations to develop an intuition for how the\\nmeasures diﬀer in practice.\\n\\x88Recursive Model . A recursive formulation of the model could be used where the next\\nword in the output sequence could be conditional on the input sequence and the output\\nsequence generated so far.\\nIf you explore any of these extensions, I’d love to know.\\n30.7 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n30.7.1 Dataset\\n\\x88Tab-delimited Bilingual Sentence Pairs.\\nhttp://www.manythings.org/anki/\\n\\x88German - English deu-eng.zip .\\nhttp://www.manythings.org/anki/deu-eng.zip\\n30.7.2 Neural Machine Translation\\n\\x88Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nhttps://arxiv.org/abs/1609.08144\\n\\x88Sequence to Sequence Learning with Neural Networks , 2014.\\nhttps://arxiv.org/abs/1409.3215\\n\\x88Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\nhttps://arxiv.org/abs/1406.1078\\n\\x88Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nhttps://arxiv.org/abs/1409.0473\\n\\x88On the Properties of Neural Machine Translation: Encoder-Decoder Approaches , 2014.\\nhttps://arxiv.org/abs/1409.1259\\n\\x88Massive Exploration of Neural Machine Translation Architectures , 2017.\\nhttps://arxiv.org/abs/1703.03906'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 388}, page_content='30.8. Summary 372\\n30.8 Summary\\nIn this tutorial, you discovered how to develop a neural machine translation system for translating\\nGerman phrases to English. Speciﬁcally, you learned:\\n\\x88How to clean and prepare data ready to train a neural machine translation system.\\n\\x88How to develop an encoder-decoder model for machine translation.\\n\\x88How to use a trained model for inference on new input phrases and evaluate the model\\nskill.\\n30.8.1 Next\\nThis is the ﬁnal chapter for the machine translation part. In the next part you will discover\\nhelpful information in the appendix.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 389}, page_content='Part X\\nAppendix\\n373'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 390}, page_content='Appendix A\\nGetting Help\\nThis is just the beginning of your journey with deep learning for natural language processing in\\nPython. As you start to work on projects and expand your existing knowledge of the techniques,\\nyou may need help. This appendix points out some of the best sources of help.\\nA.1 Oﬃcial Keras Destinations\\nThis section lists the oﬃcial Keras sites that you may ﬁnd helpful.\\n\\x88Keras Oﬃcial Blog.\\nhttps://blog.keras.io/\\n\\x88Keras API Documentation.\\nhttps://keras.io/\\n\\x88Keras Source Code Project.\\nhttps://github.com/fchollet/keras\\nA.2 Where to Get Help with Keras\\nThis section lists the 9 best places I know where you can get help with Keras and LSTMs.\\n\\x88Keras Users Google Group.\\nhttps://groups.google.com/forum/#!forum/keras-users\\n\\x88Keras Slack Channel (you must request to join).\\nhttps://keras-slack-autojoin.herokuapp.com/\\n\\x88Keras on Gitter.\\nhttps://gitter.im/Keras-io/Lobby#\\n\\x88Keras tag on StackOverﬂow.\\nhttps://stackoverflow.com/questions/tagged/keras\\n\\x88Keras tag on CrossValidated.\\nhttps://stats.stackexchange.com/questions/tagged/keras\\n374'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 391}, page_content='A.3. Where to Get Help with Natural Language 375\\n\\x88Keras tag on DataScience.\\nhttps://datascience.stackexchange.com/questions/tagged/keras\\n\\x88Keras Topic on Quora.\\nhttps://www.quora.com/topic/Keras\\n\\x88Keras Github Issues.\\nhttps://github.com/fchollet/keras/issues\\n\\x88Keras on Twitter.\\nhttps://twitter.com/hashtag/keras\\nA.3 Where to Get Help with Natural Language\\nThis section lists the best places I know where you can get help with natural language processing.\\n\\x88Language Technology Subreddit.\\nhttps://www.reddit.com/r/LanguageTechnology/\\n\\x88NLP Tag on StackOverﬂow.\\nhttps://stackoverflow.com/questions/tagged/nlp\\n\\x88Linguistics Stack Exchange.\\nhttps://linguistics.stackexchange.com/\\n\\x88Natural Language Processing Topic on Quora.\\nhttps://www.quora.com/topic/Natural-Language-Processing\\nA.4 How to Ask Questions\\nKnowing where to get help is the ﬁrst step, but you need to know how to get the most out of\\nthese resources. Below are some tips that you can use:\\n\\x88Boil your question down to the simplest form. E.g. not something broad like my model\\ndoes not work orhow does x work .\\n\\x88Search for answers before asking questions.\\n\\x88Provide complete code and error messages.\\n\\x88Boil your code down to the smallest possible working example that demonstrates the issue.\\nA.5 Contact the Author\\nYou are not alone. If you ever have any questions about deep learning, natural language\\nprocessing, or this book, please contact me directly. I will do my best to help.\\nJason Brownlee\\nJason@MachineLearningMastery.com'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 392}, page_content='Appendix B\\nHow to Setup a Workstation for Deep\\nLearning\\nIt can be diﬃcult to install a Python machine learning environment on some platforms. Python\\nitself must be installed ﬁrst and then there are many packages to install, and it can be confusing\\nfor beginners. In this tutorial, you will discover how to setup a Python machine learning\\ndevelopment environment using Anaconda.\\nAfter completing this tutorial, you will have a working Python environment to begin learning,\\npracticing, and developing machine learning and deep learning software. These instructions are\\nsuitable for Windows, Mac OS X, and Linux platforms. I will demonstrate them on OS X, so\\nyou may see some mac dialogs and ﬁle extensions.\\nB.1 Overview\\nIn this tutorial, we will cover the following steps:\\n1. Download Anaconda\\n2. Install Anaconda\\n3. Start and Update Anaconda\\n4. Install Deep Learning Libraries\\nNote: The speciﬁc versions may diﬀer as the software and libraries are updated frequently.\\nB.2 Download Anaconda\\nIn this step, we will download the Anaconda Python package for your platform. Anaconda is a\\nfree and easy-to-use environment for scientiﬁc Python.\\n\\x881. Visit the Anaconda homepage.\\nhttps://www.continuum.io/\\n\\x882. Click Anaconda from the menu and click Download to go to the download page.\\nhttps://www.continuum.io/downloads\\n376'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 393}, page_content='B.2. Download Anaconda 377\\nFigure B.1: Click Anaconda and Download.\\n\\x883. Choose the download suitable for your platform (Windows, OSX, or Linux):\\n–Choose Python 3.6\\n–Choose the Graphical Installer'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 394}, page_content='B.3. Install Anaconda 378\\nFigure B.2: Choose Anaconda Download for Your Platform.\\nThis will download the Anaconda Python package to your workstation. I’m on OS X, so I\\nchose the OS X version. The ﬁle is about 426 MB. You should have a ﬁle with a name like:\\nAnaconda3-4.4.0-MacOSX-x86_64.pkg\\nListing B.1: Example ﬁlename on Mac OS X.\\nB.3 Install Anaconda\\nIn this step, we will install the Anaconda Python software on your system. This step assumes\\nyou have suﬃcient administrative privileges to install software on your system.\\n\\x881. Double click the downloaded ﬁle.\\n\\x882. Follow the installation wizard.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 395}, page_content='B.3. Install Anaconda 379\\nFigure B.3: Anaconda Python Installation Wizard.\\nInstallation is quick and painless. There should be no tricky questions or sticking points.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 396}, page_content='B.4. Start and Update Anaconda 380\\nFigure B.4: Anaconda Python Installation Wizard Writing Files.\\nThe installation should take less than 10 minutes and take up a little more than 1 GB of\\nspace on your hard drive.\\nB.4 Start and Update Anaconda\\nIn this step, we will conﬁrm that your Anaconda Python environment is up to date. Anaconda\\ncomes with a suite of graphical tools called Anaconda Navigator. You can start Anaconda\\nNavigator by opening it from your application launcher.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 397}, page_content='B.4. Start and Update Anaconda 381\\nFigure B.5: Anaconda Navigator GUI.\\nYou can use the Anaconda Navigator and graphical development environments later; for now,\\nI recommend starting with the Anaconda command line environment called conda. Conda is\\nfast, simple, it’s hard for error messages to hide, and you can quickly conﬁrm your environment\\nis installed and working correctly.\\n\\x881. Open a terminal (command line window).\\n\\x882. Conﬁrm conda is installed correctly, by typing:\\nconda -V\\nListing B.2: Check the conda version.\\nYou should see the following (or something similar):\\nconda 4.3.21\\nListing B.3: Example conda version.\\n\\x883. Conﬁrm Python is installed correctly by typing:\\npython -V\\nListing B.4: Check the Python version.\\nYou should see the following (or something similar):\\nPython 3.6.1 :: Anaconda 4.4.0 (x86_64)\\nListing B.5: Example Python version.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 398}, page_content=\"B.4. Start and Update Anaconda 382\\nIf the commands do not work or have an error, please check the documentation for help for\\nyour platform. See some of the resources in the Further Reading section.\\n\\x884. Conﬁrm your conda environment is up-to-date, type:\\nconda update conda\\nconda update anaconda\\nListing B.6: Update conda and anaconda.\\nYou may need to install some packages and conﬁrm the updates.\\n\\x885. Conﬁrm your SciPy environment.\\nThe script below will print the version number of the key SciPy libraries you require for\\nmachine learning development, speciﬁcally: SciPy, NumPy, Matplotlib, Pandas, Statsmodels,\\nand Scikit-learn. You can type python and type the commands in directly. Alternatively, I\\nrecommend opening a text editor and copy-pasting the script into your editor.\\n# scipy\\nimport scipy\\nprint( 'scipy: %s '% scipy.__version__)\\n# numpy\\nimport numpy\\nprint( 'numpy: %s '% numpy.__version__)\\n# matplotlib\\nimport matplotlib\\nprint( 'matplotlib: %s '% matplotlib.__version__)\\n# pandas\\nimport pandas\\nprint( 'pandas: %s '% pandas.__version__)\\n# statsmodels\\nimport statsmodels\\nprint( 'statsmodels: %s '% statsmodels.__version__)\\n# scikit-learn\\nimport sklearn\\nprint( 'sklearn: %s '% sklearn.__version__)\\nListing B.7: Code to check that key Python libraries are installed.\\nSave the script as a ﬁle with the name: versions.py . On the command line, change your\\ndirectory to where you saved the script and type:\\npython versions.py\\nListing B.8: Run the script from the command line.\\nYou should see output like the following:\\nscipy: 0.19.1\\nnumpy: 1.13.3\\nmatplotlib: 2.1.0\\npandas: 0.20.3\\nstatsmodels: 0.8.0\\nsklearn: 0.19.0\\nListing B.9: Sample output of versions script.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 399}, page_content=\"B.5. Install Deep Learning Libraries 383\\nB.5 Install Deep Learning Libraries\\nIn this step, we will install Python libraries used for deep learning, speciﬁcally: Theano,\\nTensorFlow, and Keras. Note: I recommend using Keras for deep learning and Keras only\\nrequires one of Theano or TensorFlow to be installed. You do not need both. There may be\\nproblems installing TensorFlow on some Windows machines.\\n\\x881. Install the Theano deep learning library by typing:\\nconda install theano\\nListing B.10: Install Theano with conda.\\n\\x882. Install the TensorFlow deep learning library by typing:\\nconda install -c conda-forge tensorflow\\nListing B.11: Install TensorFlow with conda.\\nAlternatively, you may choose to install using pipand a speciﬁc version of TensorFlow for\\nyour platform.\\n\\x883. Install Keras by typing:\\npip install keras\\nListing B.12: Install Keras with pip.\\n\\x884. Conﬁrm your deep learning environment is installed and working correctly.\\nCreate a script that prints the version numbers of each library, as we did before for the SciPy\\nenvironment.\\n# theano\\nimport theano\\nprint( 'theano: %s '% theano.__version__)\\n# tensorflow\\nimport tensorflow\\nprint( 'tensorflow: %s '% tensorflow.__version__)\\n# keras\\nimport keras\\nprint( 'keras: %s '% keras.__version__)\\nListing B.13: Code to check that key deep learning libraries are installed.\\nSave the script to a ﬁle deep versions.py . Run the script by typing:\\npython deep_versions.py\\nListing B.14: Run script from the command line.\\nYou should see output like:\\ntheano: 0.9.0\\ntensorflow: 1.3.0\\nkeras: 2.0.8\\nListing B.15: Sample output of the deep learning versions script.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 400}, page_content='B.6. Further Reading 384\\nB.6 Further Reading\\nThis section provides resources if you want to know more about Anaconda.\\n\\x88Anaconda homepage.\\nhttps://www.continuum.io/\\n\\x88Anaconda Navigator.\\nhttps://docs.continuum.io/anaconda/navigator.html\\n\\x88The conda command line tool.\\nhttp://conda.pydata.org/docs/index.html\\n\\x88Instructions for installing TensorFlow in Anaconda.\\nhttps://www.tensorflow.org/get_started/os_setup#anaconda_installation\\nB.7 Summary\\nCongratulations, you now have a working Python development environment for machine learning\\nand deep learning. You can now learn and practice machine learning and deep learning on your\\nworkstation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 401}, page_content='Appendix C\\nHow to Use Deep Learning in the\\nCloud\\nLarge deep learning models require a lot of compute time to run. You can run them on your\\nCPU but it can take hours or days to get a result. If you have access to a GPU on your desktop,\\nyou can drastically speed up the training time of your deep learning models. In this project you\\nwill discover how you can get access to GPUs to speed up the training of your deep learning\\nmodels by using the Amazon Web Service (AWS) infrastructure. For less than a dollar per hour\\nand often a lot cheaper you can use this service from your workstation or laptop. After working\\nthrough this project you will know:\\n\\x88How to create an account and log-in to Amazon Web Service.\\n\\x88How to launch a server instance for deep learning.\\n\\x88How to conﬁgure a server instance for faster deep learning on the GPU.\\nLet’s get started.\\nC.1 Overview\\nThe process is quite simple because most of the work has already been done for us. Below is an\\noverview of the process.\\n\\x88Setup Your AWS Account.\\n\\x88Launch Your Server Instance.\\n\\x88Login and Run Your Code.\\n\\x88Close Your Server Instance.\\nNote, it costs money to use a virtual server instance on Amazon . The cost is low for\\nmodel development (e.g. less than one US dollar per hour), which is why this is so attractive,\\nbut it is not free. The server instance runs Linux. It is desirable although not required that you\\nknow how to navigate Linux or a Unix-like environment. We’re just running our Python scripts,\\nso no advanced skills are needed.\\nNote: The speciﬁc versions may diﬀer as the software and libraries are updated frequently.\\n385'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 402}, page_content='C.2. Setup Your AWS Account 386\\nC.2 Setup Your AWS Account\\nYou need an account on Amazon Web Services1.\\n\\x881. You can create account by the Amazon Web Services portal and click Sign in to the\\nConsole . From there you can sign in using an existing Amazon account or create a new\\naccount.\\nFigure C.1: AWS Sign-in Button\\n\\x882. You will need to provide your details as well as a valid credit card that Amazon can\\ncharge. The process is a lot quicker if you are already an Amazon customer and have your\\ncredit card on ﬁle.\\n1https://aws.amazon.com'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 403}, page_content='C.3. Launch Your Server Instance 387\\nFigure C.2: AWS Sign-In Form\\nOnce you have an account you can log into the Amazon Web Services console. You will see\\na range of diﬀerent services that you can access.\\nC.3 Launch Your Server Instance\\nNow that you have an AWS account, you want to launch an EC2 virtual server instance on\\nwhich you can run Keras. Launching an instance is as easy as selecting the image to load and\\nstarting the virtual server. Thankfully there is already an image available that has almost\\neverything we need it is called the Deep Learning AMI Amazon Linux Version and was\\ncreated and is maintained by Amazon. Let’s launch it as an instance.\\n\\x881. Login to your AWS console if you have not already.\\nhttps://console.aws.amazon.com/console/home'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 404}, page_content='C.3. Launch Your Server Instance 388\\nFigure C.3: AWS Console\\n\\x882. Click on EC2 for launching a new virtual server.\\n\\x883. Select US West Orgeon from the drop-down in the top right hand corner. This is\\nimportant otherwise you will not be able to ﬁnd the image we plan to use.\\n\\x884. Click the Launch Instance button.\\n\\x885. Click Community AMIs . An AMI is an Amazon Machine Image. It is a frozen instance\\nof a server that you can select and instantiate on a new virtual server.\\nFigure C.4: Community AMIs\\n\\x886. Enter ami-df77b6a7 in the Search community AMIs search box and press enter (this\\nis the current AMI id for v3.3 but the AMI may have been updated since, you check for a\\nmore recent id2). You should be presented with a single result.\\n2https://aws.amazon.com/marketplace/pp/B01M0AXXQB'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 405}, page_content='C.3. Launch Your Server Instance 389\\nFigure C.5: Select a Speciﬁc AMI\\n\\x887. Click Select to choose the AMI in the search result.\\n\\x888. Now you need to select the hardware on which to run the image. Scroll down and select\\ntheg2.2xlarge hardware. This includes a GPU that we can use to signiﬁcantly increase\\nthe training speed of our models. The choice of hardware will impact the price, I generally\\nrecommend g2 and p2 hardware. See the AMI page for estimated pricing per hour for\\ndiﬀerent hardware conﬁgurations3.\\nFigure C.6: Select g2.2xlarge Hardware\\n\\x889. Click Review and Launch to ﬁnalize the conﬁguration of your server instance.\\n\\x8810. Click the Launch button.\\n\\x8811. Select Your Key Pair.\\nIf you have a key pair because you have used EC2 before, select Choose an existing key pair\\nand choose your key pair from the list. Then check I acknowledge... . If you do not have a key\\npair, select the option Create a new key pair and enter a Key pair name such as keras-keypair.\\nClick the Download Key Pair button.\\n3https://aws.amazon.com/marketplace/pp/B01M0AXXQB'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 406}, page_content='C.3. Launch Your Server Instance 390\\nFigure C.7: Select Your Key Pair\\n\\x8812. Open a Terminal and change directory to where you downloaded your key pair.\\n\\x8813. If you have not already done so, restrict the access permissions on your key pair ﬁle.\\nThis is required as part of the SSH access to your server. For example, open a terminal on\\nyour workstation and type:\\ncd Downloads\\nchmod 600 keras-aws-keypair.pem\\nListing C.1: Change Permissions of Your Key Pair File.\\n\\x8814. Click Launch Instances . If this is your ﬁrst time using AWS, Amazon may have to\\nvalidate your request and this could take up to 2 hours (often just a few minutes).\\n\\x8815. Click View Instances to review the status of your instance.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 407}, page_content='C.4. Login, Conﬁgure and Run 391\\nFigure C.8: Review Your Running Instance\\nYour server is now running and ready for you to log in.\\nC.4 Login, Conﬁgure and Run\\nNow that you have launched your server instance, it is time to log in and start using it.\\n\\x881. Click View Instances in your Amazon EC2 console if you have not done so already.\\n\\x882. Copy the Public IP (down the bottom of the screen in Description) to your clipboard.\\nIn this example my IP address is 54.186.97.77 .Do not use this IP address, it will\\nnot work as your server IP address will be diﬀerent .\\n\\x883. Open a Terminal and change directory to where you downloaded your key pair. Login\\nto your server using SSH, for example:\\nssh -i keras-aws-keypair.pem ec2-user@54.186.97.77\\nListing C.2: Log-in To Your AWS Instance.\\n\\x884. If prompted, type yesand press enter.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 408}, page_content='C.5. Build and Run Models on AWS 392\\nYou are now logged into your server.\\nFigure C.9: Log in Screen for Your AWS Server\\nWe need to make two small changes before we can start using Keras. This will just take a\\nminute. You will have to do these changes each time you start the instance.\\nC.4.1 Update Keras\\nUpdate to a speciﬁc version of Keras known to work on this conﬁguration, at the time of writing\\nthe latest version of Keras is version 2.0.8. We can specify this version as part of the upgrade of\\nKeras via pip.\\nsudo pip install --upgrade keras==2.0.8\\nListing C.3: Update Keras Using pip.\\nYou can also conﬁrm that Keras is installed and is working correctly by typing:\\npython -c \"import keras; print(keras.__version__)\"\\nListing C.4: Script To Check Keras Conﬁguration.\\nYou should see:\\nUsing TensorFlow backend.\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcublas.so.7.5 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcudnn.so.5 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcufft.so.7.5 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcuda.so.1 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcurand.so.7.5 locally\\n2.0.8\\nListing C.5: Sample Output of Script to Check Keras Conﬁguration.\\nYou are now free to run your code.\\nC.5 Build and Run Models on AWS\\nThis section oﬀers some tips for running your code on AWS.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 409}, page_content='C.6. Close Your EC2 Instance 393\\nC.5.1 Copy Scripts and Data to AWS\\nYou can get started quickly by copying your ﬁles to your running AWS instance. For example,\\nyou can copy the examples provided with this book to your AWS instance using the scp\\ncommand as follows:\\nscp -i keras-aws-keypair.pem -r src ec2-user@54.186.97.77:~/\\nListing C.6: Example for Copying Sample Code to AWS.\\nThis will copy the entire src/ directory to your home directory on your AWS instance. You\\ncan easily adapt this example to get your larger datasets from your workstation onto your AWS\\ninstance. Note that Amazon may impose charges for moving very large amounts of data in and\\nout of your AWS instance. Refer to Amazon documentation for relevant charges.\\nC.5.2 Run Models on AWS\\nYou can run your scripts on your AWS instance as per normal:\\npython filename.py\\nListing C.7: Example of Running a Python script on AWS.\\nYou are using AWS to create large neural network models that may take hours or days to\\ntrain. As such, it is a better idea to run your scripts as a background job. This allows you to\\nclose your terminal and your workstation while your AWS instance continues to run your script.\\nYou can easily run your script as a background process as follows:\\nnohup /path/to/script >/path/to/script.log 2>&1 < /dev/null &\\nListing C.8: Run Script as a Background Process.\\nYou can then check the status and results in your script.log ﬁle later.\\nC.6 Close Your EC2 Instance\\nWhen you are ﬁnished with your work you must close your instance. Remember you are charged\\nby the amount of time that you use the instance. It is cheap, but you do not want to leave an\\ninstance on if you are not using it.\\n\\x881. Log out of your instance at the terminal, for example you can type:\\nexit\\nListing C.9: Log-out of Server Instance.\\n\\x882. Log in to your AWS account with your web browser.\\n\\x883. Click EC2.\\n\\x884. Click Instances from the left-hand side menu.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 410}, page_content='C.6. Close Your EC2 Instance 394\\nFigure C.10: Review Your List of Running Instances\\n\\x885. Select your running instance from the list (it may already be selected if you only have\\none running instance).\\nFigure C.11: Select Your Running AWS Instance\\n\\x886. Click the Actions button and select Instance State and choose Terminate . Conﬁrm\\nthat you want to terminate your running instance.\\nIt may take a number of seconds for the instance to close and to be removed from your list\\nof instances.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 411}, page_content='C.7. Tips and Tricks for Using Keras on AWS 395\\nC.7 Tips and Tricks for Using Keras on AWS\\nBelow are some tips and tricks for getting the most out of using Keras on AWS instances.\\n\\x88Design a suite of experiments to run beforehand . Experiments can take a long\\ntime to run and you are paying for the time you use. Make time to design a batch of\\nexperiments to run on AWS. Put each in a separate ﬁle and call them in turn from another\\nscript. This will allow you to answer multiple questions from one long run, perhaps\\novernight.\\n\\x88Always close your instance at the end of your experiments . You do not want to\\nbe surprised with a very large AWS bill.\\n\\x88Try spot instances for a cheaper but less reliable option . Amazon sell unused\\ntime on their hardware at a much cheaper price, but at the cost of potentially having your\\ninstance closed at any second. If you are learning or your experiments are not critical, this\\nmight be an ideal option for you. You can access spot instances from the Spot Instance\\noption on the left hand side menu in your EC2 web console.\\nC.8 Further Reading\\nBelow is a list of resources to learn more about AWS and developing deep learning models in\\nthe cloud.\\n\\x88An introduction to Amazon Elastic Compute Cloud (EC2) if you are new to all of this.\\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html\\n\\x88An introduction to Amazon Machine Images (AMI).\\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\\n\\x88Deep Learning AMI Amazon Linux Version on the AMI Marketplace.\\nhttps://aws.amazon.com/marketplace/pp/B01M0AXXQB\\nC.9 Summary\\nIn this lesson you discovered how you can develop and evaluate your large deep learning models\\nin Keras using GPUs on the Amazon Web Service. You learned:\\n\\x88Amazon Web Services with their Elastic Compute Cloud oﬀers an aﬀordable way to run\\nlarge deep learning models on GPU hardware.\\n\\x88How to setup and launch an EC2 server for deep learning experiments.\\n\\x88How to update the Keras version on the server and conﬁrm that the system is working\\ncorrectly.\\n\\x88How to run Keras experiments on AWS instances in batch as background tasks.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 412}, page_content='Part XI\\nConclusions\\n396'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 413}, page_content='How Far You Have Come\\nYou made it. Well done. Take a moment and look back at how far you have come. You now\\nknow:\\n1. What natural language processing is and why it is challenging.\\n2. What deep learning is and how it is diﬀerent from other machine learning methods.\\n3. The promise of deep learning methods for natural language processing problems.\\n4. How to prepare text data for modeling using best-of-breed Python libraries.\\n5. How to develop distributed representations of text using word embedding models.\\n6.How to develop a bag-of-words model, a representation technique that can be used for\\nmachine learning and deep learning methods.\\n7.How to develop a neural sentiment analysis model for automatically predicting the class\\nlabel for a text document.\\n8.How to develop a neural language model, required for any text generating neural network.\\n9.How to develop a photo captioning system to automatically generate textual descriptions\\nof photographs.\\n10.How to develop a neural machine translation system for translating text from one language\\nto another.\\nDon’t make light of this. You have come a long way in a short amount of time. You have\\ndeveloped the important and valuable skill of being able to implement and work through natural\\nlanguage prediction problems using deep learning in Python. You can now conﬁdently bring\\ndeep learning models to your own natural language processing problems. The sky’s the limit.\\nThank You!\\nI want to take a moment and sincerely thank you for letting me help you start your deep learning\\nfor natural language processing journey. I hope you keep learning and have fun as you continue\\nto master machine learning.\\nJason Brownlee\\n2017\\n397')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('Deep_Learning_for_Natural_Language_processing.pdf')\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 0}, page_content='Deep Learning for \\nNatural Language Processing\\nDevelop Deep Learning Models for \\nNatural Language in Python\\nJason Brownlee'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 1}, page_content='i\\nDisclaimer\\nThe information contained within this eBook is strictly for educational purposes. If you wish to apply\\nideas contained in this eBook, you are taking full responsibility for your actions.\\nThe author has made every eﬀort to ensure the accuracy of the information within this book was\\ncorrect at time of publication. The author does not assume and hereby disclaims any liability to any\\nparty for any loss, damage, or disruption caused by errors or omissions, whether such errors or\\nomissions result from accident, negligence, or any other cause.\\nNo part of this eBook may be reproduced or transmitted in any form or by any means, electronic or\\nmechanical, recording or by any information storage and retrieval system, without written permission\\nfrom the author.\\nAcknowledgements\\nSpecial thanks to my copy editor Sarah Martin and my technical editors Arun Koshy and Andrei\\nCheremskoy.\\nCopyright\\nDeep Learning for Natural Language Processing'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 1}, page_content='©Copyright 2017 Jason Brownlee. All Rights Reserved.\\nEdition: v1.1'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 2}, page_content='Contents\\nCopyright i\\nContents ii\\nPreface iii\\nI Introductions iv\\nWelcome v\\nWho Is This Book For? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v\\nAbout Your Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\\nHow to Read This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\\nAbout the Book Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\\nAbout Python Code Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\nAbout Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\nAbout Getting Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\\nII Foundations 1\\n1 Natural Language Processing 2\\n1.1 Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 2}, page_content='1.2 Challenge of Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 From Linguistics to Natural Language Processing . . . . . . . . . . . . . . . . . 3\\n1.4 Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2 Deep Learning 8\\n2.1 Deep Learning is Large Neural Networks . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Deep Learning is Hierarchical Feature Learning . . . . . . . . . . . . . . . . . . 11\\n2.3 Deep Learning as Scalable Learning Across Domains . . . . . . . . . . . . . . . 12\\n2.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\nii')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 0}, page_content='Deep Learning for \\nNatural Language Processing\\nDevelop Deep Learning Models for \\nNatural Language in Python\\nJason Brownlee'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 1}, page_content='i\\nDisclaimer\\nThe information contained within this eBook is strictly for educational purposes. If you wish to apply\\nideas contained in this eBook, you are taking full responsibility for your actions.\\nThe author has made every eﬀort to ensure the accuracy of the information within this book was\\ncorrect at time of publication. The author does not assume and hereby disclaims any liability to any\\nparty for any loss, damage, or disruption caused by errors or omissions, whether such errors or\\nomissions result from accident, negligence, or any other cause.\\nNo part of this eBook may be reproduced or transmitted in any form or by any means, electronic or\\nmechanical, recording or by any information storage and retrieval system, without written permission\\nfrom the author.\\nAcknowledgements\\nSpecial thanks to my copy editor Sarah Martin and my technical editors Arun Koshy and Andrei\\nCheremskoy.\\nCopyright\\nDeep Learning for Natural Language Processing'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 1}, page_content='©Copyright 2017 Jason Brownlee. All Rights Reserved.\\nEdition: v1.1'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 2}, page_content='Contents\\nCopyright i\\nContents ii\\nPreface iii\\nI Introductions iv\\nWelcome v\\nWho Is This Book For? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v\\nAbout Your Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\\nHow to Read This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\\nAbout the Book Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\\nAbout Python Code Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\nAbout Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\\nAbout Getting Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\\nII Foundations 1\\n1 Natural Language Processing 2\\n1.1 Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 2}, page_content='1.2 Challenge of Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 From Linguistics to Natural Language Processing . . . . . . . . . . . . . . . . . 3\\n1.4 Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2 Deep Learning 8\\n2.1 Deep Learning is Large Neural Networks . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2 Deep Learning is Hierarchical Feature Learning . . . . . . . . . . . . . . . . . . 11\\n2.3 Deep Learning as Scalable Learning Across Domains . . . . . . . . . . . . . . . 12\\n2.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\nii'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 3}, page_content='CONTENTS iii\\n3 Promise of Deep Learning for Natural Language 16\\n3.1 Promise of Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.2 Promise of Drop-in Replacement Models . . . . . . . . . . . . . . . . . . . . . . 17\\n3.3 Promise of New NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.4 Promise of Feature Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.5 Promise of Continued Improvement . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.6 Promise of End-to-End Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n3.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4 How to Develop Deep Learning Models With Keras 21\\n4.1 Keras Model Life-Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 3}, page_content='4.2 Keras Functional Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n4.3 Standard Network Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\nIII Data Preparation 34\\n5 How to Clean Text Manually and with NLTK 35\\n5.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n5.2 Metamorphosis by Franz Kafka . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n5.3 Text Cleaning Is Task Speciﬁc . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n5.4 Manual Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n5.5 Tokenization and Cleaning with NLTK . . . . . . . . . . . . . . . . . . . . . . . 41\\n5.6 Additional Text Cleaning Considerations . . . . . . . . . . . . . . . . . . . . . . 46'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 3}, page_content='5.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n5.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n6 How to Prepare Text Data with scikit-learn 48\\n6.1 The Bag-of-Words Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n6.2 Word Counts with CountVectorizer . . . . . . . . . . . . . . . . . . . . . . . . 49\\n6.3 Word Frequencies with TfidfVectorizer . . . . . . . . . . . . . . . . . . . . . . 50\\n6.4 Hashing with HashingVectorizer . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n6.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\n6.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n7 How to Prepare Text Data With Keras 54\\n7.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 3}, page_content='7.2 Split Words with text toword sequence . . . . . . . . . . . . . . . . . . . . . 54\\n7.3 Encoding with onehot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n7.4 Hash Encoding with hashing trick . . . . . . . . . . . . . . . . . . . . . . . . 56\\n7.5 Tokenizer API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n7.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 4}, page_content='CONTENTS iv\\nIV Bag-of-Words 61\\n8 The Bag-of-Words Model 62\\n8.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n8.2 The Problem with Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n8.3 What is a Bag-of-Words? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n8.4 Example of the Bag-of-Words Model . . . . . . . . . . . . . . . . . . . . . . . . 63\\n8.5 Managing Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n8.6 Scoring Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n8.7 Limitations of Bag-of-Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n8.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n8.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n9 How to Prepare Movie Review Data for Sentiment Analysis 69'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 4}, page_content='9.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n9.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n9.3 Load Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n9.4 Clean Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\n9.5 Develop Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n9.6 Save Prepared Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n9.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n9.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n10 Project: Develop a Neural Bag-of-Words Model for Sentiment Analysis 85\\n10.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 4}, page_content='10.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n10.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n10.4 Bag-of-Words Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n10.5 Sentiment Analysis Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\\n10.6 Comparing Word Scoring Methods . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n10.7 Predicting Sentiment for New Reviews . . . . . . . . . . . . . . . . . . . . . . . 108\\n10.8 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\\n10.9 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\\n10.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\\nV Word Embeddings 114\\n11 The Word Embedding Model 115\\n11.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 4}, page_content='11.2 What Are Word Embeddings? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n11.3 Word Embedding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n11.4 Using Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n11.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 5}, page_content='CONTENTS v\\n12 How to Develop Word Embeddings with Gensim 122\\n12.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n12.2 Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.3 Gensim Python Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.4 Develop Word2Vec Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.5 Visualize Word Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\n12.6 Load Google’s Word2Vec Embedding . . . . . . . . . . . . . . . . . . . . . . . . 128\\n12.7 Load Stanford’s GloVe Embedding . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n12.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\\n12.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n13 How to Learn and Load Word Embeddings in Keras 133'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 5}, page_content='13.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\\n13.2 Word Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\\n13.3 Keras Embedding Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\\n13.4 Example of Learning an Embedding . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n13.5 Example of Using Pre-Trained GloVe Embedding . . . . . . . . . . . . . . . . . 138\\n13.6 Tips for Cleaning Text for Word Embedding . . . . . . . . . . . . . . . . . . . . 142\\n13.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\\n13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\\nVI Text Classiﬁcation 144\\n14 Neural Models for Document Classiﬁcation 145\\n14.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 5}, page_content='14.2 Word Embeddings + CNN = Text Classiﬁcation . . . . . . . . . . . . . . . . . . 146\\n14.3 Use a Single Layer CNN Architecture . . . . . . . . . . . . . . . . . . . . . . . . 147\\n14.4 Dial in CNN Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n14.5 Consider Character-Level CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n14.6 Consider Deeper CNNs for Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . 151\\n14.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n14.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n15 Project: Develop an Embedding + CNN Model for Sentiment Analysis 153\\n15.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n15.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 5}, page_content='15.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n15.4 Train CNN With Embedding Layer . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n15.5 Evaluate Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n15.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n15.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n15.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 6}, page_content='CONTENTS vi\\n16 Project: Develop an n-gram CNN Model for Sentiment Analysis 174\\n16.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n16.2 Movie Review Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n16.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\\n16.4 Develop Multichannel Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n16.5 Evaluate Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n16.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n16.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\\n16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\\nVII Language Modeling 189\\n17 Neural Language Modeling 190'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 6}, page_content='17.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\\n17.2 Problem of Modeling Language . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\\n17.3 Statistical Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n17.4 Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n17.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\\n17.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\\n18 How to Develop a Character-Based Neural Language Model 197\\n18.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\\n18.2 Sing a Song of Sixpence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\n18.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 6}, page_content='18.4 Train Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\\n18.5 Generate Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\\n18.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\\n18.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\\n19 How to Develop a Word-Based Neural Language Model 211\\n19.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\\n19.2 Framing Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\\n19.3 Jack and Jill Nursery Rhyme . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\\n19.4 Model 1: One-Word-In, One-Word-Out Sequences . . . . . . . . . . . . . . . . . 212\\n19.5 Model 2: Line-by-Line Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\\n19.6 Model 3: Two-Words-In, One-Word-Out Sequence . . . . . . . . . . . . . . . . . 222'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 6}, page_content='19.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\\n19.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\\n20 Project: Develop a Neural Language Model for Text Generation 226\\n20.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\\n20.2 The Republic by Plato . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\\n20.3 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\\n20.4 Train Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\\n20.5 Use Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\\n20.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 7}, page_content='CONTENTS vii\\n20.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\\n20.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\\nVIII Image Captioning 245\\n21 Neural Image Caption Generation 246\\n21.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\\n21.2 Describing an Image with Text . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\\n21.3 Neural Captioning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\n21.4 Encoder-Decoder Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\\n21.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\\n21.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\\n22 Neural Network Models for Caption Generation 252\\n22.1 Image Caption Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 7}, page_content='22.2 Inject Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\\n22.3 Merge Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\\n22.4 More on the Merge Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\\n22.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\\n22.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\\n23 How to Load and Use a Pre-Trained Object Recognition Model 257\\n23.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\\n23.2 ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\\n23.3 The Oxford VGG Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\\n23.4 Load the VGG Model in Keras . . . . . . . . . . . . . . . . . . . . . . . . . . . 259'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 7}, page_content='23.5 Develop a Simple Photo Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n23.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\\n23.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\\n24 How to Evaluate Generated Text With the BLEU Score 268\\n24.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\\n24.2 Bilingual Evaluation Understudy Score . . . . . . . . . . . . . . . . . . . . . . . 268\\n24.3 Calculate BLEU Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\\n24.4 Cumulative and Individual BLEU Scores . . . . . . . . . . . . . . . . . . . . . . 271\\n24.5 Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\\n24.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 7}, page_content='24.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\\n25 How to Prepare a Photo Caption Dataset For Modeling 277\\n25.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\\n25.2 Download the Flickr8K Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\\n25.3 How to Load Photographs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\\n25.4 Pre-Calculate Photo Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\\n25.5 How to Load Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n25.6 Prepare Description Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 8}, page_content='CONTENTS viii\\n25.7 Whole Description Sequence Model . . . . . . . . . . . . . . . . . . . . . . . . . 287\\n25.8 Word-By-Word Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\\n25.9 Progressive Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\\n25.10Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\\n25.11Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\\n26 Project: Develop a Neural Image Caption Generation Model 299\\n26.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\\n26.2 Photo and Caption Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\\n26.3 Prepare Photo Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\\n26.4 Prepare Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 8}, page_content='26.5 Develop Deep Learning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\\n26.6 Evaluate Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\\n26.7 Generate New Captions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\\n26.8 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\\n26.9 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\\n26.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\\nIX Machine Translation 332\\n27 Neural Machine Translation 333\\n27.1 What is Machine Translation? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\\n27.2 What is Statistical Machine Translation? . . . . . . . . . . . . . . . . . . . . . . 334\\n27.3 What is Neural Machine Translation? . . . . . . . . . . . . . . . . . . . . . . . . 335'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 8}, page_content='27.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\\n27.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\\n28 What are Encoder-Decoder Models for Neural Machine Translation 339\\n28.1 Encoder-Decoder Architecture for NMT . . . . . . . . . . . . . . . . . . . . . . 339\\n28.2 Sutskever NMT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\\n28.3 Cho NMT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\\n28.4 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\\n28.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346\\n29 How to Conﬁgure Encoder-Decoder Models for Machine Translation 347\\n29.1 Encoder-Decoder Model for Neural Machine Translation . . . . . . . . . . . . . 347\\n29.2 Baseline Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 8}, page_content='29.3 Word Embedding Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\\n29.4 RNN Cell Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\\n29.5 Encoder-Decoder Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\\n29.6 Direction of Encoder Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\\n29.7 Attention Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\\n29.8 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\\n29.9 Final Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\\n29.10Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\\n29.11Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 9}, page_content='CONTENTS ix\\n30 Project: Develop a Neural Machine Translation Model 354\\n30.1 Tutorial Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\n30.2 German to English Translation Dataset . . . . . . . . . . . . . . . . . . . . . . . 354\\n30.3 Preparing the Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\\n30.4 Train Neural Translation Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\\n30.5 Evaluate Neural Translation Model . . . . . . . . . . . . . . . . . . . . . . . . . 366\\n30.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\\n30.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n30.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\\nX Appendix 373\\nA Getting Help 374\\nA.1 Oﬃcial Keras Destinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 9}, page_content='A.2 Where to Get Help with Keras . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\\nA.3 Where to Get Help with Natural Language . . . . . . . . . . . . . . . . . . . . . 375\\nA.4 How to Ask Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\\nA.5 Contact the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\\nB How to Setup a Workstation for Deep Learning 376\\nB.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\\nB.2 Download Anaconda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\\nB.3 Install Anaconda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378\\nB.4 Start and Update Anaconda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\\nB.5 Install Deep Learning Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\\nB.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 9}, page_content='B.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\nC How to Use Deep Learning in the Cloud 385\\nC.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\\nC.2 Setup Your AWS Account . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386\\nC.3 Launch Your Server Instance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\\nC.4 Login, Conﬁgure and Run . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\\nC.5 Build and Run Models on AWS . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\\nC.6 Close Your EC2 Instance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\\nC.7 Tips and Tricks for Using Keras on AWS . . . . . . . . . . . . . . . . . . . . . . 395\\nC.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\\nC.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\\nXI Conclusions 396'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 9}, page_content='XI Conclusions 396\\nHow Far You Have Come 397'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 10}, page_content='Preface\\nWe are awash with text, from books, papers, blogs, tweets, news, and increasingly text from\\nspoken utterances. Every day, I get questions asking how to develop machine learning models\\nfor text data. Working with text is hard as it requires drawing upon knowledge from diverse\\ndomains such as linguistics, machine learning, statistical natural language processing, and these\\ndays, deep learning.\\nI have done my best to write blog posts to answer frequently asked questions on the topic\\nand decided to pull together my best knowledge on the matter into this book. I designed this\\nbook to teach you step-by-step how to bring modern deep learning methods to your natural\\nlanguage processing projects. I chose the programming language, programming libraries, and\\ntutorial topics to give you the skills you need.\\nPython is the go-to language for applied machine learning and deep learning, both in terms\\nof demand from employers and employees. This is not least because it could be a renaissance'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 10}, page_content='for machine learning tools. I have focused on showing you how to use the best of breed Python\\ntools for natural language processing such as Gensim and NLTK, and even a little scikit-learn.\\nKey to getting results is speed of development, and for this reason, we use the Keras deep\\nlearning library as you can deﬁne, train, and use complex deep learning models with just a few\\nlines of Python code.\\nThere are three key areas that you must know when working with text:\\n1.How to clean text. This includes loading, analyzing, ﬁltering and cleaning tasks required\\nprior to modeling.\\n2.How to represent text. This includes the classical bag-of-words model and the modern\\nand powerful distributed representation in word embeddings.\\n3.How to generate text. This includes the range of most interesting problems, such as image\\ncaptioning and translation.\\nThese key topics provide the backbone for the book and the tutorials you will work through.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 10}, page_content='I believe that after completing this book, you will have the skills that you need to both work\\nthrough your own natural language processing projects and bring modern deep learning methods\\nto bare.\\nJason Brownlee\\n2017\\nx'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 11}, page_content='Part I\\nIntroductions\\nxi'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 12}, page_content='Welcome\\nWelcome to Deep Learning for Natural Language Processing . Natural language processing is the\\narea of study dedicated to the automatic manipulation of speech and text by software. It is\\nan old ﬁeld of study, originally dominated by rule-based methods designed by linguists, then\\nstatistical methods, and, more recently, deep learning methods that show great promise in the\\nﬁeld. So much so that the heart of the Google Translate service uses a deep learning method, a\\ntopic that you will learn more about in this book.\\nI designed this book to teach you step-by-step how to bring modern deep learning models to\\nyour own natural language processing projects.\\nWho Is This Book For?\\nBefore we get started, let’s make sure you are in the right place. This book is for developers\\nthat know some applied machine learning and some deep learning. Maybe you want or need\\nto start using deep learning for text on your research project or on a project at work. This'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 12}, page_content='guide was written to help you do that quickly and eﬃciently by compressing years worth of\\nknowledge and experience into a laser-focused course of hands-on tutorials. The lessons in this\\nbook assume a few things about you, such as:\\n\\x88You know your way around basic Python for programming.\\n\\x88You know your way around basic NumPy for array manipulation.\\n\\x88You know your way around basic scikit-learn for machine learning.\\n\\x88You know your way around basic Keras for deep learning.\\nFor some bonus points, perhaps some of the below points apply to you. Don’t panic if they\\ndon’t.\\n\\x88You may know how to work through a predictive modeling problem end-to-end.\\n\\x88You may know a little bit of natural language processing.\\n\\x88You may know a little bit of natural language libraries such as NLTK or Gensim.\\nThis guide was written in the top-down and results-ﬁrst machine learning style that you’re\\nused to from MachineLearningMastery.com.\\nxii'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 13}, page_content='xiii\\nAbout Your Outcomes\\nThis book will teach you how to get results as a machine learning practitioner interested in\\nusing deep learning on your natural language processing project. After reading and working\\nthrough this book, you will know:\\n\\x88What natural language processing is and why it is challenging.\\n\\x88What deep learning is and how it is diﬀerent from other machine learning methods.\\n\\x88The promise of deep learning methods for natural language processing problems.\\n\\x88How to prepare text data for modeling using best-of-breed Python libraries.\\n\\x88How to develop distributed representations of text using word embedding models.\\n\\x88How to develop a bag-of-words model, a representation technique that can be used for\\nmachine learning and deep learning methods.\\n\\x88How to develop a neural sentiment analysis model for automatically predicting the class\\nlabel for a text document.\\n\\x88How to develop a neural language model, required for any text generating neural network.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 13}, page_content='\\x88How to develop a photo captioning system to automatically generate textual descriptions\\nof photographs.\\n\\x88How to develop a neural machine translation system for translating text from one language\\nto another.\\nThis book will NOT teach you how to be a research scientist and all the theory behind why\\nspeciﬁc methods work. For that, I would recommend good research papers and textbooks. See\\ntheFurther Reading section at the end of each tutorial for a good starting point.\\nHow to Read This Book\\nThis book was written to be read linearly, from start to ﬁnish. That being said, if you know\\nthe basics and need help with a speciﬁc problem type or technique, then you can ﬂip straight\\nto that section and get started. This book was designed for you to read on your workstation,\\non the screen, not on an eReader. My hope is that you have the book open right next to your\\neditor and run the examples as you read about them.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 13}, page_content='This book is not intended to be read passively or be placed in a folder as a reference text.\\nIt is a playbook, a workbook, and a guidebook intended for you to learn by doing and then\\napply your new understanding to your own natural language projects. To get the most out of\\nthe book, I would recommend playing with the examples in each tutorial. Extend them, break\\nthem, then ﬁx them. Try some of the extensions presented at the end of each lesson and let me\\nknow how you do.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 14}, page_content='xiv\\nAbout the Book Structure\\nThis book was designed around major activities, techniques, and natural language processing\\nproblems. There are a lot of things you could learn about deep learning and natural language\\nprocessing, from theory to applications to APIs. My goal is to take you straight to getting\\nresults with laser-focused tutorials. I designed the tutorials to focus on how to get things done.\\nThey give you the tools to both rapidly understand and apply each technique to your own\\nnatural language processing prediction problems.\\nEach of the tutorials are designed to take you about one hour to read through and complete,\\nexcluding the extensions and further reading. You can choose to work through the lessons one\\nper day, one per week, or at your own pace. I think momentum is critically important, and this\\nbook is intended to be read and used, not to sit idle. I would recommend picking a schedule\\nand sticking to it. The tutorials are divided into eight parts:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 14}, page_content='\\x88Part 1: Foundations . Discover a gentle introduction to natural language processing,\\ndeep learning, and the promise of combining the two, as well as tutorials on how to get\\nstarted with Keras.\\n\\x88Part 2: Data Preparation : Discover tutorials that show how to clean, prepare and\\nencode text ready for modeling with neural networks.\\n\\x88Part 3: Bag-of-Words . Discover the bag-of-words model, a staple representation for\\nmachine learning and a good starting point for neural networks for sentiment analysis.\\n\\x88Part 4: Word Embeddings . Discover a more powerful word representation in word\\nembeddings, how to develop them as standalone models, and how to learn them as part of\\nneural network models.\\n\\x88Part 5: Text Classiﬁcation . Discover how to leverage word embeddings and convolu-\\ntional neural networks to learn spatial invariant models of text for sentiment analysis, a\\nsuccessor to the bag-of-words model.\\n\\x88Part 6: Language Modeling . Discover how to develop character-based and word-based'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 14}, page_content='language models, a technique that is required as part of any modern text generating model.\\n\\x88Part 7: Image Captioning . Discover how to combine a pre-trained object recognition\\nmodel with a language model to automatically caption images.\\n\\x88Part 8: Machine Translation . Discover how to combine two language models to\\nautomatically translate text from one language to another.\\nEach part targets a speciﬁc learning outcome, and so does each tutorial within each part.\\nThis acts as a ﬁlter to ensure you are only focused on the things you need to know to get to a\\nspeciﬁc result and do not get bogged down in the math or near-inﬁnite number of conﬁguration\\nparameters. The tutorials were not designed to teach you everything there is to know about\\neach of the techniques or natural language processing problems. They were designed to give you\\nan understanding of how they work, how to use them on your projects the fastest way I know\\nhow: to learn by doing.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 15}, page_content='xv\\nAbout Python Code Examples\\nThe code examples were carefully designed to demonstrate the purpose of a given lesson. For\\nthis reason, the examples are highly targeted.\\n\\x88Models were demonstrated on real-world datasets to give you the context and conﬁdence\\nto bring the techniques to your own natural language processing problems.\\n\\x88Model conﬁgurations used were discovered through trial and error are skillful, but not\\noptimized. This leaves the door open for you to explore new and possibly better conﬁgu-\\nrations.\\n\\x88Code examples are complete and standalone. The code for each lesson will run as-is with\\nno code from prior lessons or third parties required beyond the installation of the required\\npackages.\\nA complete working example is presented with each tutorial for you to inspect and copy-\\nand-paste. All source code is also provided with the book and I would recommend running\\nthe provided ﬁles whenever possible to avoid any copy-paste issues. The provided code was'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 15}, page_content='developed in a text editor and intended to be run on the command line. No special IDE or\\nnotebooks are required. If you are using a more advanced development environment and are\\nhaving trouble, try running the example from the command line instead.\\nNeural network algorithms are stochastic. This means that they will make diﬀerent predictions\\nwhen the same model conﬁguration is trained on the same training data. On top of that, each\\nexperimental problem in this book is based around generating stochastic predictions. As a\\nresult, this means you will not get exactly the same sample output presented in this book. This\\nis by design. I want you to get used to the stochastic nature of the neural network algorithms.\\nIf this bothers you, please note:\\n\\x88You can re-run a given example a few times and your results should be close to the values\\nreported.\\n\\x88You can make the output consistent by ﬁxing the NumPy random number seed.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 15}, page_content='\\x88You can develop a robust estimate of the skill of a model by ﬁtting and evaluating it\\nmultiple times and taking the average of the ﬁnal skill score (highly recommended).\\nAll code examples were tested on a POSIX-compaitable machine with Python 3 and Keras\\n2. All code examples will run on modest and modern computer hardware and were executed on\\na CPU. No GPUs are required to run the presented examples, although a GPU would make the\\ncode run faster. I am only human and there may be a bug in the sample code. If you discover a\\nbug, please let me know so I can ﬁx it and update the book and send out a free update.\\nAbout Further Reading\\nEach lesson includes a list of further reading resources. This may include:\\n\\x88Research papers.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 16}, page_content='xvi\\n\\x88Books and book chapters.\\n\\x88Webpages.\\n\\x88API documentation.\\nWherever possible, I try to list and link to the relevant API documentation for key objects\\nand functions used in each lesson so you can learn more about them. When it comes to research\\npapers, I try to list papers that are ﬁrst to use a speciﬁc technique or ﬁrst in a speciﬁc problem\\ndomain. These are not required reading, but can give you more technical details, theory, and\\nconﬁguration details if you’re looking for it. Wherever possible, I have tried to link to the freely\\navailable version of the paper on arxiv.org . You can search for and download any of the papers\\nlisted on Google Scholar Search scholar.google.com . Wherever possible, I have tried to link\\nto books on Amazon. I don’t know everything, and if you discover a good resource related to a\\ngiven lesson, please let me know so I can update the book.\\nAbout Getting Help\\nYou might need help along the way. Don’t worry, you are not alone.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 16}, page_content='\\x88Help with a Technique? If you need help with the technical aspects of a speciﬁc model or\\nmethod, see the Further Reading sections at the end of each lesson.\\n\\x88Help with Keras? If you need help with using the Keras library, see the list of resources\\nin Appendix A.\\n\\x88Help with your workstation? If you need help setting up your environment, I would\\nrecommend using Anaconda and following my tutorial in Appendix B.\\n\\x88Help running large models? I recommend renting time on Amazon Web Service (AWS) to\\nrun large models. If you need help getting started on AWS, see the tutorial in Appendix\\nC.\\n\\x88Help in general? You can shoot me an email. My details are in Appendix A.\\nSummary\\nAre you ready? Let’s dive in!\\nNext up you will discover a concrete idea of what natural language processing actually\\nmeans.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 17}, page_content='Part II\\nFoundations\\n1'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 18}, page_content='Chapter 1\\nNatural Language Processing\\nNatural Language Processing, or NLP for short, is broadly deﬁned as the automatic manipulation\\nof natural language, like speech and text, by software. The study of natural language processing\\nhas been around for more than 50 years and grew out of the ﬁeld of linguistics with the rise of\\ncomputers. In this chapter, you will discover what natural language processing is and why it is\\nso important. After reading this chapter, you will know:\\n\\x88What natural language is and how it is diﬀerent from other types of data.\\n\\x88What makes working with natural language so challenging.\\n\\x88Where the ﬁeld of NLP came from and how it is deﬁned by modern practitioners.\\nLet’s get started.\\n1.1 Natural Language\\nNatural language refers to the way we, humans, communicate with each other. Namely, speech\\nand text. We are surrounded by text. Think about how much text you see each day:\\n\\x88Signs\\n\\x88Menus\\n\\x88Email\\n\\x88SMS\\n\\x88Web Pages\\n\\x88and so much more...'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 18}, page_content='The list is endless. Now think about speech. We may speak to each other, as a species, more\\nthan we write. It may even be easier to learn to speak than to write. Voice and text are how we\\ncommunicate with each other. Given the importance of this type of data, we must have methods\\nto understand and reason about natural language, just like we do for other types of data.\\n2'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 19}, page_content='1.2. Challenge of Natural Language 3\\n1.2 Challenge of Natural Language\\nWorking with natural language data is not solved. It has been studied for half a century, and it\\nis really hard.\\nIt is hard from the standpoint of the child, who must spend many years acquiring\\na language ... it is hard for the adult language learner, it is hard for the scientist\\nwho attempts to model the relevant phenomena, and it is hard for the engineer who\\nattempts to build systems that deal with natural language input or output. These\\ntasks are so hard that Turing could rightly make ﬂuent conversation in natural\\nlanguage the centerpiece of his test for intelligence.\\n— Page 248, Mathematical Linguistics , 2010.\\nNatural language is primarily hard because it is messy. There are few rules. And yet we can\\neasily understand each other most of the time.\\nHuman language is highly ambiguous ... It is also ever changing and evolving. People\\nare great at producing language and understanding language, and are capable of'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 19}, page_content='expressing, perceiving, and interpreting very elaborate and nuanced meanings. At\\nthe same time, while we humans are great users of language, we are also very poor\\nat formally understanding and describing the rules that govern language.\\n— Page 1, Neural Network Methods in Natural Language Processing , 2017.\\n1.3 From Linguistics to Natural Language Processing\\n1.3.1 Linguistics\\nLinguistics is the scientiﬁc study of language, including its grammar, semantics, and phonetics.\\nClassical linguistics involved devising and evaluating rules of language. Great progress was made\\non formal methods for syntax and semantics, but for the most part, the interesting problems in\\nnatural language understanding resist clean mathematical formalisms.\\nBroadly, a linguist is anyone who studies language, but perhaps more colloquially, a self-\\ndeﬁning linguist may be more focused on being out in the ﬁeld. Mathematics is the tool of'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 19}, page_content='science. Mathematicians working on natural language may refer to their study as mathematical\\nlinguistics, focusing exclusively on the use of discrete mathematical formalisms and theory for\\nnatural language (e.g. formal languages and automata theory).\\n1.3.2 Computational Linguistics\\nComputational linguistics is the modern study of linguistics using the tools of computer science.\\nYesterday’s linguistics may be today’s computational linguist as the use of computational tools\\nand thinking has overtaken most ﬁelds of study.\\nComputational linguistics is the study of computer systems for understanding and\\ngenerating natural language. ... One natural function for computational linguistics\\nwould be the testing of grammars proposed by theoretical linguists.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 20}, page_content='1.3. From Linguistics to Natural Language Processing 4\\n— Pages 4-5, Computational Linguistics: An Introduction , 1986.\\nLarge data and fast computers mean that new and diﬀerent things can be discovered from\\nlarge datasets of text by writing and running software. In the 1990s, statistical methods and\\nstatistical machine learning began to and eventually replaced the classical top-down rule-based\\napproaches to language, primarily because of their better results, speed, and robustness. The\\nstatistical approach to studying natural language now dominates the ﬁeld; it may deﬁne the\\nﬁeld.\\nData-Driven methods for natural language processing have now become so popular\\nthat they must be considered mainstream approaches to computational linguistics.\\n... A strong contributing factor to this development is undoubtedly the increase\\namount of available electronically stored data to which these methods can be applied;\\nanother factor might be a certain disenchantment with approaches relying exclusively'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 20}, page_content='on hand-crafted rules, due to their observed brittleness.\\n— Page 358, The Oxford Handbook of Computational Linguistics , 2005.\\nThe statistical approach to natural language is not limited to statistics per-se, but also to\\nadvanced inference methods like those used in applied machine learning.\\n... understanding natural language require large amounts of knowledge about\\nmorphology, syntax, semantics and pragmatics as well as general knowledge about\\nthe world. Acquiring and encoding all of this knowledge is one of the fundamental\\nimpediments to developing eﬀective and robust language systems. Like the statistical\\nmethods ... machine learning methods oﬀ the promise of te automatic acquisition of\\nthis knowledge from annotated or unannotated language corpora.\\n— Page 377, The Oxford Handbook of Computational Linguistics , 2005.\\n1.3.3 Statistical Natural Language Processing\\nComputational linguistics also became known by the name of natural language process, or'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 20}, page_content='NLP, to reﬂect the more engineer-based or empirical approach of the statistical methods. The\\nstatistical dominance of the ﬁeld also often leads to NLP being described as Statistical Natural\\nLanguage Processing, perhaps to distance it from the classical computational linguistics methods.\\nI view computational linguistics as having both a scientiﬁc and an engineering side.\\nThe engineering side of computational linguistics, often called natural language\\nprocessing (NLP), is largely concerned with building computational tools that do\\nuseful things with language, e.g., machine translation, summarization, question-\\nanswering, etc. Like any engineering discipline, natural language processing draws\\non a variety of diﬀerent scientiﬁc disciplines.\\n—How the statistical revolution changes (computational) linguistics , 2009.\\nLinguistics is a large topic of study, and, although the statistical approach to NLP has shown'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 20}, page_content='great success in some areas, there is still room and great beneﬁt from the classical top-down\\nmethods.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 21}, page_content='1.4. Natural Language Processing 5\\nRoughly speaking, statistical NLP associates probabilities with the alternatives\\nencountered in the course of analyzing an utterance or a text and accepts the\\nmost probable outcome as the correct one. ... Not surprisingly, words that name\\nphenomena that are closely related in the world, or our perception of it, frequently\\noccur close to one another so that crisp facts about the world are reﬂected in\\nsomewhat fuzzier facts about texts. There is much room for debate in this view.\\n— Page xix, The Oxford Handbook of Computational Linguistics , 2005.\\n1.4 Natural Language Processing\\nAs machine learning practitioners interested in working with text data, we are concerned with\\nthe tools and methods from the ﬁeld of Natural Language Processing. We have seen the path\\nfrom linguistics to NLP in the previous section. Now, let’s take a look at how modern researchers\\nand practitioners deﬁne what NLP is all about. In perhaps one of the more widely known'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 21}, page_content='textbooks written by top researchers in the ﬁeld, they refer to the subject as linguistic science ,\\npermitting discussion of both classical linguistics and modern statistical methods.\\nThe aim of a linguistic science is to be able to characterize and explain the multitude\\nof linguistic observations circling around us, in conversations, writing, and other\\nmedia. Part of that has to do with the cognitive size of how humans acquire, produce\\nand understand language, part of it has to do with understanding the relationship\\nbetween linguistic utterances and the world, and part of it has to do with understand\\nthe linguistic structures by which language communicates.\\n— Page 3, Foundations of Statistical Natural Language Processing , 1999.\\nThey go on to focus on inference through the use of statistical methods in natural language\\nprocessing.\\nStatistical NLP aims to do statistical inference for the ﬁeld of natural language.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 21}, page_content='Statistical inference in general consists of taking some data (generated in accordance\\nwith some unknown probability distribution) and then making some inference about\\nthis distribution.\\n— Page 191, Foundations of Statistical Natural Language Processing , 1999.\\nIn their text on applied natural language processing, the authors and contributors to the\\npopular NLTK Python library for NLP describe the ﬁeld broadly as using computers to work\\nwith natural language data.\\nWe will take Natural Language Processing - or NLP for short - in a wide sense to\\ncover any kind of computer manipulation of natural language. At one extreme, it\\ncould be as simple as counting word frequencies to compare diﬀerent writing styles.\\nAt the other extreme, NLP involves “understanding” complete human utterances,\\nat least to the extent of being able to give useful responses to them.\\n— Page ix, Natural Language Processing with Python , 2009.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 22}, page_content='1.5. Further Reading 6\\nStatistical NLP has turned another corner and is now strongly focused on the use of deep\\nlearning neural networks to both perform inference on speciﬁc tasks and for developing robust\\nend-to-end systems. In one of the ﬁrst textbooks dedicated to this emerging topic, Yoav Goldberg\\nsuccinctly deﬁnes NLP as automatic methods that take natural language as input or produce\\nnatural language as output.\\nNatural language processing (NLP) is a collective term referring to automatic\\ncomputational processing of human languages. This includes both algorithms that\\ntake human-produced text as input, and algorithms that produce natural looking\\ntext as outputs.\\n— Page xvii, Neural Network Methods in Natural Language Processing , 2017.\\n1.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n1.5.1 Books\\n\\x88Mathematical Linguistics , 2010.\\nhttp://amzn.to/2tO1cOO\\n\\x88Neural Network Methods in Natural Language Processing , 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 22}, page_content='http://amzn.to/2u0JtPl\\n\\x88Computational Linguistics: An Introduction , 1986.\\nhttp://amzn.to/2h6U4qY\\n\\x88The Oxford Handbook of Computational Linguistics , 2005.\\nhttp://amzn.to/2uHeERE\\n\\x88Foundations of Statistical Natural Language Processing , 1999.\\nhttp://amzn.to/2uzwxDE\\n\\x88Natural Language Processing with Python , 2009.\\nhttp://amzn.to/2uZMF27\\n1.5.2 Wikipedia\\n\\x88Linguistics.\\nhttps://en.wikipedia.org/wiki/Linguistics\\n\\x88Computational linguistics.\\nhttps://en.wikipedia.org/wiki/Computational_linguistics\\n\\x88Natural language processing.\\nhttps://en.wikipedia.org/wiki/Natural_language_processing'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 23}, page_content='1.6. Summary 7\\n\\x88History of natural language processing.\\nhttps://en.wikipedia.org/wiki/History_of_natural_language_processing\\n\\x88Outline of natural language processing.\\nhttps://en.wikipedia.org/wiki/Outline_of_natural_language_processing\\n1.6 Summary\\nIn this chapter, you discovered what natural language processing is why it is so important.\\nSpeciﬁcally, you learned:\\n\\x88What natural language is and how it is diﬀerent from other types of data.\\n\\x88What makes working with natural language so challenging.\\n\\x88Where the ﬁeld of NLP came from and how it is deﬁned by modern practitioners.\\n1.6.1 Next\\nIn the next chapter, you will discover what deep learning is and the motivation behind using\\ndeep artiﬁcial neural networks.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 24}, page_content='Chapter 2\\nDeep Learning\\nDeep Learning is a subﬁeld of machine learning concerned with algorithms inspired by the\\nstructure and function of the brain called artiﬁcial neural networks. If you are just starting out\\nin the ﬁeld of deep learning or you had some experience with neural networks some time ago,\\nyou may be confused. I know I was confused initially and so were many of my colleagues and\\nfriends who learned and used neural networks in the 1990s and early 2000s.\\nThe leaders and experts in the ﬁeld have ideas of what deep learning is and these speciﬁc\\nand nuanced perspectives shed a lot of light on what deep learning is all about. In this chapter,\\nyou will discover exactly what deep learning is by hearing from a range of experts and leaders\\nin the ﬁeld. After reading this chapter, you will know:\\n\\x88The motivation for exploring and adopting large neural network models.\\n\\x88The perspective on deep learning as hierarchical feature learning.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 24}, page_content='\\x88The promise of scalability of deep learning with the size of data.\\nLet’s dive in.\\n2.1 Deep Learning is Large Neural Networks\\nAndrew Ng from Coursera and formally Chief Scientist at Baidu Research and founder of Google\\nBrain that eventually resulted in the productization of deep learning technologies across a large\\nnumber of Google services. He has spoken and written a lot about what deep learning is and\\nis a good place to start. In early talks on deep learning, Andrew described deep learning in\\nthe context of traditional artiﬁcial neural networks. In the 2013 talk titled Deep Learning,\\nSelf-Taught Learning and Unsupervised Feature Learning he described the idea of deep learning\\nas:\\nUsing brain simulations, hope to:\\n- Make learning algorithms much better and easier to use.\\n- Make revolutionary advances in machine learning and AI.\\nI believe this is our best shot at progress towards real AI\\n—Deep Learning, Self-Taught Learning and Unsupervised Feature Learning , 2013.\\n8'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 25}, page_content='2.1. Deep Learning is Large Neural Networks 9\\nLater his comments became more nuanced. The core of deep learning according to Andrew\\nis that we now have fast enough computers and enough data to actually train large neural\\nnetworks. When discussing why now is the time that deep learning is taking oﬀ at ExtractConf\\n2015 in a talk titled What data scientists should know about deep learning , he commented:\\n... very large neural networks we can now have and ... huge amounts of data that\\nwe have access to\\n—What data scientists should know about deep learning , 2015.\\nHe also commented on the important point that it is all about scale. That as we construct\\nlarger neural networks and train them with more and more data, their performance continues to\\nincrease. This is generally diﬀerent to other machine learning techniques that reach a plateau in\\nperformance.\\n... for most ﬂavors of the old generations of learning algorithms ... performance will'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 25}, page_content='plateau. ... deep learning ... is the ﬁrst class of algorithms ... that is scalable. ...\\nperformance just keeps getting better as you feed them more data\\n—What data scientists should know about deep learning , 2015.\\nHe provides a nice cartoon of this in his slides:\\nFigure 2.1: Why Deep Learning? Slide by Andrew Ng, taken from What data scientists should\\nknow about deep learning .\\nFinally, he is clear to point out that the beneﬁts from deep learning that we are seeing in\\npractice come from supervised learning. From the 2015 ExtractConf talk, he commented:\\n... almost all the value today of deep learning is through supervised learning or\\nlearning from labeled data'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 26}, page_content='2.1. Deep Learning is Large Neural Networks 10\\n—What data scientists should know about deep learning , 2015.\\nEarlier at a talk to Stanford University titled Deep Learning in 2014 he made a similar\\ncomment:\\n... one reason that deep learning has taken oﬀ like crazy is because it is fantastic at\\nsupervised learning\\n—Invited Talk: Andrew Ng (Stanford University): Deep Learning , 2014.\\nAndrew often mentions that we should and will see more beneﬁts coming from the unsu-\\npervised side of the tracks as the ﬁeld matures to deal with the abundance of unlabeled data\\navailable. Jeﬀ Dean is a Wizard and Google Senior Fellow in the Systems and Infrastructure\\nGroup at Google and has been involved and perhaps partially responsible for the scaling and\\nadoption of deep learning within Google. Jeﬀ was involved in the Google Brain project and the\\ndevelopment of large-scale deep learning software DistBelief and later TensorFlow. In a 2016'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 26}, page_content='talk titled Deep Learning for Building Intelligent Computer Systems he made a comment in the\\nsimilar vein, that deep learning is really all about large neural networks.\\nWhen you hear the term deep learning, just think of a large deep neural net. Deep\\nrefers to the number of layers typically and so this kind of the popular term that’s\\nbeen adopted in the press. I think of them as deep neural networks generally.\\n—Deep Learning for Building Intelligent Computer Systems , 2016.\\nHe has given this talk a few times, and in a modiﬁed set of slides for the same talk, he\\nhighlights the scalability of neural networks indicating that results get better with more data\\nand larger models, that in turn require more computation to train.\\nFigure 2.2: Results Get Better With More Data, Larger Models, More Compute. Taken from\\nDeep Learning for Building Intelligent Computer Systems .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 27}, page_content='2.2. Deep Learning is Hierarchical Feature Learning 11\\n2.2 Deep Learning is Hierarchical Feature Learning\\nIn addition to scalability, another often cited beneﬁt of deep learning models is their ability to\\nperform automatic feature extraction from raw data, also called feature learning. Yoshua Bengio\\nis another leader in deep learning although began with a strong interest in the automatic feature\\nlearning that large neural networks are capable of achieving. He describes deep learning in terms\\nof the algorithms ability to discover and learn good representations using feature learning. In\\nhis 2012 paper titled Deep Learning of Representations for Unsupervised and Transfer Learning\\nhe commented:\\nDeep learning algorithms seek to exploit the unknown structure in the input dis-\\ntribution in order to discover good representations, often at multiple levels, with\\nhigher-level learned features deﬁned in terms of lower-level features'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 27}, page_content='—Deep Learning of Representations for Unsupervised and Transfer Learning , 2012.\\nAn elaborated perspective of deep learning along these lines is provided in his 2009 technical\\nreport titled Learning deep architectures for AI where he emphasizes the importance the\\nhierarchy in feature learning.\\nDeep learning methods aim at learning feature hierarchies with features from higher\\nlevels of the hierarchy formed by the composition of lower level features. Auto-\\nmatically learning features at multiple levels of abstraction allow a system to learn\\ncomplex functions mapping the input to the output directly from data, without\\ndepending completely on human-crafted features.\\n—Learning deep architectures for AI , 2009.\\nIn the published book titled Deep Learning co-authored with Ian Goodfellow and Aaron\\nCourville, they deﬁne deep learning in terms of the depth of the architecture of the models.\\nThe hierarchy of concepts allows the computer to learn complicated concepts by'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 27}, page_content='building them out of simpler ones. If we draw a graph showing how these concepts\\nare built on top of each other, the graph is deep, with many layers. For this reason,\\nwe call this approach to AI deep learning.\\n—Deep Learning , 2016.\\nThis is an important book and will likely may be the deﬁnitive resource for the ﬁeld for some\\ntime. The book goes on to describe Multilayer Perceptrons as an algorithm used in the ﬁeld of\\ndeep learning, giving the idea that deep learning has subsumed artiﬁcial neural networks.\\nThe quintessential example of a deep learning model is the feedforward deep network\\nor multilayer perceptron (MLP).\\n—Deep Learning , 2016.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 28}, page_content='2.3. Deep Learning as Scalable Learning Across Domains 12\\nPeter Norvig is the Director of Research at Google and famous for his textbook on AI titled\\nArtiﬁcial Intelligence: A Modern Approach . In a 2016 talk he gave titled Deep Learning and\\nUnderstandability versus Software Engineering and Veriﬁcation he deﬁned deep learning in a\\nvery similar way to Yoshua, focusing on the power of abstraction permitted by using a deeper\\nnetwork structure.\\na kind of learning where the representation you form have several levels of abstraction,\\nrather than a direct input to output\\n—Deep Learning and Understandability versus Software Engineering and Veriﬁcation , 2016.\\n2.3 Deep Learning as Scalable Learning Across Domains\\nDeep learning excels on problem domains where the inputs (and even output) are analog.\\nMeaning, they are not a few quantities in a tabular format but instead are images of pixel\\ndata, documents of text data or ﬁles of audio data. Yann LeCun is the director of Facebook'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 28}, page_content='Research and is the father of the network architecture that excels at object recognition in image\\ndata called the Convolutional Neural Network (CNN). This technique is seeing great success\\nbecause like multilayer perceptron feedforward neural networks, the technique scales with data\\nand model size and can be trained with backpropagation.\\nThis biases his deﬁnition of deep learning as the development of very large CNNs, which have\\nhad great success on object recognition in photographs. In a 2016 talk at Lawrence Livermore\\nNational Laboratory titled Accelerating Understanding: Deep Learning, Intelligent Applications,\\nand GPUs he described deep learning generally as learning hierarchical representations and\\ndeﬁnes it as a scalable approach to building object recognition systems:\\ndeep learning [is] ... a pipeline of modules all of which are trainable. ... deep because\\n[has] multiple stages in the process of recognizing an object and all of those stages\\nare part of the training'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 28}, page_content='—Accelerating Understanding: Deep Learning, Intelligent Applications, and GPUs , 2016.\\nJurgen Schmidhuber is the father of another popular algorithm that like MLPs and CNNs\\nalso scales with model size and dataset size and can be trained with backpropagation, but\\nis instead tailored to learning sequence data, called the Long Short-Term Memory Network\\n(LSTM), a type of recurrent neural network. We do see some confusion in the phrasing of the\\nﬁeld as deep learning . In his 2014 paper titled Deep Learning in Neural Networks: An Overview\\nhe does comment on the problematic naming of the ﬁeld and the diﬀerentiation of deep from\\nshallow learning. He also interestingly describes depth in terms of the complexity of the problem\\nrather than the model used to solve the problem.\\nAt which problem depth does Shallow Learning end, and Deep Learning begin?\\nDiscussions with DL experts have not yet yielded a conclusive response to this'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 28}, page_content='question. [...], let me just deﬁne for the purposes of this overview: problems of depth\\n>10 require Very Deep Learning.\\n—Deep Learning in Neural Networks: An Overview , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 29}, page_content='2.4. Further Reading 13\\nDemis Hassabis is the founder of DeepMind, later acquired by Google. DeepMind made\\nthe breakthrough of combining deep learning techniques with reinforcement learning to handle\\ncomplex learning problems like game playing, famously demonstrated in playing Atari games\\nand the game Go with Alpha Go. In keeping with the naming, they called their new technique\\na Deep Q-Network, combining Deep Learning with Q-Learning. They also name the broader\\nﬁeld of study Deep Reinforcement Learning .\\nIn their 2015 nature paper titled Human-level control through deep reinforcement learning\\nthey comment on the important role of deep neural networks in their breakthrough and highlight\\nthe need for hierarchical abstraction.\\nTo achieve this, we developed a novel agent, a deep Q-network (DQN), which is\\nable to combine reinforcement learning with a class of artiﬁcial neural network\\nknown as deep neural networks. Notably, recent advances in deep neural networks,'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 29}, page_content='in which several layers of nodes are used to build up progressively more abstract\\nrepresentations of the data, have made it possible for artiﬁcial neural networks to\\nlearn concepts such as object categories directly from raw sensory data.\\n—Human-level control through deep reinforcement learning , 2015.\\nFinally, in what may be considered a deﬁning paper in the ﬁeld, Yann LeCun, Yoshua Bengio\\nand Geoﬀrey Hinton published a paper in Nature titled simply Deep Learning . In it, they open\\nwith a clean deﬁnition of deep learning highlighting the multilayered approach.\\nDeep learning allows computational models that are composed of multiple processing\\nlayers to learn representations of data with multiple levels of abstraction.\\n—Deep Learning , 2015.\\nLater the multilayered approach is described in terms of representation learning and abstrac-\\ntion.\\nDeep-learning methods are representation-learning methods with multiple levels of'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 29}, page_content='representation, obtained by composing simple but non-linear modules that each\\ntransform the representation at one level (starting with the raw input) into a\\nrepresentation at a higher, slightly more abstract level. [...] The key aspect of deep\\nlearning is that these layers of features are not designed by human engineers: they\\nare learned from data using a general-purpose learning procedure.\\n—Deep Learning , 2015.\\nThis is a nice and generic description, and could easily describe most artiﬁcial neural network\\nalgorithms. It is also a good note to end on.\\n2.4 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 30}, page_content='2.5. Summary 14\\n2.4.1 Videos\\n\\x88Deep Learning, Self-Taught Learning and Unsupervised Feature Learning , 2013.\\nhttps://www.youtube.com/watch?v=n1ViNeWhC24\\n\\x88What data scientists should know about deep learning , 2015.\\nhttps://www.youtube.com/watch?v=O0VN0pGgBZM\\n\\x88Invited Talk: Andrew Ng (Stanford University): Deep Learning 2014.\\nhttps://www.youtube.com/watch?v=W15K9PegQt0\\n\\x88Deep Learning for Building Intelligent Computer Systems , 2016.\\nhttps://www.youtube.com/watch?v=QSaZGT4-6EY\\n\\x88Deep Learning and Understandability versus Software Engineering and Veriﬁcation , 2016.\\nhttps://www.youtube.com/watch?v=X769cyzBNVw\\n\\x88Accelerating Understanding: Deep Learning, Intelligent Applications, and GPUs , 2016.\\nhttps://www.youtube.com/watch?v=Qk4SqF9FT-M\\n2.4.2 Books\\n\\x88Deep Learning , 2016.\\nhttp://amzn.to/2goLnbO\\n2.4.3 Articles\\n\\x88Deep Learning of Representations for Unsupervised and Transfer Learning , 2012.\\nhttp://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 30}, page_content='\\x88Learning deep architectures for AI , 2009.\\nhttp://www.iro.umontreal.ca/ ~lisa/publications2/index.php/publications/show/\\n239\\n\\x88Deep Learning in Neural Networks: An Overview , 2014.\\nhttp://arxiv.org/pdf/1404.7828v4.pdf\\n\\x88Human-level control through deep reinforcement learning , 2015.\\nhttp://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\\n\\x88Deep Learning , 2015.\\nhttp://www.nature.com/nature/journal/v521/n7553/full/nature14539.html\\n2.5 Summary\\nIn this chapter you discovered that deep learning is just very big neural networks on a lot\\nmore data, requiring bigger computers. Although early approaches published by Hinton and\\ncollaborators focus on greedy layer-wise training and unsupervised methods like autoencoders,\\nmodern state-of-the-art deep learning is focused on training deep (many layered) neural network'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 31}, page_content='2.5. Summary 15\\nmodels using the backpropagation algorithm. The most popular techniques that we will focus\\non are:\\n\\x88Multilayer Perceptron Networks (MLP).\\n\\x88Convolutional Neural Networks (CNN).\\n\\x88Long Short-Term Memory Recurrent Neural Networks (LSTM).\\nI hope this has cleared up what deep learning is and how leading deﬁnitions ﬁt together\\nunder the one umbrella.\\n2.5.1 Next\\nIn the next chapter, you will discover the promise of deep learning neural networks for the ﬁeld\\nof natural language processing.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 32}, page_content='Chapter 3\\nPromise of Deep Learning for Natural\\nLanguage\\nThe promise of deep learning in the ﬁeld of natural language processing is the better performance\\nby models that may require more data but less linguistic expertise to train and operate. There\\nis a lot of hype and large claims around deep learning methods, but beyond the hype, deep\\nlearning methods are achieving state-of-the-art results on challenging problems. Notably in\\nnatural language processing. In this chapter, you will discover the speciﬁc promises that deep\\nlearning methods have for tackling natural language processing problems. After reading this\\nchapter, you will know:\\n\\x88The promises of deep learning for natural language processing.\\n\\x88What practitioners and research scientists have to say about the promise of deep learning\\nin NLP.\\n\\x88Key deep learning methods and applications for natural language processing.\\nLet’s get started.\\n3.1 Promise of Deep Learning'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 32}, page_content='Deep learning methods are popular, primarily because they are delivering on their promise.\\nThat is not to say that there is no hype around the technology, but that the hype is based\\non very real results that are being demonstrated across a suite of very challenging artiﬁcial\\nintelligence problems from computer vision and natural language processing. Some of the\\nﬁrst large demonstrations of the power of deep learning were in natural language processing,\\nspeciﬁcally speech recognition. More recently in machine translation.\\nIn this chapter, we will look at ﬁve speciﬁc promises of deep learning methods in the ﬁeld of\\nnatural language processing. Promises highlighted recently by researchers and practitioners in\\nthe ﬁeld, people who may be more tempered than the average reported in what the promises\\nmay be. In summary, they are:\\n\\x88The Promise of Drop-in Replacement Models . That is, deep learning methods can'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 32}, page_content='be dropped into existing natural language systems as replacement models that can achieve\\ncommensurate or better performance.\\n16'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 33}, page_content='3.2. Promise of Drop-in Replacement Models 17\\n\\x88The Promise of New NLP Models . That is, deep learning methods oﬀer the op-\\nportunity of new modeling approaches to challenging natural language problems like\\nsequence-to-sequence prediction.\\n\\x88The Promise of Feature Learning . That is, that deep learning methods can learn\\nthe features from natural language required by the model, rather than requiring that the\\nfeatures be speciﬁed and extracted by an expert.\\n\\x88The Promise of Continued Improvement . That is, that the performance of deep\\nlearning in natural language processing is based on real results and that the improvements\\nappear to be continuing and perhaps speeding up.\\n\\x88The Promise of End-to-End Models . That is, that large end-to-end deep learning\\nmodels can be ﬁt on natural language problems oﬀering a more general and better-\\nperforming approach.\\nWe will now take a closer look at each. There are other promises of deep learning for natural'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 33}, page_content='language processing; these were just the 5 that I chose to highlight.\\n3.2 Promise of Drop-in Replacement Models\\nThe ﬁrst promise for deep learning in natural language processing is the ability to replace\\nexisting linear models with better performing models capable of learning and exploiting nonlinear\\nrelationships. Yoav Goldberg, in his primer on neural networks for NLP researchers, highlights\\nboth that deep learning methods are achieving impressive results.\\nMore recently, neural network models started to be applied also to textual natural\\nlanguage signals, again with very promising results.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\nHe goes on to highlight that the methods are easy to use and can sometimes be used to\\nwholesale replace existing linear methods.\\nRecently, the ﬁeld has seen some success in switching from such linear models over\\nsparse inputs to non-linear neural-network models over dense inputs. While most'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 33}, page_content='of the neural network techniques are easy to apply, sometimes as almost drop-in\\nreplacements of the old linear classiﬁers, there is in many cases a strong barrier of\\nentry.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\n3.3 Promise of New NLP Models\\nAnother promise is that deep learning methods facilitate developing entirely new models. One\\nstrong example is the use of recurrent neural networks that are able learn and condition output\\nover very long sequences. The approach is suﬃciently diﬀerent in that they allow the practitioner'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 34}, page_content='3.4. Promise of Feature Learning 18\\nto break free of traditional modeling assumptions and in turn achieve state-of-the-art results.\\nIn his book expanding on deep learning for NLP, Yoav Goldberg comments that sophisticated\\nneural network models like recurrent neural networks allow for wholly new NLP modeling\\nopportunities.\\nAround 2014, the ﬁeld has started to see some success in switching from such\\nlinear models over sparse inputs to nonlinear neural network models over dense\\ninputs. ... Others are more advanced, require a change of mindset, and provide new\\nmodeling opportunities, In particular, a family of approaches based on recurrent\\nneural networks (RNNs) alleviates the reliance on the Markov Assumption that was\\nprevalent in sequence models, allowing to condition on arbitrary long sequences\\nand produce eﬀective feature extractors. These advance lead to breakthroughs in\\nlanguage modeling, automatic machine translations and other applications.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 34}, page_content='— Page xvii, Neural Network Methods in Natural Language Processing , 2017.\\n3.4 Promise of Feature Learning\\nDeep learning methods have the ability to learn feature representations rather than requiring\\nexperts to manually specify and extract features from natural language. The NLP researcher\\nChris Manning, in the ﬁrst lecture of his course on deep learning for natural language processing,\\nhighlights a diﬀerent perspective. He describes the limitations of manually deﬁned input features,\\nwhere prior applications of machine learning in statistical NLP were really a testament to the\\nhumans deﬁning the features and that the computers did very little learning.\\nChris suggests that the promise of deep learning methods is the automatic feature learning.\\nHe highlights that feature learning is automatic rather than manual, easy to adapt rather than\\nbrittle, and can continually and automatically improve.\\nIn general our manually designed features tend to be overspeciﬁed, incomplete, take a'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 34}, page_content='long time to design and validated, and only get you to a certain level of performance\\nat the end of the day. Where the learned features are easy to adapt, fast to train\\nand they can keep on learning so that they get to a better level of performance they\\nwe’ve been able to achieve previously.\\n— Chris Manning, Lecture 1 – Natural Language Processing with Deep Learning, 2017.\\n3.5 Promise of Continued Improvement\\nAnother promise of deep learning for NLP is continued and rapid improvement on challenging\\nproblems. In the same initial lecture on deep learning for NLP, Chris Manning goes on to\\ndescribe that deep learning methods are popular for natural language because they are working.\\nThe real reason why deep learning is so exciting to most people is it has been\\nworking.\\n— Chris Manning, Lecture 1 – Natural Language Processing with Deep Learning, 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 35}, page_content='3.6. Promise of End-to-End Models 19\\nHe highlights that initial results were impressive and achieved results in speech better than\\nany other methods in the last 30 years. Chris goes on to mention that it is not just the\\nstate-of-the-art results being achieved, but also the rate of improvement.\\n... what has just been totally stunning is over the last 6 or 7 years, there’s just\\nbeen this amazing ramp in which deep learning methods have been keeping on being\\nimproved and getting better at just an amazing speed. ... I’d actually just say\\nit unprecedented, in terms of seeming a ﬁeld that has been progressing quite so\\nquickly in its ability to be sort of rolling out better methods of doing things month\\non month.\\n— Chris Manning, Lecture 1 – Natural Language Processing with Deep Learning, 2017.\\n3.6 Promise of End-to-End Models\\nA ﬁnal promise of deep learning is the ability to develop and train end-to-end models for natural'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 35}, page_content='language problems instead of developing pipelines of specialized models. This is desirable both\\nfor the speed and simplicity of development in addition to the improved performance of these\\nmodels.\\nNeural machine translation, or NMT for short, refers to large neural networks that attempt\\nto learn to translate one language to another. This was a task traditionally handled by a pipeline\\nof classical hand-tuned models, each of which required specialized expertise. This is described\\nby Chris Manning in lecture 10 of his Stanford course on deep learning for NLP.\\nNeural machine translation is used to mean what we want to do is build one big\\nneural network which we can train entire end-to-end machine translation process in\\nand optimize end-to-end.\\n[...]\\nThis move away from hand customized piecewise models towards end-to-end sequence-\\nto-sequence prediction models has been the trend in speech recognition. Systems\\nthat do that are referred to as an NMT [neural machine translation] system.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 35}, page_content='— Chris Manning, Lecture 10: Neural Machine Translation and Models with Attention, 2017.\\nThis trend towards end-to-end models rather than pipelines of specialized systems is also\\na trend in speech recognition. In his presentation of speech recognition in the Stanford NLP\\ncourse, the NLP researcher Navdeep Jaitly, now at Nvidia, highlights that each component of a\\nspeech recognition can be replaced with a neural network. The large blocks of an automatic\\nspeech recognition pipeline are speech processing, acoustic models, pronunciation models, and\\nlanguage models. The problem is, the properties and importantly the errors of each sub-system\\nare diﬀerent. This motivates the need to develop one neural network to learn the whole problem\\nend-to-end.\\nOver time people starting noticing that each of these components could be done\\nbetter if we used a neural network. ... However, there’s still a problem. There’s\\nneural networks in every component, but errors in each one are diﬀerent, so they'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 35}, page_content='may not play well together. So that is the basic motivation for trying to go to a\\nprocess where you train entire model as one big model itself.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 36}, page_content='3.7. Further Reading 20\\n— Navdeep Jaitly, Lecture 12: End-to-End Models for Speech Processing, Natural Language\\nProcessing with Deep Learning, 2017.\\n3.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88A Primer on Neural Network Models for Natural Language Processing , 2015.\\nhttps://arxiv.org/abs/1510.00726\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2eScGtY\\n\\x88Stanford CS224n: Natural Language Processing with Deep Learning , 2017.\\nhttp://web.stanford.edu/class/cs224n/\\n3.8 Summary\\nIn this chapter, you discovered the promise of deep learning neural networks for natural language\\nprocessing. Speciﬁcally, you learned:\\n\\x88The promises of deep learning for natural language processing.\\n\\x88What practitioners and research scientists have to say about the promise of deep learning\\nin NLP.\\n\\x88Key deep learning methods and applications for natural language processing.\\n3.8.1 Next'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 36}, page_content='3.8.1 Next\\nIn the next chapter, you will discover how you can develop deep learning neural networks using\\nthe Keras Python library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 37}, page_content='Chapter 4\\nHow to Develop Deep Learning Models\\nWith Keras\\nDeep learning neural networks are very easy to create and evaluate in Python with Keras, but\\nyou must follow a strict model life-cycle. In this chapter you will discover the step-by-step\\nlife-cycle for creating, training and evaluating deep learning neural networks in Keras and how\\nto make predictions with a trained model. You will also discover how to use the functional API\\nthat provides more ﬂexibility when designing models. After reading this chapter you will know:\\n\\x88How to deﬁne, compile, ﬁt and evaluate a deep learning neural network in Keras.\\n\\x88How to select standard defaults for regression and classiﬁcation predictive modeling\\nproblems.\\n\\x88How to use the functional API to develop standard Multilayer Perceptron, convolutional\\nand recurrent neural networks.\\nLet’s get started.\\nNote: It is assumed that you have a basic familiarity with deep learning and Keras, this chapter'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 37}, page_content='should provide a refresher for the Keras API, and perhaps an introduction to the Keras\\nfunctional API. See the Appendix for installation instructions. Most code snippets in this\\ntutorial are just for reference and are not complete examples.\\n4.1 Keras Model Life-Cycle\\nBelow is an overview of the 5 steps in the neural network model life-cycle in Keras:\\n1. Deﬁne Network.\\n2. Compile Network.\\n3. Fit Network.\\n4. Evaluate Network.\\n5. Make Predictions.\\n21'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 38}, page_content='4.1. Keras Model Life-Cycle 22\\nFigure 4.1: 5 Step Life-Cycle for Neural Network Models in Keras.\\nLet’s take a look at each step in turn using the easy-to-use Keras Sequential API.\\n4.1.1 Step 1. Deﬁne Network\\nThe ﬁrst step is to deﬁne your neural network. Neural networks are deﬁned in Keras as a\\nsequence of layers. The container for these layers is the Sequential class. The ﬁrst step is to\\ncreate an instance of the Sequential class. Then you can create your layers and add them in\\nthe order that they should be connected. For example, we can do this in two steps:\\nmodel = Sequential()\\nmodel.add(Dense(2))\\nListing 4.1: Sequential model with one Dense layer with 2 neurons.\\nBut we can also do this in one step by creating an array of layers and passing it to the\\nconstructor of the Sequential class.\\nlayers = [Dense(2)]\\nmodel = Sequential(layers)\\nListing 4.2: Layers for a Sequential model deﬁned as an array.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 38}, page_content='The ﬁrst layer in the network must deﬁne the number of inputs to expect. The way that this\\nis speciﬁed can diﬀer depending on the network type, but for a Multilayer Perceptron model\\nthis is speciﬁed by the input dimattribute. For example, a small Multilayer Perceptron model\\nwith 2 inputs in the visible layer, 5 neurons in the hidden layer and one neuron in the output\\nlayer can be deﬁned as:\\nmodel = Sequential()\\nmodel.add(Dense(5, input_dim=2))\\nmodel.add(Dense(1))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 39}, page_content=\"4.1. Keras Model Life-Cycle 23\\nListing 4.3: Sequential model with 2 inputs.\\nThink of a Sequential model as a pipeline with your raw data fed in at the bottom and\\npredictions that come out at the top. This is a helpful conception in Keras as concerns that were\\ntraditionally associated with a layer can also be split out and added as separate layers, clearly\\nshowing their role in the transform of data from input to prediction. For example, activation\\nfunctions that transform a summed signal from each neuron in a layer can be extracted and\\nadded to the Sequential as a layer-like object called the Activation class.\\nmodel = Sequential()\\nmodel.add(Dense(5, input_dim=2))\\nmodel.add(Activation( 'relu '))\\nmodel.add(Dense(1))\\nmodel.add(Activation( 'sigmoid '))\\nListing 4.4: Sequential model with Activation functions deﬁned separately from layers.\\nThe choice of activation function is most important for the output layer as it will deﬁne the\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 39}, page_content='format that predictions will take. For example, below are some common predictive modeling\\nproblem types and the structure and standard activation function that you can use in the output\\nlayer:\\n\\x88Regression : Linear activation function, or linear , and the number of neurons matching\\nthe number of outputs.\\n\\x88Binary Classiﬁcation (2 class) : Logistic activation function, or sigmoid , and one\\nneuron the output layer.\\n\\x88Multiclass Classiﬁcation ( >2 class) : Softmax activation function, or softmax , and\\none output neuron per class value, assuming a one hot encoded output pattern.\\n4.1.2 Step 2. Compile Network\\nOnce we have deﬁned our network, we must compile it. Compilation is an eﬃciency step. It\\ntransforms the simple sequence of layers that we deﬁned into a highly eﬃcient series of matrix\\ntransforms in a format intended to be executed on your GPU or CPU, depending on how Keras\\nis conﬁgured. Think of compilation as a precompute step for your network. It is always required'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 39}, page_content=\"after deﬁning a model.\\nCompilation requires a number of parameters to be speciﬁed, speciﬁcally tailored to training\\nyour network. Speciﬁcally, the optimization algorithm to use to train the network and the loss\\nfunction used to evaluate the network that is minimized by the optimization algorithm. For\\nexample, below is a case of compiling a deﬁned model and specifying the stochastic gradient\\ndescent ( sgd) optimization algorithm and the mean squared error ( mean squared error ) loss\\nfunction, intended for a regression type problem.\\nmodel.compile(optimizer= 'sgd ', loss= 'mean_squared_error ')\\nListing 4.5: Example of compiling a deﬁned model.\\nAlternately, the optimizer can be created and conﬁgured before being provided as an argument\\nto the compilation step.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 40}, page_content=\"4.1. Keras Model Life-Cycle 24\\nalgorithm = SGD(lr=0.1, momentum=0.3)\\nmodel.compile(optimizer=algorithm, loss= 'mean_squared_error ')\\nListing 4.6: Example of deﬁning the optimization algorithm separately.\\nThe type of predictive modeling problem imposes constraints on the type of loss function\\nthat can be used. For example, below are some standard loss functions for diﬀerent predictive\\nmodel types:\\n\\x88Regression : Mean Squared Error or mean squared error .\\n\\x88Binary Classiﬁcation (2 class) : Logarithmic Loss, also called cross entropy or\\nbinary crossentropy .\\n\\x88Multiclass Classiﬁcation ( >2 class) : Multiclass Logarithmic Loss or\\ncategorical crossentropy .\\nThe most common optimization algorithm is stochastic gradient descent, but Keras also\\nsupports a suite of other state-of-the-art optimization algorithms that work well with little or\\nno conﬁguration. Perhaps the most commonly used optimization algorithms because of their\\ngenerally better performance are:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 40}, page_content=\"\\x88Stochastic Gradient Descent , orsgd, that requires the tuning of a learning rate and\\nmomentum.\\n\\x88Adam , oradam , that requires the tuning of learning rate.\\n\\x88RMSprop , orrmsprop , that requires the tuning of learning rate.\\nFinally, you can also specify metrics to collect while ﬁtting your model in addition to the\\nloss function. Generally, the most useful additional metric to collect is accuracy for classiﬁcation\\nproblems. The metrics to collect are speciﬁed by name in an array. For example:\\nmodel.compile(optimizer= 'sgd ', loss= 'mean_squared_error ', metrics=[ 'accuracy '])\\nListing 4.7: Example of deﬁning metrics when compiling the model.\\n4.1.3 Step 3. Fit Network\\nOnce the network is compiled, it can be ﬁt, which means adapt the weights on a training dataset.\\nFitting the network requires the training data to be speciﬁed, both a matrix of input patterns, X,\\nand an array of matching output patterns, y. The network is trained using the backpropagation\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 40}, page_content='algorithm and optimized according to the optimization algorithm and loss function speciﬁed\\nwhen compiling the model.\\nThe backpropagation algorithm requires that the network be trained for a speciﬁed number\\nof epochs or exposures to the training dataset. Each epoch can be partitioned into groups\\nof input-output pattern pairs called batches. This deﬁnes the number of patterns that the\\nnetwork is exposed to before the weights are updated within an epoch. It is also an eﬃciency\\noptimization, ensuring that not too many input patterns are loaded into memory at a time. A\\nminimal example of ﬁtting a network is as follows:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 41}, page_content='4.1. Keras Model Life-Cycle 25\\nhistory = model.fit(X, y, batch_size=10, epochs=100)\\nListing 4.8: Example of ﬁtting a compiled model.\\nOnce ﬁt, a history object is returned that provides a summary of the performance of the\\nmodel during training. This includes both the loss and any additional metrics speciﬁed when\\ncompiling the model, recorded each epoch. Training can take a long time, from seconds to hours\\nto days depending on the size of the network and the size of the training data.\\nBy default, a progress bar is displayed on the command line for each epoch. This may create\\ntoo much noise for you, or may cause problems for your environment, such as if you are in an\\ninteractive notebook or IDE. You can reduce the amount of information displayed to just the\\nloss each epoch by setting the verbose argument to 2. You can turn oﬀ all output by setting\\nverbose to 0. For example:\\nhistory = model.fit(X, y, batch_size=10, epochs=100, verbose=0)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 41}, page_content='Listing 4.9: Example of turning oﬀ verbose output when ﬁtting the model.\\n4.1.4 Step 4. Evaluate Network\\nOnce the network is trained, it can be evaluated. The network can be evaluated on the training\\ndata, but this will not provide a useful indication of the performance of the network as a\\npredictive model, as it has seen all of this data before. We can evaluate the performance of\\nthe network on a separate dataset, unseen during testing. This will provide an estimate of the\\nperformance of the network at making predictions for unseen data in the future.\\nThe model evaluates the loss across all of the test patterns, as well as any other metrics\\nspeciﬁed when the model was compiled, like classiﬁcation accuracy. A list of evaluation metrics\\nis returned. For example, for a model compiled with the accuracy metric, we could evaluate it\\non a new dataset as follows:\\nloss, accuracy = model.evaluate(X, y)\\nListing 4.10: Example of evaluating a ﬁt model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 41}, page_content='As with ﬁtting the network, verbose output is provided to give an idea of the progress of\\nevaluating the model. We can turn this oﬀ by setting the verbose argument to 0.\\nloss, accuracy = model.evaluate(X, y, verbose=0)\\nListing 4.11: Example of turning oﬀ verbose output when evaluating a ﬁt model.\\n4.1.5 Step 5. Make Predictions\\nOnce we are satisﬁed with the performance of our ﬁt model, we can use it to make predictions\\non new data. This is as easy as calling the predict() function on the model with an array of\\nnew input patterns. For example:\\npredictions = model.predict(X)\\nListing 4.12: Example of making a prediction with a ﬁt model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 42}, page_content='4.2. Keras Functional Models 26\\nThe predictions will be returned in the format provided by the output layer of the network.\\nIn the case of a regression problem, these predictions may be in the format of the problem\\ndirectly, provided by a linear activation function. For a binary classiﬁcation problem, the\\npredictions may be an array of probabilities for the ﬁrst class that can be converted to a 1 or 0\\nby rounding.\\nFor a multiclass classiﬁcation problem, the results may be in the form of an array of\\nprobabilities (assuming a one hot encoded output variable) that may need to be converted to a\\nsingle class output prediction using the argmax() NumPy function. Alternately, for classiﬁcation\\nproblems, we can use the predict classes() function that will automatically convert uncrisp\\npredictions to crisp integer class values.\\npredictions = model.predict_classes(X)\\nListing 4.13: Example of predicting classes with a ﬁt model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 42}, page_content='As with ﬁtting and evaluating the network, verbose output is provided to give an idea of the\\nprogress of the model making predictions. We can turn this oﬀ by setting the verbose argument\\nto 0.\\npredictions = model.predict(X, verbose=0)\\nListing 4.14: Example of disabling verbose output when making predictions.\\n4.2 Keras Functional Models\\nThe sequential API allows you to create models layer-by-layer for most problems. It is limited\\nin that it does not allow you to create models that share layers or have multiple inputs or\\noutputs. The functional API in Keras is an alternate way of creating models that oﬀers a lot\\nmore ﬂexibility, including creating more complex models.\\nIt speciﬁcally allows you to deﬁne multiple input or output models as well as models that\\nshare layers. More than that, it allows you to deﬁne ad hoc acyclic network graphs. Models are\\ndeﬁned by creating instances of layers and connecting them directly to each other in pairs, then'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 42}, page_content='deﬁning a Model that speciﬁes the layers to act as the input and output to the model. Let’s\\nlook at the three unique aspects of Keras functional API in turn:\\n4.2.1 Deﬁning Input\\nUnlike the Sequential model, you must create and deﬁne a standalone Input layer that speciﬁes\\nthe shape of input data. The input layer takes a shape argument that is a tuple that indicates the\\ndimensionality of the input data. When input data is one-dimensional, such as for a Multilayer\\nPerceptron, the shape must explicitly leave room for the shape of the mini-batch size used when\\nsplitting the data when training the network. Therefore, the shape tuple is always deﬁned with\\na hanging last dimension (2,) , for example:\\nfrom keras.layers import Input\\nvisible = Input(shape=(2,))\\nListing 4.15: Example of deﬁning input for a functional model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 43}, page_content='4.3. Standard Network Models 27\\n4.2.2 Connecting Layers\\nThe layers in the model are connected pairwise. This is done by specifying where the input\\ncomes from when deﬁning each new layer. A bracket notation is used, such that after the layer\\nis created, the layer from which the input to the current layer comes from is speciﬁed. Let’s\\nmake this clear with a short example. We can create the input layer as above, then create a\\nhidden layer as a Dense that receives input only from the input layer.\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nvisible = Input(shape=(2,))\\nhidden = Dense(2)(visible)\\nListing 4.16: Example of connecting a hidden layer to the visible layer.\\nNote it is the visible after the creation of the Dense layer that connects the input layer’s\\noutput as the input to the Dense hidden layer. It is this way of connecting layers piece by piece\\nthat gives the functional API its ﬂexibility. For example, you can see how easy it would be to'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 43}, page_content='start deﬁning ad hoc graphs of layers.\\n4.2.3 Creating the Model\\nAfter creating all of your model layers and connecting them together, you must deﬁne the model.\\nAs with the Sequential API, the model is the thing you can summarize, ﬁt, evaluate, and use to\\nmake predictions. Keras provides a Model class that you can use to create a model from your\\ncreated layers. It requires that you only specify the input and output layers. For example:\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nvisible = Input(shape=(2,))\\nhidden = Dense(2)(visible)\\nmodel = Model(inputs=visible, outputs=hidden)\\nListing 4.17: Example of creating a full model with the functional API.\\nNow that we know all of the key pieces of the Keras functional API, let’s work through\\ndeﬁning a suite of diﬀerent models and build up some practice with it. Each example is\\nexecutable and prints the structure and creates a diagram of the graph. I recommend doing'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 43}, page_content='this for your own models to make it clear what exactly you have deﬁned. My hope is that\\nthese examples provide templates for you when you want to deﬁne your own models using the\\nfunctional API in the future.\\n4.3 Standard Network Models\\nWhen getting started with the functional API, it is a good idea to see how some standard\\nneural network models are deﬁned. In this section, we will look at deﬁning a simple Multilayer\\nPerceptron, convolutional neural network, and recurrent neural network. These examples will\\nprovide a foundation for understanding the more elaborate examples later.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 44}, page_content=\"4.3. Standard Network Models 28\\n4.3.1 Multilayer Perceptron\\nIn this section, we deﬁne a Multilayer Perceptron model for binary classiﬁcation. The model\\nhas 10 inputs, 3 hidden layers with 10, 20, and 10 neurons, and an output layer with 1 output.\\nRectiﬁed linear activation functions are used in each hidden layer and a sigmoid activation\\nfunction is used in the output layer, for binary classiﬁcation.\\n# Multilayer Perceptron\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nvisible = Input(shape=(10,))\\nhidden1 = Dense(10, activation= 'relu ')(visible)\\nhidden2 = Dense(20, activation= 'relu ')(hidden1)\\nhidden3 = Dense(10, activation= 'relu ')(hidden2)\\noutput = Dense(1, activation= 'sigmoid ')(hidden3)\\nmodel = Model(inputs=visible, outputs=output)\\n# summarize layers\\nmodel.summary()\\n# plot graph\\nplot_model(model, to_file= 'multilayer_perceptron_graph.png ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 44}, page_content='Listing 4.18: Example of deﬁning an MLP with the functional API.\\nRunning the example prints the structure of the network.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 10) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 10) 110\\n_________________________________________________________________\\ndense_2 (Dense) (None, 20) 220\\n_________________________________________________________________\\ndense_3 (Dense) (None, 10) 210\\n_________________________________________________________________\\ndense_4 (Dense) (None, 1) 11\\n=================================================================\\nTotal params: 551\\nTrainable params: 551\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 4.19: Summary of MLP model deﬁned with the functional API.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 44}, page_content='A plot of the model graph is also created and saved to ﬁle.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 45}, page_content='4.3. Standard Network Models 29\\nFigure 4.2: Plot of the MLP Model Graph.\\nNote, creating plots of Keras models requires that you install pydot andpygraphviz (the\\ngraphviz library and the python wrapper). Instructions for installing these libraries vary for\\ndiﬀerent systems. If this is a challenge for you (e.g. you’re on windows), consider commenting\\nout the calls to plot model() when you see them.\\n4.3.2 Convolutional Neural Network\\nIn this section, we will deﬁne a convolutional neural network for image classiﬁcation. The model\\nreceives black and white 64 x 64 images as input, then has a sequence of two convolutional and\\npooling layers as feature extractors, followed by a fully connected layer to interpret the features\\nand an output layer with a sigmoid activation for two-class predictions.\\n# Convolutional Neural Network\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 45}, page_content=\"from keras.layers.convolutional import Conv2D\\nfrom keras.layers.pooling import MaxPooling2D\\nvisible = Input(shape=(64,64,1))\\nconv1 = Conv2D(32, kernel_size=4, activation= 'relu ')(visible)\\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\\nconv2 = Conv2D(16, kernel_size=4, activation= 'relu ')(pool1)\\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\\nhidden1 = Dense(10, activation= 'relu ')(pool2)\\noutput = Dense(1, activation= 'sigmoid ')(hidden1)\\nmodel = Model(inputs=visible, outputs=output)\\n# summarize layers\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 46}, page_content=\"4.3. Standard Network Models 30\\nmodel.summary()\\n# plot graph\\nplot_model(model, to_file= 'convolutional_neural_network.png ')\\nListing 4.20: Example of deﬁning an CNN with the functional API.\\nRunning the example summarizes the model layers.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 64, 64, 1) 0\\n_________________________________________________________________\\nconv2d_1 (Conv2D) (None, 61, 61, 32) 544\\n_________________________________________________________________\\nmax_pooling2d_1 (MaxPooling2 (None, 30, 30, 32) 0\\n_________________________________________________________________\\nconv2d_2 (Conv2D) (None, 27, 27, 16) 8208\\n_________________________________________________________________\\nmax_pooling2d_2 (MaxPooling2 (None, 13, 13, 16) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 13, 13, 10) 170\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 46}, page_content='_________________________________________________________________\\ndense_2 (Dense) (None, 13, 13, 1) 11\\n=================================================================\\nTotal params: 8,933\\nTrainable params: 8,933\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 4.21: Summary of CNN model deﬁned with the functional API.\\nA plot of the model graph is also created and saved to ﬁle.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 47}, page_content=\"4.3. Standard Network Models 31\\nFigure 4.3: Plot of the CNN Model Graph.\\n4.3.3 Recurrent Neural Network\\nIn this section, we will deﬁne a long short-term memory recurrent neural network for sequence\\nclassiﬁcation. The model expects 100 time steps of one feature as input. The model has a single\\nLSTM hidden layer to extract features from the sequence, followed by a fully connected layer to\\ninterpret the LSTM output, followed by an output layer for making binary predictions.\\n# Recurrent Neural Network\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers.recurrent import LSTM\\nvisible = Input(shape=(100,1))\\nhidden1 = LSTM(10)(visible)\\nhidden2 = Dense(10, activation= 'relu ')(hidden1)\\noutput = Dense(1, activation= 'sigmoid ')(hidden2)\\nmodel = Model(inputs=visible, outputs=output)\\n# summarize layers\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 48}, page_content=\"4.4. Further Reading 32\\nmodel.summary()\\n# plot graph\\nplot_model(model, to_file= 'recurrent_neural_network.png ')\\nListing 4.22: Example of deﬁning an RNN with the functional API.\\nRunning the example summarizes the model layers.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 100, 1) 0\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 10) 480\\n_________________________________________________________________\\ndense_1 (Dense) (None, 10) 110\\n_________________________________________________________________\\ndense_2 (Dense) (None, 1) 11\\n=================================================================\\nTotal params: 601\\nTrainable params: 601\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 4.23: Summary of RNN model deﬁned with the functional API.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 48}, page_content='A plot of the model graph is also created and saved to ﬁle.\\nFigure 4.4: Plot of the RNN Model Graph.\\n4.4 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Keras documentation for Sequential Models.\\nhttps://keras.io/models/sequential/'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 49}, page_content='4.5. Summary 33\\n\\x88Keras documentation for Functional Models.\\nhttps://keras.io/models/model/\\n\\x88Getting started with the Keras Sequential model.\\nhttps://keras.io/models/model/\\n\\x88Getting started with the Keras functional API.\\nhttps://keras.io/models/model/\\n\\x88Keras documentation for optimization algorithms.\\nhttps://keras.io/optimizers/\\n\\x88Keras documentation for loss functions.\\nhttps://keras.io/losses/\\n4.5 Summary\\nIn this tutorial, you discovered the step-by-step life-cycle for creating, training and evaluating\\ndeep learning neural networks in Keras and how to use the functional API that provides more\\nﬂexibility when deigning models. Speciﬁcally, you learned:\\n\\x88How to deﬁne, compile, ﬁt and evaluate a deep learning neural network in Keras.\\n\\x88How to select standard defaults for regression and classiﬁcation predictive modeling\\nproblems.\\n\\x88How to use the functional API to develop standard Multilayer Perceptron, convolutional\\nand recurrent neural networks.\\n4.5.1 Next'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 49}, page_content='4.5.1 Next\\nThis is the last chapter in the foundations part. In the next part, you will discover how you can\\nprepare text data ready for modeling.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 50}, page_content='Part III\\nData Preparation\\n34'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 51}, page_content='Chapter 5\\nHow to Clean Text Manually and with\\nNLTK\\nYou cannot go straight from raw text to ﬁtting a machine learning or deep learning model. You\\nmust clean your text ﬁrst, which means splitting it into words and handling punctuation and\\ncase. In fact, there is a whole suite of text preparation methods that you may need to use, and\\nthe choice of methods really depends on your natural language processing task. In this tutorial,\\nyou will discover how you can clean and prepare your text ready for modeling with machine\\nlearning. After completing this tutorial, you will know:\\n\\x88How to get started by developing your own very simple text cleaning tools.\\n\\x88How to take a step up and use the more sophisticated methods in the NLTK library.\\n\\x88Considerations when preparing text for natural language processing models.\\nLet’s get started.\\n5.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Metamorphosis by Franz Kafka\\n2. Text Cleaning is Task Speciﬁc\\n3. Manual Tokenization'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 51}, page_content='4. Tokenization and Cleaning with NLTK\\n5. Additional Text Cleaning Considerations\\n35'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 52}, page_content='5.2. Metamorphosis by Franz Kafka 36\\n5.2 Metamorphosis by Franz Kafka\\nLet’s start oﬀ by selecting a dataset. In this tutorial, we will use the text from the book\\nMetamorphosis by Franz Kafka. No speciﬁc reason, other than it’s short, I like it, and you may\\nlike it too. I expect it’s one of those classics that most students have to read in school. The full\\ntext for Metamorphosis is available for free from Project Gutenberg. You can download the\\nASCII text version of the text here:\\n\\x88Metamorphosis by Franz Kafka Plain Text UTF-8 (may need to load the page twice).\\nhttp://www.gutenberg.org/cache/epub/5200/pg5200.txt\\nDownload the ﬁle and place it in your current working directory with the ﬁle name\\nmetamorphosis.txt . The ﬁle contains header and footer information that we are not in-\\nterested in, speciﬁcally copyright and license information. Open the ﬁle and delete the header\\nand footer information and save the ﬁle as metamorphosis clean.txt . The start of the clean\\nﬁle should look like:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 52}, page_content='One morning, when Gregor Samsa woke from troubled dreams, he found himself\\ntransformed in his bed into a horrible vermin.\\nThe ﬁle should end with:\\nAnd, as if in conﬁrmation of their new dreams and good intentions, as soon as they\\nreached their destination Grete was the ﬁrst to get up and stretch out her young\\nbody.\\nPoor Gregor...\\n5.3 Text Cleaning Is Task Speciﬁc\\nAfter actually getting a hold of your text data, the ﬁrst step in cleaning up text data is to have\\na strong idea about what you’re trying to achieve, and in that context review your text to see\\nwhat exactly might help. Take a moment to look at the text. What do you notice? Here’s what\\nI see:\\n\\x88It’s plain text so there is no markup to parse (yay!).\\n\\x88The translation of the original German uses UK English (e.g. travelling ).\\n\\x88The lines are artiﬁcially wrapped with new lines at about 70 characters (meh).\\n\\x88There are no obvious typos or spelling mistakes.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 52}, page_content='\\x88There’s punctuation like commas, apostrophes, quotes, question marks, and more.\\n\\x88There’s hyphenated descriptions like armour-like .\\n\\x88There’s a lot of use of the em dash ( -) to continue sentences (maybe replace with commas?).\\n\\x88There are names (e.g. Mr. Samsa )'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 53}, page_content='5.4. Manual Tokenization 37\\n\\x88There does not appear to be numbers that require handling (e.g. 1999)\\n\\x88There are section markers (e.g. IIand III).\\nI’m sure there is a lot more going on to the trained eye. We are going to look at general\\ntext cleaning steps in this tutorial. Nevertheless, consider some possible objectives we may have\\nwhen working with this text document. For example:\\n\\x88If we were interested in developing a Kafkaesque language model, we may want to keep all\\nof the case, quotes, and other punctuation in place.\\n\\x88If we were interested in classifying documents as Kafka and Not Kafka , maybe we would\\nwant to strip case, punctuation, and even trim words back to their stem.\\nUse your task as the lens by which to choose how to ready your text data.\\n5.4 Manual Tokenization\\nText cleaning is hard, but the text we have chosen to work with is pretty clean already. We\\ncould just write some Python code to clean it up manually, and this is a good exercise for those'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 53}, page_content=\"simple problems that you encounter. Tools like regular expressions and splitting strings can get\\nyou a long way.\\n5.4.1 Load Data\\nLet’s load the text data so that we can work with it. The text is small and will load quickly\\nand easily ﬁt into memory. This will not always be the case and you may need to write code\\nto memory map the ﬁle. Tools like NLTK (covered in the next section) will make working\\nwith large ﬁles much easier. We can load the entire metamorphosis clean.txt into memory as\\nfollows:\\n# load text\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\nListing 5.1: Manually load the ﬁle.\\n5.4.2 Split by Whitespace\\nClean text often means a list of words or tokens that we can work with in our machine learning\\nmodels. This means converting the raw text into a list of words and saving it again. A very\\nsimple way to do this would be to split the document by white space, including “ ” (space), new\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 53}, page_content=\"lines, tabs and more. We can do this in Python with the split() function on the loaded string.\\n# load text\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 54}, page_content=\"5.4. Manual Tokenization 38\\nfile.close()\\n# split into words by white space\\nwords = text.split()\\nprint(words[:100])\\nListing 5.2: Manually split words by white space.\\nRunning the example splits the document into a long list of words and prints the ﬁrst 100 for\\nus to review. We can see that punctuation is preserved (e.g. wasn’t and armour-like ), which is\\nnice. We can also see that end of sentence punctuation is kept with the last word (e.g. thought .),\\nwhich is not great.\\n[ 'One ', 'morning, ', 'when ', 'Gregor ', 'Samsa ', 'woke ', 'from ', 'troubled ', 'dreams, ', 'he ',\\n'found ', 'himself ', 'transformed ', 'in ', 'his ', 'bed ', 'into ', 'a ', 'horrible ',\\n'vermin. ', 'He ', 'lay ', 'on ', 'his ', 'armour-like ', 'back, ', 'and ', 'if ', 'he ',\\n'lifted ', 'his ', 'head ', 'a ', 'little ', 'he ', 'could ', 'see ', 'his ', 'brown ', 'belly, ',\\n'slightly ', 'domed ', 'and ', 'divided ', 'by ', 'arches ', 'into ', 'stiff ', 'sections. ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 54}, page_content='\\'The \\', \\'bedding \\', \\'was \\', \\'hardly \\', \\'able \\', \\'to \\', \\'cover \\', \\'it \\', \\'and \\', \\'seemed \\',\\n\\'ready \\', \\'to \\', \\'slide \\', \\'off \\', \\'any \\', \\'moment. \\', \\'His \\', \\'many \\', \\'legs, \\', \\'pitifully \\',\\n\\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him, \\', \\'waved \\',\\n\\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked. \\', \\'\"What\\\\ \\'s \\', \\'happened \\', \\'to \\', \\'me?\" \\',\\n\\'he \\', \\'thought. \\', \\'It \\', \"wasn \\'t\", \\'a \\', \\'dream. \\', \\'His \\', \\'room, \\', \\'a \\', \\'proper \\', \\'human \\']\\nListing 5.3: Example output of splitting words by white space.\\n5.4.3 Select Words\\nAnother approach might be to use the regex model ( re) and split the document into words by\\nselecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and ‘ ’). For example:\\nimport re\\n# load text\\nfilename = \\'metamorphosis_clean.txt \\'\\nfile = open(filename, \\'rt \\')\\ntext = file.read()\\nfile.close()\\n# split based on words only\\nwords = re.split(r \\'\\\\W+ \\', text)\\nprint(words[:100])\\nListing 5.4: Manually select words with regex.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 54}, page_content=\"Again, running the example we can see that we get our list of words. This time, we can see\\nthat armour-like is now two words armour and like(ﬁne) but contractions like What’s is also\\ntwo words What and s(not great).\\n[ 'One ', 'morning ', 'when ', 'Gregor ', 'Samsa ', 'woke ', 'from ', 'troubled ', 'dreams ', 'he ',\\n'found ', 'himself ', 'transformed ', 'in ', 'his ', 'bed ', 'into ', 'a ', 'horrible ',\\n'vermin ', 'He ', 'lay ', 'on ', 'his ', 'armour ', 'like ', 'back ', 'and ', 'if ', 'he ',\\n'lifted ', 'his ', 'head ', 'a ', 'little ', 'he ', 'could ', 'see ', 'his ', 'brown ', 'belly ',\\n'slightly ', 'domed ', 'and ', 'divided ', 'by ', 'arches ', 'into ', 'stiff ', 'sections ',\\n'The ', 'bedding ', 'was ', 'hardly ', 'able ', 'to ', 'cover ', 'it ', 'and ', 'seemed ',\\n'ready ', 'to ', 'slide ', 'off ', 'any ', 'moment ', 'His ', 'many ', 'legs ', 'pitifully ',\\n'thin ', 'compared ', 'with ', 'the ', 'size ', 'of ', 'the ', 'rest ', 'of ', 'him ', 'waved ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 54}, page_content=\"'about ', 'helplessly ', 'as ', 'he ', 'looked ', 'What ', 's ', 'happened ', 'to ', 'me ', 'he ',\\n'thought ', 'It ', 'wasn ', 't ', 'a ', 'dream ', 'His ', 'room ']\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 55}, page_content='5.4. Manual Tokenization 39\\nListing 5.5: Example output of selecting words with regex.\\n5.4.4 Split by Whitespace and Remove Punctuation\\nWe may want the words, but without the punctuation like commas and quotes. We also want to\\nkeep contractions together. One way would be to split the document into words by white space\\n(as in the section Split by Whitespace ), then use string translation to replace all punctuation with\\nnothing (e.g. remove it). Python provides a constant called string.punctuation that provides a\\ngreat list of punctuation characters. For example:\\nprint(string.punctuation)\\nListing 5.6: Print the known punctuation characters.\\nResults in:\\n!\"# $%& \\'()*+,-./:;<=>?@[\\\\]^_ `{|}~\\nListing 5.7: Example output of printing the known punctuation characters.\\nWe can use regular expressions to select for the punctuation characters and use the sub()\\nfunction to replace them with nothing. For example:\\nre_punc = re.compile( \\'[%s] \\'% re.escape(string.punctuation))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 55}, page_content=\"# remove punctuation from each word\\nstripped = [re_punc.sub( '', w) for w in words]\\nListing 5.8: Example of constructing a translation table that will remove punctuation.\\nWe can put all of this together, load the text ﬁle, split it into words by white space, then\\ntranslate each word to remove the punctuation.\\nimport string\\nimport re\\n# load text\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words by white space\\nwords = text.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\nstripped = [re_punc.sub( '', w) for w in words]\\nprint(stripped[:100])\\nListing 5.9: Manually remove punctuation.\\nWe can see that this has had the desired eﬀect, mostly. Contractions like What’s have\\nbecome Whats butarmour-like has become armourlike .\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 56}, page_content=\"5.4. Manual Tokenization 40\\n[ 'One ', 'morning ', 'when ', 'Gregor ', 'Samsa ', 'woke ', 'from ', 'troubled ', 'dreams ', 'he ',\\n'found ', 'himself ', 'transformed ', 'in ', 'his ', 'bed ', 'into ', 'a ', 'horrible ',\\n'vermin ', 'He ', 'lay ', 'on ', 'his ', 'armourlike ', 'back ', 'and ', 'if ', 'he ', 'lifted ',\\n'his ', 'head ', 'a ', 'little ', 'he ', 'could ', 'see ', 'his ', 'brown ', 'belly ',\\n'slightly ', 'domed ', 'and ', 'divided ', 'by ', 'arches ', 'into ', 'stiff ', 'sections ',\\n'The ', 'bedding ', 'was ', 'hardly ', 'able ', 'to ', 'cover ', 'it ', 'and ', 'seemed ',\\n'ready ', 'to ', 'slide ', 'off ', 'any ', 'moment ', 'His ', 'many ', 'legs ', 'pitifully ',\\n'thin ', 'compared ', 'with ', 'the ', 'size ', 'of ', 'the ', 'rest ', 'of ', 'him ', 'waved ',\\n'about ', 'helplessly ', 'as ', 'he ', 'looked ', 'Whats ', 'happened ', 'to ', 'me ', 'he ',\\n'thought ', 'It ', 'wasnt ', 'a ', 'dream ', 'His ', 'room ', 'a ', 'proper ', 'human ']\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 56}, page_content=\"Listing 5.10: Example output of removing punctuation with translation tables.\\nSometimes text data may contain non-printable characters. We can use a similar approach to\\nﬁlter out all non-printable characters by selecting the inverse of the string.printable constant.\\nFor example:\\n...\\nre_print = re.compile( '[^%s] '% re.escape(string.printable))\\nresult = [re_print.sub( '', w) for w in words]\\nListing 5.11: Example of removing non-printable characters.\\n5.4.5 Normalizing Case\\nIt is common to convert all words to one case. This means that the vocabulary will shrink in\\nsize, but some distinctions are lost (e.g. Apple the company vs apple the fruit is a commonly\\nused example). We can convert all words to lowercase by calling the lower() function on each\\nword. For example:\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words by white space\\nwords = text.split()\\n# convert to lower case\\nwords = [word.lower() for word in words]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 56}, page_content='print(words[:100])\\nListing 5.12: Manually normalize case.\\nRunning the example, we can see that all words are now lowercase.\\n[ \\'one \\', \\'morning, \\', \\'when \\', \\'gregor \\', \\'samsa \\', \\'woke \\', \\'from \\', \\'troubled \\', \\'dreams, \\', \\'he \\',\\n\\'found \\', \\'himself \\', \\'transformed \\', \\'in \\', \\'his \\', \\'bed \\', \\'into \\', \\'a \\', \\'horrible \\',\\n\\'vermin. \\', \\'he \\', \\'lay \\', \\'on \\', \\'his \\', \\'armour-like \\', \\'back, \\', \\'and \\', \\'if \\', \\'he \\',\\n\\'lifted \\', \\'his \\', \\'head \\', \\'a \\', \\'little \\', \\'he \\', \\'could \\', \\'see \\', \\'his \\', \\'brown \\', \\'belly, \\',\\n\\'slightly \\', \\'domed \\', \\'and \\', \\'divided \\', \\'by \\', \\'arches \\', \\'into \\', \\'stiff \\', \\'sections. \\',\\n\\'the \\', \\'bedding \\', \\'was \\', \\'hardly \\', \\'able \\', \\'to \\', \\'cover \\', \\'it \\', \\'and \\', \\'seemed \\',\\n\\'ready \\', \\'to \\', \\'slide \\', \\'off \\', \\'any \\', \\'moment. \\', \\'his \\', \\'many \\', \\'legs, \\', \\'pitifully \\',\\n\\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him, \\', \\'waved \\',\\n\\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked. \\', \\'\"what\\\\ \\'s \\', \\'happened \\', \\'to \\', \\'me?\" \\','),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 56}, page_content='\\'he \\', \\'thought. \\', \\'it \\', \"wasn \\'t\", \\'a \\', \\'dream. \\', \\'his \\', \\'room, \\', \\'a \\', \\'proper \\', \\'human \\']\\nListing 5.13: Example output of removing punctuation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 57}, page_content='5.5. Tokenization and Cleaning with NLTK 41\\n5.4.6 Note on Cleaning Text\\nCleaning text is really hard, problem speciﬁc, and full of tradeoﬀs. Remember, simple is better.\\nSimpler text data, simpler models, smaller vocabularies. You can always make things more\\ncomplex later to see if it results in better model skill. Next, we’ll look at some of the tools in\\nthe NLTK library that oﬀer more than simple string splitting.\\n5.5 Tokenization and Cleaning with NLTK\\nThe Natural Language Toolkit, or NLTK for short, is a Python library written for working and\\nmodeling text. It provides good tools for loading and cleaning text that we can use to get our\\ndata ready for working with machine learning and deep learning algorithms.\\n5.5.1 Install NLTK\\nYou can install NLTK using your favorite package manager, such as pip. On a POSIX-compaitable\\nmachine, this would be:\\nsudo pip install -U nltk\\nListing 5.14: Command to install the NLTK library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 57}, page_content='After installation, you will need to install the data used with the library, including a great\\nset of documents that you can use later for testing other tools in NLTK. There are few ways to\\ndo this, such as from within a script:\\nimport nltk\\nnltk.download()\\nListing 5.15: NLTK script to download required text data.\\nOr from the command line:\\npython -m nltk.downloader all\\nListing 5.16: Command to download NLTK required text data.\\n5.5.2 Split into Sentences\\nA good useful ﬁrst step is to split the text into sentences. Some modeling tasks prefer input\\nto be in the form of paragraphs or sentences, such as Word2Vec. You could ﬁrst split your\\ntext into sentences, split each sentence into words, then save each sentence to ﬁle, one per line.\\nNLTK provides the sent tokenize() function to split text into sentences. The example below\\nloads the metamorphosis clean.txt ﬁle into memory, splits it into sentences, and prints the\\nﬁrst sentence.\\nfrom nltk import sent_tokenize\\n# load data'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 57}, page_content=\"# load data\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 58}, page_content=\"5.5. Tokenization and Cleaning with NLTK 42\\n# split into sentences\\nsentences = sent_tokenize(text)\\nprint(sentences[0])\\nListing 5.17: NLTK script to split text into sentences.\\nRunning the example, we can see that although the document is split into sentences, that\\neach sentence still preserves the new line from the artiﬁcial wrap of the lines in the original\\ndocument.\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.\\nListing 5.18: Example output of splitting text into sentences.\\n5.5.3 Split into Words\\nNLTK provides a function called word tokenize() for splitting strings into tokens (nominally\\nwords). It splits tokens based on white space and punctuation. For example, commas and\\nperiods are taken as separate tokens. Contractions are split apart (e.g. What’s becomes What\\nand ’s). Quotes are kept, and so on. For example:\\nfrom nltk.tokenize import word_tokenize\\n# load data\\nfilename = 'metamorphosis_clean.txt '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 58}, page_content=\"file = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\nprint(tokens[:100])\\nListing 5.19: NLTK script to split text into words.\\nRunning the code, we can see that punctuation are now tokens that we could then decide to\\nspeciﬁcally ﬁlter out.\\n[ 'One ', 'morning ', ', ', 'when ', 'Gregor ', 'Samsa ', 'woke ', 'from ', 'troubled ', 'dreams ',\\n', ', 'he ', 'found ', 'himself ', 'transformed ', 'in ', 'his ', 'bed ', 'into ', 'a ',\\n'horrible ', 'vermin ', '. ', 'He ', 'lay ', 'on ', 'his ', 'armour-like ', 'back ', ', ', 'and ',\\n'if ', 'he ', 'lifted ', 'his ', 'head ', 'a ', 'little ', 'he ', 'could ', 'see ', 'his ',\\n'brown ', 'belly ', ', ', 'slightly ', 'domed ', 'and ', 'divided ', 'by ', 'arches ', 'into ',\\n'stiff ', 'sections ', '. ', 'The ', 'bedding ', 'was ', 'hardly ', 'able ', 'to ', 'cover ',\\n'it ', 'and ', 'seemed ', 'ready ', 'to ', 'slide ', 'off ', 'any ', 'moment ', '. ', 'His ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 58}, page_content='\\'many \\', \\'legs \\', \\', \\', \\'pitifully \\', \\'thin \\', \\'compared \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\',\\n\\'the \\', \\'rest \\', \\'of \\', \\'him \\', \\', \\', \\'waved \\', \\'about \\', \\'helplessly \\', \\'as \\', \\'he \\', \\'looked \\',\\n\\'. \\', \\'``\\' , \\'What \\', \" \\'s\", \\'happened \\', \\'to \\']\\nListing 5.20: Example output of splitting text into words.\\n5.5.4 Filter Out Punctuation\\nWe can ﬁlter out all tokens that we are not interested in, such as all standalone punctuation. This\\ncan be done by iterating over all tokens and only keeping those tokens that are all alphabetic.\\nPython has the function isalpha() that can be used. For example:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 59}, page_content=\"5.5. Tokenization and Cleaning with NLTK 43\\nfrom nltk.tokenize import word_tokenize\\n# load data\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\n# remove all tokens that are not alphabetic\\nwords = [word for word in tokens if word.isalpha()]\\nprint(words[:100])\\nListing 5.21: NLTK script to remove punctuation.\\nRunning the example, you can see that not only punctuation tokens, but examples like\\narmour-like and ’swere also ﬁltered out.\\n[ 'One ', 'morning ', 'when ', 'Gregor ', 'Samsa ', 'woke ', 'from ', 'troubled ', 'dreams ', 'he ',\\n'found ', 'himself ', 'transformed ', 'in ', 'his ', 'bed ', 'into ', 'a ', 'horrible ',\\n'vermin ', 'He ', 'lay ', 'on ', 'his ', 'back ', 'and ', 'if ', 'he ', 'lifted ', 'his ', 'head ',\\n'a ', 'little ', 'he ', 'could ', 'see ', 'his ', 'brown ', 'belly ', 'slightly ', 'domed ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 59}, page_content=\"'and ', 'divided ', 'by ', 'arches ', 'into ', 'stiff ', 'sections ', 'The ', 'bedding ', 'was ',\\n'hardly ', 'able ', 'to ', 'cover ', 'it ', 'and ', 'seemed ', 'ready ', 'to ', 'slide ', 'off ',\\n'any ', 'moment ', 'His ', 'many ', 'legs ', 'pitifully ', 'thin ', 'compared ', 'with ', 'the ',\\n'size ', 'of ', 'the ', 'rest ', 'of ', 'him ', 'waved ', 'about ', 'helplessly ', 'as ', 'he ',\\n'looked ', 'What ', 'happened ', 'to ', 'me ', 'he ', 'thought ', 'It ', 'was ', 'a ', 'dream ',\\n'His ', 'room ', 'a ', 'proper ', 'human ', 'room ']\\nListing 5.22: Example output of removing punctuation.\\n5.5.5 Filter out Stop Words (and Pipeline)\\nStop words are those words that do not contribute to the deeper meaning of the phrase. They\\nare the most common words such as: the,a, and is. For some applications like documentation\\nclassiﬁcation, it may make sense to remove stop words. NLTK provides a list of commonly\\nagreed upon stop words for a variety of languages, such as English. They can be loaded as\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 59}, page_content=\"follows:\\nfrom nltk.corpus import stopwords\\nstop_words = stopwords.words( 'english ')\\nprint(stop_words)\\nListing 5.23: NLTK script print stop words.\\nYou can see the full list as follows:\\n[ 'i ', 'me ', 'my ', 'myself ', 'we ', 'our ', 'ours ', 'ourselves ', 'you ', 'your ', 'yours ',\\n'yourself ', 'yourselves ', 'he ', 'him ', 'his ', 'himself ', 'she ', 'her ', 'hers ',\\n'herself ', 'it ', 'its ', 'itself ', 'they ', 'them ', 'their ', 'theirs ', 'themselves ',\\n'what ', 'which ', 'who ', 'whom ', 'this ', 'that ', 'these ', 'those ', 'am ', 'is ', 'are ',\\n'was ', 'were ', 'be ', 'been ', 'being ', 'have ', 'has ', 'had ', 'having ', 'do ', 'does ',\\n'did ', 'doing ', 'a ', 'an ', 'the ', 'and ', 'but ', 'if ', 'or ', 'because ', 'as ', 'until ',\\n'while ', 'of ', 'at ', 'by ', 'for ', 'with ', 'about ', 'against ', 'between ', 'into ',\\n'through ', 'during ', 'before ', 'after ', 'above ', 'below ', 'to ', 'from ', 'up ', 'down ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 59}, page_content=\"'in ', 'out ', 'on ', 'off ', 'over ', 'under ', 'again ', 'further ', 'then ', 'once ', 'here ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 60}, page_content=\"5.5. Tokenization and Cleaning with NLTK 44\\n'there ', 'when ', 'where ', 'why ', 'how ', 'all ', 'any ', 'both ', 'each ', 'few ', 'more ',\\n'most ', 'other ', 'some ', 'such ', 'no ', 'nor ', 'not ', 'only ', 'own ', 'same ', 'so ',\\n'than ', 'too ', 'very ', 's ', 't ', 'can ', 'will ', 'just ', 'don ', 'should ', 'now ', 'd ',\\n'll ', 'm ', 'o ', 're ', 've ', 'y ', 'ain ', 'aren ', 'couldn ', 'didn ', 'doesn ', 'hadn ',\\n'hasn ', 'haven ', 'isn ', 'ma ', 'mightn ', 'mustn ', 'needn ', 'shan ', 'shouldn ', 'wasn ',\\n'weren ', 'won ', 'wouldn ']\\nListing 5.24: Example output of printing stop words.\\nYou can see that they are all lower case and have punctuation removed. You could compare\\nyour tokens to the stop words and ﬁlter them out, but you must ensure that your text is prepared\\nthe same way. Let’s demonstrate this with a small pipeline of text preparation including:\\n\\x88Load the raw text.\\n\\x88Split into tokens.\\n\\x88Convert to lowercase.\\n\\x88Remove punctuation from each token.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 60}, page_content=\"\\x88Filter out remaining tokens that are not alphabetic.\\n\\x88Filter out tokens that are stop words.\\nimport string\\nimport re\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n# load data\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\n# convert to lower case\\ntokens = [w.lower() for w in tokens]\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\nstripped = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\nwords = [word for word in stripped if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\nwords = [w for w in words if not w in stop_words]\\nprint(words[:100])\\nListing 5.25: NLTK script ﬁlter out stop words.\\nRunning this example, we can see that in addition to all of the other transforms, stop words\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 60}, page_content='likeaand tohave been removed. I note that we are still left with tokens like nt. The rabbit\\nhole is deep; there’s always more we can do.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 61}, page_content=\"5.5. Tokenization and Cleaning with NLTK 45\\n[ 'one ', 'morning ', 'gregor ', 'samsa ', 'woke ', 'troubled ', 'dreams ', 'found ', 'transformed ',\\n'bed ', 'horrible ', 'vermin ', 'lay ', 'armourlike ', 'back ', 'lifted ', 'head ', 'little ',\\n'could ', 'see ', 'brown ', 'belly ', 'slightly ', 'domed ', 'divided ', 'arches ', 'stiff ',\\n'sections ', 'bedding ', 'hardly ', 'able ', 'cover ', 'seemed ', 'ready ', 'slide ', 'moment ',\\n'many ', 'legs ', 'pitifully ', 'thin ', 'compared ', 'size ', 'rest ', 'waved ', 'helplessly ',\\n'looked ', 'happened ', 'thought ', 'nt ', 'dream ', 'room ', 'proper ', 'human ', 'room ',\\n'although ', 'little ', 'small ', 'lay ', 'peacefully ', 'four ', 'familiar ', 'walls ',\\n'collection ', 'textile ', 'samples ', 'lay ', 'spread ', 'table ', 'samsa ', 'travelling ',\\n'salesman ', 'hung ', 'picture ', 'recently ', 'cut ', 'illustrated ', 'magazine ', 'housed ',\\n'nice ', 'gilded ', 'frame ', 'showed ', 'lady ', 'fitted ', 'fur ', 'hat ', 'fur ', 'boa ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 61}, page_content=\"'sat ', 'upright ', 'raising ', 'heavy ', 'fur ', 'muff ', 'covered ', 'whole ', 'lower ',\\n'arm ', 'towards ', 'viewer ']\\nListing 5.26: Example output of ﬁltering out stop words.\\n5.5.6 Stem Words\\nStemming refers to the process of reducing each word to its root or base. For example ﬁshing ,\\nﬁshed ,ﬁsher all reduce to the stem ﬁsh. Some applications, like document classiﬁcation, may\\nbeneﬁt from stemming in order to both reduce the vocabulary and to focus on the sense or\\nsentiment of a document rather than deeper meaning. There are many stemming algorithms,\\nalthough a popular and long-standing method is the Porter Stemming algorithm. This method\\nis available in NLTK via the PorterStemmer class. For example:\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem.porter import PorterStemmer\\n# load data\\nfilename = 'metamorphosis_clean.txt '\\nfile = open(filename, 'rt ')\\ntext = file.read()\\nfile.close()\\n# split into words\\ntokens = word_tokenize(text)\\n# stemming of words\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 61}, page_content=\"# stemming of words\\nporter = PorterStemmer()\\nstemmed = [porter.stem(word) for word in tokens]\\nprint(stemmed[:100])\\nListing 5.27: NLTK script stem words.\\nRunning the example, you can see that words have been reduced to their stems, such as\\ntrouble has become troubl . You can also see that the stemming implementation has also reduced\\nthe tokens to lowercase, likely for internal look-ups in word tables.\\n[ 'one ', 'morn ', ', ', 'when ', 'gregor ', 'samsa ', 'woke ', 'from ', 'troubl ', 'dream ', ', ',\\n'he ', 'found ', 'himself ', 'transform ', 'in ', 'hi ', 'bed ', 'into ', 'a ', 'horribl ',\\n'vermin ', '. ', 'He ', 'lay ', 'on ', 'hi ', 'armour-lik ', 'back ', ', ', 'and ', 'if ', 'he ',\\n'lift ', 'hi ', 'head ', 'a ', 'littl ', 'he ', 'could ', 'see ', 'hi ', 'brown ', 'belli ', ', ',\\n'slightli ', 'dome ', 'and ', 'divid ', 'by ', 'arch ', 'into ', 'stiff ', 'section ', '. ',\\n'the ', 'bed ', 'wa ', 'hardli ', 'abl ', 'to ', 'cover ', 'it ', 'and ', 'seem ', 'readi ', 'to ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 61}, page_content='\\'slide \\', \\'off \\', \\'ani \\', \\'moment \\', \\'. \\', \\'hi \\', \\'mani \\', \\'leg \\', \\', \\', \\'piti \\', \\'thin \\',\\n\\'compar \\', \\'with \\', \\'the \\', \\'size \\', \\'of \\', \\'the \\', \\'rest \\', \\'of \\', \\'him \\', \\', \\', \\'wave \\',\\n\\'about \\', \\'helplessli \\', \\'as \\', \\'he \\', \\'look \\', \\'. \\', \\'``\\' , \\'what \\', \" \\'s\", \\'happen \\', \\'to \\''),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 62}, page_content='5.6. Additional Text Cleaning Considerations 46\\nListing 5.28: Example output of stemming words.\\nThere is a nice suite of stemming and lemmatization algorithms to choose from in NLTK, if\\nreducing words to their root is something you need for your project.\\n5.6 Additional Text Cleaning Considerations\\nWe are only getting started. Because the source text for this tutorial was reasonably clean to\\nbegin with, we skipped many concerns of text cleaning that you may need to deal with in your\\nown project. Here is a shortlist of additional considerations when cleaning text:\\n\\x88Handling large documents and large collections of text documents that do not ﬁt into\\nmemory.\\n\\x88Extracting text from markup like HTML, PDF, or other structured document formats.\\n\\x88Transliteration of characters from other languages into English.\\n\\x88Decoding Unicode characters into a normalized form, such as UTF8.\\n\\x88Handling of domain speciﬁc words, phrases, and acronyms.\\n\\x88Handling or removing numbers, such as dates and amounts.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 62}, page_content='\\x88Locating and correcting common typos and misspellings.\\n\\x88And much more...\\nThe list could go on. Hopefully, you can see that getting truly clean text is impossible, that\\nwe are really doing the best we can based on the time, resources, and knowledge we have. The\\nidea of clean is really deﬁned by the speciﬁc task or concern of your project.\\nA pro tip is to continually review your tokens after every transform. I have tried to show\\nthat in this tutorial and I hope you take that to heart. Ideally, you would save a new ﬁle after\\neach transform so that you can spend time with all of the data in the new form. Things always\\njump out at you when to take the time to review your data.\\n5.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Metamorphosis by Franz Kafka on Project Gutenberg.\\nhttp://www.gutenberg.org/ebooks/5200\\n\\x88Installing NLTK.\\nhttp://www.nltk.org/install.html\\n\\x88Installing NLTK Data.\\nhttp://www.nltk.org/data.html'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 63}, page_content='5.8. Summary 47\\n\\x88Python isalpha() function.\\nhttps://docs.python.org/3/library/stdtypes.html#str.isalpha\\n\\x88Stop Words on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Stop_words\\n\\x88Stemming on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Stemming\\n\\x88nltk.tokenize package API.\\nhttp://www.nltk.org/api/nltk.tokenize.html\\n\\x88Porer Stemming algorithm.\\nhttps://tartarus.org/martin/PorterStemmer/\\n\\x88nltk.stem package API.\\nhttp://www.nltk.org/api/nltk.stem.html\\n\\x88Processing Raw Text ,Natural Language Processing with Python .\\nhttp://www.nltk.org/book/ch03.html\\n5.8 Summary\\nIn this tutorial, you discovered how to clean text or machine learning in Python.\\nSpeciﬁcally, you learned:\\n\\x88How to get started by developing your own very simple text cleaning tools.\\n\\x88How to take a step up and use the more sophisticated methods in the NLTK library.\\n\\x88Considerations when preparing text for natural language processing models.\\n5.8.1 Next\\nIn the next chapter, you will discover how you can encode text data using the scikit-learn'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 63}, page_content='Python library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 64}, page_content='Chapter 6\\nHow to Prepare Text Data with\\nscikit-learn\\nText data requires special preparation before you can start using it for predictive modeling. The\\ntext must be parsed to remove words, called tokenization. Then the words need to be encoded\\nas integers or ﬂoating point values for use as input to a machine learning algorithm, called\\nfeature extraction (or vectorization). The scikit-learn library oﬀers easy-to-use tools to perform\\nboth tokenization and feature extraction of your text data. In this tutorial, you will discover\\nexactly how you can prepare your text data for predictive modeling in Python with scikit-learn.\\nAfter completing this tutorial, you will know:\\n\\x88How to convert text to word count vectors with CountVectorizer .\\n\\x88How to convert text to word frequency vectors with TfidfVectorizer .\\n\\x88How to convert text to unique integers with HashingVectorizer .\\nLet’s get started.\\n6.1 The Bag-of-Words Model'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 64}, page_content='We cannot work with text directly when using machine learning algorithms. Instead, we need\\nto convert the text to numbers. We may want to perform classiﬁcation of documents, so each\\ndocument is an input and a class label is the output for our predictive algorithm. Algorithms\\ntake vectors of numbers as input, therefore we need to convert documents to ﬁxed-length vectors\\nof numbers.\\nA simple and eﬀective model for thinking about text documents in machine learning is called\\nthe Bag-of-Words Model, or BoW. Note, that we cover the BoW model in great detail in the\\nnext part, starting with Chapter 8. The model is simple in that it throws away all of the order\\ninformation in the words and focuses on the occurrence of words in a document. This can be\\ndone by assigning each word a unique number. Then any document we see can be encoded\\nas a ﬁxed-length vector with the length of the vocabulary of known words. The value in each'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 64}, page_content='position in the vector could be ﬁlled with a count or frequency of each word in the encoded\\ndocument.\\n48'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 65}, page_content='6.2. Word Counts with CountVectorizer 49\\nThis is the bag-of-words model, where we are only concerned with encoding schemes that\\nrepresent what words are present or the degree to which they are present in encoded documents\\nwithout any information about order. There are many ways to extend this simple method, both\\nby better clarifying what a word is and in deﬁning what to encode about each word in the\\nvector. The scikit-learn library provides 3 diﬀerent schemes that we can use, and we will brieﬂy\\nlook at each.\\n6.2 Word Counts with CountVectorizer\\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents\\nand build a vocabulary of known words, but also to encode new documents using that vocabulary.\\nYou can use it as follows:\\n\\x88Create an instance of the CountVectorizer class.\\n\\x88Call the fit() function in order to learn a vocabulary from one or more documents.\\n\\x88Call the transform() function on one or more documents as needed to encode each as a\\nvector.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 65}, page_content='vector.\\nAn encoded vector is returned with a length of the entire vocabulary and an integer count\\nfor the number of times each word appeared in the document. Because these vectors will\\ncontain a lot of zeros, we call them sparse. Python provides an eﬃcient way of handling sparse\\nvectors in the scipy.sparse package. The vectors returned from a call to transform() will\\nbe sparse vectors, and you can transform them back to NumPy arrays to look and better\\nunderstand what is going on by calling the toarray() function. Below is an example of using\\ntheCountVectorizer to tokenize, build a vocabulary, and then encode a document.\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n# list of text documents\\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\\n# create the transform\\nvectorizer = CountVectorizer()\\n# tokenize and build vocab\\nvectorizer.fit(text)\\n# summarize\\nprint(vectorizer.vocabulary_)\\n# encode document\\nvector = vectorizer.transform(text)\\n# summarize encoded vector'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 65}, page_content='print(vector.shape)\\nprint(type(vector))\\nprint(vector.toarray())\\nListing 6.1: Example of training a CountVectorizer .\\nAbove, you can see that we access the vocabulary to see what exactly was tokenized by\\ncalling:\\nprint(vectorizer.vocabulary_)\\nListing 6.2: Print the learned vocabulary.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 66}, page_content=\"6.3. Word Frequencies with TfidfVectorizer 50\\nWe can see that all words were made lowercase by default and that the punctuation was\\nignored. These and other aspects of tokenizing can be conﬁgured and I encourage you to review\\nall of the options in the API documentation. Running the example ﬁrst prints the vocabulary,\\nthen the shape of the encoded document. We can see that there are 8 words in the vocab, and\\ntherefore encoded vectors have a length of 8. We can then see that the encoded vector is a\\nsparse matrix. Finally, we can see an array version of the encoded vector showing a count of 1\\noccurrence for each word except the (index and id 7) that has an occurrence of 2.\\n{ 'dog ': 1, 'fox ': 2, 'over ': 5, 'brown ': 0, 'quick ': 6, 'the ': 7, 'lazy ': 4, 'jumped ': 3}\\n(1, 8)\\n<class 'scipy.sparse.csr.csr_matrix '>\\n[[1 1 1 1 1 1 1 2]]\\nListing 6.3: Example output of training a CountVectorizer .\\nImportantly, the same vectorizer can be used on documents that contain words not included\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 66}, page_content='in the vocabulary. These words are ignored and no count is given in the resulting vector. For\\nexample, below is an example of using the vectorizer above to encode a document with one\\nword in the vocab and one word that is not.\\n# encode another document\\ntext2 = [\"the puppy\"]\\nvector = vectorizer.transform(text2)\\nprint(vector.toarray())\\nListing 6.4: Example of encoding another document with the ﬁt CountVectorizer .\\nRunning this example prints the array version of the encoded sparse vector showing one\\noccurrence of the one word in the vocab and the other word not in the vocab completely ignored.\\n[[0 0 0 0 0 0 0 1]]\\nListing 6.5: Example output of encoding another document.\\nThe encoded vectors can then be used directly with a machine learning algorithm.\\n6.3 Word Frequencies with TfidfVectorizer\\nWord counts are a good starting point, but are very basic. One issue with simple counts is that\\nsome words like thewill appear many times and their large counts will not be very meaningful'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 66}, page_content='in the encoded vectors. An alternative is to calculate word frequencies, and by far the most\\npopular method is called TF-IDF. This is an acronym that stands for Term Frequency - Inverse\\nDocument Frequency which are the components of the resulting scores assigned to each word.\\n\\x88Term Frequency : This summarizes how often a given word appears within a document.\\n\\x88Inverse Document Frequency : This downscales words that appear a lot across docu-\\nments.\\nWithout going into the math, TF-IDF are word frequency scores that try to highlight\\nwords that are more interesting, e.g. frequent in a document but not across documents.\\nThe TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 67}, page_content='6.4. Hashing with HashingVectorizer 51\\nfrequency weightings, and allow you to encode new documents. Alternately, if you already have a\\nlearned CountVectorizer , you can use it with a TfidfTransformer to just calculate the inverse\\ndocument frequencies and start encoding documents. The same create, ﬁt, and transform process\\nis used as with the CountVectorizer . Below is an example of using the TfidfVectorizer to\\nlearn vocabulary and inverse document frequencies across 3 small documents and then encode\\none of those documents.\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n# list of text documents\\ntext = [\"The quick brown fox jumped over the lazy dog.\",\\n\"The dog.\",\\n\"The fox\"]\\n# create the transform\\nvectorizer = TfidfVectorizer()\\n# tokenize and build vocab\\nvectorizer.fit(text)\\n# summarize\\nprint(vectorizer.vocabulary_)\\nprint(vectorizer.idf_)\\n# encode document\\nvector = vectorizer.transform([text[0]])\\n# summarize encoded vector\\nprint(vector.shape)\\nprint(vector.toarray())'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 67}, page_content=\"Listing 6.6: Example of training a TfidfVectorizer .\\nA vocabulary of 8 words is learned from the documents and each word is assigned a unique\\ninteger index in the output vector. The inverse document frequencies are calculated for each\\nword in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word:\\ntheat index 7. Finally, the ﬁrst document is encoded as an 8-element sparse array and we can\\nreview the ﬁnal scorings of each word with diﬀerent values for the,fox, and dogfrom the other\\nwords in the vocabulary.\\n{ 'fox ': 2, 'lazy ': 4, 'dog ': 1, 'quick ': 6, 'the ': 7, 'over ': 5, 'brown ': 0, 'jumped ': 3}\\n[ 1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\\n1.69314718 1. ]\\n(1, 8)\\n[[ 0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\\n0.36388646 0.42983441]]\\nListing 6.7: Example output of training a TfidfVectorizer .\\nThe scores are normalized to values between 0 and 1 and the encoded document vectors can\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 67}, page_content='then be used directly with most machine learning algorithms.\\n6.4 Hashing with HashingVectorizer\\nCounts and frequencies can be very useful, but one limitation of these methods is that the\\nvocabulary can become very large. This, in turn, will require large vectors for encoding\\ndocuments and impose large requirements on memory and slow down algorithms. A clever work\\naround is to use a one way hash of words to convert them to integers. The clever part is that\\nno vocabulary is required and you can choose an arbitrary-long ﬁxed length vector. A downside'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 68}, page_content='6.5. Further Reading 52\\nis that the hash is a one-way function so there is no way to convert the encoding back to a word\\n(which may not matter for many supervised learning tasks).\\nThe HashingVectorizer class implements this approach that can be used to consistently\\nhash words, then tokenize and encode documents as needed. The example below demonstrates\\ntheHashingVectorizer for encoding a single document. An arbitrary ﬁxed-length vector size\\nof 20 was chosen. This corresponds to the range of the hash function, where small values (like\\n20) may result in hash collisions. Remembering back to Computer Science classes, I believe\\nthere are heuristics that you can use to pick the hash length and probability of collision based\\non estimated vocabulary size (e.g. a load factor of 75%). See any good textbook on the topic.\\nNote that this vectorizer does not require a call to ﬁt on the training data documents. Instead,\\nafter instantiation, it can be used directly to start encoding documents.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 68}, page_content='from sklearn.feature_extraction.text import HashingVectorizer\\n# list of text documents\\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\\n# create the transform\\nvectorizer = HashingVectorizer(n_features=20)\\n# encode document\\nvector = vectorizer.transform(text)\\n# summarize encoded vector\\nprint(vector.shape)\\nprint(vector.toarray())\\nListing 6.8: Example of training a HashingVectorizer .\\nRunning the example encodes the sample document as a 20-element sparse array. The values\\nof the encoded document correspond to normalized word counts by default in the range of -1 to\\n1, but could be made simple integer counts by changing the default conﬁguration.\\n(1, 20)\\n[[ 0. 0. 0. 0. 0. 0.33333333\\n0. -0.33333333 0.33333333 0. 0. 0.33333333\\n0. 0. 0. -0.33333333 0. 0.\\n-0.66666667 0. ]]\\nListing 6.9: Example output of training a HashingVectorizer .\\n6.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n6.5.1 Natural Language Processing'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 68}, page_content='\\x88Bag-of-words model on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Bag-of-words_model\\n\\x88Tokenization on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\\n\\x88TF-IDF on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Tf%E2%80%93idf'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 69}, page_content='6.6. Summary 53\\n6.5.2 sciki-learn\\n\\x88Section 4.2. Feature extraction, scikit-learn User Guide.\\nhttp://scikit-learn.org/stable/modules/feature_extraction.html\\n\\x88sckit-learn Feature Extraction API.\\nhttp://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_\\nextraction\\n\\x88Working With Text Data, scikit-learn Tutorial.\\nhttp://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.\\nhtml\\n6.5.3 Class APIs\\n\\x88CountVectorizer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.CountVectorizer.html\\n\\x88TfidfVectorizer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.TfidfVectorizer.html\\n\\x88TfidfTransformer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.TfidfTransformer.html\\n\\x88HashingVectorizer scikit-learn API.\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.\\ntext.HashingVectorizer.html\\n6.6 Summary'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 69}, page_content='6.6 Summary\\nIn this tutorial, you discovered how to prepare text documents for machine learning with\\nscikit-learn for bag-of-words models. Speciﬁcally, you learned:\\n\\x88How to convert text to word count vectors with CountVectorizer .\\n\\x88How to convert text to word frequency vectors with TfidfVectorizer .\\n\\x88How to convert text to unique integers with HashingVectorizer .\\nWe have only scratched the surface in these examples and I want to highlight that there are\\nmany conﬁguration details for these classes to inﬂuence the tokenizing of documents that are\\nworth exploring.\\n6.6.1 Next\\nIn the next chapter, you will discover how you can prepare text data using the Keras deep\\nlearning library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 70}, page_content='Chapter 7\\nHow to Prepare Text Data With Keras\\nYou cannot feed raw text directly into deep learning models. Text data must be encoded as\\nnumbers to be used as input or output for machine learning and deep learning models, such\\nas word embeddings. The Keras deep learning library provides some basic tools to help you\\nprepare your text data. In this tutorial, you will discover how you can use Keras to prepare\\nyour text data. After completing this tutorial, you will know:\\n\\x88About the convenience methods that you can use to quickly prepare text data.\\n\\x88TheTokenizer API that can be ﬁt on training data and used to encode training, validation,\\nand test documents.\\n\\x88The range of 4 diﬀerent document encoding schemes oﬀered by the Tokenizer API.\\nLet’s get started.\\n7.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Split words with text toword sequence .\\n2. Encoding with onehot.\\n3. Hash Encoding with hashing trick .\\n4.Tokenizer API\\n7.2 Split Words with text toword sequence'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 70}, page_content='A good ﬁrst step when working with text is to split it into words. Words are called to-\\nkens and the process of splitting text into tokens is called tokenization. Keras provides the\\ntext toword sequence() function that you can use to split text into a list of words. By\\ndefault, this function automatically does 3 things:\\n\\x88Splits words by space.\\n54'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 71}, page_content=\"7.3. Encoding with onehot 55\\n\\x88Filters out punctuation.\\n\\x88Converts text to lowercase ( lower=True ).\\nYou can change any of these defaults by passing arguments to the function. Below is an\\nexample of using the text toword sequence() function to split a document (in this case a\\nsimple string) into a list of words.\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\\n# tokenize the document\\nresult = text_to_word_sequence(text)\\nprint(result)\\nListing 7.1: Example splitting words with the Tokenizer .\\nRunning the example creates an array containing all of the words in the document. The list\\nof words is printed for review.\\n[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']\\nListing 7.2: Example output for splitting words with the Tokenizer .\\nThis is a good ﬁrst step, but further pre-processing is required before you can work with the\\ntext.\\n7.3 Encoding with onehot\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 71}, page_content='It is popular to represent a document as a sequence of integer values, where each word in the\\ndocument is represented as a unique integer. Keras provides the onehot() function that you\\ncan use to tokenize and integer encode a text document in one step. The name suggests that it\\nwill create a one hot encoding of the document, which is not the case. Instead, the function\\nis a wrapper for the hashing trick() function described in the next section. The function\\nreturns an integer encoded version of the document. The use of a hash function means that\\nthere may be collisions and not all words will be assigned unique integer values. As with the\\ntext toword sequence() function in the previous section, the onehot() function will make\\nthe text lower case, ﬁlter out punctuation, and split words based on white space.\\nIn addition to the text, the vocabulary size (total words) must be speciﬁed. This could be the'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 71}, page_content=\"total number of words in the document or more if you intend to encode additional documents\\nthat contains additional words. The size of the vocabulary deﬁnes the hashing space from which\\nwords are hashed. By default, the hash function is used, although as we will see in the next\\nsection, alternate hash functions can be speciﬁed when calling the hashing trick() function\\ndirectly.\\nWe can use the text toword sequence() function from the previous section to split the\\ndocument into words and then use a set to represent only the unique words in the document.\\nThe size of this set can be used to estimate the size of the vocabulary for one document. For\\nexample:\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 72}, page_content=\"7.4. Hash Encoding with hashing trick 56\\n# estimate the size of the vocabulary\\nwords = set(text_to_word_sequence(text))\\nvocab_size = len(words)\\nprint(vocab_size)\\nListing 7.3: Example of preparing a vocabulary.\\nWe can put this together with the onehot() function and encode the words in the document.\\nThe complete example is listed below. The vocabulary size is increased by one-third to minimize\\ncollisions when hashing words.\\nfrom keras.preprocessing.text import one_hot\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\\n# estimate the size of the vocabulary\\nwords = set(text_to_word_sequence(text))\\nvocab_size = len(words)\\nprint(vocab_size)\\n# integer encode the document\\nresult = one_hot(text, round(vocab_size*1.3))\\nprint(result)\\nListing 7.4: Example of one hot encoding.\\nRunning the example ﬁrst prints the size of the vocabulary as 8. The encoded document is\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 72}, page_content='then printed as an array of integer encoded words.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n8\\n[5, 9, 8, 7, 9, 1, 5, 3, 8]\\nListing 7.5: Example output for one hot encoding with the Tokenizer .\\n7.4 Hash Encoding with hashing trick\\nA limitation of integer and count base encodings is that they must maintain a vocabulary of\\nwords and their mapping to integers. An alternative to this approach is to use a one-way hash\\nfunction to convert words to integers. This avoids the need to keep track of a vocabulary, which\\nis faster and requires less memory.\\nKeras provides the hashing trick() function that tokenizes and then integer encodes the\\ndocument, just like the onehot() function. It provides more ﬂexibility, allowing you to specify\\nthe hash function as either hash (the default) or other hash functions such as the built in md5'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 72}, page_content=\"function or your own function. Below is an example of integer encoding a document using the\\nmd5 hash function.\\nfrom keras.preprocessing.text import hashing_trick\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\ntext = 'The quick brown fox jumped over the lazy dog. '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 73}, page_content=\"7.5.Tokenizer API 57\\n# estimate the size of the vocabulary\\nwords = set(text_to_word_sequence(text))\\nvocab_size = len(words)\\nprint(vocab_size)\\n# integer encode the document\\nresult = hashing_trick(text, round(vocab_size*1.3), hash_function= 'md5 ')\\nprint(result)\\nListing 7.6: Example of hash encoding.\\nRunning the example prints the size of the vocabulary and the integer encoded document.\\nWe can see that the use of a diﬀerent hash function results in consistent, but diﬀerent integers\\nfor words as the onehot() function in the previous section.\\n8\\n[6, 4, 1, 2, 7, 5, 6, 2, 6]\\nListing 7.7: Example output for hash encoding with the Tokenizer .\\n7.5 Tokenizer API\\nSo far we have looked at one-oﬀ convenience methods for preparing text with Keras. Keras\\nprovides a more sophisticated API for preparing text that can be ﬁt and reused to prepare\\nmultiple text documents. This may be the preferred approach for large projects. Keras provides\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 73}, page_content=\"theTokenizer class for preparing text documents for deep learning. The Tokenizer must be\\nconstructed and then ﬁt on either raw text documents or integer encoded text documents. For\\nexample:\\nfrom keras.preprocessing.text import Tokenizer\\n# define 5 documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ']\\n# create the tokenizer\\nt = Tokenizer()\\n# fit the tokenizer on the documents\\nt.fit_on_texts(docs)\\nListing 7.8: Example of ﬁtting a Tokenizer .\\nOnce ﬁt, the Tokenizer provides 4 attributes that you can use to query what has been\\nlearned about your documents:\\n\\x88word counts : A dictionary of words and their counts.\\n\\x88word docs : An integer count of the total number of documents that were used to ﬁt the\\nTokenizer .\\n\\x88word index : A dictionary of words and their uniquely assigned integers.\\n\\x88document count : A dictionary of words and how many documents each appeared in.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 74}, page_content='7.5.Tokenizer API 58\\nFor example:\\n# summarize what was learned\\nprint(t.word_counts)\\nprint(t.document_count)\\nprint(t.word_index)\\nprint(t.word_docs)\\nListing 7.9: Summarize the output of the ﬁt Tokenizer .\\nOnce the Tokenizer has been ﬁt on training data, it can be used to encode documents in\\nthe train or test datasets. The texts tomatrix() function on the Tokenizer can be used to\\ncreate one vector per document provided per input. The length of the vectors is the total size\\nof the vocabulary. This function provides a suite of standard bag-of-words model text encoding\\nschemes that can be provided via a mode argument to the function. The modes available\\ninclude:\\n\\x88binary : Whether or not each word is present in the document. This is the default.\\n\\x88count : The count of each word in the document.\\n\\x88tfidf : The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word\\nin the document.\\n\\x88freq : The frequency of each word as a ratio of words within each document.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 74}, page_content=\"We can put all of this together with a worked example.\\nfrom keras.preprocessing.text import Tokenizer\\n# define 5 documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ']\\n# create the tokenizer\\nt = Tokenizer()\\n# fit the tokenizer on the documents\\nt.fit_on_texts(docs)\\n# summarize what was learned\\nprint(t.word_counts)\\nprint(t.document_count)\\nprint(t.word_index)\\nprint(t.word_docs)\\n# integer encode documents\\nencoded_docs = t.texts_to_matrix(docs, mode= 'count ')\\nprint(encoded_docs)\\nListing 7.10: Example of ﬁtting and encoding with the Tokenizer .\\nRunning the example ﬁts the Tokenizer with 5 small documents. The details of the ﬁt\\nTokenizer are printed. Then the 5 documents are encoded using a word count. Each document\\nis encoded as a 9-element vector with one position for each word and the chosen encoding\\nscheme value for each word position. In this case, a simple word count mode is used.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 75}, page_content=\"7.6. Further Reading 59\\nOrderedDict([( 'well ', 1), ( 'done ', 1), ( 'good ', 1), ( 'work ', 2), ( 'great ', 1), ( 'effort ',\\n1), ( 'nice ', 1), ( 'excellent ', 1)])\\n5\\n{ 'work ': 1, 'effort ': 6, 'done ': 3, 'great ': 5, 'good ': 4, 'excellent ': 8, 'well ': 2,\\n'nice ': 7}\\n{ 'work ': 2, 'effort ': 1, 'done ': 1, 'well ': 1, 'good ': 1, 'great ': 1, 'excellent ': 1,\\n'nice ': 1}\\n[[ 0. 0. 1. 1. 0. 0. 0. 0. 0.]\\n[ 0. 1. 0. 0. 1. 0. 0. 0. 0.]\\n[ 0. 0. 0. 0. 0. 1. 1. 0. 0.]\\n[ 0. 1. 0. 0. 0. 0. 0. 1. 0.]\\n[ 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\\nListing 7.11: Example output from ﬁtting and encoding with the Tokenizer .\\nThe Tokenizer will be the key way we will prepare text for word embeddings throughout\\nthis book.\\n7.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Text Preprocessing Keras API.\\nhttps://keras.io/preprocessing/text/\\n\\x88text toword sequence Keras API.\\nhttps://keras.io/preprocessing/text/#text_to_word_sequence\\n\\x88onehotKeras API.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 75}, page_content='\\x88onehotKeras API.\\nhttps://keras.io/preprocessing/text/#one_hot\\n\\x88hashing trick Keras API.\\nhttps://keras.io/preprocessing/text/#hashing_trick\\n\\x88Tokenizer Keras API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n7.7 Summary\\nIn this tutorial, you discovered how you can use the Keras API to prepare your text data for\\ndeep learning. Speciﬁcally, you learned:\\n\\x88About the convenience methods that you can use to quickly prepare text data.\\n\\x88TheTokenizer API that can be ﬁt on training data and used to encode training, validation,\\nand test documents.\\n\\x88The range of 4 diﬀerent document encoding schemes oﬀered by the Tokenizer API.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 76}, page_content='7.7. Summary 60\\n7.7.1 Next\\nThis is the last chapter in the data preparation part. In the next part, you will discover how to\\ndevelop bag-of-words models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 77}, page_content='Part IV\\nBag-of-Words\\n61'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 78}, page_content='Chapter 8\\nThe Bag-of-Words Model\\nThe bag-of-words model is a way of representing text data when modeling text with machine\\nlearning algorithms. The bag-of-words model is simple to understand and implement and has\\nseen great success in problems such as language modeling and document classiﬁcation. In this\\ntutorial, you will discover the bag-of-words model for feature extraction in natural language\\nprocessing. After completing this tutorial, you will know:\\n\\x88What the bag-of-words model is and why it is needed to represent text.\\n\\x88How to develop a bag-of-words model for a collection of documents.\\n\\x88How to use diﬀerent techniques to prepare a vocabulary and score words.\\nLet’s get started.\\n8.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. The Problem with Text\\n2. What is a Bag-of-Words?\\n3. Example of the Bag-of-Words Model\\n4. Managing Vocabulary\\n5. Scoring Words\\n6. Limitations of Bag-of-Words\\n8.2 The Problem with Text'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 78}, page_content='A problem with modeling text is that it is messy, and techniques like machine learning algorithms\\nprefer well deﬁned ﬁxed-length inputs and outputs. Machine learning algorithms cannot work\\nwith raw text directly; the text must be converted into numbers. Speciﬁcally, vectors of numbers.\\n62'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 79}, page_content='8.3. What is a Bag-of-Words? 63\\nIn language processing, the vectors x are derived from textual data, in order to\\nreﬂect various linguistic properties of the text.\\n— Page 65, Neural Network Methods in Natural Language Processing , 2017.\\nThis is called feature extraction or feature encoding. A popular and simple method of feature\\nextraction with text data is called the bag-of-words model of text.\\n8.3 What is a Bag-of-Words?\\nA bag-of-words model, or BoW for short, is a way of extracting features from text for use in\\nmodeling, such as with machine learning algorithms. The approach is very simple and ﬂexible,\\nand can be used in a myriad of ways for extracting features from documents. A bag-of-words is\\na representation of text that describes the occurrence of words within a document. It involves\\ntwo things:\\n\\x88A vocabulary of known words.\\n\\x88A measure of the presence of known words.\\nIt is called a bag-of-words , because any information about the order or structure of words'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 79}, page_content='in the document is discarded. The model is only concerned with whether known words occur in\\nthe document, not where in the document.\\nA very common feature extraction procedures for sentences and documents is the\\nbag-of-words approach (BOW). In this approach, we look at the histogram of the\\nwords within the text, i.e. considering each word count as a feature.\\n— Page 69, Neural Network Methods in Natural Language Processing , 2017.\\nThe intuition is that documents are similar if they have similar content. Further, that from\\nthe content alone we can learn something about the meaning of the document. The bag-of-words\\ncan be as simple or complex as you like. The complexity comes both in deciding how to design\\nthe vocabulary of known words (or tokens) and how to score the presence of known words. We\\nwill take a closer look at both of these concerns.\\n8.4 Example of the Bag-of-Words Model\\nLet’s make the bag-of-words model concrete with a worked example.\\n8.4.1 Step 1: Collect Data'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 79}, page_content='Below is a snippet of the ﬁrst few lines of text from the book A Tale of Two Cities by Charles\\nDickens, taken from Project Gutenberg.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 80}, page_content='8.4. Example of the Bag-of-Words Model 64\\nIt was the best of times,\\nit was the worst of times,\\nit was the age of wisdom,\\nit was the age of foolishness,\\nListing 8.1: Sample of text from A Tale of Two Cities by Charles Dickens.\\nFor this small example, let’s treat each line as a separate document and the 4 lines as our\\nentire corpus of documents.\\n8.4.2 Step 2: Design the Vocabulary\\nNow we can make a list of all of the words in our model vocabulary. The unique words here\\n(ignoring case and punctuation) are:\\nit\\nwas\\nthe\\nbest\\nof\\ntimes\\nworst\\nage\\nwisdom\\nfoolishness\\nListing 8.2: List of unique words.\\nThat is a vocabulary of 10 words from a corpus containing 24 words.\\n8.4.3 Step 3: Create Document Vectors\\nThe next step is to score the words in each document. The objective is to turn each document of\\nfree text into a vector that we can use as input or output for a machine learning model. Because\\nwe know the vocabulary has 10 words, we can use a ﬁxed-length document representation of 10,'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 80}, page_content='with one position in the vector to score each word. The simplest scoring method is to mark the\\npresence of words as a boolean value, 0 for absent, 1 for present. Using the arbitrary ordering of\\nwords listed above in our vocabulary, we can step through the ﬁrst document ( It was the best of\\ntimes ) and convert it into a binary vector. The scoring of the document would look as follows:\\nit = 1\\nwas = 1\\nthe = 1\\nbest = 1\\nof = 1\\ntimes = 1\\nworst = 0\\nage = 0\\nwisdom = 0\\nfoolishness = 0\\nListing 8.3: List of unique words and their occurrence in a document.\\nAs a binary vector, this would look as follows:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 81}, page_content='8.5. Managing Vocabulary 65\\n[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\\nListing 8.4: First line of text as a binary vector.\\nThe other three documents would look as follows:\\n\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\\n\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\\n\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\\nListing 8.5: Remaining three lines of text as binary vectors.\\nAll ordering of the words is nominally discarded and we have a consistent way of extracting\\nfeatures from any document in our corpus, ready for use in modeling. New documents that\\noverlap with the vocabulary of known words, but may contain words outside of the vocabulary,\\ncan still be encoded, where only the occurrence of known words are scored and unknown\\nwords are ignored. You can see how this might naturally scale to large vocabularies and larger\\ndocuments.\\n8.5 Managing Vocabulary\\nAs the vocabulary size increases, so does the vector representation of documents. In the previous'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 81}, page_content='example, the length of the document vector is equal to the number of known words. You can\\nimagine that for a very large corpus, such as thousands of books, that the length of the vector\\nmight be thousands or millions of positions. Further, each document may contain very few of\\nthe known words in the vocabulary.\\nThis results in a vector with lots of zero scores, called a sparse vector or sparse representation.\\nSparse vectors require more memory and computational resources when modeling and the\\nvast number of positions or dimensions can make the modeling process very challenging for\\ntraditional algorithms. As such, there is pressure to decrease the size of the vocabulary when\\nusing a bag-of-words model.\\nThere are simple text cleaning techniques that can be used as a ﬁrst step, such as:\\n\\x88Ignoring case.\\n\\x88Ignoring punctuation.\\n\\x88Ignoring frequent words that don’t contain much information, called stop words, like a,of,\\netc.\\n\\x88Fixing misspelled words.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 81}, page_content='\\x88Reducing words to their stem (e.g. play from playing ) using stemming algorithms.\\nA more sophisticated approach is to create a vocabulary of grouped words. This both\\nchanges the scope of the vocabulary and allows the bag-of-words to capture a little bit more\\nmeaning from the document. In this approach, each word or token is called a gram . Creating a\\nvocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that\\nappear in the corpus are modeled, not all possible bigrams.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 82}, page_content='8.6. Scoring Words 66\\nAn n-gram is an n-token sequence of words: a 2-gram (more commonly called a\\nbigram) is a two-word sequence of words like “please turn”, “turn your”, or “your\\nhomework”, and a 3-gram (more commonly called a trigram) is a three-word sequence\\nof words like “please turn your”, or “turn your homework”.\\n— Page 85, Speech and Language Processing , 2009.\\nFor example, the bigrams in the ﬁrst line of text in the previous section: It was the best of\\ntimes are as follows:\\nit was\\nwas the\\nthe best\\nbest of\\nof times\\nListing 8.6: List of bi-grams for a document.\\nA vocabulary that tracks triplets of words is called a trigram model and the general approach\\nis called the n-gram model, where nrefers to the number of grouped words. Often a simple\\nbigram approach is better than a 1-gram bag-of-words model for tasks like documentation\\nclassiﬁcation.\\na bag-of-bigrams representation is much more powerful than bag-of-words, and in\\nmany cases proves very hard to beat.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 82}, page_content='— Page 75, Neural Network Methods in Natural Language Processing , 2017.\\n8.6 Scoring Words\\nOnce a vocabulary has been chosen, the occurrence of words in example documents needs to be\\nscored. In the worked example, we have already seen one very simple approach to scoring: a\\nbinary scoring of the presence or absence of words. Some additional simple scoring methods\\ninclude:\\n\\x88Counts . Count the number of times each word appears in a document.\\n\\x88Frequencies . Calculate the frequency that each word appears in a document out of all\\nthe words in the document.\\n8.6.1 Word Hashing\\nYou may remember from computer science that a hash function is a bit of math that maps data\\nto a ﬁxed size set of numbers. For example, we use them in hash tables when programming where\\nperhaps names are converted to numbers for fast lookup. We can use a hash representation of\\nknown words in our vocabulary. This addresses the problem of having a very large vocabulary'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 82}, page_content='for a large text corpus because we can choose the size of the hash space, which is in turn the\\nsize of the vector representation of the document.\\nWords are hashed deterministically to the same integer index in the target hash space. A\\nbinary score or count can then be used to score the word. This is called the hash trick orfeature\\nhashing . The challenge is to choose a hash space to accommodate the chosen vocabulary size to\\nminimize the probability of collisions and trade-oﬀ sparsity.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 83}, page_content='8.7. Limitations of Bag-of-Words 67\\n8.6.2 TF-IDF\\nA problem with scoring word frequency is that highly frequent words start to dominate in the\\ndocument (e.g. larger score), but may not contain as much informational content to the model\\nas rarer but perhaps domain speciﬁc words. One approach is to rescale the frequency of words\\nby how often they appear in all documents, so that the scores for frequent words like thethat\\nare also frequent across all documents are penalized. This approach to scoring is called Term\\nFrequency - Inverse Document Frequency, or TF-IDF for short, where:\\n\\x88Term Frequency : is a scoring of the frequency of the word in the current document.\\n\\x88Inverse Document Frequency : is a scoring of how rare the word is across documents.\\nThe scores are a weighting where not all words are equally as important or interesting. The\\nscores have the eﬀect of highlighting words that are distinct (contain useful information) in a\\ngiven document.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 83}, page_content='given document.\\nThus the idf of a rare term is high, whereas the idf of a frequent term is likely to be\\nlow.\\n— Page 118, An Introduction to Information Retrieval , 2008.\\n8.7 Limitations of Bag-of-Words\\nThe bag-of-words model is very simple to understand and implement and oﬀers a lot of ﬂexibility\\nfor customization on your speciﬁc text data. It has been used with great success on prediction\\nproblems like language modeling and documentation classiﬁcation. Nevertheless, it suﬀers from\\nsome shortcomings, such as:\\n\\x88Vocabulary : The vocabulary requires careful design, most speciﬁcally in order to manage\\nthe size, which impacts the sparsity of the document representations.\\n\\x88Sparsity : Sparse representations are harder to model both for computational reasons\\n(space and time complexity) and also for information reasons, where the challenge is for\\nthe models to harness so little information in such a large representational space.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 83}, page_content='\\x88Meaning : Discarding word order ignores the context, and in turn meaning of words in\\nthe document (semantics). Context and meaning can oﬀer a lot to the model, that if\\nmodeled could tell the diﬀerence between the same words diﬀerently arranged ( this is\\ninteresting vsis this interesting ), synonyms ( old bike vsused bike ), and much more.\\n8.8 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 84}, page_content='8.9. Summary 68\\n8.8.1 Books\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2wycQKA\\n\\x88Speech and Language Processing , 2009.\\nhttp://amzn.to/2vaEb7T\\n\\x88An Introduction to Information Retrieval , 2008.\\nhttp://amzn.to/2vvnPHP\\n\\x88Foundations of Statistical Natural Language Processing , 1999.\\nhttp://amzn.to/2vvnPHP\\n8.8.2 Wikipedia\\n\\x88Bag-of-words model.\\nhttps://en.wikipedia.org/wiki/N-gram\\n\\x88n-gram.\\nhttps://en.wikipedia.org/wiki/N-gram\\n\\x88Feature hashing.\\nhttps://en.wikipedia.org/wiki/Feature_hashing\\n\\x88tf-idf.\\nhttps://en.wikipedia.org/wiki/Tf-idf\\n8.9 Summary\\nIn this tutorial, you discovered the bag-of-words model for feature extraction with text data.\\nSpeciﬁcally, you learned:\\n\\x88What the bag-of-words model is and why we need it.\\n\\x88How to work through the application of a bag-of-words model to a collection of documents.\\n\\x88What techniques can be used for preparing a vocabulary and scoring words.\\n8.9.1 Next'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 84}, page_content='8.9.1 Next\\nIn the next chapter, you will can prepare movie review data for the bag-of-words model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 85}, page_content='Chapter 9\\nHow to Prepare Movie Review Data\\nfor Sentiment Analysis\\nText data preparation is diﬀerent for each problem. Preparation starts with simple steps, like\\nloading data, but quickly gets diﬃcult with cleaning tasks that are very speciﬁc to the data you\\nare working with. You need help as to where to begin and what order to work through the steps\\nfrom raw data to data ready for modeling. In this tutorial, you will discover how to prepare\\nmovie review text data for sentiment analysis, step-by-step. After completing this tutorial, you\\nwill know:\\n\\x88How to load text data and clean it to remove punctuation and other non-words.\\n\\x88How to develop a vocabulary, tailor it, and save it to ﬁle.\\n\\x88How to prepare movie reviews using cleaning and a pre-deﬁned vocabulary and save them\\nto new ﬁles ready for modeling.\\nLet’s get started.\\n9.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Movie Review Dataset\\n2. Load Text Data\\n3. Clean Text Data\\n4. Develop Vocabulary'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 85}, page_content='5. Save Prepared Data\\n69'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 86}, page_content='9.2. Movie Review Dataset 70\\n9.2 Movie Review Dataset\\nThe Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in\\nthe early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available\\nas part of their research on natural language processing. The reviews were originally released\\nin 2002, but an updated and cleaned up version was released in 2004, referred to as v2.0. The\\ndataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive\\nof the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as\\nthepolarity dataset .\\nOur data contains 1000 positive and 1000 negative reviews all written before 2002,\\nwith a cap of 20 reviews per author (312 authors total) per category. We refer to\\nthis corpus as the polarity dataset.\\n— A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on\\nMinimum Cuts, 2004.\\nThe data has been cleaned up somewhat, for example:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 86}, page_content='\\x88The dataset is comprised of only English reviews.\\n\\x88All text has been converted to lowercase.\\n\\x88There is white space around punctuation like periods, commas, and brackets.\\n\\x88Text has been split into one sentence per line.\\nThe data has been used for a few related natural language processing tasks. For classiﬁcation,\\nthe performance of classical models (such as Support Vector Machines) on the data is in the\\nrange of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see\\nresults as high as 86% with 10-fold cross-validation. This gives us a ballpark of low-to-mid 80s\\nif we were looking to use this dataset in experiments on modern methods.\\n... depending on choice of downstream polarity classiﬁer, we can achieve highly\\nstatistically signiﬁcant improvement (from 82.8% to 86.4%)\\n— A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on\\nMinimum Cuts, 2004.\\nYou can download the dataset from here:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 86}, page_content='\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention from cv000 tocv999 for each of negand pos. Next, let’s\\nlook at loading the text data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 87}, page_content=\"9.3. Load Text Data 71\\n9.3 Load Text Data\\nIn this section, we will look at loading individual text ﬁles, then processing the directories of\\nﬁles. We will assume that the review data is downloaded and available in the current working\\ndirectory in the folder txtsentoken . We can load an individual text ﬁle by opening it, reading\\nin the ASCII text, and closing the ﬁle. This is standard ﬁle handling stuﬀ. For example, we can\\nload the ﬁrst negative review ﬁle cv000 29416.txt as follows:\\n# load one file\\nfilename = 'txt_sentoken/neg/cv000_29416.txt '\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nListing 9.1: Example of loading a single movie review.\\nThis loads the document as ASCII and preserves any white space, like new lines. We can\\nturn this into a function called load doc() that takes a ﬁlename of the document to load and\\nreturns the text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 87}, page_content=\"file = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 9.2: Function to load a document into memory.\\nWe have two directories each with 1,000 documents each. We can process each directory in\\nturn by ﬁrst getting a list of ﬁles in the directory using the listdir() function, then loading\\neach ﬁle in turn. For example, we can load each document in the negative directory using the\\nload doc() function to do the actual loading.\\nfrom os import listdir\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# specify directory to load\\ndirectory = 'txt_sentoken/neg '\\n# walk through all files in the folder\\nfor filename in listdir(directory):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 88}, page_content='9.3. Load Text Data 72\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load document\\ndoc = load_doc(path)\\nprint( \\'Loaded %s \\'% filename)\\nListing 9.3: Example of loading a all movie reviews.\\nRunning this example prints the ﬁlename of each review after it is loaded.\\n...\\nLoaded cv995_23113.txt\\nLoaded cv996_12447.txt\\nLoaded cv997_5152.txt\\nLoaded cv998_15691.txt\\nLoaded cv999_14636.txt\\nListing 9.4: Example output of loading all movie reviews.\\nWe can turn the processing of the documents into a function as well and use it as a template\\nlater for developing a function to clean all documents in a folder. For example, below we deﬁne\\naprocess docs() function to do the same thing.\\nfrom os import listdir\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, \\'r \\')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 88}, page_content='return text\\n# load all docs in a directory\\ndef process_docs(directory):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load document\\ndoc = load_doc(path)\\nprint( \\'Loaded %s \\'% filename)\\n# specify directory to load\\ndirectory = \\'txt_sentoken/neg \\'\\nprocess_docs(directory)\\nListing 9.5: Example of loading a all movie reviews with functions.\\nNow that we know how to load the movie review text data, let’s look at cleaning it.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 89}, page_content=\"9.4. Clean Text Data 73\\n9.4 Clean Text Data\\nIn this section, we will look at what data cleaning we might want to do to the movie review\\ndata. We will assume that we will be using a bag-of-words model or perhaps a word embedding\\nthat does not require too much preparation.\\n9.4.1 Split into Tokens\\nFirst, let’s load one document and look at the raw tokens split by white space. We will use the\\nload doc() function developed in the previous section. We can use the split() function to\\nsplit the loaded document into tokens separated by white space.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load the document\\nfilename = 'txt_sentoken/neg/cv000_29416.txt '\\ntext = load_doc(filename)\\n# split into tokens by white space\\ntokens = text.split()\\nprint(tokens)\\nListing 9.6: Load a movie review and split by white space.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 89}, page_content='Running the example gives a nice long list of raw tokens from the document.\\n...\\n\\'years \\', \\'ago \\', \\'and \\', \\'has \\', \\'been \\', \\'sitting \\', \\'on \\', \\'the \\', \\'shelves \\', \\'ever \\', \\'since \\',\\n\\'. \\', \\'whatever \\', \\'. \\', \\'. \\', \\'. \\', \\'skip \\', \\'it \\', \\'! \\', \"where \\'s\", \\'joblo \\', \\'coming \\',\\n\\'from \\', \\'? \\', \\'a \\', \\'nightmare \\', \\'of \\', \\'elm \\', \\'street \\', \\'3 \\', \\'( \\', \\'7/10 \\', \\') \\', \\'- \\',\\n\\'blair \\', \\'witch \\', \\'2 \\', \\'( \\', \\'7/10 \\', \\') \\', \\'- \\', \\'the \\', \\'crow \\', \\'( \\', \\'9/10 \\', \\') \\', \\'- \\',\\n\\'the \\', \\'crow \\', \\': \\', \\'salvation \\', \\'( \\', \\'4/10 \\', \\') \\', \\'- \\', \\'lost \\', \\'highway \\', \\'( \\',\\n\\'10/10 \\', \\') \\', \\'- \\', \\'memento \\', \\'( \\', \\'10/10 \\', \\') \\', \\'- \\', \\'the \\', \\'others \\', \\'( \\', \\'9/10 \\',\\n\\') \\', \\'- \\', \\'stir \\', \\'of \\', \\'echoes \\', \\'( \\', \\'8/10 \\', \\') \\']\\nListing 9.7: Example output of spitting a review by white space.\\nJust looking at the raw tokens can give us a lot of ideas of things to try, such as:\\n\\x88Remove punctuation from words (e.g. ‘what’s’).\\n\\x88Removing tokens that are just punctuation (e.g. ‘-’).'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 89}, page_content='\\x88Removing tokens that contain numbers (e.g. ‘10/10’).\\n\\x88Remove tokens that have one character (e.g. ‘a’).\\n\\x88Remove tokens that don’t have much meaning (e.g. ‘and’).'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 90}, page_content=\"9.4. Clean Text Data 74\\nSome ideas:\\n\\x88We can ﬁlter out punctuation from tokens using regular expressions.\\n\\x88We can remove tokens that are just punctuation or contain numbers by using an isalpha()\\ncheck on each token.\\n\\x88We can remove English stop words using the list loaded using NLTK.\\n\\x88We can ﬁlter out short tokens by checking their length.\\nBelow is an updated version of cleaning this review.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load the document\\nfilename = 'txt_sentoken/neg/cv000_29416.txt '\\ntext = load_doc(filename)\\n# split into tokens by white space\\ntokens = text.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 90}, page_content=\"# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nprint(tokens)\\nListing 9.8: Load and clean one movie review.\\nRunning the example gives a much cleaner looking list of tokens.\\n...\\n'explanation ', 'craziness ', 'came ', 'oh ', 'way ', 'horror ', 'teen ', 'slasher ', 'flick ',\\n'packaged ', 'look ', 'way ', 'someone ', 'apparently ', 'assuming ', 'genre ', 'still ',\\n'hot ', 'kids ', 'also ', 'wrapped ', 'production ', 'two ', 'years ', 'ago ', 'sitting ',\\n'shelves ', 'ever ', 'since ', 'whatever ', 'skip ', 'wheres ', 'joblo ', 'coming ',\\n'nightmare ', 'elm ', 'street ', 'blair ', 'witch ', 'crow ', 'crow ', 'salvation ', 'lost ',\\n'highway ', 'memento ', 'others ', 'stir ', 'echoes ']\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 90}, page_content='Listing 9.9: Example output of cleaning one movie review.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 91}, page_content=\"9.4. Clean Text Data 75\\nWe can put this into a function called clean doc() and test it on another review, this time\\na positive review.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 91}, page_content=\"# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 9.10: Function to clean movie reviews.\\nAgain, the cleaning procedure seems to produce a good set of tokens, at least as a ﬁrst cut.\\n...\\n'comic ', 'oscar ', 'winner ', 'martin ', 'childs ', 'shakespeare ', 'love ', 'production ',\\n'design ', 'turns ', 'original ', 'prague ', 'surroundings ', 'one ', 'creepy ', 'place ',\\n'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ', 'typically ', 'strong ',\\n'performance ', 'deftly ', 'handling ', 'british ', 'accent ', 'ians ', 'holm ', 'joe ',\\n'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ', 'supporting ', 'roles ',\\n'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ', 'opened ', 'mouth ',\\n'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ', 'half ', 'bad ', 'film ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 91}, page_content=\"'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ', 'language ', 'drug ', 'content ']\\nListing 9.11: Example output of a function to clean movie reviews.\\nThere are many more cleaning steps we could take and I leave them to your imagination.\\nNext, let’s look at how we can manage a preferred vocabulary of tokens.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 92}, page_content='9.5. Develop Vocabulary 76\\n9.5 Develop Vocabulary\\nWhen working with predictive models of text, like a bag-of-words model, there is a pressure to\\nreduce the size of the vocabulary. The larger the vocabulary, the more sparse the representation\\nof each word or document. A part of preparing text for sentiment analysis involves deﬁning and\\ntailoring the vocabulary of words supported by the model. We can do this by loading all of the\\ndocuments in the dataset and building a set of words. We may decide to support all of these\\nwords, or perhaps discard some. The ﬁnal chosen vocabulary can then be saved to ﬁle for later\\nuse, such as ﬁltering words in new documents in the future.\\nWe can keep track of the vocabulary in a Counter , which is a dictionary of words and their\\ncount with some additional convenience functions. We need to develop a new function to process\\na document and add it to the vocabulary. The function needs to load a document by calling the'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 92}, page_content='previously developed load doc() function. It needs to clean the loaded document using the\\npreviously developed clean doc() function, then it needs to add all the tokens to the Counter ,\\nand update counts. We can do this last step by calling the update() function on the counter\\nobject. Below is a function called adddoctovocab() that takes as arguments a document\\nﬁlename and a Counter vocabulary.\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\nListing 9.12: Function add a movie review to a vocabulary.\\nFinally, we can use our template above for processing all documents in a directory called\\nprocess docs() and update it to call adddoctovocab() .\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 92}, page_content='if not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\nListing 9.13: Updated process documents function.\\nWe can put all of this together and develop a full vocabulary from all documents in the\\ndataset.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 93}, page_content=\"9.5. Develop Vocabulary 77\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 93}, page_content='# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( \\'txt_sentoken/neg \\', vocab)\\nprocess_docs( \\'txt_sentoken/pos \\', vocab)\\n# print the size of the vocab\\nprint(len(vocab))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 94}, page_content=\"9.5. Develop Vocabulary 78\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\\nListing 9.14: Example of cleaning all reviews and building a vocabulary.\\nRunning the example creates a vocabulary with all documents in the dataset, including\\npositive and negative reviews. We can see that there are a little over 46,000 unique words across\\nall reviews and the top 3 words are ﬁlm,one, and movie .\\n46557\\n[( 'film ', 8860), ( 'one ', 5521), ( 'movie ', 5440), ( 'like ', 3553), ( 'even ', 2555), ( 'good ',\\n2320), ( 'time ', 2283), ( 'story ', 2118), ( 'films ', 2102), ( 'would ', 2042), ( 'much ',\\n2024), ( 'also ', 1965), ( 'characters ', 1947), ( 'get ', 1921), ( 'character ', 1906),\\n( 'two ', 1825), ( 'first ', 1768), ( 'see ', 1730), ( 'well ', 1694), ( 'way ', 1668), ( 'make ',\\n1590), ( 'really ', 1563), ( 'little ', 1491), ( 'life ', 1472), ( 'plot ', 1451), ( 'people ',\\n1420), ( 'movies ', 1416), ( 'could ', 1395), ( 'bad ', 1374), ( 'scene ', 1373), ( 'never ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 94}, page_content=\"1364), ( 'best ', 1301), ( 'new ', 1277), ( 'many ', 1268), ( 'doesnt ', 1267), ( 'man ', 1266),\\n( 'scenes ', 1265), ( 'dont ', 1210), ( 'know ', 1207), ( 'hes ', 1150), ( 'great ', 1141),\\n( 'another ', 1111), ( 'love ', 1089), ( 'action ', 1078), ( 'go ', 1075), ( 'us ', 1065),\\n( 'director ', 1056), ( 'something ', 1048), ( 'end ', 1047), ( 'still ', 1038)]\\nListing 9.15: Example output of building a vocabulary.\\nPerhaps the least common words, those that only appear once across all reviews, are not\\npredictive. Perhaps some of the most common words are not useful too. These are good\\nquestions and really should be tested with a speciﬁc predictive model. Generally, words that\\nonly appear once or a few times across 2,000 reviews are probably not predictive and can be\\nremoved from the vocabulary, greatly cutting down on the tokens we need to model. We can do\\nthis by stepping through words and their counts and only keeping those with a count above a\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 94}, page_content=\"chosen threshold. Here we will use 5 occurrences.\\n# keep tokens with > 5 occurrence\\nmin_occurane = 5\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\nListing 9.16: Example of ﬁltering the vocabulary by an occurrence count.\\nThis reduces the vocabulary from 46,557 to 14,803 words, a huge drop. Perhaps a minimum\\nof 5 occurrences is too aggressive; you can experiment with diﬀerent values. We can then save\\nthe chosen vocabulary of words to a new ﬁle. I like to save the vocabulary as ASCII with one\\nword per line. Below deﬁnes a function called save list() to save a list of items, in this case,\\ntokens to ﬁle, one per line.\\ndef save_list(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nListing 9.17: Function to save the vocabulary to ﬁle.\\nThe complete example for deﬁning and saving the vocabulary is listed below.\\nimport string\\nimport re\\nfrom os import listdir\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 95}, page_content=\"9.5. Develop Vocabulary 79\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 95}, page_content='# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# save list to file\\ndef save_list(lines, filename):\\ndata = \\'\\\\n \\'.join(lines)\\nfile = open(filename, \\'w \\')\\nfile.write(data)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 96}, page_content=\"9.6. Save Prepared Data 80\\nfile.close()\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\\n# keep tokens with > 5 occurrence\\nmin_occurane = 5\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 9.18: Example building and saving a ﬁnal vocabulary.\\nRunning this ﬁnal snippet after creating the vocabulary will save the chosen words to ﬁle. It\\nis a good idea to take a look at, and even study, your chosen vocabulary in order to get ideas\\nfor better preparing this data, or text data in the future.\\nhasnt\\nupdating\\nfiguratively\\nsymphony\\ncivilians\\nmight\\nfisherman\\nhokum\\nwitch\\nbuffoons\\n...\\nListing 9.19: Sample of the saved vocabulary ﬁle.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 96}, page_content=\"Next, we can look at using the vocabulary to create a prepared version of the movie review\\ndataset.\\n9.6 Save Prepared Data\\nWe can use the data cleaning and chosen vocabulary to prepare each movie review and save the\\nprepared versions of the reviews ready for modeling. This is a good practice as it decouples\\nthe data preparation from modeling, allowing you to focus on modeling and circle back to data\\nprep if you have new ideas. We can start oﬀ by loading the vocabulary from vocab.txt .\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 97}, page_content=\"9.6. Save Prepared Data 81\\nfile.close()\\nreturn text\\n# load vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = vocab.split()\\nvocab = set(vocab)\\nListing 9.20: Load the saved vocabulary.\\nNext, we can clean the reviews, use the loaded vocab to ﬁlter out unwanted tokens, and\\nsave the clean reviews in a new ﬁle. One approach could be to save all the positive reviews\\nin one ﬁle and all the negative reviews in another ﬁle, with the ﬁltered tokens separated by\\nwhite space for each review on separate lines. First, we can deﬁne a function to process a\\ndocument, clean it, ﬁlter it, and return it as a single line that could be saved in a ﬁle. Below\\ndeﬁnes the doctoline() function to do just that, taking a ﬁlename and vocabulary (as a set)\\nas arguments. It calls the previously deﬁned load doc() function to load the document and\\nclean doc() to tokenize the document.\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 97}, page_content='# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn \\' \\'.join(tokens)\\nListing 9.21: Function to ﬁlter a review by the vocabulary\\nNext, we can deﬁne a new version of process docs() to step through all reviews in a folder\\nand convert them to lines by calling doctoline() for each document. A list of lines is then\\nreturned.\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\nListing 9.22: Updated process docs function to ﬁlter all documents by the vocabulary.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 97}, page_content='We can then call process docs() for both the directories of positive and negative reviews,\\nthen call save list() from the previous section to save each list of processed reviews to a ﬁle.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 98}, page_content=\"9.6. Save Prepared Data 82\\nThe complete code listing is provided below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# save list to file\\ndef save_list(lines, filename):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 98}, page_content=\"data = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 99}, page_content='9.7. Further Reading 83\\n# skip files that do not have the right extension\\nif not filename.endswith(\".txt\"):\\nnext\\n# create the full path of the file to open\\npath = directory + \\'/ \\'+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load vocabulary\\nvocab_filename = \\'vocab.txt \\'\\nvocab = load_doc(vocab_filename)\\nvocab = vocab.split()\\nvocab = set(vocab)\\n# prepare negative reviews\\nnegative_lines = process_docs( \\'txt_sentoken/neg \\', vocab)\\nsave_list(negative_lines, \\'negative.txt \\')\\n# prepare positive reviews\\npositive_lines = process_docs( \\'txt_sentoken/pos \\', vocab)\\nsave_list(positive_lines, \\'positive.txt \\')\\nListing 9.23: Example of cleaning and ﬁltering all reviews by the vocab and saving the results\\nto ﬁle.\\nRunning the example saves two new ﬁles, negative.txt andpositive.txt , that contain the\\nprepared negative and positive reviews respectively. The data is ready for use in a bag-of-words\\nor even word embedding model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 99}, page_content='9.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n9.7.1 Dataset\\n\\x88Movie Review Data.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\n\\x88A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based\\non Minimum Cuts , 2004.\\nhttp://xxx.lanl.gov/abs/cs/0409058\\n\\x88Movie Review Polarity Dataset.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\n\\x88Dataset Readme v2.0 and v1.1.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/poldata.README.2.0.\\ntxt\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/README.1.1'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 100}, page_content='9.8. Summary 84\\n9.7.2 APIs\\n\\x88nltk.tokenize package API.\\nhttp://www.nltk.org/api/nltk.tokenize.html\\n\\x88Chapter 2, Accessing Text Corpora and Lexical Resources .\\nhttp://www.nltk.org/book/ch02.html\\n\\x88osAPI Miscellaneous operating system interfaces.\\nhttps://docs.python.org/3/library/os.html\\n\\x88collections API - Container datatypes.\\nhttps://docs.python.org/3/library/collections.html\\n9.8 Summary\\nIn this tutorial, you discovered how to prepare movie review text data for sentiment analysis,\\nstep-by-step. Speciﬁcally, you learned:\\n\\x88How to load text data and clean it to remove punctuation and other non-words.\\n\\x88How to develop a vocabulary, tailor it, and save it to ﬁle.\\n\\x88How to prepare movie reviews using cleaning and a predeﬁned vocabulary and save them\\nto new ﬁles ready for modeling.\\n9.8.1 Next\\nIn the next chapter, you will discover how you can develop a neural bag-of-words model for\\nmovie review sentiment analysis.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 101}, page_content='Chapter 10\\nProject: Develop a Neural\\nBag-of-Words Model for Sentiment\\nAnalysis\\nMovie reviews can be classiﬁed as either favorable or not. The evaluation of movie review text\\nis a classiﬁcation problem often called sentiment analysis. A popular technique for developing\\nsentiment analysis models is to use a bag-of-words model that transforms documents into vectors\\nwhere each word in the document is assigned a score. In this tutorial, you will discover how you\\ncan develop a deep learning predictive model using the bag-of-words representation for movie\\nreview sentiment classiﬁcation. After completing this tutorial, you will know:\\n\\x88How to prepare the review text data for modeling with a restricted vocabulary.\\n\\x88How to use the bag-of-words model to prepare train and test data.\\n\\x88How to develop a Multilayer Perceptron bag-of-words model and use it to make predictions\\non new review text data.\\nLet’s get started.\\n10.1 Tutorial Overview\\nThis tutorial is divided into the following parts:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 101}, page_content='1. Movie Review Dataset\\n2. Data Preparation\\n3. Bag-of-Words Representation\\n4. Sentiment Analysis Models\\n5. Comparing Word Scoring Methods\\n6. Predicting Sentiment for New Reviews\\n85'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 102}, page_content='10.2. Movie Review Dataset 86\\n10.2 Movie Review Dataset\\nIn this tutorial, we will use the Movie Review Dataset. This dataset designed for sentiment\\nanalysis was described previously in Chapter 9. You can download the dataset from here:\\n\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention cv000 tocv999 for each of negand pos. Next, let’s look\\nat loading the text data.\\n10.3 Data Preparation\\nNote : The preparation of the movie review dataset was ﬁrst described in Chapter 9. In this\\nsection, we will look at 3 things:\\n1. Separation of data into training and test sets.\\n2. Loading and cleaning the data to remove punctuation and numbers.\\n3. Deﬁning a vocabulary of preferred words.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 102}, page_content='10.3.1 Split into Train and Test Sets\\nWe are pretending that we are developing a system that can predict the sentiment of a textual\\nmovie review as either positive or negative. This means that after the model is developed, we\\nwill need to make predictions on new textual reviews. This will require all of the same data\\npreparation to be performed on those new reviews as is performed on the training data for the\\nmodel.\\nWe will ensure that this constraint is built into the evaluation of our models by splitting the\\ntraining and test datasets prior to any data preparation. This means that any knowledge in the\\ntest set that could help us better prepare the data (e.g. the words used) is unavailable during\\nthe preparation of data and the training of the model. That being said, we will use the last 100\\npositive reviews and the last 100 negative reviews as a test set (100 reviews) and the remaining\\n1,800 reviews as the training dataset. This is a 90% train, 10% split of the data. The split can'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 102}, page_content='be imposed easily by using the ﬁlenames of the reviews where reviews named 000 to 899 are for\\ntraining data and reviews named 900 onwards are for testing the model.\\n10.3.2 Loading and Cleaning Reviews\\nThe text data is already pretty clean, so not much preparation is required. Without getting too\\nmuch into the details, we will prepare the data using the following method:\\n\\x88Split tokens on white space.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 103}, page_content=\"10.3. Data Preparation 87\\n\\x88Remove all punctuation from words.\\n\\x88Remove all words that are not purely comprised of alphabetical characters.\\n\\x88Remove all words that are known stop words.\\n\\x88Remove all words that have a length ≤1 character.\\nWe can put all of these steps into a function called clean doc() that takes as an argument\\nthe raw text loaded from a ﬁle and returns a list of cleaned tokens. We can also deﬁne a function\\nload doc() that loads a document from ﬁle ready for use with the clean doc() function. An\\nexample of cleaning the ﬁrst positive review is listed below.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 103}, page_content=\"re_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 10.1: Example of cleaning a movie review.\\nRunning the example prints a long list of clean tokens. There are many more cleaning steps\\nwe may want to explore, and I leave them as further exercises. I’d love to see what you can\\ncome up with.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 104}, page_content=\"10.3. Data Preparation 88\\n...\\n'creepy ', 'place ', 'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ',\\n'typically ', 'strong ', 'performance ', 'deftly ', 'handling ', 'british ', 'accent ',\\n'ians ', 'holm ', 'joe ', 'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ',\\n'supporting ', 'roles ', 'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ',\\n'opened ', 'mouth ', 'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ',\\n'half ', 'bad ', 'film ', 'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ',\\n'language ', 'drug ', 'content ']\\nListing 10.2: Example output of cleaning a movie review.\\n10.3.3 Deﬁne a Vocabulary\\nIt is important to deﬁne a vocabulary of known words when using a bag-of-words model. The\\nmore words, the larger the representation of documents, therefore it is important to constrain\\nthe words to only those believed to be predictive. This is diﬃcult to know beforehand and often\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 104}, page_content=\"it is important to test diﬀerent hypotheses about how to construct a useful vocabulary. We\\nhave already seen how we can remove punctuation and numbers from the vocabulary in the\\nprevious section. We can repeat this for all documents and build a set of all known words.\\nWe can develop a vocabulary as a Counter , which is a dictionary mapping of words and\\ntheir count that allows us to easily update and query. Each document can be added to the\\ncounter (a new function called adddoctovocab() ) and we can step over all of the reviews in\\nthe negative directory and then the positive directory (a new function called process docs() ).\\nThe complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 104}, page_content=\"def clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 105}, page_content=\"10.3. Data Preparation 89\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 105}, page_content=\"Listing 10.3: Example of selecting a vocabulary for the dataset.\\nRunning the example shows that we have a vocabulary of 44,276 words. We also can see\\na sample of the top 50 most used words in the movie reviews. Note that this vocabulary was\\nconstructed based on only those reviews in the training dataset.\\n44276\\n[( 'film ', 7983), ( 'one ', 4946), ( 'movie ', 4826), ( 'like ', 3201), ( 'even ', 2262), ( 'good ',\\n2080), ( 'time ', 2041), ( 'story ', 1907), ( 'films ', 1873), ( 'would ', 1844), ( 'much ',\\n1824), ( 'also ', 1757), ( 'characters ', 1735), ( 'get ', 1724), ( 'character ', 1703),\\n( 'two ', 1643), ( 'first ', 1588), ( 'see ', 1557), ( 'way ', 1515), ( 'well ', 1511), ( 'make ',\\n1418), ( 'really ', 1407), ( 'little ', 1351), ( 'life ', 1334), ( 'plot ', 1288), ( 'people ',\\n1269), ( 'could ', 1248), ( 'bad ', 1248), ( 'scene ', 1241), ( 'movies ', 1238), ( 'never ',\\n1201), ( 'best ', 1179), ( 'new ', 1140), ( 'scenes ', 1135), ( 'man ', 1131), ( 'many ', 1130),\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 105}, page_content=\"( 'doesnt ', 1118), ( 'know ', 1092), ( 'dont ', 1086), ( 'hes ', 1024), ( 'great ', 1014),\\n( 'another ', 992), ( 'action ', 985), ( 'love ', 977), ( 'us ', 967), ( 'go ', 952),\\n( 'director ', 948), ( 'end ', 946), ( 'something ', 945), ( 'still ', 936)]\\nListing 10.4: Example output of selecting a vocabulary for the dataset.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 106}, page_content=\"10.3. Data Preparation 90\\nWe can step through the vocabulary and remove all words that have a low occurrence, such\\nas only being used once or twice in all reviews. For example, the following snippet will retrieve\\nonly the tokens that appear 2 or more times in all reviews.\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\nListing 10.5: Example of ﬁltering the vocabulary by occurrence.\\nFinally, the vocabulary can be saved to a new ﬁle called vocab.txt that we can later load\\nand use to ﬁlter movie reviews prior to encoding them for modeling. We deﬁne a new function\\ncalled save list() that saves the vocabulary to ﬁle, with one word per ﬁle. For example:\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# save tokens to a vocabulary file\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 106}, page_content=\"save_list(tokens, 'vocab.txt ')\\nListing 10.6: Example of saving the ﬁltered vocabulary.\\nPulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 107}, page_content=\"10.3. Data Preparation 91\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 107}, page_content=\"# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 10.7: Example of ﬁltering the vocabulary for the dataset.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 108}, page_content='10.4. Bag-of-Words Representation 92\\nRunning the above example with this addition shows that the vocabulary size drops by a\\nlittle more than half its size, from about 44,000 to about 25,000 words.\\n25767\\nListing 10.8: Example output of ﬁltering the vocabulary by min occurrence.\\nRunning the min occurrence ﬁlter on the vocabulary and saving it to ﬁle, you should now\\nhave a new ﬁle called vocab.txt with only the words we are interested in.\\nThe order of words in your ﬁle will diﬀer, but should look something like the following:\\naberdeen\\ndupe\\nburt\\nlibido\\nhamlet\\narlene\\navailable\\ncorners\\nweb\\ncolumbia\\n...\\nListing 10.9: Sample of the vocabulary ﬁle vocab.txt .\\nWe are now ready to look at extracting features from the reviews ready for modeling.\\n10.4 Bag-of-Words Representation\\nIn this section, we will look at how we can convert each review into a representation that we\\ncan provide to a Multilayer Perceptron model. A bag-of-words model is a way of extracting'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 108}, page_content='features from text so the text input can be used with machine learning algorithms like neural\\nnetworks. Each document, in this case a review, is converted into a vector representation.\\nThe number of items in the vector representing a document corresponds to the number of\\nwords in the vocabulary. The larger the vocabulary, the longer the vector representation, hence\\nthe preference for smaller vocabularies in the previous section. The bag-of-words model was\\nintroduced previously in Chapter 8.\\nWords in a document are scored and the scores are placed in the corresponding location in\\nthe representation. We will look at diﬀerent word scoring methods in the next section. In this\\nsection, we are concerned with converting reviews into vectors ready for training a ﬁrst neural\\nnetwork model. This section is divided into 2 steps:\\n1. Converting reviews to lines of tokens.\\n2. Encoding reviews with a bag-of-words model representation.\\n10.4.1 Reviews to Lines of Tokens'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 108}, page_content='Before we can convert reviews to vectors for modeling, we must ﬁrst clean them up. This\\ninvolves loading them, performing the cleaning operation developed above, ﬁltering out words\\nnot in the chosen vocabulary, and converting the remaining tokens into a single string or line'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 109}, page_content=\"10.4. Bag-of-Words Representation 93\\nready for encoding. First, we need a function to prepare one document. Below lists the function\\ndoctoline() that will load a document, clean it, ﬁlter out tokens not in the vocabulary, then\\nreturn the document as a string of white space separated tokens.\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\nListing 10.10: Function to ﬁlter a review by pre-deﬁned vocabulary.\\nNext, we need a function to work through all documents in a directory (such as posand\\nneg) to convert the documents into lines. Below lists the process docs() function that does\\njust this, expecting a directory name and a vocabulary set as input arguments and returning a\\nlist of processed documents.\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 109}, page_content=\"lines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\nListing 10.11: Function to ﬁlter all movie reviews by vocabulary.\\nWe can call the process docs() consistently for positive and negative reviews to construct\\na dataset of review text and their associated output labels, 0 for negative and 1 for positive.\\nThe load clean dataset() function below implements this behavior.\\n# load and clean a dataset\\ndef load_clean_dataset(vocab):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab)\\npos = process_docs( 'txt_sentoken/pos ', vocab)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 109}, page_content='return docs, labels\\nListing 10.12: Function to load movie reviews and prepare output labels.\\nFinally, we need to load the vocabulary and turn it into a set for use in cleaning reviews.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 110}, page_content=\"10.4. Bag-of-Words Representation 94\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\nListing 10.13: Load the pre-deﬁned vocabulary of words.\\nWe can put all of this together, reusing the loading and cleaning functions developed in\\nprevious sections. The complete example is listed below, demonstrating how to prepare the\\npositive and negative reviews from the training dataset.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 110}, page_content=\"# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 111}, page_content=\"10.4. Bag-of-Words Representation 95\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab)\\npos = process_docs( 'txt_sentoken/pos ', vocab)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = vocab.split()\\nvocab = set(vocab)\\n# load all training reviews\\ndocs, labels = load_clean_dataset(vocab)\\n# summarize what we have\\nprint(len(docs), len(labels))\\nListing 10.14: Filter all movie reviews by the pre-deﬁned vocabulary.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 111}, page_content='Running this example loads and cleans the review text and returns the labels.\\n1800 1800\\nListing 10.15: Example output from loading, cleaning and ﬁltering movie review data by a\\nconstrained vocabulary.\\n10.4.2 Movie Reviews to Bag-of-Words Vectors\\nWe will use the Keras API to convert reviews to encoded document vectors. Keras provides\\ntheTokenizer class that can do some of the cleaning and vocab deﬁnition tasks that we took\\ncare of in the previous section. It is better to do this ourselves to know exactly what was done\\nand why. Nevertheless, the Tokenizer class is convenient and will easily transform documents\\ninto encoded vectors. First, the Tokenizer must be created, then ﬁt on the text documents\\nin the training dataset. In this case, these are the aggregation of the positive lines and\\nnegative lines arrays developed in the previous section.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 112}, page_content=\"10.4. Bag-of-Words Representation 96\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 10.16: Function to ﬁt a Tokenizer on the clean and ﬁltered movie reviews.\\nThis process determines a consistent way to convert the vocabulary to a ﬁxed-length vector\\nwith 25,768 elements, which is the total number of words in the vocabulary ﬁle vocab.txt .\\nNext, documents can then be encoded using the Tokenizer by calling texts tomatrix() . The\\nfunction takes both a list of documents to encode and an encoding mode, which is the method\\nused to score words in the document. Here we specify freq to score words based on their\\nfrequency in the document. This can be used to encode the loaded training and test data, for\\nexample:\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'freq ')\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'freq ')\\nListing 10.17: Encode training data.\\nThis encodes all of the positive and negative reviews in the training dataset.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 112}, page_content=\"Next, the process docs() function from the previous section needs to be modiﬁed to\\nselectively process reviews in the test or train dataset. We support the loading of both the\\ntraining and test datasets by adding an istrain argument and using that to decide what\\nreview ﬁle names to skip.\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\nListing 10.18: Updated process documents function to load all reviews.\\nSimilarly, the load clean dataset() dataset must be updated to load either train or test\\ndata.\\n# load and clean a dataset\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 112}, page_content=\"def load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 113}, page_content=\"10.4. Bag-of-Words Representation 97\\nreturn docs, labels\\nListing 10.19: Function to load text data and labels.\\nWe can put all of this together in a single example.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 113}, page_content=\"# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 114}, page_content=\"10.5. Sentiment Analysis Models 98\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 114}, page_content=\"tokenizer = create_tokenizer(train_docs)\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'freq ')\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'freq ')\\nprint(Xtrain.shape, Xtest.shape)\\nListing 10.20: Complete example of preparing train and test data.\\nRunning the example prints both the shape of the encoded training dataset and test dataset\\nwith 1,800 and 200 documents respectively, each with the same sized encoding vocabulary\\n(vector length).\\n(1800, 25768) (200, 25768)\\nListing 10.21: Example output of loading and preparing the datasets.\\n10.5 Sentiment Analysis Models\\nIn this section, we will develop Multilayer Perceptron (MLP) models to classify encoded\\ndocuments as either positive or negative. The models will be simple feedforward network models\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 115}, page_content='10.5. Sentiment Analysis Models 99\\nwith fully connected layers called Dense in the Keras deep learning library. This section is\\ndivided into 3 sections:\\n1. First sentiment analysis model\\n2. Comparing word scoring modes\\n3. Making a prediction for new reviews\\n10.5.1 First Sentiment Analysis Model\\nWe can develop a simple MLP model to predict the sentiment of encoded reviews. The model\\nwill have an input layer that equals the number of words in the vocabulary, and in turn the\\nlength of the input documents. We can store this in a new variable called nwords , as follows:\\nn_words = Xtest.shape[1]\\nListing 10.22: Example of calculating the number of words.\\nWe can now deﬁne the network. All model conﬁguration was found with very little trial and\\nerror and should not be considered tuned for this problem. We will use a single hidden layer\\nwith 50 neurons and a rectiﬁed linear activation function. The output layer is a single neuron'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 115}, page_content=\"with a sigmoid activation function for predicting 0 for negative and 1 for positive reviews. The\\nnetwork will be trained using the eﬃcient Adam implementation of gradient descent and the\\nbinary cross entropy loss function, suited to binary classiﬁcation problems. We will keep track\\nof accuracy when training and evaluating the model.\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 10.23: Example of deﬁning an MLP for the bag-of-words model.\\nNext, we can ﬁt the model on the training data; in this case, the model is small and is easily\\nﬁt in 10 epochs.\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 115}, page_content='Listing 10.24: Example of ﬁtting the deﬁned model.\\nFinally, once the model is trained, we can evaluate its performance by making predictions in\\nthe test dataset and printing the accuracy.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 116}, page_content=\"10.5. Sentiment Analysis Models 100\\n# evaluate\\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %f '% (acc*100))\\nListing 10.25: Example of evaluating the ﬁt model.\\nThe complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 116}, page_content=\"tokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 117}, page_content=\"10.5. Sentiment Analysis Models 101\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 117}, page_content=\"model.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'freq ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 118}, page_content=\"10.5. Sentiment Analysis Models 102\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'freq ')\\n# define the model\\nn_words = Xtest.shape[1]\\nmodel = define_model(n_words)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# evaluate\\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %f '% (acc*100))\\nListing 10.26: Complete example of training and evaluating an MLP bag-of-words model.\\nRunning the example ﬁrst prints a summary of the deﬁned model.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ndense_1 (Dense) (None, 50) 1288450\\n_________________________________________________________________\\ndense_2 (Dense) (None, 1) 51\\n=================================================================\\nTotal params: 1,288,501\\nTrainable params: 1,288,501\\nNon-trainable params: 0\\n_________________________________________________________________\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 118}, page_content='Listing 10.27: Summary of the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\\nFigure 10.1: Plot of the deﬁned bag-of-words model.\\nWe can see that the model easily ﬁts the training data within the 10 epochs, achieving close\\nto 100% accuracy. Evaluating the model on the test dataset, we can see that model does well,\\nachieving an accuracy of above 87%, well within the ballpark of low-to-mid 80s seen in the\\noriginal paper. Although, it is important to note that this is not an apples-to-apples comparison,\\nas the original paper used 10-fold cross-validation to estimate model skill instead of a single\\ntrain/test split.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 119}, page_content='10.6. Comparing Word Scoring Methods 103\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n...\\nEpoch 6/10\\n0s - loss: 0.5319 - acc: 0.9428\\nEpoch 7/10\\n0s - loss: 0.4839 - acc: 0.9506\\nEpoch 8/10\\n0s - loss: 0.4368 - acc: 0.9567\\nEpoch 9/10\\n0s - loss: 0.3927 - acc: 0.9611\\nEpoch 10/10\\n0s - loss: 0.3516 - acc: 0.9689\\nTest Accuracy: 87.000000\\nListing 10.28: Example output of training and evaluating the MLP model.\\nNext, let’s look at testing diﬀerent word scoring methods for the bag-of-words model.\\n10.6 Comparing Word Scoring Methods\\nThe texts tomatrix() function for the Tokenizer in the Keras API provides 4 diﬀerent\\nmethods for scoring words; they are:\\n\\x88binary Where words are marked as present (1) or absent (0).\\n\\x88count Where the occurrence count for each word is marked as an integer.\\n\\x88tfidf Where each word is scored based on their frequency, where words that are common\\nacross all documents are penalized.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 119}, page_content='\\x88freq Where words are scored based on their frequency of occurrence within the document.\\nWe can evaluate the skill of the model developed in the previous section ﬁt using each of the\\n4 supported word scoring modes. This ﬁrst involves the development of a function to create an\\nencoding of the loaded documents based on a chosen scoring model. The function creates the\\ntokenizer, ﬁts it on the training documents, then creates the train and test encodings using the\\nchosen model. The function prepare data() implements this behavior given lists of train and\\ntest documents.\\n# prepare bag-of-words encoding of docs\\ndef prepare_data(train_docs, test_docs, mode):\\n# create the tokenizer\\ntokenizer = Tokenizer()\\n# fit the tokenizer on the documents\\ntokenizer.fit_on_texts(train_docs)\\n# encode training data set\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\\n# encode training data set\\nXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 120}, page_content='10.6. Comparing Word Scoring Methods 104\\nreturn Xtrain, Xtest\\nListing 10.29: Updated data preparation to take encoding mode as a parameter.\\nWe also need a function to evaluate the MLP given a speciﬁc encoding of the data. Because\\nneural networks are stochastic, they can produce diﬀerent results when the same model is ﬁt on\\nthe same data. This is mainly because of the random initial weights and the shuﬄing of patterns\\nduring mini-batch gradient descent. This means that any one scoring of a model is unreliable\\nand we should estimate model skill based on an average of multiple runs. The function below,\\nnamed evaluate mode() , takes encoded documents and evaluates the MLP by training it on\\nthe train set and estimating skill on the test set 10 times and returns a list of the accuracy\\nscores across all of these runs.\\n# evaluate a neural network model\\ndef evaluate_mode(Xtrain, ytrain, Xtest, ytest):\\nscores = list()\\nn_repeats = 30\\nn_words = Xtest.shape[1]\\nfor i in range(n_repeats):'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 120}, page_content=\"# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# evaluate\\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\\nscores.append(acc)\\nprint( '%d accuracy: %s '% ((i+1), acc))\\nreturn scores\\nListing 10.30: Function to create, ﬁt and evaluate a model multiple times.\\nWe are now ready to evaluate the performance of the 4 diﬀerent word scoring methods.\\nPulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom pandas import DataFrame\\nfrom matplotlib import pyplot\\n# load doc into memory\\ndef load_doc(filename):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 120}, page_content=\"# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 121}, page_content=\"10.6. Comparing Word Scoring Methods 105\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\nlines = list()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 121}, page_content=\"lines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 122}, page_content=\"10.6. Comparing Word Scoring Methods 106\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\nreturn model\\n# evaluate a neural network model\\ndef evaluate_mode(Xtrain, ytrain, Xtest, ytest):\\nscores = list()\\nn_repeats = 10\\nn_words = Xtest.shape[1]\\nfor i in range(n_repeats):\\n# define network\\nmodel = define_model(n_words)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=0)\\n# evaluate\\n_, acc = model.evaluate(Xtest, ytest, verbose=0)\\nscores.append(acc)\\nprint( '%d accuracy: %s '% ((i+1), acc))\\nreturn scores\\n# prepare bag of words encoding of docs\\ndef prepare_data(train_docs, test_docs, mode):\\n# create the tokenizer\\ntokenizer = Tokenizer()\\n# fit the tokenizer on the documents\\ntokenizer.fit_on_texts(train_docs)\\n# encode training data set\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 122}, page_content=\"Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\\n# encode training data set\\nXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\\nreturn Xtrain, Xtest\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# run experiment\\nmodes = [ 'binary ', 'count ', 'tfidf ', 'freq ']\\nresults = DataFrame()\\nfor mode in modes:\\n# prepare data for mode\\nXtrain, Xtest = prepare_data(train_docs, test_docs, mode)\\n# evaluate model on data for mode\\nresults[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\\n# summarize results\\nprint(results.describe())\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 123}, page_content='10.6. Comparing Word Scoring Methods 107\\n# plot results\\nresults.boxplot()\\npyplot.show()\\nListing 10.31: Complete example of comparing document encoding schemes.\\nAt the end of the run, summary statistics for each word scoring method are provided,\\nsummarizing the distribution of model skill scores across each of the 10 runs per mode. We can\\nsee that the mean score of both the count andbinary methods appear to be better than freq\\nandtfidf .\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nbinary count tfidf freq\\ncount 10.000000 10.000000 10.000000 10.000000\\nmean 0.927000 0.903500 0.876500 0.871000\\nstd 0.011595 0.009144 0.017958 0.005164\\nmin 0.910000 0.885000 0.855000 0.865000\\n25% 0.921250 0.900000 0.861250 0.866250\\n50% 0.927500 0.905000 0.875000 0.870000\\n75% 0.933750 0.908750 0.888750 0.875000\\nmax 0.945000 0.915000 0.910000 0.880000\\nListing 10.32: Example output of comparing document encoding schemes.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 123}, page_content='A box and whisker plot of the results is also presented, summarizing the accuracy distributions\\nper conﬁguration. We can see that binary achieved the best results with a modest spread and\\nmight be the preferred approach for this dataset.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 124}, page_content='10.7. Predicting Sentiment for New Reviews 108\\nFigure 10.2: Box and Whisker Plot for Model Accuracy with Diﬀerent Word Scoring Methods.\\n10.7 Predicting Sentiment for New Reviews\\nFinally, we can develop and use a ﬁnal model to make predictions for new textual reviews. This\\nis why we wanted the model in the ﬁrst place. First we will train a ﬁnal model on all of the\\navailable data. We will use the binary mode for scoring the bag-of-words model that was shown\\nto give the best results in the previous section.\\nPredicting the sentiment of new reviews involves following the same steps used to prepare\\nthe test data. Speciﬁcally, loading the text, cleaning the document, ﬁltering tokens by the\\nchosen vocabulary, converting the remaining tokens to a line, encoding it using the Tokenizer ,\\nand making a prediction. We can make a prediction of a class value directly with the ﬁt model\\nby calling predict() that will return an integer of 0 for a negative review and 1 for a positive'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 124}, page_content='review. All of these steps can be put into a new function called predict sentiment() that\\nrequires the review text, the vocabulary, the tokenizer, and the ﬁt model and returns the\\npredicted sentiment and an associated percentage or conﬁdence-like output.\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, model):\\n# clean\\ntokens = clean_doc(review)\\n# filter by vocab'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 125}, page_content=\"10.7. Predicting Sentiment for New Reviews 109\\ntokens = [w for w in tokens if w in vocab]\\n# convert to line\\nline = ' '.join(tokens)\\n# encode\\nencoded = tokenizer.texts_to_matrix([line], mode= 'binary ')\\n# predict sentiment\\nyhat = model.predict(encoded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\nListing 10.33: Function for making predictions for new reviews.\\nWe can now make predictions for new review texts. Below is an example with both a clearly\\npositive and a clearly negative review using the simple MLP developed above with the frequency\\nword scoring mode.\\n# test positive text\\ntext = 'Best movie ever! It was great, I recommend it. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\n# test negative text\\ntext = 'This is a bad movie. '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 125}, page_content=\"percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\nListing 10.34: Exampling of making predictions for new reviews.\\nPulling this all together, the complete example for making predictions for new reviews is\\nlisted below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 126}, page_content=\"10.7. Predicting Sentiment for New Reviews 110\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc, clean and return line of tokens\\ndef doc_to_line(filename, vocab):\\n# load the doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\nreturn ' '.join(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\nlines = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 126}, page_content=\"# load and clean the doc\\nline = doc_to_line(path, vocab)\\n# add to list\\nlines.append(line)\\nreturn lines\\n# load and clean a dataset\\ndef load_clean_dataset(vocab):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab)\\npos = process_docs( 'txt_sentoken/pos ', vocab)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# define the model\\ndef define_model(n_words):\\n# define network\\nmodel = Sequential()\\nmodel.add(Dense(50, input_shape=(n_words,), activation= 'relu '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 127}, page_content=\"10.7. Predicting Sentiment for New Reviews 111\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, model):\\n# clean\\ntokens = clean_doc(review)\\n# filter by vocab\\ntokens = [w for w in tokens if w in vocab]\\n# convert to line\\nline = ' '.join(tokens)\\n# encode\\nencoded = tokenizer.texts_to_matrix([line], mode= 'binary ')\\n# predict sentiment\\nyhat = model.predict(encoded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 127}, page_content=\"# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab)\\ntest_docs, ytest = load_clean_dataset(vocab)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# encode data\\nXtrain = tokenizer.texts_to_matrix(train_docs, mode= 'binary ')\\nXtest = tokenizer.texts_to_matrix(test_docs, mode= 'binary ')\\n# define network\\nn_words = Xtrain.shape[1]\\nmodel = define_model(n_words)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# test positive text\\ntext = 'Best movie ever! It was great, I recommend it. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\n# test negative text\\ntext = 'This is a bad movie. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\nListing 10.35: Complete example of making predictions for new review data.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 127}, page_content='Running the example correctly classiﬁes these reviews.\\nReview: [Best movie ever! It was great, I recommend it.]'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 128}, page_content='10.8. Extensions 112\\nSentiment: POSITIVE (57.124%)\\nReview: [This is a bad movie.]\\nSentiment: NEGATIVE (64.404%)\\nListing 10.36: Example output of making predictions for new reviews.\\nIdeally, we would ﬁt the model on all available data (train and test) to create a ﬁnal model\\nand save the model and tokenizer to ﬁle so that they can be loaded and used in new software.\\n10.8 Extensions\\nThis section lists some extensions if you are looking to get more out of this tutorial.\\n\\x88Manage Vocabulary . Explore using a larger or smaller vocabulary. Perhaps you can\\nget better performance with a smaller set of words.\\n\\x88Tune the Network Topology . Explore alternate network topologies such as deeper or\\nwider networks. Perhaps you can get better performance with a more suited network.\\n\\x88Use Regularization . Explore the use of regularization techniques, such as dropout.\\nPerhaps you can delay the convergence of the model and achieve better test set performance.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 128}, page_content='\\x88More Data Cleaning . Explore more or less cleaning of the review text and see how it\\nimpacts the model skill.\\n\\x88Training Diagnostics . Use the test dataset as a validation dataset during training and\\ncreate plots of train and test loss. Use these diagnostics to tune the batch size and number\\nof training epochs.\\n\\x88Trigger Words . Explore whether there are speciﬁc words in reviews that are highly\\npredictive of the sentiment.\\n\\x88Use Bigrams . Prepare the model to score bigrams of words and evaluate the performance\\nunder diﬀerent scoring schemes.\\n\\x88Truncated Reviews . Explore how using a truncated version of the movie reviews results\\nimpacts model skill, try truncating the start, end and middle of reviews.\\n\\x88Ensemble Models . Create models with diﬀerent word scoring schemes and see if using\\nensembles of the models results in improves to model skill.\\n\\x88Real Reviews . Train a ﬁnal model on all data and evaluate the model on real movie\\nreviews taken from the internet.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 128}, page_content='If you explore any of these extensions, I’d love to know.\\n10.9 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 129}, page_content='10.10. Summary 113\\n10.9.1 Dataset\\n\\x88Movie Review Data.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\n\\x88A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based\\non Minimum Cuts , 2004.\\nhttp://xxx.lanl.gov/abs/cs/0409058\\n\\x88Movie Review Polarity Dataset.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\n10.9.2 APIs\\n\\x88nltk.tokenize package API.\\nhttp://www.nltk.org/api/nltk.tokenize.html\\n\\x88Chapter 2, Accessing Text Corpora and Lexical Resources .\\nhttp://www.nltk.org/book/ch02.html\\n\\x88osAPI Miscellaneous operating system interfaces.\\nhttps://docs.python.org/3/library/os.html\\n\\x88collections API - Container datatypes.\\nhttps://docs.python.org/3/library/collections.html\\n\\x88Tokenizer Keras API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n10.10 Summary\\nIn this tutorial, you discovered how to develop a bag-of-words model for predicting the sentiment\\nof movie reviews. Speciﬁcally, you learned:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 129}, page_content='\\x88How to prepare the review text data for modeling with a restricted vocabulary.\\n\\x88How to use the bag-of-words model to prepare train and test data.\\n\\x88How to develop a Multilayer Perceptron bag-of-words model and use it to make predictions\\non new review text data.\\n10.10.1 Next\\nThis is the ﬁnal chapter in the bag-of-words part. In the next part, you will discover how to\\ndevelop word embedding models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 130}, page_content='Part V\\nWord Embeddings\\n114'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 131}, page_content='Chapter 11\\nThe Word Embedding Model\\nWord embeddings are a type of word representation that allows words with similar meaning to\\nhave a similar representation. They are a distributed representation for text that is perhaps one\\nof the key breakthroughs for the impressive performance of deep learning methods on challenging\\nnatural language processing problems. In this chapter, you will discover the word embedding\\napproach for representing text data. After completing this chapter, you will know:\\n\\x88What the word embedding approach for representing text is and how it diﬀers from other\\nfeature extraction methods.\\n\\x88That there are 3 main algorithms for learning a word embedding from text data.\\n\\x88That you can either train a new embedding or use a pre-trained embedding on your natural\\nlanguage processing task.\\nLet’s get started.\\n11.1 Overview\\nThis tutorial is divided into the following parts:\\n1. What Are Word Embeddings?\\n2. Word Embedding Algorithms\\n3. Using Word Embeddings'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 131}, page_content='11.2 What Are Word Embeddings?\\nA word embedding is a learned representation for text where words that have the same meaning\\nhave a similar representation. It is this approach to representing words and documents that may\\nbe considered one of the key breakthroughs of deep learning on challenging natural language\\nprocessing problems.\\n115'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 132}, page_content='11.3. Word Embedding Algorithms 116\\nOne of the beneﬁts of using dense and low-dimensional vectors is computational:\\nthe majority of neural network toolkits do not play well with very high-dimensional,\\nsparse vectors. ... The main beneﬁt of the dense representations is generalization\\npower: if we believe some features may provide similar clues, it is worthwhile to\\nprovide a representation that is able to capture these similarities.\\n— Page 92, Neural Network Methods in Natural Language Processing , 2017.\\nWord embeddings are in fact a class of techniques where individual words are represented as\\nreal-valued vectors in a predeﬁned vector space. Each word is mapped to one vector and the\\nvector values are learned in a way that resembles a neural network, and hence the technique is\\noften lumped into the ﬁeld of deep learning.\\nKey to the approach is the idea of using a dense distributed representation for each word.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 132}, page_content='Each word is represented by a real-valued vector, often tens or hundreds of dimensions. This is\\ncontrasted to the thousands or millions of dimensions required for sparse word representations,\\nsuch as a one hot encoding.\\nassociate with each word in the vocabulary a distributed word feature vector ... The\\nfeature vector represents diﬀerent aspects of the word: each word is associated with\\na point in a vector space. The number of features ... is much smaller than the size\\nof the vocabulary\\n—A Neural Probabilistic Language Model , 2003.\\nThe distributed representation is learned based on the usage of words. This allows words\\nthat are used in similar ways to result in having similar representations, naturally capturing\\ntheir meaning. This can be contrasted with the crisp but fragile representation in a bag-of-words\\nmodel where, unless explicitly managed, diﬀerent words have diﬀerent representations, regardless\\nof how they are used.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 132}, page_content='There is deeper linguistic theory behind the approach, namely the distributional hypothesis\\nby Zellig Harris that could be summarized as: words that have similar context will have similar\\nmeanings. For more depth see Harris’ 1956 paper Distributional structure . This notion of letting\\nthe usage of the word deﬁne its meaning can be summarized by an oft repeated quip by John\\nFirth:\\nYou shall know a word by the company it keeps!\\n— Page 11, A synopsis of linguistic theory 1930-1955 , in Studies in Linguistic Analysis\\n1930-1955, 1962.\\n11.3 Word Embedding Algorithms\\nWord embedding methods learn a real-valued vector representation for a predeﬁned ﬁxed sized\\nvocabulary from a corpus of text. The learning process is either joint with the neural network\\nmodel on some task, such as document classiﬁcation, or is an unsupervised process, using\\ndocument statistics. This section reviews three techniques that can be used to learn a word\\nembedding from text data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 133}, page_content='11.3. Word Embedding Algorithms 117\\n11.3.1 Embedding Layer\\nAn embedding layer, for lack of a better name, is a word embedding that is learned jointly\\nwith a neural network model on a speciﬁc natural language processing task, such as language\\nmodeling or document classiﬁcation. It requires that document text be cleaned and prepared\\nsuch that each word is one hot encoded. The size of the vector space is speciﬁed as part of\\nthe model, such as 50, 100, or 300 dimensions. The vectors are initialized with small random\\nnumbers. The embedding layer is used on the front end of a neural network and is ﬁt in a\\nsupervised way using the Backpropagation algorithm.\\n... when the input to a neural network contains symbolic categorical features\\n(e.g. features that take one of kdistinct symbols, such as words from a closed\\nvocabulary), it is common to associate each possible feature value (i.e., each word\\nin the vocabulary) with a d-dimensional vector for some d. These vectors are'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 133}, page_content='then considered parameters of the model, and are trained jointly with the other\\nparameters.\\n— Page 49, Neural Network Methods in Natural Language Processing , 2017.\\nThe one hot encoded words are mapped to the word vectors. If a Multilayer Perceptron\\nmodel is used, then the word vectors are concatenated before being fed as input to the model.\\nIf a recurrent neural network is used, then each word may be taken as one input in a sequence.\\nThis approach of learning an embedding layer requires a lot of training data and can be slow,\\nbut will learn an embedding both targeted to the speciﬁc text data and the NLP task.\\n11.3.2 Word2Vec\\nWord2Vec is a statistical method for eﬃciently learning a standalone word embedding from a\\ntext corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make\\nthe neural-network-based training of the embedding more eﬃcient and since then has become\\nthe de facto standard for developing pre-trained word embedding.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 133}, page_content='Additionally, the work involved analysis of the learned vectors and the exploration of vector\\nmath on the representations of words. For example, that subtracting the man-ness from King\\nand adding women-ness results in the word Queen , capturing the analogy king is to queen as\\nman is to woman .\\nWe ﬁnd that these representations are surprisingly good at capturing syntactic and\\nsemantic regularities in language, and that each relationship is characterized by a\\nrelation-speciﬁc vector oﬀset. This allows vector-oriented reasoning based on the\\noﬀsets between words. For example, the male/female relationship is automatically\\nlearned, and with the induced vector representations, King - Man + Woman results\\nin a vector very close to Queen .\\n—Linguistic Regularities in Continuous Space Word Representations , 2013.\\nTwo diﬀerent learning models were introduced that can be used as part of the Word2Vec\\napproach to learn the word embedding; they are:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 134}, page_content='11.3. Word Embedding Algorithms 118\\n\\x88Continuous Bag-of-Words, or CBOW model.\\n\\x88Continuous Skip-Gram Model.\\nThe CBOW model learns the embedding by predicting the current word based on its context.\\nThe continuous skip-gram model learns by predicting the surrounding words given a current\\nword.\\nFigure 11.1: Word2Vec Training Models. Taken from Eﬃcient Estimation of Word Representa-\\ntions in Vector Space , 2013\\nBoth models are focused on learning about words given their local usage context, where the\\ncontext is deﬁned by a window of neighboring words. This window is a conﬁgurable parameter\\nof the model.\\nThe size of the sliding window has a strong eﬀect on the resulting vector similarities.\\nLarge windows tend to produce more topical similarities [...], while smaller windows\\ntend to produce more functional and syntactic similarities.\\n— Page 128, Neural Network Methods in Natural Language Processing , 2017.\\nThe key beneﬁt of the approach is that high-quality word embeddings can be learned'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 134}, page_content='eﬃciently (low space and time complexity), allowing larger embeddings to be learned (more\\ndimensions) from much larger corpora of text (billions of words).\\n11.3.3 GloVe\\nThe Global Vectors for Word Representation, or GloVe, algorithm is an extension to the\\nWord2Vec method for eﬃciently learning word vectors, developed by Pennington, et al. at\\nStanford. Classical vector space model representations of words were developed using matrix'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 135}, page_content='11.4. Using Word Embeddings 119\\nfactorization techniques such as Latent Semantic Analysis (LSA) that do a good job of using\\nglobal text statistics but are not as good as the learned methods like Word2Vec at capturing\\nmeaning and demonstrating it on tasks like calculating analogies (e.g. the King and Queen\\nexample above).\\nGloVe is an approach to marry both the global statistics of matrix factorization techniques\\nlike LSA with the local context-based learning in Word2Vec. Rather than using a window to\\ndeﬁne local context, GloVe constructs an explicit word-context or word co-occurrence matrix\\nusing statistics across the whole text corpus. The result is a learning model that may result in\\ngenerally better word embeddings.\\nGloVe, is a new global log-bilinear regression model for the unsupervised learning of\\nword representations that outperforms other models on word analogy, word similarity,\\nand named entity recognition tasks.\\n—GloVe: Global Vectors for Word Representation , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 135}, page_content='11.4 Using Word Embeddings\\nYou have some options when it comes time to using word embeddings on your natural language\\nprocessing project. This section outlines those options.\\n11.4.1 Learn an Embedding\\nYou may choose to learn a word embedding for your problem. This will require a large amount\\nof text data to ensure that useful embeddings are learned, such as millions or billions of words.\\nYou have two main options when training your word embedding:\\n\\x88Learn it Standalone , where a model is trained to learn the embedding, which is saved\\nand used as a part of another model for your task later. This is a good approach if you\\nwould like to use the same embedding in multiple models.\\n\\x88Learn Jointly , where the embedding is learned as part of a large task-speciﬁc model.\\nThis is a good approach if you only intend to use the embedding on one task.\\n11.4.2 Reuse an Embedding\\nIt is common for researchers to make pre-trained word embeddings available for free, often under'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 135}, page_content='a permissive license so that you can use them on your own academic or commercial projects. For\\nexample, both Word2Vec and GloVe word embeddings are available for free download. These\\ncan be used on your project instead of training your own embeddings from scratch. You have\\ntwo main options when it comes to using pre-trained embeddings:\\n\\x88Static , where the embedding is kept static and is used as a component of your model.\\nThis is a suitable approach if the embedding is a good ﬁt for your problem and gives good\\nresults.\\n\\x88Updated , where the pre-trained embedding is used to seed the model, but the embedding\\nis updated jointly during the training of the model. This may be a good option if you are\\nlooking to get the most out of the model and embedding on your task.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 136}, page_content='11.5. Further Reading 120\\n11.4.3 Which Option Should You Use?\\nExplore the diﬀerent options, and if possible, test to see which gives the best results on your\\nproblem. Perhaps start with fast methods, like using a pre-trained embedding, and only use a\\nnew embedding if it results in better performance on your problem.\\n11.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n11.5.1 Books\\n\\x88Neural Network Methods in Natural Language Processing, 2017.\\nhttp://amzn.to/2wycQKA\\n11.5.2 Articles\\n\\x88Word embedding on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word_embedding\\n\\x88Word2Vec on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word2vec\\n\\x88GloVe on Wikipedia\\nhttps://en.wikipedia.org/wiki/GloVe_(machine_learning)\\n\\x88An overview of word embeddings and their connection to distributional semantic models ,\\n2016.\\nhttp://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/\\n\\x88Deep Learning, NLP, and Representations , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 136}, page_content='http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\\n11.5.3 Papers\\n\\x88Distributional structure , 1956.\\nhttp://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520\\n\\x88A Neural Probabilistic Language Model , 2003.\\nhttp://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\\n\\x88A Uniﬁed Architecture for Natural Language Processing: Deep Neural Networks with\\nMultitask Learning , 2008.\\nhttps://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf\\n\\x88Continuous space language models , 2007.\\nhttps://pdfs.semanticscholar.org/0fcc/184b3b90405ec3ceafd6a4007c749df7c363.\\npdf'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 137}, page_content='11.6. Summary 121\\n\\x88Eﬃcient Estimation of Word Representations in Vector Space , 2013.\\nhttps://arxiv.org/pdf/1301.3781.pdf\\n\\x88Distributed Representations of Words and Phrases and their Compositionality , 2013.\\nhttps://arxiv.org/pdf/1310.4546.pdf\\n\\x88GloVe: Global Vectors for Word Representation , 2014.\\nhttps://nlp.stanford.edu/pubs/glove.pdf\\n11.5.4 Projects\\n\\x88Word2Vec on Google Code.\\nhttps://code.google.com/archive/p/word2vec/\\n\\x88GloVe: Global Vectors for Word Representation.\\nhttps://nlp.stanford.edu/projects/glove/\\n11.6 Summary\\nIn this chapter, you discovered Word Embeddings as a representation method for text in deep\\nlearning applications. Speciﬁcally, you learned:\\n\\x88What the word embedding approach for representation text is and how it diﬀers from\\nother feature extraction methods.\\n\\x88That there are 3 main algorithms for learning a word embedding from text data.\\n\\x88That you can either train a new embedding or use a pre-trained embedding on your natural\\nlanguage processing task.\\n11.6.1 Next'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 137}, page_content='11.6.1 Next\\nIn the next chapter, you will discover how you can train and manipulate word embeddings using\\nthe Gensim Python library.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 138}, page_content='Chapter 12\\nHow to Develop Word Embeddings\\nwith Gensim\\nWord embeddings are a modern approach for representing text in natural language processing.\\nEmbedding algorithms like Word2Vec and GloVe are key to the state-of-the-art results achieved\\nby neural network models on natural language processing problems like machine translation.\\nIn this tutorial, you will discover how to train and load word embedding models for natural\\nlanguage processing applications in Python using Gensim. After completing this tutorial, you\\nwill know:\\n\\x88How to train your own Word2Vec word embedding model on text data.\\n\\x88How to visualize a trained word embedding model using Principal Component Analysis.\\n\\x88How to load pre-trained Word2Vec and GloVe word embedding models from Google and\\nStanford.\\nLet’s get started.\\n12.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Word Embeddings\\n2. Gensim Library\\n3. Develop Word2Vec Embedding\\n4. Visualize Word Embedding\\n5. Load Google’s Word2Vec Embedding'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 138}, page_content='6. Load Stanford’s GloVe Embedding\\n122'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 139}, page_content='12.2. Word Embeddings 123\\n12.2 Word Embeddings\\nA word embedding is an approach to provide a dense vector representation of words that capture\\nsomething about their meaning. Word embeddings are an improvement over simpler bag-of-word\\nmodel word encoding schemes like word counts and frequencies that result in large and sparse\\nvectors (mostly 0 values) that describe documents but not the meaning of the words.\\nWord embeddings work by using an algorithm to train a set of ﬁxed-length dense and\\ncontinuous-valued vectors based on a large corpus of text. Each word is represented by a\\npoint in the embedding space and these points are learned and moved around based on the\\nwords that surround the target word. It is deﬁning a word by the company that it keeps that\\nallows the word embedding to learn something about the meaning of words. The vector space\\nrepresentation of the words provides a projection where words with similar meanings are locally\\nclustered within the space.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 139}, page_content='The use of word embeddings over other text representations is one of the key methods that\\nhas led to breakthrough performance with deep neural networks on problems like machine\\ntranslation. In this tutorial, we are going to look at how to use two diﬀerent word embedding\\nmethods called Word2Vec by researchers at Google and GloVe by researchers at Stanford.\\n12.3 Gensim Python Library\\nGensim is an open source Python library for natural language processing, with a focus on topic\\nmodeling. It is billed as “ topic modeling for humans ”. Gensim was developed and is maintained\\nby the Czech natural language processing researcher Radim Rehurek and his company RaRe\\nTechnologies. It is not an everything-including-the-kitchen-sink NLP research library (like\\nNLTK); instead, Gensim is a mature, focused, and eﬃcient suite of NLP tools for topic modeling.\\nMost notably for this tutorial, it supports an implementation of the Word2Vec word embedding\\nfor learning new word vectors from text.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 139}, page_content='It also provides tools for loading pre-trained word embeddings in a few formats and for\\nmaking use and querying a loaded embedding. We will use the Gensim library in this tutorial.\\nGensim can be installed easily using piporeasy install . For example, you can install Gensim\\nwith pipby typing the following on your command line:\\nsudo pip install -U gensim\\nListing 12.1: Install the Gensim library with pip.\\nIf you need help installing Gensim on your system, you can see the Gensim Installation\\nInstructions (linked at the end of the chapter).\\n12.4 Develop Word2Vec Embedding\\nWord2Vec is one algorithm for learning a word embedding from a text corpus. There are two\\nmain training algorithms that can be used to learn the embedding from text; they are Continuous\\nBag-of-Words (CBOW) and skip grams. We will not get into the algorithms other than to say\\nthat they generally look at a window of words for each target word to provide context and in'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 139}, page_content='turn meaning for words. The approach was developed by Tomas Mikolov, formerly at Google\\nand currently at Facebook.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 140}, page_content='12.4. Develop Word2Vec Embedding 124\\nWord2Vec models require a lot of text, e.g. the entire Wikipedia corpus. Nevertheless, we\\nwill demonstrate the principles using a small in-memory example of text. Gensim provides the\\nWord2Vec class for working with a Word2Vec model. Learning a word embedding from text\\ninvolves loading and organizing the text into sentences and providing them to the constructor\\nof a new Word2Vec() instance. For example:\\nsentences = ...\\nmodel = Word2Vec(sentences)\\nListing 12.2: Example of creating a Word2Vec model.\\nSpeciﬁcally, each sentence must be tokenized, meaning divided into words and prepared (e.g.\\nperhaps pre-ﬁltered and perhaps converted to a preferred case). The sentences could be text\\nloaded into memory, or an iterator that progressively loads text, required for very large text\\ncorpora. There are many parameters on this constructor; a few noteworthy arguments you may\\nwish to conﬁgure are:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 140}, page_content='\\x88size : (default 100) The number of dimensions of the embedding, e.g. the length of the\\ndense vector to represent each token (word).\\n\\x88window : (default 5) The maximum distance between a target word and words around the\\ntarget word.\\n\\x88mincount : (default 5) The minimum count of words to consider when training the model;\\nwords with an occurrence less than this count will be ignored.\\n\\x88workers : (default 3) The number of threads to use while training.\\n\\x88sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\\nThe defaults are often good enough when just getting started. If you have a lot of cores, as\\nmost modern computers do, I strongly encourage you to increase workers to match the number\\nof cores (e.g. 8). After the model is trained, it is accessible via the wvattribute. This is the\\nactual word vector model in which queries can be made. For example, you can print the learned\\nvocabulary of tokens (words) as follows:\\nwords = list(model.wv.vocab)\\nprint(words)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 140}, page_content=\"print(words)\\nListing 12.3: Example of summarizing the words in the model vocabulary.\\nYou can review the embedded vector for a speciﬁc token as follows:\\nprint(model[ 'word '])\\nListing 12.4: Example of printing the embedding for a speciﬁc word.\\nFinally, a trained model can then be saved to ﬁle by calling the save word2vec format()\\nfunction on the word vector model. By default, the model is saved in a binary format to save\\nspace. For example:\\nmodel.wv.save_word2vec_format( 'model.bin ')\\nListing 12.5: Example of saving a word embedding.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 141}, page_content=\"12.4. Develop Word2Vec Embedding 125\\nWhen getting started, you can save the learned model in ASCII format and review the\\ncontents. You can do this by setting binary=False when calling the save word2vec format()\\nfunction, for example:\\nmodel.wv.save_word2vec_format( 'model.txt ', binary=False)\\nListing 12.6: Example of saving a word embedding in ASCII format.\\nThe saved model can then be loaded again by calling the Word2Vec.load() function. For\\nexample:\\nmodel = Word2Vec.load( 'model.bin ')\\nListing 12.7: Example of loading a saved word embedding.\\nWe can tie all of this together with a worked example. Rather than loading a large text\\ndocument or corpus from ﬁle, we will work with a small, in-memory list of pre-tokenized\\nsentences. The model is trained and the minimum count for words is set to 1 so that no words\\nare ignored. After the model is learned, we summarize, print the vocabulary, then print a single\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 141}, page_content=\"vector for the word “ sentence ”. Finally, the model is saved to a ﬁle in binary format, loaded,\\nand then summarized.\\nfrom gensim.models import Word2Vec\\n# define training data\\nsentences = [[ 'this ', 'is ', 'the ', 'first ', 'sentence ', 'for ', 'word2vec '],\\n[ 'this ', 'is ', 'the ', 'second ', 'sentence '],\\n[ 'yet ', 'another ', 'sentence '],\\n[ 'one ', 'more ', 'sentence '],\\n[ 'and ', 'the ', 'final ', 'sentence ']]\\n# train model\\nmodel = Word2Vec(sentences, min_count=1)\\n# summarize the loaded model\\nprint(model)\\n# summarize vocabulary\\nwords = list(model.wv.vocab)\\nprint(words)\\n# access vector for one word\\nprint(model[ 'sentence '])\\n# save model\\nmodel.save( 'model.bin ')\\n# load model\\nnew_model = Word2Vec.load( 'model.bin ')\\nprint(new_model)\\nListing 12.8: Example demonstrating the Word2Vec model in Gensim.\\nRunning the example prints the following output.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 141}, page_content=\"Word2Vec(vocab=14, size=100, alpha=0.025)\\n[ 'second ', 'sentence ', 'and ', 'this ', 'final ', 'word2vec ', 'for ', 'another ', 'one ',\\n'first ', 'more ', 'the ', 'yet ', 'is ']\\n[ -4.61881841e-03 -4.88735968e-03 -3.19508743e-03 4.08568839e-03\\n-3.38211656e-03 1.93076557e-03 3.90265253e-03 -1.04349572e-03\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 142}, page_content='12.5. Visualize Word Embedding 126\\n4.14286414e-03 1.55219622e-03 3.85653134e-03 2.22428422e-03\\n-3.52565176e-03 2.82056746e-03 -2.11121864e-03 -1.38054823e-03\\n-1.12888147e-03 -2.87318649e-03 -7.99703528e-04 3.67874932e-03\\n2.68940022e-03 6.31021452e-04 -4.36326629e-03 2.38655557e-04\\n-1.94210222e-03 4.87691024e-03 -4.04118607e-03 -3.17813386e-03\\n4.94802603e-03 3.43150692e-03 -1.44031656e-03 4.25637932e-03\\n-1.15106850e-04 -3.73274647e-03 2.50349124e-03 4.28692997e-03\\n-3.57313151e-03 -7.24728088e-05 -3.46099050e-03 -3.39612062e-03\\n3.54845310e-03 1.56780297e-03 4.58260969e-04 2.52689526e-04\\n3.06256465e-03 2.37558200e-03 4.06933809e-03 2.94650183e-03\\n-2.96231941e-03 -4.47433954e-03 2.89590308e-03 -2.16034567e-03\\n-2.58548348e-03 -2.06163677e-04 1.72605237e-03 -2.27384618e-04\\n-3.70194600e-03 2.11557443e-03 2.03793868e-03 3.09839356e-03\\n-4.71800892e-03 2.32995977e-03 -6.70911541e-05 1.39375112e-03\\n-3.84263694e-03 -1.03898917e-03 4.13251948e-03 1.06330717e-03'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 142}, page_content='1.38514000e-03 -1.18144893e-03 -2.60811858e-03 1.54952740e-03\\n2.49916781e-03 -1.95435272e-03 8.86975031e-05 1.89820060e-03\\n-3.41996481e-03 -4.08187555e-03 5.88635216e-04 4.13103355e-03\\n-3.25899688e-03 1.02130906e-03 -3.61028523e-03 4.17646067e-03\\n4.65870230e-03 3.64110398e-04 4.95479070e-03 -1.29743712e-03\\n-5.03367570e-04 -2.52546836e-03 3.31060472e-03 -3.12870182e-03\\n-1.14580349e-03 -4.34387522e-03 -4.62882593e-03 3.19007039e-03\\n2.88707414e-03 1.62976081e-04 -6.05802808e-04 -1.06368808e-03]\\nWord2Vec(vocab=14, size=100, alpha=0.025)\\nListing 12.9: Example output of the Word2Vec model in Gensim.\\nYou can see that with a little work to prepare your text document, you can create your own\\nword embedding very easily with Gensim.\\n12.5 Visualize Word Embedding\\nAfter you learn word embedding for your text data, it can be nice to explore it with visualization.\\nYou can use classical projection methods to reduce the high-dimensional word vectors to two-'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 142}, page_content='dimensional plots and plot them on a graph. The visualizations can provide a qualitative\\ndiagnostic for your learned model. We can retrieve all of the vectors from a trained model as\\nfollows:\\nX = model[model.wv.vocab]\\nListing 12.10: Access the model vocabulary.\\nWe can then train a projection method on the vectors, such as those methods oﬀered in\\nscikit-learn, then use Matplotlib to plot the projection as a scatter plot. Let’s look at an example\\nwith Principal Component Analysis or PCA.\\n12.5.1 Plot Word Vectors Using PCA\\nWe can create a 2-dimensional PCA model of the word vectors using the scikit-learn PCAclass\\nas follows.\\npca = PCA(n_components=2)\\nresult = pca.fit_transform(X)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 143}, page_content=\"12.5. Visualize Word Embedding 127\\nListing 12.11: Example of ﬁtting a 2D PCA model to the word vectors.\\nThe resulting projection can be plotted using Matplotlib as follows, pulling out the two\\ndimensions as xandycoordinates.\\npyplot.scatter(result[:, 0], result[:, 1])\\nListing 12.12: Example of plotting a scatter plot of the PCA vectors.\\nWe can go one step further and annotate the points on the graph with the words themselves.\\nA crude version without any nice oﬀsets looks as follows.\\nwords = list(model.wv.vocab)\\nfor i, word in enumerate(words):\\npyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\\nListing 12.13: Example of plotting words on the scatter plot.\\nPutting this all together with the model from the previous section, the complete example is\\nlisted below.\\nfrom gensim.models import Word2Vec\\nfrom sklearn.decomposition import PCA\\nfrom matplotlib import pyplot\\n# define training data\\nsentences = [[ 'this ', 'is ', 'the ', 'first ', 'sentence ', 'for ', 'word2vec '],\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 143}, page_content=\"[ 'this ', 'is ', 'the ', 'second ', 'sentence '],\\n[ 'yet ', 'another ', 'sentence '],\\n[ 'one ', 'more ', 'sentence '],\\n[ 'and ', 'the ', 'final ', 'sentence ']]\\n# train model\\nmodel = Word2Vec(sentences, min_count=1)\\n# fit a 2d PCA model to the vectors\\nX = model[model.wv.vocab]\\npca = PCA(n_components=2)\\nresult = pca.fit_transform(X)\\n# create a scatter plot of the projection\\npyplot.scatter(result[:, 0], result[:, 1])\\nwords = list(model.wv.vocab)\\nfor i, word in enumerate(words):\\npyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\\npyplot.show()\\nListing 12.14: Example demonstrating how to plot word vectors.\\nRunning the example creates a scatter plot with the dots annotated with the words. It is\\nhard to pull much meaning out of the graph given such a tiny corpus was used to ﬁt the model.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 144}, page_content='12.6. Load Google’s Word2Vec Embedding 128\\nFigure 12.1: Scatter Plot of PCA Projection of Word2Vec Model\\n12.6 Load Google’s Word2Vec Embedding\\nTraining your own word vectors may be the best approach for a given NLP problem. But it\\ncan take a long time, a fast computer with a lot of RAM and disk space, and perhaps some\\nexpertise in ﬁnessing the input data and training algorithm. An alternative is to simply use an\\nexisting pre-trained word embedding. Along with the paper and code for Word2Vec, Google\\nalso published a pre-trained Word2Vec model on the Word2Vec Google Code Project.\\nA pre-trained model is nothing more than a ﬁle containing tokens and their associated word\\nvectors. The pre-trained Google Word2Vec model was trained on Google news data (about 100\\nbillion words); it contains 3 million words and phrases and was ﬁt using 300-dimensional word\\nvectors. It is a 1.53 Gigabyte ﬁle. You can download it from here:\\n\\x88GoogleNews-vectors-negative300.bin.gz .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 144}, page_content='https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\\nUnzipped, the binary ﬁle ( GoogleNews-vectors-negative300.bin ) is 3.4 Gigabytes. The\\nGensim library provides tools to load this ﬁle. Speciﬁcally, you can call the\\nKeyedVectors.load word2vec format() function to load this model into memory, for example:\\nfrom gensim.models import KeyedVectors'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 145}, page_content=\"12.7. Load Stanford’s GloVe Embedding 129\\nfilename = 'GoogleNews-vectors-negative300.bin '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)\\nListing 12.15: Example of loading the Google word vectors in Gensim.\\nNote, this example may require a workstation with 8 or more Gigabytes of RAM to execute.\\nOn my modern workstation, it takes about 43 seconds to load. Another interesting thing that\\nyou can do is do a little linear algebra arithmetic with words. For example, a popular example\\ndescribed in lectures and introduction papers is:\\nqueen = (king - man) + woman\\nListing 12.16: Example of arithmetic of word vectors.\\nThat is the word queen is the closest word given the subtraction of the notion of man from\\nking and adding the word woman. The man-ness in king is replaced with woman-ness to give\\nus queen. A very cool concept. Gensim provides an interface for performing these types of\\noperations in the most similar() function on the trained or loaded model. For example:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 145}, page_content=\"result = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.17: Example of arithmetic of word vectors in Gensim.\\nWe can put all of this together as follows.\\nfrom gensim.models import KeyedVectors\\n# load the google word2vec model\\nfilename = 'GoogleNews-vectors-negative300.bin '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)\\n# calculate: (king - man) + woman = ?\\nresult = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.18: Example demonstrating arithmetic with Google word vectors.\\nRunning the example loads the Google pre-trained Word2Vec model and then calculates the\\n(king - man) + woman = ? operation on the word vectors for those words. The answer, as we\\nwould expect, is queen .\\n[( 'queen ', 0.7118192315101624)]\\nListing 12.19: Output of arithmetic with Google word vectors.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 145}, page_content='See some of the articles in the further reading section for more interesting arithmetic examples\\nthat you can explore.\\n12.7 Load Stanford’s GloVe Embedding\\nStanford researchers also have their own word embedding algorithm like Word2Vec called Global\\nVectors for Word Representation, or GloVe for short. I won’t get into the details of the diﬀerences\\nbetween Word2Vec and GloVe here, but generally, NLP practitioners seem to prefer GloVe at\\nthe moment based on results.\\nLike Word2Vec, the GloVe researchers also provide pre-trained word vectors, in this case, a\\ngreat selection to choose from. You can download the GloVe pre-trained word vectors and load'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 146}, page_content=\"12.7. Load Stanford’s GloVe Embedding 130\\nthem easily with Gensim. The ﬁrst step is to convert the GloVe ﬁle format to the Word2Vec ﬁle\\nformat. The only diﬀerence is the addition of a small header line. This can be done by calling\\ntheglove2word2vec() function. For example (note, this example is just a demonstration with\\na mock input ﬁlename):\\nfrom gensim.scripts.glove2word2vec import glove2word2vec\\nglove_input_file = 'glove.txt '\\nword2vec_output_file = 'word2vec.txt '\\nglove2word2vec(glove_input_file, word2vec_output_file)\\nListing 12.20: Example of converting a ﬁle from GloVe to Word2Vec format.\\nOnce converted, the ﬁle can be loaded just like Word2Vec ﬁle above. Let’s make this concrete\\nwith an example. You can download the smallest GloVe pre-trained model from the GloVe\\nwebsite. It an 822 Megabyte zip ﬁle with 4 diﬀerent models (50, 100, 200 and 300-dimensional\\nvectors) trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary. The\\ndirect download link is here:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 146}, page_content=\"\\x88glove.6B.zip .\\nhttp://nlp.stanford.edu/data/glove.6B.zip\\nWorking with the 100-dimensional version of the model, we can convert the ﬁle to Word2Vec\\nformat as follows:\\nfrom gensim.scripts.glove2word2vec import glove2word2vec\\nglove_input_file = 'glove.6B.100d.txt '\\nword2vec_output_file = 'glove.6B.100d.txt.word2vec '\\nglove2word2vec(glove_input_file, word2vec_output_file)\\nListing 12.21: Example of converting a speciﬁc GloVe ﬁle to Word2Vec format.\\nYou now have a copy of the GloVe model in Word2Vec format with the ﬁlename\\nglove.6B.100d.txt.word2vec . Now we can load it and perform the same (king - man) +\\nwoman = ? test as in the previous section. The complete code listing is provided below. Note\\nthat the converted ﬁle is ASCII format, not binary, so we set binary=False when loading.\\nfrom gensim.models import KeyedVectors\\n# load the Stanford GloVe model\\nfilename = 'glove.6B.100d.txt.word2vec '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=False)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 146}, page_content=\"# calculate: (king - man) + woman = ?\\nresult = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.22: Example of arithmetic with converted GloVe word vectors.\\nPulling all of this together, the complete example is listed below.\\nfrom gensim.models import KeyedVectors\\nfrom gensim.scripts.glove2word2vec import glove2word2vec\\n# convert glove to word2vec format\\nglove_input_file = 'glove.6B.100d.txt '\\nword2vec_output_file = 'glove.6B.100d.txt.word2vec '\\nglove2word2vec(glove_input_file, word2vec_output_file)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 147}, page_content=\"12.8. Further Reading 131\\n# load the converted model\\nfilename = 'glove.6B.100d.txt.word2vec '\\nmodel = KeyedVectors.load_word2vec_format(filename, binary=False)\\n# calculate: (king - man) + woman = ?\\nresult = model.most_similar(positive=[ 'woman ', 'king '], negative=[ 'man '], topn=1)\\nprint(result)\\nListing 12.23: Example demonstrating how to load and use GloVe word embeddings.\\nRunning the example prints the same result of queen .\\n[( 'queen ', 0.7698540687561035)]\\nListing 12.24: Example output of arithmetic with converted GloVe word vectors.\\n12.8 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n12.8.1 Word Embeddings\\n\\x88Word Embedding on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word2vec\\n\\x88Word2Vec on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word2vec\\n\\x88Google Word2Vec project.\\nhttps://code.google.com/archive/p/word2vec/\\n\\x88Stanford GloVe project.\\nhttps://nlp.stanford.edu/projects/glove/\\n12.8.2 Gensim\\n\\x88Gensim Python Library.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 147}, page_content='https://radimrehurek.com/gensim/index.html\\n\\x88Gensim Installation Instructions.\\nhttps://radimrehurek.com/gensim/install.html\\n\\x88models.word2vec Gensim API.\\nhttps://radimrehurek.com/gensim/models/keyedvectors.html\\n\\x88models.keyedvectors Gensim API.\\nhttps://radimrehurek.com/gensim/models/keyedvectors.html\\n\\x88scripts.glove2word2vec Gensim API.\\nhttps://radimrehurek.com/gensim/scripts/glove2word2vec.html'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 148}, page_content='12.9. Summary 132\\n12.8.3 Articles\\n\\x88Messing Around With Word2Vec , 2016.\\nhttps://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/\\n\\x88Vector Space Models for the Digital Humanities , 2015.\\nhttp://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html\\n\\x88Gensim Word2Vec Tutorial , 2014.\\nhttps://rare-technologies.com/word2vec-tutorial/\\n12.9 Summary\\nIn this tutorial, you discovered how to develop and load word embedding layers in Python using\\nGensim. Speciﬁcally, you learned:\\n\\x88How to train your own Word2Vec word embedding model on text data.\\n\\x88How to visualize a trained word embedding model using Principal Component Analysis.\\n\\x88How to load pre-trained Word2Vec and GloVe word embedding models from Google and\\nStanford.\\n12.9.1 Next\\nIn the next chapter, you will discover how you can develop a neural network model with a\\nlearned word embedding.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 149}, page_content='Chapter 13\\nHow to Learn and Load Word\\nEmbeddings in Keras\\nWord embeddings provide a dense representation of words and their relative meanings. They are\\nan improvement over sparse representations used in simpler bag of word model representations.\\nWord embeddings can be learned from text data and reused among projects. They can also be\\nlearned as part of ﬁtting a neural network on text data. In this tutorial, you will discover how\\nto use word embeddings for deep learning in Python with Keras. After completing this tutorial,\\nyou will know:\\n\\x88About word embeddings and that Keras supports word embeddings via the Embedding\\nlayer.\\n\\x88How to learn a word embedding while ﬁtting a neural network.\\n\\x88How to use a pre-trained word embedding in a neural network.\\nLet’s get started.\\n13.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Word Embedding\\n2. Keras Embedding Layer\\n3. Example of Learning an Embedding\\n4. Example of Using Pre-Trained GloVe Embedding'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 149}, page_content='5. Tips for Cleaning Text for Word Embedding\\n133'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 150}, page_content='13.2. Word Embedding 134\\n13.2 Word Embedding\\nA word embedding is a class of approaches for representing words and documents using a\\ndense vector representation. It is an improvement over more the traditional bag-of-word model\\nencoding schemes where large sparse vectors were used to represent each word or to score each\\nword within a vector to represent an entire vocabulary. These representations were sparse\\nbecause the vocabularies were vast and a given word or document would be represented by a\\nlarge vector comprised mostly of zero values.\\nInstead, in an embedding, words are represented by dense vectors where a vector represents\\nthe projection of the word into a continuous vector space. The position of a word within the\\nvector space is learned from text and is based on the words that surround the word when it is\\nused. The position of a word in the learned vector space is referred to as its embedding. Two\\npopular examples of methods of learning word embeddings from text include:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 150}, page_content='\\x88Word2Vec.\\n\\x88GloVe.\\nIn addition to these carefully designed methods, a word embedding can be learned as part\\nof a deep learning model. This can be a slower approach, but tailors the model to a speciﬁc\\ntraining dataset.\\n13.3 Keras Embedding Layer\\nKeras oﬀers an Embedding layer that can be used for neural networks on text data. It requires\\nthat the input data be integer encoded, so that each word is represented by a unique integer.\\nThis data preparation step can be performed using the Tokenizer API also provided with\\nKeras.\\nThe Embedding layer is initialized with random weights and will learn an embedding for all\\nof the words in the training dataset. It is a ﬂexible layer that can be used in a variety of ways,\\nsuch as:\\n\\x88It can be used alone to learn a word embedding that can be saved and used in another\\nmodel later.\\n\\x88It can be used as part of a deep learning model where the embedding is learned along\\nwith the model itself.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 150}, page_content='\\x88It can be used to load a pre-trained word embedding model, a type of transfer learning.\\nThe Embedding layer is deﬁned as the ﬁrst hidden layer of a network. It must specify 3\\narguments:\\n\\x88input dim: This is the size of the vocabulary in the text data. For example, if your data\\nis integer encoded to values between 0-10, then the size of the vocabulary would be 11\\nwords.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 151}, page_content='13.4. Example of Learning an Embedding 135\\n\\x88output dim: This is the size of the vector space in which words will be embedded. It\\ndeﬁnes the size of the output vectors from this layer for each word. For example, it could\\nbe 32 or 100 or even larger. Test diﬀerent values for your problem.\\n\\x88input length : This is the length of input sequences, as you would deﬁne for any input\\nlayer of a Keras model. For example, if all of your input documents are comprised of 1000\\nwords, this would be 1000.\\nFor example, below we deﬁne an Embedding layer with a vocabulary of 200 (e.g. integer\\nencoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be\\nembedded, and input documents that have 50 words each.\\ne = Embedding(200, 32, input_length=50)\\nListing 13.1: Example of creating a word embedding layer.\\nThe Embedding layer has weights that are learned. If you save your model to ﬁle, this will'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 151}, page_content=\"include weights for the Embedding layer. The output of the Embedding layer is a 2D vector with\\none embedding for each word in the input sequence of words (input document). If you wish\\nto connect a Dense layer directly to an Embedding layer, you must ﬁrst ﬂatten the 2D output\\nmatrix to a 1D vector using the Flatten layer. Now, let’s see how we can use an Embedding\\nlayer in practice.\\n13.4 Example of Learning an Embedding\\nIn this section, we will look at how we can learn a word embedding while ﬁtting a neural\\nnetwork on a text classiﬁcation problem. We will deﬁne a small problem where we have 10\\ntext documents, each with a comment about a piece of work a student submitted. Each text\\ndocument is classiﬁed as positive 1or negative 0. This is a simple sentiment analysis problem.\\nFirst, we will deﬁne the documents and their class labels.\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 151}, page_content=\"'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\nListing 13.2: Example of a small contrived classiﬁcation problem.\\nNext, we can integer encode each document. This means that as input the Embedding layer\\nwill have sequences of integers. We could experiment with other more sophisticated bag of word\\nmodel encoding like counts or TF-IDF. Keras provides the onehot() function that creates a\\nhash of each word as an eﬃcient integer encoding. We will estimate the vocabulary size of 50,\\nwhich is much larger than needed to reduce the probability of collisions from the hash function.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 152}, page_content=\"13.4. Example of Learning an Embedding 136\\n# integer encode the documents\\nvocab_size = 50\\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\\nprint(encoded_docs)\\nListing 13.3: Integer encode the text.\\nThe sequences have diﬀerent lengths and Keras prefers inputs to be vectorized and all inputs\\nto have the same length. We will pad all input sequences to have the length of 4. Again, we can\\ndo this with a built in Keras function, in this case the padsequences() function.\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\nListing 13.4: Pad the encoded text.\\nWe are now ready to deﬁne our Embedding layer as part of our neural network model.\\nThe Embedding layer has a vocabulary of 50 and an input length of 4. We will choose a\\nsmall embedding space of 8 dimensions. The model is a simple binary classiﬁcation model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 152}, page_content=\"Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one\\nfor each word. We ﬂatten this to a one 32-element vector to pass on to the Dense output layer.\\n# define the model\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile the model\\nmodel.compile(optimizer= 'adam ', loss= 'binary_crossentropy ', metrics=[ 'acc '])\\n# summarize the model\\nmodel.summary()\\nListing 13.5: Deﬁne a simple model with an Embedding input.\\nFinally, we can ﬁt and evaluate the classiﬁcation model.\\n# fit the model\\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\\n# evaluate the model\\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\\nprint( 'Accuracy: %f '% (accuracy*100))\\nListing 13.6: Fit the deﬁned model and print model accuracy.\\nThe complete code listing is provided below.\\nfrom keras.preprocessing.text import one_hot\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 152}, page_content=\"from keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers.embeddings import Embedding\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 153}, page_content=\"13.4. Example of Learning an Embedding 137\\n'Great effort ',\\n'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\n# integer encode the documents\\nvocab_size = 50\\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\\nprint(encoded_docs)\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\n# define the model\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile the model\\nmodel.compile(optimizer= 'adam ', loss= 'binary_crossentropy ', metrics=[ 'acc '])\\n# summarize the model\\nmodel.summary()\\n# fit the model\\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\\n# evaluate the model\\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\\nprint( 'Accuracy: %f '% (accuracy*100))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 153}, page_content='Listing 13.7: Example of ﬁtting and evaluating a Keras model with an Embedding input layer.\\nRunning the example ﬁrst prints the integer encoded documents.\\n[[6, 16], [42, 24], [2, 17], [42, 24], [18], [17], [22, 17], [27, 42], [22, 24], [49, 46,\\n16, 34]]\\nListing 13.8: Example output of the encoded documents.\\nThen the padded versions of each document are printed, making them all uniform length.\\n[[ 6 16 0 0]\\n[42 24 0 0]\\n[ 2 17 0 0]\\n[42 24 0 0]\\n[18 0 0 0]\\n[17 0 0 0]\\n[22 17 0 0]\\n[27 42 0 0]\\n[22 24 0 0]\\n[49 46 16 34]]\\nListing 13.9: Example output of the padded encoded documents.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 154}, page_content='13.5. Example of Using Pre-Trained GloVe Embedding 138\\nAfter the network is deﬁned, a summary of the layers is printed. We can see that as expected,\\nthe output of the Embedding layer is a 4 x 8 matrix and this is squashed to a 32-element vector\\nby the Flatten layer.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 4, 8) 400\\n_________________________________________________________________\\nflatten_1 (Flatten) (None, 32) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 1) 33\\n=================================================================\\nTotal params: 433\\nTrainable params: 433\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 13.10: Example output of the model summary.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 154}, page_content='Finally, the accuracy of the trained model is printed, showing that it learned the training\\ndataset perfectly (which is not surprising).\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nAccuracy: 100.000000\\nListing 13.11: Example output of the model accuracy.\\nYou could save the learned weights from the Embedding layer to ﬁle for later use in other\\nmodels. You could also use this model generally to classify other documents that have the\\nsame kind vocabulary seen in the test dataset. Next, let’s look at loading a pre-trained word\\nembedding in Keras.\\n13.5 Example of Using Pre-Trained GloVe Embedding\\nThe Keras Embedding layer can also use a word embedding learned elsewhere. It is common\\nin the ﬁeld of Natural Language Processing to learn, save, and make freely available word\\nembeddings. For example, the researchers behind GloVe method provide a suite of pre-trained'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 154}, page_content='word embeddings on their website released under a public domain license.\\nThe smallest package of embeddings is 822 Megabytes, called glove.6B.zip . It was trained\\non a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. There\\nare a few diﬀerent embedding vector sizes, including 50, 100, 200 and 300 dimensions. You\\ncan download this collection of embeddings and we can seed the Keras Embedding layer with\\nweights from the pre-trained embedding for the words in your training dataset.\\nThis example is inspired by an example in the Keras project: pretrained word embeddings.py .\\nAfter downloading and unzipping, you will see a few ﬁles, one of which is glove.6B.100d.txt ,\\nwhich contains a 100-dimensional version of the embedding. If you peek inside the ﬁle, you will\\nsee a token (word) followed by the weights (100 numbers) on each line. For example, below are\\nthe ﬁrst line of the embedding ASCII text ﬁle showing the embedding for the.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 155}, page_content='13.5. Example of Using Pre-Trained GloVe Embedding 139\\nthe -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459\\n0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336\\n0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107\\n-0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378\\n-0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857\\n-0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201\\n-0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044\\n0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624\\n0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217\\n0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\\nListing 13.12: Example GloVe word vector for the word ’the’.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 155}, page_content=\"As in the previous section, the ﬁrst step is to deﬁne the examples, encode them as integers,\\nthen pad the sequences to be the same length. In this case, we need to be able to map words to\\nintegers as well as integers to words. Keras provides a Tokenizer class that can be ﬁt on the\\ntraining data, can convert text to sequences consistently by calling the texts tosequences()\\nmethod on the Tokenizer class, and provides access to the dictionary mapping of words to\\nintegers in a word index attribute.\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\n# prepare tokenizer\\nt = Tokenizer()\\nt.fit_on_texts(docs)\\nvocab_size = len(t.word_index) + 1\\n# integer encode the documents\\nencoded_docs = t.texts_to_sequences(docs)\\nprint(encoded_docs)\\n# pad documents to a max length of 4 words\\nmax_length = 4\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 155}, page_content=\"max_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\nListing 13.13: Deﬁne encode and pad sample documents.\\nNext, we need to load the entire GloVe word embedding ﬁle into memory as a dictionary of\\nword to embedding array.\\n# load the whole embedding into memory\\nembeddings_index = dict()\\nf = open( 'glove.6B.100d.txt ')\\nfor line in f:\\nvalues = line.split()\\nword = values[0]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 156}, page_content=\"13.5. Example of Using Pre-Trained GloVe Embedding 140\\ncoefs = asarray(values[1:], dtype= 'float32 ')\\nembeddings_index[word] = coefs\\nf.close()\\nprint( 'Loaded %s word vectors. '% len(embeddings_index))\\nListing 13.14: Load the GloVe word embedding into memory.\\nThis is pretty slow. It might be better to ﬁlter the embedding for the unique words in your\\ntraining data. Next, we need to create a matrix of one embedding for each word in the training\\ndataset. We can do that by enumerating all unique words in the Tokenizer.word index and\\nlocating the embedding weight vector from the loaded GloVe embedding. The result is a matrix\\nof weights only for words we will see during training.\\n# create a weight matrix for words in training docs\\nembedding_matrix = zeros((vocab_size, 100))\\nfor word, i in t.word_index.items():\\nembedding_vector = embeddings_index.get(word)\\nif embedding_vector is not None:\\nembedding_matrix[i] = embedding_vector\\nListing 13.15: Covert the word embedding into a weight matrix.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 156}, page_content=\"Now we can deﬁne our model, ﬁt, and evaluate it as before. The key diﬀerence is that\\ntheEmbedding layer can be seeded with the GloVe word embedding weights. We chose the\\n100-dimensional version, therefore the Embedding layer must be deﬁned with output dimset to\\n100. Finally, we do not want to update the learned word weights in this model, therefore we\\nwill set the trainable attribute for the model to be False .\\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\\nListing 13.16: Create an Embedding layer with the pre-loaded weights.\\nThe complete worked example is listed below.\\nfrom numpy import asarray\\nfrom numpy import zeros\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import Embedding\\n# define documents\\ndocs = [ 'Well done! ',\\n'Good work ',\\n'Great effort ',\\n'nice work ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 156}, page_content=\"'nice work ',\\n'Excellent! ',\\n'Weak ',\\n'Poor effort! ',\\n'not good ',\\n'poor work ',\\n'Could have done better. ']\\n# define class labels\\nlabels = [1,1,1,1,1,0,0,0,0,0]\\n# prepare tokenizer\\nt = Tokenizer()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 157}, page_content=\"13.5. Example of Using Pre-Trained GloVe Embedding 141\\nt.fit_on_texts(docs)\\nvocab_size = len(t.word_index) + 1\\n# integer encode the documents\\nencoded_docs = t.texts_to_sequences(docs)\\nprint(encoded_docs)\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding= 'post ')\\nprint(padded_docs)\\n# load the whole embedding into memory\\nembeddings_index = dict()\\nf = open( 'glove.6B.100d.txt ', mode= 'rt ', encoding= 'utf-8 ')\\nfor line in f:\\nvalues = line.split()\\nword = values[0]\\ncoefs = asarray(values[1:], dtype= 'float32 ')\\nembeddings_index[word] = coefs\\nf.close()\\nprint( 'Loaded %s word vectors. '% len(embeddings_index))\\n# create a weight matrix for words in training docs\\nembedding_matrix = zeros((vocab_size, 100))\\nfor word, i in t.word_index.items():\\nembedding_vector = embeddings_index.get(word)\\nif embedding_vector is not None:\\nembedding_matrix[i] = embedding_vector\\n# define model\\nmodel = Sequential()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 157}, page_content=\"e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\\nmodel.add(e)\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile the model\\nmodel.compile(optimizer= 'adam ', loss= 'binary_crossentropy ', metrics=[ 'acc '])\\n# summarize the model\\nmodel.summary()\\n# fit the model\\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\\n# evaluate the model\\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\\nprint( 'Accuracy: %f '% (accuracy*100))\\nListing 13.17: Example loading pre-trained GloVe weights into an Embedding input layer.\\nRunning the example may take a bit longer, but then demonstrates that it is just as capable\\nof ﬁtting this simple problem.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n...\\nAccuracy: 100.000000\\nListing 13.18: Example output of loading pre-trained GloVe weights into an Embedding input\\nlayer.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 158}, page_content='13.6. Tips for Cleaning Text for Word Embedding 142\\nIn practice, I would encourage you to experiment with learning a word embedding using\\na pre-trained embedding that is ﬁxed and trying to perform learning on top of a pre-trained\\nembedding. See what works best for your speciﬁc problem.\\n13.6 Tips for Cleaning Text for Word Embedding\\nRecently, the ﬁeld of natural language processing has been moving away from bag-of-word\\nmodels and word encoding toward word embeddings. The beneﬁt of word embeddings is that\\nthey encode each word into a dense vector that captures something about its relative meaning\\nwithin the training text. This means that variations of words like case, spelling, punctuation,\\nand so on will automatically be learned to be similar in the embedding space. In turn, this\\ncan mean that the amount of cleaning required from your text may be less and perhaps quite\\ndiﬀerent to classical text cleaning. For example, it may no-longer make sense to stem words or'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 158}, page_content='remove punctuation for contractions.\\nTomas Mikolov is one of the developers of Word2Vec, a popular word embedding method.\\nHe suggests only very minimal text cleaning is required when learning a word embedding model.\\nBelow is his response when pressed with the question about how to best prepare text data for\\nWord2Vec.\\nThere is no universal answer. It all depends on what you plan to use the vectors\\nfor. In my experience, it is usually good to disconnect (or remove) punctuation from\\nwords, and sometimes also convert all characters to lowercase. One can also replace\\nall numbers (possibly greater than some constant) with some single token such as .\\nAll these pre-processing steps aim to reduce the vocabulary size without removing\\nany important content (which in some cases may not be true when you lowercase\\ncertain words, ie. ‘Bush’ is diﬀerent than ‘bush’, while ‘Another’ has usually the\\nsame sense as ‘another’). The smaller the vocabulary is, the lower is the memory'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 158}, page_content='complexity, and the more robustly are the parameters for the words estimated. You\\nalso have to pre-process the test data in the same way.\\n[...]\\nIn short, you will understand all this much better if you will run experiments.\\n— Tomas Mikolov, word2vec-toolkit: google groups thread. , 2015.\\nhttps://goo.gl/KtDGst\\n13.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Word Embedding on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Word_embedding\\n\\x88Keras Embedding Layer API.\\nhttps://keras.io/layers/embeddings/#embedding'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 159}, page_content='13.8. Summary 143\\n\\x88Using pre-trained word embeddings in a Keras model , 2016.\\nhttps://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\\n\\x88Example of using a pre-trained GloVe Embedding in Keras.\\nhttps://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.\\npy\\n\\x88GloVe Embedding.\\nhttps://nlp.stanford.edu/projects/glove/\\n\\x88An overview of word embeddings and their connection to distributional semantic models ,\\n2016.\\nhttp://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/\\n\\x88Deep Learning, NLP, and Representations , 2014.\\nhttp://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\\n13.8 Summary\\nIn this tutorial, you discovered how to use word embeddings for deep learning in Python with\\nKeras. Speciﬁcally, you learned:\\n\\x88About word embeddings and that Keras supports word embeddings via the Embedding\\nlayer.\\n\\x88How to learn a word embedding while ﬁtting a neural network.\\n\\x88How to use a pre-trained word embedding in a neural network.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 159}, page_content='13.8.1 Next\\nThis is the end of the part on word embeddings. In the next part you will discover neural text\\nclassiﬁcation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 160}, page_content='Part VI\\nText Classiﬁcation\\n144'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 161}, page_content='Chapter 14\\nNeural Models for Document\\nClassiﬁcation\\nText classiﬁcation describes a general class of problems such as predicting the sentiment of\\ntweets and movie reviews, as well as classifying email as spam or not. Deep learning methods are\\nproving very good at text classiﬁcation, achieving state-of-the-art results on a suite of standard\\nacademic benchmark problems. In this chapter, you will discover some best practices to consider\\nwhen developing deep learning models for text classiﬁcation. After reading this chapter, you\\nwill know:\\n\\x88The general combination of deep learning methods to consider when starting your text\\nclassiﬁcation problems.\\n\\x88The ﬁrst architecture to try with speciﬁc advice on how to conﬁgure hyperparameters.\\n\\x88That deeper networks may be the future of the ﬁeld in terms of ﬂexibility and capability.\\nLet’s get started.\\n14.1 Overview\\nThis tutorial is divided into the following parts:\\n1. Word Embeddings + CNN = Text Classiﬁcation\\n2. Use a Single Layer CNN Architecture'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 161}, page_content='3. Dial in CNN Hyperparameters\\n4. Consider Character-Level CNNs\\n5. Consider Deeper CNNs for Classiﬁcation\\n145'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 162}, page_content='14.2. Word Embeddings + CNN = Text Classiﬁcation 146\\n14.2 Word Embeddings + CNN = Text Classiﬁcation\\nThe modus operandi for text classiﬁcation involves the use of a word embedding for representing\\nwords and a Convolutional Neural Network (CNN) for learning how to discriminate documents\\non classiﬁcation problems. Yoav Goldberg, in his primer on deep learning for natural language\\nprocessing, comments that neural networks in general oﬀer better performance than classical\\nlinear classiﬁers, especially when used with pre-trained word embeddings.\\nThe non-linearity of the network, as well as the ability to easily integrate pre-trained\\nword embeddings, often lead to superior classiﬁcation accuracy.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\nHe also comments that convolutional neural networks are eﬀective at document classiﬁcation,\\nnamely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 162}, page_content='a way that is invariant to their position within the input sequences.\\nNetworks with convolutional and pooling layers are useful for classiﬁcation tasks in\\nwhich we expect to ﬁnd strong local clues regarding class membership, but these\\nclues can appear in diﬀerent places in the input. [...] We would like to learn that\\ncertain sequences of words are good indicators of the topic, and do not necessarily\\ncare where they appear in the document. Convolutional and pooling layers allow\\nthe model to learn to ﬁnd such local indicators, regardless of their position.\\n—A Primer on Neural Network Models for Natural Language Processing , 2015.\\nThe architecture is therefore comprised of three key pieces:\\n\\x88Word Embedding : A distributed representation of words where diﬀerent words that\\nhave a similar meaning (based on their usage) also have a similar representation.\\n\\x88Convolutional Model : A feature extraction model that learns to extract salient features'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 162}, page_content='from documents represented using a word embedding.\\n\\x88Fully Connected Model : The interpretation of extracted features in terms of a predictive\\noutput.\\nYoav Goldberg highlights the CNNs role as a feature extractor model in his book:\\n... the CNN is in essence a feature-extracting architecture. It does not constitute a\\nstandalone, useful network on its own, but rather is meant to be integrated into a\\nlarger network, and to be trained to work in tandem with it in order to produce an\\nend result. The CNNs layer’s responsibility is to extract meaningful sub-structures\\nthat are useful for the overall prediction task at hand.\\n— Page 152, Neural Network Methods for Natural Language Processing , 2017.\\nThe tying together of these three elements is demonstrated in perhaps one of the most widely\\ncited examples of the combination, described in the next section.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 163}, page_content='14.3. Use a Single Layer CNN Architecture 147\\n14.3 Use a Single Layer CNN Architecture\\nYou can get good results for document classiﬁcation with a single layer CNN, perhaps with\\ndiﬀerently sized kernels across the ﬁlters to allow grouping of word representations at diﬀerent\\nscales. Yoon Kim in his study of the use of pre-trained word vectors for classiﬁcation tasks with\\nConvolutional Neural Networks found that using pre-trained static word vectors does very well.\\nHe suggests that pre-trained word embeddings that were trained on very large text corpora,\\nsuch as the freely available Word2Vec vectors trained on 100 billion tokens from Google news\\nmay oﬀer good universal features for use in natural language processing.\\nDespite little tuning of hyperparameters, a simple CNN with one layer of convolution\\nperforms remarkably well. Our results add to the well-established evidence that\\nunsupervised pre-training of word vectors is an important ingredient in deep learning\\nfor NLP'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 163}, page_content='for NLP\\n—Convolutional Neural Networks for Sentence Classiﬁcation , 2014.\\nHe also discovered that further task-speciﬁc tuning of the word vectors oﬀer a small additional\\nimprovement in performance. Kim describes the general approach of using CNN for natural\\nlanguage processing. Sentences are mapped to embedding vectors and are available as a matrix\\ninput to the model. Convolutions are performed across the input word-wise using diﬀerently\\nsized kernels, such as 2 or 3 words at a time. The resulting feature maps are then processed\\nusing a max pooling layer to condense or summarize the extracted features.\\nThe architecture is based on the approach used by Ronan Collobert, et al. in their paper\\nNatural Language Processing (almost) from Scratch , 2011. In it, they develop a single end-to-end\\nneural network model with convolutional and pooling layers for use across a range of fundamental\\nnatural language processing problems. Kim provides a diagram that helps to see the sampling'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 163}, page_content='of the ﬁlters using diﬀerently sized kernels as diﬀerent colors (red and yellow).\\nFigure 14.1: An example of a CNN Filter and Polling Architecture for Natural Language\\nProcessing. Taken from Convolutional Neural Networks for Sentence Classiﬁcation .\\nUsefully, he reports his chosen model conﬁguration, discovered via grid search and used\\nacross a suite of 7 text classiﬁcation tasks, summarized as follows:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 164}, page_content='14.4. Dial in CNN Hyperparameters 148\\n\\x88Transfer function: rectiﬁed linear.\\n\\x88Kernel sizes: 2, 4, 5.\\n\\x88Number of ﬁlters: 100.\\n\\x88Dropout rate: 0.5.\\n\\x88Weight regularization (L2): 3.\\n\\x88Batch Size: 50.\\n\\x88Update Rule: Adadelta.\\nThese conﬁgurations could be used to inspire a starting point for your own experiments.\\n14.4 Dial in CNN Hyperparameters\\nSome hyperparameters matter more than others when tuning a convolutional neural network on\\nyour document classiﬁcation problem. Ye Zhang and Byron Wallace performed a sensitivity\\nanalysis into the hyperparameters needed to conﬁgure a single layer convolutional neural network\\nfor document classiﬁcation. The study is motivated by their claim that the models are sensitive\\nto their conﬁguration.\\nUnfortunately, a downside to CNN-based models - even simple ones - is that they\\nrequire practitioners to specify the exact model architecture to be used and to set\\nthe accompanying hyperparameters. To the uninitiated, making such decisions can'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 164}, page_content='seem like something of a black art because there are many free parameters in the\\nmodel.\\n—A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for\\nSentence Classiﬁcation , 2015.\\nTheir aim was to provide general conﬁgurations that can be used for conﬁguring CNNs on\\nnew text classiﬁcation tasks. They provide a nice depiction of the model architecture and the\\ndecision points for conﬁguring the model, reproduced below.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 165}, page_content='14.4. Dial in CNN Hyperparameters 149\\nFigure 14.2: Convolutional Neural Network Architecture for Sentence Classiﬁcation. Taken\\nfrom A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for\\nSentence Classiﬁcation .\\nThe study makes a number of useful ﬁndings that could be used as a starting point for\\nconﬁguring shallow CNN models for text classiﬁcation. The general ﬁndings were as follows:\\n\\x88The choice of pre-trained Word2Vec and GloVe embeddings diﬀer from problem to problem,\\nand both performed better than using one hot encoded word vectors.\\n\\x88The size of the kernel is important and should be tuned for each problem.\\n\\x88The number of feature maps is also important and should be tuned.\\n\\x88The 1-max pooling generally outperformed other types of pooling.\\n\\x88Dropout has little eﬀect on the model performance.\\nThey go on to provide more speciﬁc heuristics, as follows:\\n\\x88Use Word2Vec or GloVe word embeddings as a starting point and tune them while ﬁtting\\nthe model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 165}, page_content='the model.\\n\\x88Grid search across diﬀerent kernel sizes to ﬁnd the optimal conﬁguration for your problem,\\nin the range 1-10.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 166}, page_content='14.5. Consider Character-Level CNNs 150\\n\\x88Search the number of ﬁlters from 100-600 and explore a dropout of 0.0-0.5 as part of the\\nsame search.\\n\\x88Explore using tanh, relu, and linear activation functions.\\nThe key caveat is that the ﬁndings are based on empirical results on binary text classiﬁcation\\nproblems using single sentences as input.\\n14.5 Consider Character-Level CNNs\\nText documents can be modeled at the character level using convolutional neural networks\\nthat are capable of learning the relevant hierarchical structure of words, sentences, paragraphs,\\nand more. Xiang Zhang, et al. use a character-based representation of text as input for a\\nconvolutional neural network. The promise of the approach is that all of the labor-intensive\\neﬀort required to clean and prepare text could be overcome if a CNN can learn to abstract the\\nsalient details.\\n... deep ConvNets do not require the knowledge of words, in addition to the conclusion'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 166}, page_content='from previous research that ConvNets do not require the knowledge about the\\nsyntactic or semantic structure of a language. This simpliﬁcation of engineering could\\nbe crucial for a single system that can work for diﬀerent languages, since characters\\nalways constitute a necessary construct regardless of whether segmentation into\\nwords is possible. Working on only characters also has the advantage that abnormal\\ncharacter combinations such as misspellings and emoticons may be naturally learnt.\\n—Character-level Convolutional Networks for Text Classiﬁcation , 2015.\\nThe model reads in one hot encoded characters in a ﬁxed-sized alphabet. Encoded characters\\nare read in blocks or sequences of 1,024 characters. A stack of 6 convolutional layers with\\npooling follows, with 3 fully connected layers at the output end of the network in order to make\\na prediction.\\nFigure 14.3: Character-based Convolutional Neural Network for Text Classiﬁcation. Taken from'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 166}, page_content='Character-level Convolutional Networks for Text Classiﬁcation .\\nThe model achieves some success, performing better on problems that oﬀer a larger corpus\\nof text.\\n... analysis shows that character-level ConvNet is an eﬀective method. [...] how well\\nour model performs in comparisons depends on many factors, such as dataset size,\\nwhether the texts are curated and choice of alphabet.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 167}, page_content='14.6. Consider Deeper CNNs for Classiﬁcation 151\\n—Character-level Convolutional Networks for Text Classiﬁcation , 2015.\\nResults using an extended version of this approach were pushed to the state-of-the-art in a\\nfollow-up paper covered in the next section.\\n14.6 Consider Deeper CNNs for Classiﬁcation\\nBetter performance can be achieved with very deep convolutional neural networks, although\\nstandard and reusable architectures have not been adopted for classiﬁcation tasks, yet. Alexis\\nConneau, et al. comment on the relatively shallow networks used for natural language processing\\nand the success of much deeper networks used for computer vision applications. For example,\\nKim (above) restricted the model to a single convolutional layer.\\nOther architectures used for natural language reviewed in the paper are limited to 5 and 6\\nlayers. These are contrasted with successful architectures used in computer vision with 19 or'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 167}, page_content='even up to 152 layers. They suggest and demonstrate that there are beneﬁts for hierarchical\\nfeature learning with very deep convolutional neural network model, called VDCNN.\\n... we propose to use deep architectures of many convolutional layers to approach\\nthis goal, using up to 29 layers. The design of our architecture is inspired by recent\\nprogress in computer vision [...] The proposed deep convolutional network shows\\nsigniﬁcantly better results than previous ConvNets approach.\\n—Very Deep Convolutional Networks for Text Classiﬁcation , 2016.\\nKey to their approach is an embedding of individual characters, rather than a word embed-\\nding.\\nWe present a new architecture (VDCNN) for text processing which operates directly\\nat the character level and uses only small convolutions and pooling operations.\\n—Very Deep Convolutional Networks for Text Classiﬁcation , 2016.\\nResults on a suite of 8 large text classiﬁcation tasks show better performance than more'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 167}, page_content='shallow networks. Speciﬁcally, state-of-the-art results on all but two of the datasets tested,\\nat the time of writing. Generally, they make some key ﬁndings from exploring the deeper\\narchitectural approach:\\n\\x88The very deep architecture worked well on small and large datasets.\\n\\x88Deeper networks decrease classiﬁcation error.\\n\\x88Max-pooling achieves better results than other, more sophisticated types of pooling.\\n\\x88Generally going deeper degrades accuracy; the shortcut connections used in the architecture\\nare important.\\n... this is the ﬁrst time that the “beneﬁt of depths” was shown for convolutional\\nneural networks in NLP.\\n—Very Deep Convolutional Networks for Text Classiﬁcation , 2016.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 168}, page_content='14.7. Further Reading 152\\n14.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88A Primer on Neural Network Models for Natural Language Processing , 2015.\\nhttps://arxiv.org/abs/1510.00726\\n\\x88Convolutional Neural Networks for Sentence Classiﬁcation , 2014.\\nhttps://arxiv.org/abs/1103.0398\\n\\x88Natural Language Processing (almost) from Scratch , 2011.\\nhttps://arxiv.org/abs/1103.0398\\n\\x88Very Deep Convolutional Networks for Text Classiﬁcation , 2016.\\nhttps://arxiv.org/abs/1606.01781\\n\\x88Character-level Convolutional Networks for Text Classiﬁcation , 2015.\\nhttps://arxiv.org/abs/1509.01626\\n\\x88A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks\\nfor Sentence Classiﬁcation , 2015.\\nhttps://arxiv.org/abs/1510.03820\\n14.8 Summary\\nIn this chapter, you discovered some best practices for developing deep learning models for\\ndocument classiﬁcation. Speciﬁcally, you learned:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 168}, page_content='\\x88That a key approach is to use word embeddings and convolutional neural networks for\\ntext classiﬁcation.\\n\\x88That a single layer model can do well on moderate-sized problems, and ideas on how to\\nconﬁgure it.\\n\\x88That deeper models that operate directly on text may be the future of natural language\\nprocessing.\\n14.8.1 Next\\nIn the next chapter, you will discover how you can develop a neural text classiﬁcation model\\nwith word embeddings and a convolutional neural network.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 169}, page_content='Chapter 15\\nProject: Develop an Embedding +\\nCNN Model for Sentiment Analysis\\nWord embeddings are a technique for representing text where diﬀerent words with similar\\nmeaning have a similar real-valued vector representation. They are a key breakthrough that has\\nled to great performance of neural network models on a suite of challenging natural language\\nprocessing problems. In this tutorial, you will discover how to develop word embedding models\\nwith convolutional neural networks to classify movie reviews. After completing this tutorial,\\nyou will know:\\n\\x88How to prepare movie review text data for classiﬁcation with deep learning methods.\\n\\x88How to develop a neural classiﬁcation model with word embedding and convolutional\\nlayers.\\n\\x88How to evaluate the developed a neural classiﬁcation model.\\nLet’s get started.\\n15.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Movie Review Dataset\\n2. Data Preparation\\n3. Train CNN With Embedding Layer\\n4. Evaluate Model'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 169}, page_content='4. Evaluate Model\\n15.2 Movie Review Dataset\\nIn this tutorial, we will use the Movie Review Dataset. This dataset designed for sentiment\\nanalysis was described previously in Chapter 9. You can download the dataset from here:\\n153'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 170}, page_content='15.3. Data Preparation 154\\n\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention cv000 tocv999 for each of negand pos.\\n15.3 Data Preparation\\nNote : The preparation of the movie review dataset was ﬁrst described in Chapter 9. In this\\nsection, we will look at 3 things:\\n1. Separation of data into training and test sets.\\n2. Loading and cleaning the data to remove punctuation and numbers.\\n3. Deﬁning a vocabulary of preferred words.\\n15.3.1 Split into Train and Test Sets\\nWe are pretending that we are developing a system that can predict the sentiment of a textual\\nmovie review as either positive or negative. This means that after the model is developed, we'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 170}, page_content='will need to make predictions on new textual reviews. This will require all of the same data\\npreparation to be performed on those new reviews as is performed on the training data for the\\nmodel. We will ensure that this constraint is built into the evaluation of our models by splitting\\nthe training and test datasets prior to any data preparation. This means that any knowledge in\\nthe data in the test set that could help us better prepare the data (e.g. the words used) are\\nunavailable in the preparation of data used for training the model.\\nThat being said, we will use the last 100 positive reviews and the last 100 negative reviews\\nas a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a\\n90% train, 10% split of the data. The split can be imposed easily by using the ﬁlenames of the\\nreviews where reviews named 000 to 899 are for training data and reviews named 900 onwards\\nare for test.\\n15.3.2 Loading and Cleaning Reviews'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 170}, page_content='The text data is already pretty clean; not much preparation is required. Without getting bogged\\ndown too much in the details, we will prepare the data using the following way:\\n\\x88Split tokens on white space.\\n\\x88Remove all punctuation from words.\\n\\x88Remove all words that are not purely comprised of alphabetical characters.\\n\\x88Remove all words that are known stop words.\\n\\x88Remove all words that have a length ≤1 character.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 171}, page_content=\"15.3. Data Preparation 155\\nWe can put all of these steps into a function called clean doc() that takes as an argument\\nthe raw text loaded from a ﬁle and returns a list of cleaned tokens. We can also deﬁne a function\\nload doc() that loads a document from ﬁle ready for use with the clean doc() function. An\\nexample of cleaning the ﬁrst positive review is listed below.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 171}, page_content=\"# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 15.1: Example of cleaning a movie review.\\nRunning the example prints a long list of clean tokens. There are many more cleaning steps\\nwe may want to explore and I leave them as further exercises.\\n...\\n'creepy ', 'place ', 'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ',\\n'typically ', 'strong ', 'performance ', 'deftly ', 'handling ', 'british ', 'accent ',\\n'ians ', 'holm ', 'joe ', 'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ',\\n'supporting ', 'roles ', 'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 171}, page_content=\"'opened ', 'mouth ', 'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ',\\n'half ', 'bad ', 'film ', 'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ',\\n'language ', 'drug ', 'content ']\\nListing 15.2: Example output of cleaning a movie review.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 172}, page_content='15.3. Data Preparation 156\\n15.3.3 Deﬁne a Vocabulary\\nIt is important to deﬁne a vocabulary of known words when using a text model. The more\\nwords, the larger the representation of documents, therefore it is important to constrain the\\nwords to only those believed to be predictive. This is diﬃcult to know beforehand and often it\\nis important to test diﬀerent hypotheses about how to construct a useful vocabulary. We have\\nalready seen how we can remove punctuation and numbers from the vocabulary in the previous\\nsection. We can repeat this for all documents and build a set of all known words.\\nWe can develop a vocabulary as a Counter , which is a dictionary mapping of words and\\ntheir count that allows us to easily update and query. Each document can be added to the\\ncounter (a new function called adddoctovocab() ) and we can step over all of the reviews in\\nthe negative directory and then the positive directory (a new function called process docs() ).\\nThe complete example is listed below.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 172}, page_content=\"import string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 172}, page_content='# load doc\\ndoc = load_doc(filename)\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 173}, page_content=\"15.3. Data Preparation 157\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# print the top words in the vocab\\nprint(vocab.most_common(50))\\nListing 15.3: Example of selecting a vocabulary for the dataset.\\nRunning the example shows that we have a vocabulary of 44,276 words. We also can see\\na sample of the top 50 most used words in the movie reviews. Note that this vocabulary was\\nconstructed based on only those reviews in the training dataset.\\n44276\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 173}, page_content=\"44276\\n[( 'film ', 7983), ( 'one ', 4946), ( 'movie ', 4826), ( 'like ', 3201), ( 'even ', 2262), ( 'good ',\\n2080), ( 'time ', 2041), ( 'story ', 1907), ( 'films ', 1873), ( 'would ', 1844), ( 'much ',\\n1824), ( 'also ', 1757), ( 'characters ', 1735), ( 'get ', 1724), ( 'character ', 1703),\\n( 'two ', 1643), ( 'first ', 1588), ( 'see ', 1557), ( 'way ', 1515), ( 'well ', 1511), ( 'make ',\\n1418), ( 'really ', 1407), ( 'little ', 1351), ( 'life ', 1334), ( 'plot ', 1288), ( 'people ',\\n1269), ( 'could ', 1248), ( 'bad ', 1248), ( 'scene ', 1241), ( 'movies ', 1238), ( 'never ',\\n1201), ( 'best ', 1179), ( 'new ', 1140), ( 'scenes ', 1135), ( 'man ', 1131), ( 'many ', 1130),\\n( 'doesnt ', 1118), ( 'know ', 1092), ( 'dont ', 1086), ( 'hes ', 1024), ( 'great ', 1014),\\n( 'another ', 992), ( 'action ', 985), ( 'love ', 977), ( 'us ', 967), ( 'go ', 952),\\n( 'director ', 948), ( 'end ', 946), ( 'something ', 945), ( 'still ', 936)]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 173}, page_content='Listing 15.4: Example output of selecting a vocabulary for the dataset.\\nWe can step through the vocabulary and remove all words that have a low occurrence, such\\nas only being used once or twice in all reviews. For example, the following snippet will retrieve\\nonly the tokens that appear 2 or more times in all reviews.\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\\nListing 15.5: Example of ﬁltering the vocabulary by occurrence.\\nFinally, the vocabulary can be saved to a new ﬁle called vocab.txt that we can later load\\nand use to ﬁlter movie reviews prior to encoding them for modeling. We deﬁne a new function'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 174}, page_content=\"15.3. Data Preparation 158\\ncalled save list() that saves the vocabulary to ﬁle, with one word per line. For example:\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 15.6: Example of saving the ﬁltered vocabulary.\\nPulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom collections import Counter\\nfrom nltk.corpus import stopwords\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 174}, page_content=\"re_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load doc and add to vocab\\ndef add_doc_to_vocab(filename, vocab):\\n# load doc\\ndoc = load_doc(filename)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 175}, page_content=\"15.3. Data Preparation 159\\n# clean doc\\ntokens = clean_doc(doc)\\n# update counts\\nvocab.update(tokens)\\n# load all docs in a directory\\ndef process_docs(directory, vocab):\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# add doc to vocab\\nadd_doc_to_vocab(path, vocab)\\n# save list to file\\ndef save_list(lines, filename):\\n# convert lines to a single blob of text\\ndata = '\\\\n '.join(lines)\\n# open file\\nfile = open(filename, 'w ')\\n# write text\\nfile.write(data)\\n# close file\\nfile.close()\\n# define vocab\\nvocab = Counter()\\n# add all docs to vocab\\nprocess_docs( 'txt_sentoken/pos ', vocab)\\nprocess_docs( 'txt_sentoken/neg ', vocab)\\n# print the size of the vocab\\nprint(len(vocab))\\n# keep tokens with a min occurrence\\nmin_occurane = 2\\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\\nprint(len(tokens))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 175}, page_content=\"print(len(tokens))\\n# save tokens to a vocabulary file\\nsave_list(tokens, 'vocab.txt ')\\nListing 15.7: Example of ﬁltering the vocabulary for the dataset.\\nRunning the above example with this addition shows that the vocabulary size drops by a\\nlittle more than half its size, from 44,276 to 25,767 words.\\n25767\\nListing 15.8: Example output of ﬁltering the vocabulary by min occurrence.\\nRunning the min occurrence ﬁlter on the vocabulary and saving it to ﬁle, you should now\\nhave a new ﬁle called vocab.txt with only the words we are interested in. The order of words\\nin your ﬁle will diﬀer, but should look something like the following:\\naberdeen\\ndupe\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 176}, page_content='15.4. Train CNN With Embedding Layer 160\\nburt\\nlibido\\nhamlet\\narlene\\navailable\\ncorners\\nweb\\ncolumbia\\n...\\nListing 15.9: Sample of the vocabulary ﬁle vocab.txt .\\nWe are now ready to look at extracting features from the reviews ready for modeling.\\n15.4 Train CNN With Embedding Layer\\nIn this section, we will learn a word embedding while training a convolutional neural network on\\nthe classiﬁcation problem. A word embedding is a way of representing text where each word in\\nthe vocabulary is represented by a real valued vector in a high-dimensional space. The vectors\\nare learned in such a way that words that have similar meanings will have similar representation\\nin the vector space (close in the vector space). This is a more expressive representation for text\\nthan more classical methods like bag-of-words, where relationships between words or tokens are\\nignored, or forced in bigram and trigram approaches.\\nThe real valued vector representation for words can be learned while training the neural'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 176}, page_content=\"network. We can do this in the Keras deep learning library using the Embedding layer. The\\nﬁrst step is to load the vocabulary. We will use it to ﬁlter out words from movie reviews that\\nwe are not interested in. If you have worked through the previous section, you should have a\\nlocal ﬁle called vocab.txt with one word per line. We can load that ﬁle and build a vocabulary\\nas a set for checking the validity of tokens.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\nListing 15.10: Load vocabulary.\\nNext, we need to load all of the training data movie reviews. For that we can adapt the\\nprocess docs() from the previous section to load the documents, clean them, and return them\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 176}, page_content='as a list of strings, with one document per string. We want each document to be a string for\\neasy encoding as a sequence of integers later. Cleaning the document involves splitting each\\nreview based on white space, removing punctuation, and then ﬁltering out all tokens not in the\\nvocabulary. The updated clean doc() function is listed below.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 177}, page_content=\"15.4. Train CNN With Embedding Layer 161\\n# turn a doc into clean tokens\\ndef clean_doc(doc, vocab):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# filter out tokens not in vocab\\ntokens = [w for w in tokens if w in vocab]\\ntokens = ' '.join(tokens)\\nreturn tokens\\nListing 15.11: Function to load and ﬁlter a loaded review.\\nThe updated process docs() can then call the clean doc() for each document in a given\\ndirectory.\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 177}, page_content=\"path = directory + '/ '+ filename\\n# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc, vocab)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\nListing 15.12: Example to clean all movie reviews.\\nWe can call the process docs function for both the negandposdirectories and combine\\nthe reviews into a single train or test dataset. We also can deﬁne the class labels for the dataset.\\nThe load clean dataset() function below will load all reviews and prepare class labels for the\\ntraining or test dataset.\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\\nreturn docs, labels\\nListing 15.13: Function to load and clean all train or test movie reviews.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 178}, page_content='15.4. Train CNN With Embedding Layer 162\\nThe next step is to encode each document as a sequence of integers. The Keras Embedding\\nlayer requires integer inputs where each integer maps to a single token that has a speciﬁc\\nreal-valued vector representation within the embedding. These vectors are random at the\\nbeginning of training, but during training become meaningful to the network. We can encode\\nthe training documents as sequences of integers using the Tokenizer class in the Keras API.\\nFirst, we must construct an instance of the class then train it on all documents in the training\\ndataset. In this case, it develops a vocabulary of all tokens in the training dataset and develops\\na consistent mapping from words in the vocabulary to unique integers. We could just as easily\\ndevelop this mapping ourselves using our vocabulary ﬁle. The create tokenizer() function\\nbelow will prepare a Tokenizer from the training data.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 178}, page_content=\"tokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 15.14: Function to create a Tokenizer from training.\\nNow that the mapping of words to integers has been prepared, we can use it to encode the\\nreviews in the training dataset. We can do that by calling the texts tosequences() function\\non the Tokenizer . We also need to ensure that all documents have the same length. This is a\\nrequirement of Keras for eﬃcient computation. We could truncate reviews to the smallest size\\nor zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid. In this case,\\nwe will pad all reviews to the length of the longest review in the training dataset. First, we can\\nﬁnd the longest review using the max() function on the training dataset and take its length.\\nWe can then call the Keras function padsequences() to pad the sequences to the maximum\\nlength by adding 0 values on the end.\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 178}, page_content=\"Listing 15.15: Calculate the maximum movie review length.\\nWe can then use the maximum length as a parameter to a function to integer encode and\\npad the sequences.\\n# integer encode and pad documents\\ndef encode_docs(tokenizer, max_length, docs):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(docs)\\n# pad sequences\\npadded = pad_sequences(encoded, maxlen=max_length, padding= 'post ')\\nreturn padded\\nListing 15.16: Function to integer encode and pad movie reviews.\\nWe are now ready to deﬁne our neural network model. The model will use an Embedding\\nlayer as the ﬁrst hidden layer. The Embedding layer requires the speciﬁcation of the vocabulary\\nsize, the size of the real-valued vector space, and the maximum length of input documents. The\\nvocabulary size is the total number of words in our vocabulary, plus one for unknown words.\\nThis could be the vocab set length or the size of the vocab within the tokenizer used to integer\\nencode the documents, for example:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 179}, page_content=\"15.4. Train CNN With Embedding Layer 163\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\nListing 15.17: Calculate the size of the vocabulary for the Embedding layer.\\nWe will use a 100-dimensional vector space, but you could try other values, such as 50 or\\n150. Finally, the maximum document length was calculated above in the maxlength variable\\nused during padding. The complete model deﬁnition is listed below including the Embedding\\nlayer. We use a Convolutional Neural Network (CNN) as they have proven to be successful\\nat document classiﬁcation problems. A conservative CNN conﬁguration is used with 32 ﬁlters\\n(parallel ﬁelds for processing words) and a kernel size of 8 with a rectiﬁed linear ( relu ) activation\\nfunction. This is followed by a pooling layer that reduces the output of the convolutional layer\\nby half.\\nNext, the 2D output from the CNN part of the model is ﬂattened to one long 2D vector to\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 179}, page_content=\"represent the features extracted by the CNN. The back-end of the model is a standard Multilayer\\nPerceptron layers to interpret the CNN features. The output layer uses a sigmoid activation\\nfunction to output a value between 0 and 1 for the negative and positive sentiment in the review.\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 100, input_length=max_length))\\nmodel.add(Conv1D(filters=32, kernel_size=8, activation= 'relu '))\\nmodel.add(MaxPooling1D(pool_size=2))\\nmodel.add(Flatten())\\nmodel.add(Dense(10, activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 15.18: Deﬁne a CNN model with the Embedding Layer.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 179}, page_content='Running just this piece provides a summary of the deﬁned network. We can see that the\\nEmbedding layer expects documents with a length of 1,317 words as input and encodes each\\nword in the document as a 100 element vector.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 1317, 100) 2576800\\n_________________________________________________________________\\nconv1d_1 (Conv1D) (None, 1310, 32) 25632\\n_________________________________________________________________\\nmax_pooling1d_1 (MaxPooling1 (None, 655, 32) 0\\n_________________________________________________________________\\nflatten_1 (Flatten) (None, 20960) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 10) 209610\\n_________________________________________________________________'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 180}, page_content='15.4. Train CNN With Embedding Layer 164\\ndense_2 (Dense) (None, 1) 11\\n=================================================================\\nTotal params: 2,812,053\\nTrainable params: 2,812,053\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 15.19: Summary of the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\\nFigure 15.1: Plot of the deﬁned CNN classiﬁcation model.\\nNext, we ﬁt the network on the training data. We use a binary cross entropy loss function\\nbecause the problem we are learning is a binary classiﬁcation problem. The eﬃcient Adam\\nimplementation of stochastic gradient descent is used and we keep track of accuracy in addition\\nto loss during training. The model is trained for 10 epochs, or 10 passes through the training\\ndata. The network conﬁguration and training schedule were found with a little trial and error,'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 180}, page_content=\"but are by no means optimal for this problem. If you can get better results with a diﬀerent\\nconﬁguration, let me know.\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\nListing 15.20: Train the deﬁned classiﬁcation model.\\nAfter the model is ﬁt, it is saved to a ﬁle named model.h5 for later evaluation.\\n# save the model\\nmodel.save( 'model.h5 ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 181}, page_content=\"15.4. Train CNN With Embedding Layer 165\\nListing 15.21: Save the ﬁt model to ﬁle.\\nWe can tie all of this together. The complete code listing is provided below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import Embedding\\nfrom keras.layers.convolutional import Conv1D\\nfrom keras.layers.convolutional import MaxPooling1D\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc, vocab):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 181}, page_content=\"re_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# filter out tokens not in vocab\\ntokens = [w for w in tokens if w in vocab]\\ntokens = ' '.join(tokens)\\nreturn tokens\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load the doc\\ndoc = load_doc(path)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 182}, page_content=\"15.4. Train CNN With Embedding Layer 166\\n# clean doc\\ntokens = clean_doc(doc, vocab)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\\nreturn docs, labels\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# integer encode and pad documents\\ndef encode_docs(tokenizer, max_length, docs):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(docs)\\n# pad sequences\\npadded = pad_sequences(encoded, maxlen=max_length, padding= 'post ')\\nreturn padded\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 100, input_length=max_length))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 182}, page_content=\"model.add(Conv1D(filters=32, kernel_size=8, activation= 'relu '))\\nmodel.add(MaxPooling1D(pool_size=2))\\nmodel.add(Flatten())\\nmodel.add(Dense(10, activation= 'relu '))\\nmodel.add(Dense(1, activation= 'sigmoid '))\\n# compile network\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load training data\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 183}, page_content=\"15.5. Evaluate Model 167\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# calculate the maximum sequence length\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\\n# encode data\\nXtrain = encode_docs(tokenizer, max_length, train_docs)\\n# define model\\nmodel = define_model(vocab_size, max_length)\\n# fit network\\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\\n# save the model\\nmodel.save( 'model.h5 ')\\nListing 15.22: Complete example of ﬁtting a CNN model with an Embedding input layer.\\nRunning the example will ﬁrst provide a summary of the training dataset vocabulary (25,768)\\nand maximum input sequence length in words (1,317). The example should run in a few minutes\\nand the ﬁt model will be saved to ﬁle.\\n...\\nVocabulary size: 25768\\nMaximum length: 1317\\nEpoch 1/10\\n8s - loss: 0.6927 - acc: 0.4800\\nEpoch 2/10\\n7s - loss: 0.6610 - acc: 0.5922\\nEpoch 3/10\\n7s - loss: 0.3461 - acc: 0.8844\\nEpoch 4/10\\n7s - loss: 0.0441 - acc: 0.9889\\nEpoch 5/10\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 183}, page_content='Epoch 5/10\\n7s - loss: 0.0058 - acc: 1.0000\\nEpoch 6/10\\n7s - loss: 0.0024 - acc: 1.0000\\nEpoch 7/10\\n7s - loss: 0.0015 - acc: 1.0000\\nEpoch 8/10\\n7s - loss: 0.0011 - acc: 1.0000\\nEpoch 9/10\\n7s - loss: 8.0111e-04 - acc: 1.0000\\nEpoch 10/10\\n7s - loss: 5.4109e-04 - acc: 1.0000\\nListing 15.23: Example output from ﬁtting the model.\\n15.5 Evaluate Model\\nIn this section, we will evaluate the trained model and use it to make predictions on new data.\\nFirst, we can use the built-in evaluate() function to estimate the skill of the model on both\\nthe training and test dataset. This requires that we load and encode both the training and test\\ndatasets.\\n# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 184}, page_content=\"15.5. Evaluate Model 168\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# calculate the maximum sequence length\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\\n# encode data\\nXtrain = encode_docs(tokenizer, max_length, train_docs)\\nXtest = encode_docs(tokenizer, max_length, test_docs)\\nListing 15.24: Load and encode both training and test datasets.\\nWe can then load the model and evaluate it on both datasets and print the accuracy.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# evaluate model on training dataset\\n_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\\nprint( 'Train Accuracy: %f '% (acc*100))\\n# evaluate model on test dataset\\n_, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %f '% (acc*100))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 184}, page_content=\"Listing 15.25: Load and evaluate model on both train and test datasets.\\nNew data must then be prepared using the same text encoding and encoding schemes as was\\nused on the training dataset. Once prepared, a prediction can be made by calling the predict()\\nfunction on the model. The function below named predict sentiment() will encode and pad\\na given movie review text and return a prediction in terms of both the percentage and a label.\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, max_length, model):\\n# clean review\\nline = clean_doc(review, vocab)\\n# encode and pad review\\npadded = encode_docs(tokenizer, max_length, [line])\\n# predict sentiment\\nyhat = model.predict(padded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\nListing 15.26: Function to predict the sentiment for an ad hoc movie review.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 184}, page_content='We can test out this model with two ad hoc movie reviews. The complete example is listed\\nbelow.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 185}, page_content=\"15.5. Evaluate Model 169\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc, vocab):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# filter out tokens not in vocab\\ntokens = [w for w in tokens if w in vocab]\\ntokens = ' '.join(tokens)\\nreturn tokens\\n# load all docs in a directory\\ndef process_docs(directory, vocab, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 185}, page_content=\"continue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc, vocab)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\n# load and clean a dataset\\ndef load_clean_dataset(vocab, is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', vocab, is_train)\\npos = process_docs( 'txt_sentoken/pos ', vocab, is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\\nreturn docs, labels\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 186}, page_content=\"15.5. Evaluate Model 170\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# integer encode and pad documents\\ndef encode_docs(tokenizer, max_length, docs):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(docs)\\n# pad sequences\\npadded = pad_sequences(encoded, maxlen=max_length, padding= 'post ')\\nreturn padded\\n# classify a review as negative or positive\\ndef predict_sentiment(review, vocab, tokenizer, max_length, model):\\n# clean review\\nline = clean_doc(review, vocab)\\n# encode and pad review\\npadded = encode_docs(tokenizer, max_length, [line])\\n# predict sentiment\\nyhat = model.predict(padded, verbose=0)\\n# retrieve predicted percentage and label\\npercent_pos = yhat[0,0]\\nif round(percent_pos) == 0:\\nreturn (1-percent_pos), 'NEGATIVE '\\nreturn percent_pos, 'POSITIVE '\\n# load the vocabulary\\nvocab_filename = 'vocab.txt '\\nvocab = load_doc(vocab_filename)\\nvocab = set(vocab.split())\\n# load all reviews\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 186}, page_content=\"# load all reviews\\ntrain_docs, ytrain = load_clean_dataset(vocab, True)\\ntest_docs, ytest = load_clean_dataset(vocab, False)\\n# create the tokenizer\\ntokenizer = create_tokenizer(train_docs)\\n# define vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# calculate the maximum sequence length\\nmax_length = max([len(s.split()) for s in train_docs])\\nprint( 'Maximum length: %d '% max_length)\\n# encode data\\nXtrain = encode_docs(tokenizer, max_length, train_docs)\\nXtest = encode_docs(tokenizer, max_length, test_docs)\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# evaluate model on training dataset\\n_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\\nprint( 'Train Accuracy: %.2f '% (acc*100))\\n# evaluate model on test dataset\\n_, acc = model.evaluate(Xtest, ytest, verbose=0)\\nprint( 'Test Accuracy: %.2f '% (acc*100))\\n# test positive text\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 187}, page_content=\"15.6. Extensions 171\\ntext = 'Everyone will enjoy this film. I love it, recommended! '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\n# test negative text\\ntext = 'This is a bad movie. Do not watch it. It sucks. '\\npercent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\\nprint( 'Review: [%s]\\\\nSentiment: %s (%.3f%%) '% (text, sentiment, percent*100))\\nListing 15.27: Complete example of making a prediction on new text data.\\nRunning the example ﬁrst prints the skill of the model on the training and test dataset. We\\ncan see that the model achieves 100% accuracy on the training dataset and 87.5% on the test\\ndataset, an impressive score.\\nNext, we can see that the model makes the correct prediction on two contrived movie reviews.\\nWe can see that the percentage or conﬁdence of the prediction is close to 50% for both, this\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 187}, page_content='may be because the two contrived reviews are very short and the model is expecting sequences\\nof 1,000 or more words.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nTrain Accuracy: 100.00\\nTest Accuracy: 87.50\\nReview: [Everyone will enjoy this film. I love it, recommended!]\\nSentiment: POSITIVE (55.431%)\\nReview: [This is a bad movie. Do not watch it. It sucks.]\\nSentiment: NEGATIVE (54.746%)\\nListing 15.28: Example output from making a prediction on new reviews.\\n15.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Data Cleaning . Explore better data cleaning, perhaps leaving some punctuation in tact\\nor normalizing contractions.\\n\\x88Truncated Sequences . Padding all sequences to the length of the longest sequence\\nmight be extreme if the longest sequence is very diﬀerent to all other reviews. Study the'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 187}, page_content='distribution of review lengths and truncate reviews to a mean length.\\n\\x88Truncated Vocabulary . We removed infrequently occurring words, but still had a large\\nvocabulary of more than 25,000 words. Explore further reducing the size of the vocabulary\\nand the eﬀect on model skill.\\n\\x88Filters and Kernel Size . The number of ﬁlters and kernel size are important to model\\nskill and were not tuned. Explore tuning these two CNN parameters.\\n\\x88Epochs and Batch Size . The model appears to ﬁt the training dataset quickly. Explore\\nalternate conﬁgurations of the number of training epochs and batch size and use the test\\ndataset as a validation set to pick a better stopping point for training the model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 188}, page_content='15.7. Further Reading 172\\n\\x88Deeper Network . Explore whether a deeper network results in better skill, either in\\nterms of CNN layers, MLP layers and both.\\n\\x88Pre-Train an Embedding . Explore pre-training a Word2Vec word embedding in the\\nmodel and the impact on model skill with and without further ﬁne tuning during training.\\n\\x88Use GloVe Embedding . Explore loading the pre-trained GloVe embedding and the\\nimpact on model skill with and without further ﬁne tuning during training.\\n\\x88Longer Test Reviews . Explore whether the skill of model predictions is dependent on\\nthe length of movie reviews as suspected in the ﬁnal section on evaluating the model.\\n\\x88Train Final Model . Train a ﬁnal model on all available data and use it make predictions\\non real ad hoc movie reviews from the internet.\\nIf you explore any of these extensions, I’d love to know.\\n15.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n15.7.1 Dataset\\n\\x88Movie Review Data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 188}, page_content='\\x88Movie Review Data.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\n\\x88A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based\\non Minimum Cuts , 2004.\\nhttp://xxx.lanl.gov/abs/cs/0409058\\n\\x88Movie Review Polarity Dataset.\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\n15.7.2 APIs\\n\\x88collections API - Container datatypes.\\nhttps://docs.python.org/3/library/collections.html\\n\\x88Tokenizer Keras API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n\\x88Embedding Keras API.\\nhttps://keras.io/layers/embeddings/'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 189}, page_content='15.8. Summary 173\\n15.8 Summary\\nIn this tutorial, you discovered how to develop word embeddings for the classiﬁcation of movie\\nreviews. Speciﬁcally, you learned:\\n\\x88How to prepare movie review text data for classiﬁcation with deep learning methods.\\n\\x88How to develop a neural classiﬁcation model with word embedding and convolutional\\nlayers.\\n\\x88How to evaluate the developed a neural classiﬁcation model.\\n15.8.1 Next\\nIn the next chapter, you will discover how you can develop an n-gram multichannel convolutional\\nneural network for text classiﬁcation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 190}, page_content='Chapter 16\\nProject: Develop an n-gram CNN\\nModel for Sentiment Analysis\\nA standard deep learning model for text classiﬁcation and sentiment analysis uses a word\\nembedding layer and one-dimensional convolutional neural network. The model can be expanded\\nby using multiple parallel convolutional neural networks that read the source document using\\ndiﬀerent kernel sizes. This, in eﬀect, creates a multichannel convolutional neural network for\\ntext that reads text with diﬀerent n-gram sizes (groups of words). In this tutorial, you will\\ndiscover how to develop a multichannel convolutional neural network for sentiment prediction\\non text movie review data. After completing this tutorial, you will know:\\n\\x88How to prepare movie review text data for modeling.\\n\\x88How to develop a multichannel convolutional neural network for text in Keras.\\n\\x88How to evaluate a ﬁt model on unseen movie review data.\\nLet’s get started.\\n16.1 Tutorial Overview\\nThis tutorial is divided into the following parts:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 190}, page_content='1. Movie Review Dataset.\\n2. Data Preparation.\\n3. Develop Multichannel Model.\\n4. Evaluate Model.\\n16.2 Movie Review Dataset\\nIn this tutorial, we will use the Movie Review Dataset. This dataset designed for sentiment\\nanalysis was described previously in Chapter 9. You can download the dataset from here:\\n174'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 191}, page_content='16.3. Data Preparation 175\\n\\x88Movie Review Polarity Dataset ( review polarity.tar.gz , 3MB).\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.\\ngz\\nAfter unzipping the ﬁle, you will have a directory called txtsentoken with two sub-\\ndirectories containing the text negandposfor negative and positive reviews. Reviews are stored\\none per ﬁle with a naming convention cv000 tocv999 for each of negand pos.\\n16.3 Data Preparation\\nNote : The preparation of the movie review dataset was ﬁrst described in Chapter 9. In this\\nsection, we will look at 3 things:\\n1. Separation of data into training and test sets.\\n2. Loading and cleaning the data to remove punctuation and numbers.\\n3. Clean All Reviews and Save.\\n16.3.1 Split into Train and Test Sets\\nWe are pretending that we are developing a system that can predict the sentiment of a textual\\nmovie review as either positive or negative. This means that after the model is developed, we'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 191}, page_content='will need to make predictions on new textual reviews. This will require all of the same data\\npreparation to be performed on those new reviews as is performed on the training data for the\\nmodel. We will ensure that this constraint is built into the evaluation of our models by splitting\\nthe training and test datasets prior to any data preparation. This means that any knowledge in\\nthe data in the test set that could help us better prepare the data (e.g. the words used) are\\nunavailable in the preparation of data used for training the model.\\nThat being said, we will use the last 100 positive reviews and the last 100 negative reviews\\nas a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a\\n90% train, 10% split of the data. The split can be imposed easily by using the ﬁlenames of the\\nreviews where reviews named 000 to 899 are for training data and reviews named 900 onwards\\nare for test.\\n16.3.2 Loading and Cleaning Reviews'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 191}, page_content='The text data is already pretty clean; not much preparation is required. Without getting bogged\\ndown too much in the details, we will prepare the data using the following way:\\n\\x88Split tokens on white space.\\n\\x88Remove all punctuation from words.\\n\\x88Remove all words that are not purely comprised of alphabetical characters.\\n\\x88Remove all words that are known stop words.\\n\\x88Remove all words that have a length ≤1 character.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 192}, page_content=\"16.3. Data Preparation 176\\nWe can put all of these steps into a function called clean doc() that takes as an argument\\nthe raw text loaded from a ﬁle and returns a list of cleaned tokens. We can also deﬁne a function\\nload doc() that loads a document from ﬁle ready for use with the clean doc() function. An\\nexample of cleaning the ﬁrst positive review is listed below.\\nfrom nltk.corpus import stopwords\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 192}, page_content=\"# filter out stop words\\nstop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\nreturn tokens\\n# load the document\\nfilename = 'txt_sentoken/pos/cv000_29590.txt '\\ntext = load_doc(filename)\\ntokens = clean_doc(text)\\nprint(tokens)\\nListing 16.1: Example of cleaning a movie review.\\nRunning the example prints a long list of clean tokens. There are many more cleaning steps\\nwe may want to explore and I leave them as further exercises.\\n...\\n'creepy ', 'place ', 'even ', 'acting ', 'hell ', 'solid ', 'dreamy ', 'depp ', 'turning ',\\n'typically ', 'strong ', 'performance ', 'deftly ', 'handling ', 'british ', 'accent ',\\n'ians ', 'holm ', 'joe ', 'goulds ', 'secret ', 'richardson ', 'dalmatians ', 'log ', 'great ',\\n'supporting ', 'roles ', 'big ', 'surprise ', 'graham ', 'cringed ', 'first ', 'time ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 192}, page_content=\"'opened ', 'mouth ', 'imagining ', 'attempt ', 'irish ', 'accent ', 'actually ', 'wasnt ',\\n'half ', 'bad ', 'film ', 'however ', 'good ', 'strong ', 'violencegore ', 'sexuality ',\\n'language ', 'drug ', 'content ']\\nListing 16.2: Example output of cleaning a movie review.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 193}, page_content=\"16.3. Data Preparation 177\\n16.3.3 Clean All Reviews and Save\\nWe can now use the function to clean reviews and apply it to all reviews. To do this, we will\\ndevelop a new function named process docs() below that will walk through all reviews in a\\ndirectory, clean them, and return them as a list. We will also add an argument to the function\\nto indicate whether the function is processing train or test reviews, that way the ﬁlenames can\\nbe ﬁltered (as described above) and only those train or test reviews requested will be cleaned\\nand returned. The full function is listed below.\\n# load all docs in a directory\\ndef process_docs(directory, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\\n# load the doc\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 193}, page_content=\"# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\nListing 16.3: Function for cleaning multiple review documents.\\nWe can call this function with negative training reviews. We also need labels for the train\\nand test documents. We know that we have 900 training documents and 100 test documents.\\nWe can use a Python list comprehension to create the labels for the negative (0) and positive\\n(1) reviews for both train and test sets. The function below named load clean dataset() will\\nload and clean the movie review text and also create the labels for the reviews.\\n# load and clean a dataset\\ndef load_clean_dataset(is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', is_train)\\npos = process_docs( 'txt_sentoken/pos ', is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 193}, page_content='return docs, labels\\nListing 16.4: Function to prepare reviews and labels for a dataset.\\nFinally, we want to save the prepared train and test sets to ﬁle so that we can load them\\nlater for modeling and model evaluation. The function below-named save dataset() will save\\na given prepared dataset ( Xandyelements) to a ﬁle using the pickle API (this is the standard\\nAPI for saving objects in Python).\\n# save a dataset to file\\ndef save_dataset(dataset, filename):'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 194}, page_content=\"16.3. Data Preparation 178\\ndump(dataset, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\nListing 16.5: Function for saving clean documents to ﬁle.\\n16.3.4 Complete Example\\nWe can tie all of these data preparation steps together. The complete example is listed below.\\nimport string\\nimport re\\nfrom os import listdir\\nfrom nltk.corpus import stopwords\\nfrom pickle import dump\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# filter out stop words\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 194}, page_content=\"stop_words = set(stopwords.words( 'english '))\\ntokens = [w for w in tokens if not w in stop_words]\\n# filter out short tokens\\ntokens = [word for word in tokens if len(word) > 1]\\ntokens = ' '.join(tokens)\\nreturn tokens\\n# load all docs in a directory\\ndef process_docs(directory, is_train):\\ndocuments = list()\\n# walk through all files in the folder\\nfor filename in listdir(directory):\\n# skip any reviews in the test set\\nif is_train and filename.startswith( 'cv9 '):\\ncontinue\\nif not is_train and not filename.startswith( 'cv9 '):\\ncontinue\\n# create the full path of the file to open\\npath = directory + '/ '+ filename\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 195}, page_content=\"16.4. Develop Multichannel Model 179\\n# load the doc\\ndoc = load_doc(path)\\n# clean doc\\ntokens = clean_doc(doc)\\n# add to list\\ndocuments.append(tokens)\\nreturn documents\\n# load and clean a dataset\\ndef load_clean_dataset(is_train):\\n# load documents\\nneg = process_docs( 'txt_sentoken/neg ', is_train)\\npos = process_docs( 'txt_sentoken/pos ', is_train)\\ndocs = neg + pos\\n# prepare labels\\nlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\\nreturn docs, labels\\n# save a dataset to file\\ndef save_dataset(dataset, filename):\\ndump(dataset, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\n# load and clean all reviews\\ntrain_docs, ytrain = load_clean_dataset(True)\\ntest_docs, ytest = load_clean_dataset(False)\\n# save training datasets\\nsave_dataset([train_docs, ytrain], 'train.pkl ')\\nsave_dataset([test_docs, ytest], 'test.pkl ')\\nListing 16.6: Complete example of cleaning and saving all movie reviews.\\nRunning the example cleans the text movie review documents, creates labels, and saves the\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 195}, page_content='prepared data for both train and test datasets in train.pkl andtest.pkl respectively. Now\\nwe are ready to develop our model.\\n16.4 Develop Multichannel Model\\nIn this section, we will develop a multichannel convolutional neural network for the sentiment\\nanalysis prediction problem. This section is divided into 3 parts:\\n1. Encode Data\\n2. Deﬁne Model.\\n3. Complete Example.\\n16.4.1 Encode Data\\nThe ﬁrst step is to load the cleaned training dataset. The function below-named load dataset()\\ncan be called to load the pickled training dataset.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 196}, page_content=\"16.4. Develop Multichannel Model 180\\n# load a clean dataset\\ndef load_dataset(filename):\\nreturn load(open(filename, 'rb '))\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\nListing 16.7: Example of loading the cleaned and saved reviews.\\nNext, we must ﬁt a Keras Tokenizer on the training dataset. We will use this tokenizer to\\nboth deﬁne the vocabulary for the Embedding layer and encode the review documents as integers.\\nThe function create tokenizer() below will create a Tokenizer given a list of documents.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 16.8: Function for creating a Tokenizer .\\nWe also need to know the maximum length of input sequences as input for the model and\\nto pad all sequences to the ﬁxed length. The function maxlength() below will calculate the\\nmaximum length (number of words) for all reviews in the training dataset.\\n# calculate the maximum document length\\ndef max_length(lines):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 196}, page_content=\"return max([len(s.split()) for s in lines])\\nListing 16.9: Function to calculate the maximum movie review length.\\nWe also need to know the size of the vocabulary for the Embedding layer. This can be\\ncalculated from the prepared Tokenizer , as follows:\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nListing 16.10: Calculate the size of the vocabulary.\\nFinally, we can integer encode and pad the clean movie review text. The function below\\nnamed encode text() will both encode and pad text data to the maximum review length.\\n# encode a list of lines\\ndef encode_text(tokenizer, lines, length):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(lines)\\n# pad encoded sequences\\npadded = pad_sequences(encoded, maxlen=length, padding= 'post ')\\nreturn padded\\nListing 16.11: Function to encode and pad movie review text.\\n16.4.2 Deﬁne Model\\nA standard model for document classiﬁcation is to use an Embedding layer as input, followed by\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 196}, page_content='a one-dimensional convolutional neural network, pooling layer, and then a prediction output\\nlayer. The kernel size in the convolutional layer deﬁnes the number of words to consider as'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 197}, page_content='16.4. Develop Multichannel Model 181\\nthe convolution is passed across the input text document, providing a grouping parameter. A\\nmulti-channel convolutional neural network for document classiﬁcation involves using multiple\\nversions of the standard model with diﬀerent sized kernels. This allows the document to be\\nprocessed at diﬀerent resolutions or diﬀerent n-grams (groups of words) at a time, whilst the\\nmodel learns how to best integrate these interpretations.\\nThis approach was ﬁrst described by Yoon Kim in his 2014 paper titled Convolutional Neural\\nNetworks for Sentence Classiﬁcation . In the paper, Kim experimented with static and dynamic\\n(updated) embedding layers, we can simplify the approach and instead focus only on the use of\\ndiﬀerent kernel sizes. This approach is best understood with a diagram taken from Kim’s paper,\\nsee Chapter 14.\\nIn Keras, a multiple-input model can be deﬁned using the functional API. We will deﬁne a'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 197}, page_content='model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review\\ntext. Each channel is comprised of the following elements:\\n\\x88Input layer that deﬁnes the length of input sequences.\\n\\x88Embedding layer set to the size of the vocabulary and 100-dimensional real-valued repre-\\nsentations.\\n\\x88Conv1D layer with 32 ﬁlters and a kernel size set to the number of words to read at once.\\n\\x88MaxPooling1D layer to consolidate the output from the convolutional layer.\\n\\x88Flatten layer to reduce the three-dimensional output to two dimensional for concatenation.\\nThe output from the three channels are concatenated into a single vector and process by a\\nDense layer and an output layer. The function below deﬁnes and returns the model. As part of\\ndeﬁning the model, a summary of the deﬁned model is printed and a plot of the model graph is\\ncreated and saved to ﬁle.\\n# define the model\\ndef define_model(length, vocab_size):\\n# channel 1\\ninputs1 = Input(shape=(length,))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 197}, page_content=\"embedding1 = Embedding(vocab_size, 100)(inputs1)\\nconv1 = Conv1D(filters=32, kernel_size=4, activation= 'relu ')(embedding1)\\ndrop1 = Dropout(0.5)(conv1)\\npool1 = MaxPooling1D(pool_size=2)(drop1)\\nflat1 = Flatten()(pool1)\\n# channel 2\\ninputs2 = Input(shape=(length,))\\nembedding2 = Embedding(vocab_size, 100)(inputs2)\\nconv2 = Conv1D(filters=32, kernel_size=6, activation= 'relu ')(embedding2)\\ndrop2 = Dropout(0.5)(conv2)\\npool2 = MaxPooling1D(pool_size=2)(drop2)\\nflat2 = Flatten()(pool2)\\n# channel 3\\ninputs3 = Input(shape=(length,))\\nembedding3 = Embedding(vocab_size, 100)(inputs3)\\nconv3 = Conv1D(filters=32, kernel_size=8, activation= 'relu ')(embedding3)\\ndrop3 = Dropout(0.5)(conv3)\\npool3 = MaxPooling1D(pool_size=2)(drop3)\\nflat3 = Flatten()(pool3)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 198}, page_content=\"16.4. Develop Multichannel Model 182\\n# merge\\nmerged = concatenate([flat1, flat2, flat3])\\n# interpretation\\ndense1 = Dense(10, activation= 'relu ')(merged)\\noutputs = Dense(1, activation= 'sigmoid ')(dense1)\\nmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\\n# compile\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize\\nmodel.summary()\\nplot_model(model, show_shapes=True, to_file= 'multichannel.png ')\\nreturn model\\nListing 16.12: Function for deﬁning the classiﬁcation model.\\n16.4.3 Complete Example\\nPulling all of this together, the complete example is listed below.\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import Dropout\\nfrom keras.layers import Embedding\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 198}, page_content=\"from keras.layers.convolutional import Conv1D\\nfrom keras.layers.convolutional import MaxPooling1D\\nfrom keras.layers.merge import concatenate\\n# load a clean dataset\\ndef load_dataset(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the maximum document length\\ndef max_length(lines):\\nreturn max([len(s.split()) for s in lines])\\n# encode a list of lines\\ndef encode_text(tokenizer, lines, length):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(lines)\\n# pad encoded sequences\\npadded = pad_sequences(encoded, maxlen=length, padding= 'post ')\\nreturn padded\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 199}, page_content=\"16.4. Develop Multichannel Model 183\\n# define the model\\ndef define_model(length, vocab_size):\\n# channel 1\\ninputs1 = Input(shape=(length,))\\nembedding1 = Embedding(vocab_size, 100)(inputs1)\\nconv1 = Conv1D(filters=32, kernel_size=4, activation= 'relu ')(embedding1)\\ndrop1 = Dropout(0.5)(conv1)\\npool1 = MaxPooling1D(pool_size=2)(drop1)\\nflat1 = Flatten()(pool1)\\n# channel 2\\ninputs2 = Input(shape=(length,))\\nembedding2 = Embedding(vocab_size, 100)(inputs2)\\nconv2 = Conv1D(filters=32, kernel_size=6, activation= 'relu ')(embedding2)\\ndrop2 = Dropout(0.5)(conv2)\\npool2 = MaxPooling1D(pool_size=2)(drop2)\\nflat2 = Flatten()(pool2)\\n# channel 3\\ninputs3 = Input(shape=(length,))\\nembedding3 = Embedding(vocab_size, 100)(inputs3)\\nconv3 = Conv1D(filters=32, kernel_size=8, activation= 'relu ')(embedding3)\\ndrop3 = Dropout(0.5)(conv3)\\npool3 = MaxPooling1D(pool_size=2)(drop3)\\nflat3 = Flatten()(pool3)\\n# merge\\nmerged = concatenate([flat1, flat2, flat3])\\n# interpretation\\ndense1 = Dense(10, activation= 'relu ')(merged)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 199}, page_content=\"outputs = Dense(1, activation= 'sigmoid ')(dense1)\\nmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\\n# compile\\nmodel.compile(loss= 'binary_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize\\nmodel.summary()\\nplot_model(model, show_shapes=True, to_file= 'model.png ')\\nreturn model\\n# load training dataset\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\n# create tokenizer\\ntokenizer = create_tokenizer(trainLines)\\n# calculate max document length\\nlength = max_length(trainLines)\\nprint( 'Max document length: %d '% length)\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# encode data\\ntrainX = encode_text(tokenizer, trainLines, length)\\n# define model\\nmodel = define_model(length, vocab_size)\\n# fit model\\nmodel.fit([trainX,trainX,trainX], trainLabels, epochs=7, batch_size=16)\\n# save the model\\nmodel.save( 'model.h5 ')\\nListing 16.13: Complete example of ﬁtting the n-gram CNN model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 200}, page_content='16.4. Develop Multichannel Model 184\\nRunning the example ﬁrst prints a summary of the prepared training dataset.\\nMax document length: 1380\\nVocabulary size: 44277\\nListing 16.14: Example output from preparing the training data.\\nThe model is ﬁt relatively quickly and appears to show good skill on the training dataset.\\n...\\nEpoch 3/7\\n1800/1800 [==============================] - 29s - loss: 0.0460 - acc: 0.9894\\nEpoch 4/7\\n1800/1800 [==============================] - 30s - loss: 0.0041 - acc: 1.0000\\nEpoch 5/7\\n1800/1800 [==============================] - 31s - loss: 0.0010 - acc: 1.0000\\nEpoch 6/7\\n1800/1800 [==============================] - 30s - loss: 3.0271e-04 - acc: 1.0000\\nEpoch 7/7\\n1800/1800 [==============================] - 28s - loss: 1.3875e-04 - acc: 1.0000\\nListing 16.15: Example output from ﬁtting the model.\\nA plot of the deﬁned model is saved to ﬁle, clearly showing the three input channels for the\\nmodel.\\nFigure 16.1: Plot of the Multichannel Convolutional Neural Network For Text.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 200}, page_content='The model is ﬁt for a number of epochs and saved to the ﬁle model.h5 for later evaluation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 201}, page_content=\"16.5. Evaluate Model 185\\n16.5 Evaluate Model\\nIn this section, we can evaluate the ﬁt model by predicting the sentiment on all reviews in the\\nunseen test dataset. Using the data loading functions developed in the previous section, we can\\nload and encode both the training and test datasets.\\n# load datasets\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\ntestLines, testLabels = load_dataset( 'test.pkl ')\\n# create tokenizer\\ntokenizer = create_tokenizer(trainLines)\\n# calculate max document length\\nlength = max_length(trainLines)\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Max document length: %d '% length)\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# encode data\\ntrainX = encode_text(tokenizer, trainLines, length)\\ntestX = encode_text(tokenizer, testLines, length)\\nprint(trainX.shape, testX.shape)\\nListing 16.16: Prepare train and test data for evaluating the model.\\nWe can load the saved model and evaluate it on both the training and test datasets. The\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 201}, page_content=\"complete example is listed below.\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\\n# load a clean dataset\\ndef load_dataset(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the maximum document length\\ndef max_length(lines):\\nreturn max([len(s.split()) for s in lines])\\n# encode a list of lines\\ndef encode_text(tokenizer, lines, length):\\n# integer encode\\nencoded = tokenizer.texts_to_sequences(lines)\\n# pad encoded sequences\\npadded = pad_sequences(encoded, maxlen=length, padding= 'post ')\\nreturn padded\\n# load datasets\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 202}, page_content=\"16.6. Extensions 186\\ntrainLines, trainLabels = load_dataset( 'train.pkl ')\\ntestLines, testLabels = load_dataset( 'test.pkl ')\\n# create tokenizer\\ntokenizer = create_tokenizer(trainLines)\\n# calculate max document length\\nlength = max_length(trainLines)\\nprint( 'Max document length: %d '% length)\\n# calculate vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary size: %d '% vocab_size)\\n# encode data\\ntrainX = encode_text(tokenizer, trainLines, length)\\ntestX = encode_text(tokenizer, testLines, length)\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# evaluate model on training dataset\\n_, acc = model.evaluate([trainX,trainX,trainX], trainLabels, verbose=0)\\nprint( 'Train Accuracy: %.2f '% (acc*100))\\n# evaluate model on test dataset dataset\\n_, acc = model.evaluate([testX,testX,testX], testLabels, verbose=0)\\nprint( 'Test Accuracy: %.2f '% (acc*100))\\nListing 16.17: Complete example of evaluating the ﬁt model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 202}, page_content='Running the example prints the skill of the model on both the training and test datasets. We\\ncan see that, as expected, the skill on the training dataset is excellent, here at 100% accuracy.\\nWe can also see that the skill of the model on the unseen test dataset is also very impressive,\\nachieving 88.5%, which is above the skill of the model reported in the 2014 paper (although not\\na direct apples-to-apples comparison).\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nTrain Accuracy: 100.00\\nTest Accuracy: 88.50\\nListing 16.18: Example output from evaluating the ﬁt model.\\n16.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Diﬀerent n-grams . Explore the model by changing the kernel size (number of n-grams)\\nused by the channels in the model to see how it impacts model skill.\\n\\x88More or Fewer Channels . Explore using more or fewer channels in the model and see'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 202}, page_content='how it impacts model skill.\\n\\x88Shared Embedding . Explore conﬁgurations where each channel shares the same word\\nembedding and report on the impact on model skill.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 203}, page_content='16.7. Further Reading 187\\n\\x88Deeper Network . Convolutional neural networks perform better in computer vision\\nwhen they are deeper. Explore using deeper models here and see how it impacts model\\nskill.\\n\\x88Truncated Sequences . Padding all sequences to the length of the longest sequence\\nmight be extreme if the longest sequence is very diﬀerent to all other reviews. Study the\\ndistribution of review lengths and truncate reviews to a mean length.\\n\\x88Truncated Vocabulary . We removed infrequently occurring words, but still had a large\\nvocabulary of more than 25,000 words. Explore further reducing the size of the vocabulary\\nand the eﬀect on model skill.\\n\\x88Epochs and Batch Size . The model appears to ﬁt the training dataset quickly. Explore\\nalternate conﬁgurations of the number of training epochs and batch size and use the test\\ndataset as a validation set to pick a better stopping point for training the model.\\n\\x88Pre-Train an Embedding . Explore pre-training a Word2Vec word embedding in the'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 203}, page_content='model and the impact on model skill with and without further ﬁne tuning during training.\\n\\x88Use GloVe Embedding . Explore loading the pre-trained GloVe embedding and the\\nimpact on model skill with and without further ﬁne tuning during training.\\n\\x88Train Final Model . Train a ﬁnal model on all available data and use it make predictions\\non real ad hoc movie reviews from the internet.\\nIf you explore any of these extensions, I’d love to know.\\n16.7 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Convolutional Neural Networks for Sentence Classiﬁcation , 2014.\\nhttps://arxiv.org/abs/1408.5882\\n\\x88Convolutional Neural Networks for Sentence Classiﬁcation (code).\\nhttps://github.com/yoonkim/CNN_sentence\\n\\x88Keras Functional API.\\nhttps://keras.io/getting-started/functional-api-guide/\\n16.8 Summary\\nIn this tutorial, you discovered how to develop a multichannel convolutional neural network for'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 203}, page_content='sentiment prediction on text movie review data. Speciﬁcally, you learned:\\n\\x88How to prepare movie review text data for modeling.\\n\\x88How to develop a multichannel convolutional neural network for text in Keras.\\n\\x88How to evaluate a ﬁt model on unseen movie review data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 204}, page_content='16.8. Summary 188\\n16.8.1 Next\\nThis chapter is the last in the text classiﬁcation part. In the next part, you will discover how to\\ndevelop neural language models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 205}, page_content='Part VII\\nLanguage Modeling\\n189'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 206}, page_content='Chapter 17\\nNeural Language Modeling\\nLanguage modeling is central to many important natural language processing tasks. Recently,\\nneural-network-based language models have demonstrated better performance than classical\\nmethods both standalone and as part of more challenging natural language processing tasks. In\\nthis chapter, you will discover language modeling for natural language processing. After reading\\nthis chapter, you will know:\\n\\x88Why language modeling is critical to addressing tasks in natural language processing.\\n\\x88What a language model is and some examples of where they are used.\\n\\x88How neural networks can be used for language modeling.\\nLet’s get started.\\n17.1 Overview\\nThis tutorial is divided into the following parts:\\n1. Problem of Modeling Language\\n2. Statistical Language Modeling\\n3. Neural Language Models\\n17.2 Problem of Modeling Language\\nFormal languages, like programming languages, can be fully speciﬁed. All the reserved words'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 206}, page_content='can be deﬁned and the valid ways that they can be used can be precisely deﬁned. We cannot do\\nthis with natural language. Natural languages are not designed; they emerge, and therefore\\nthere is no formal speciﬁcation.\\nThere may be formal rules and heuristics for parts of the language, but as soon as rules\\nare deﬁned, you will devise or encounter counter examples that contradict the rules. Natural\\nlanguages involve vast numbers of terms that can be used in ways that introduce all kinds of\\nambiguities, yet can still be understood by other humans. Further, languages change, word\\n190'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 207}, page_content='17.3. Statistical Language Modeling 191\\nusages change: it is a moving target. Nevertheless, linguists try to specify the language with\\nformal grammars and structures. It can be done, but it is very diﬃcult and the results can\\nbe fragile. An alternative approach to specifying the model of the language is to learn it from\\nexamples.\\n17.3 Statistical Language Modeling\\nStatistical Language Modeling, or Language Modeling and LM for short, is the development of\\nprobabilistic models that are able to predict the next word in the sequence given the words that\\nprecede it.\\nLanguage modeling is the task of assigning a probability to sentences in a language.\\n[...] Besides assigning a probability to each sequence of words, the language models\\nalso assigns a probability for the likelihood of a given word (or a sequence of words)\\nto follow a sequence of words\\n— Page 105, Neural Network Methods in Natural Language Processing , 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 207}, page_content='A language model learns the probability of word occurrence based on examples of text.\\nSimpler models may look at a context of a short sequence of words, whereas larger models may\\nwork at the level of sentences or paragraphs. Most commonly, language models operate at the\\nlevel of words.\\nThe notion of a language model is inherently probabilistic. A language model is a\\nfunction that puts a probability measure over strings drawn from some vocabulary.\\n— Page 238, An Introduction to Information Retrieval , 2008.\\nA language model can be developed and used standalone, such as to generate new sequences\\nof text that appear to have come from the corpus. Language modeling is a root problem for a\\nlarge range of natural language processing tasks. More practically, language models are used\\non the front-end or back-end of a more sophisticated model for a task that requires language\\nunderstanding.\\n... language modeling is a crucial component in real-world applications such as'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 207}, page_content='machine-translation and automatic speech recognition, [...] For these reasons, lan-\\nguage modeling plays a central role in natural-language processing, AI, and machine-\\nlearning research.\\n— Page 105, Neural Network Methods in Natural Language Processing , 2017.\\nA good example is speech recognition, where audio data is used as an input to the model\\nand the output requires a language model that interprets the input signal and recognizes each\\nnew word within the context of the words already recognized.\\nSpeech recognition is principally concerned with the problem of transcribing the\\nspeech signal as a sequence of words. [...] From this point of view, speech is assumed\\nto be a generated by a language model which provides estimates of Pr(w) for all word\\nstrings w independently of the observed signal [...] The goal of speech recognition is\\nto ﬁnd the most likely word sequence given the observed acoustic signal.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 208}, page_content='17.4. Neural Language Models 192\\n— Pages 205-206, The Oxford Handbook of Computational Linguistics , 2005\\nSimilarly, language models are used to generate text in many similar natural language\\nprocessing tasks, for example:\\n\\x88Optical Character Recognition\\n\\x88Handwriting Recognition.\\n\\x88Machine Translation.\\n\\x88Spelling Correction.\\n\\x88Image Captioning.\\n\\x88Text Summarization\\n\\x88And much more.\\nLanguage modeling is the art of determining the probability of a sequence of words.\\nThis is useful in a large variety of areas including speech recognition, optical character\\nrecognition, handwriting recognition, machine translation, and spelling correction\\n—A Bit of Progress in Language Modeling , 2001.\\nDeveloping better language models often results in models that perform better on their\\nintended natural language processing task. This is the motivation for developing better and\\nmore accurate language models.\\n[language models] have played a key role in traditional NLP tasks such as speech'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 208}, page_content='recognition, machine translation, or text summarization. Often (although not\\nalways), training better language models improves the underlying metrics of the\\ndownstream task (such as word error rate for speech recognition, or BLEU score for\\ntranslation), which makes the task of training better LMs valuable by itself.\\n—Exploring the Limits of Language Modeling , 2016.\\n17.4 Neural Language Models\\nRecently, the use of neural networks in the development of language models has become very\\npopular, to the point that it may now be the preferred approach. The use of neural networks in\\nlanguage modeling is often called Neural Language Modeling, or NLM for short. Neural network\\napproaches are achieving better results than classical methods both on standalone language\\nmodels and when models are incorporated into larger models on challenging tasks like speech\\nrecognition and machine translation. A key reason for the leaps in improved performance may\\nbe the method’s ability to generalize.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 209}, page_content='17.4. Neural Language Models 193\\nNonlinear neural network models solve some of the shortcomings of traditional\\nlanguage models: they allow conditioning on increasingly large context sizes with\\nonly a linear increase in the number of parameters, they alleviate the need for\\nmanually designing backoﬀ orders, and they support generalization across diﬀerent\\ncontexts.\\n— Page 109, Neural Network Methods in Natural Language Processing , 2017.\\nSpeciﬁcally, a word embedding is adopted that uses a real-valued vector to represent each\\nword in a projected vector space. This learned representation of words based on their usage\\nallows words with a similar meaning to have a similar representation.\\nNeural Language Models (NLM) address the n-gram data sparsity issue through\\nparameterization of words as vectors (word embeddings) and using them as inputs to\\na neural network. The parameters are learned as part of the training process. Word'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 209}, page_content='embeddings obtained through NLMs exhibit the property whereby semantically close\\nwords are likewise close in the induced vector space.\\n—Character-Aware Neural Language Model , 2015.\\nThis generalization is something that the representation used in classical statistical language\\nmodels cannot easily achieve.\\n“True generalization” is diﬃcult to obtain in a discrete word indice space, since\\nthere is no obvious relation between the word indices.\\n—Connectionist language modeling for large vocabulary continuous speech recognition , 2002.\\nFurther, the distributed representation approach allows the embedding representation to scale\\nbetter with the size of the vocabulary. Classical methods that have one discrete representation\\nper word ﬁght the curse of dimensionality with larger and larger vocabularies of words that\\nresult in longer and more sparse representations. The neural network approach to language\\nmodeling can be described using the three following model properties, taken from A Neural'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 209}, page_content='Probabilistic Language Model , 2003.\\n1. Associate each word in the vocabulary with a distributed word feature vector.\\n2.Express the joint probability function of word sequences in terms of the feature vectors of\\nthese words in the sequence.\\n3.Learn simultaneously the word feature vector and the parameters of the probability\\nfunction.\\nThis represents a relatively simple model where both the representation and probabilistic\\nmodel are learned together directly from raw text data. Recently, the neural based approaches\\nhave started to outperform the classical statistical approaches.\\nWe provide ample empirical evidence to suggest that connectionist language mod-\\nels are superior to standard n-gram techniques, except their high computational\\n(training) complexity.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 210}, page_content='17.5. Further Reading 194\\n—Recurrent neural network based language model , 2010.\\nInitially, feedforward neural network models were used to introduce the approach. More\\nrecently, recurrent neural networks and then networks with a long-term memory like the Long\\nShort-Term Memory network, or LSTM, allow the models to learn the relevant context over\\nmuch longer input sequences than the simpler feedforward networks.\\n[an RNN language model] provides further generalization: instead of considering\\njust several preceding words, neurons with input from recurrent connections are\\nassumed to represent short term memory. The model learns itself from the data how\\nto represent memory. While shallow feedforward neural networks (those with just\\none hidden layer) can only cluster similar words, recurrent neural network (which\\ncan be considered as a deep architecture) can perform clustering of similar histories.\\nThis allows for instance eﬃcient representation of patterns with variable length.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 210}, page_content='—Extensions of recurrent neural network language model , 2011.\\nRecently, researchers have been seeking the limits of these language models. In the paper\\nExploring the Limits of Language Modeling , evaluating language models over large datasets,\\nsuch as the corpus of one million words, the authors ﬁnd that LSTM-based neural language\\nmodels out-perform the classical methods.\\n... we have shown that RNN LMs can be trained on large amounts of data, and\\noutperform competing models including carefully tuned N-grams.\\n—Exploring the Limits of Language Modeling , 2016.\\nFurther, they propose some heuristics for developing high-performing neural language models\\nin general:\\n\\x88Size matters . The best models were the largest models, speciﬁcally number of memory\\nunits.\\n\\x88Regularization matters . Use of regularization like dropout on input connections\\nimproves results.\\n\\x88CNNs vs Embeddings . Character-level Convolutional Neural Network (CNN) models'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 210}, page_content='can be used on the front-end instead of word embeddings, achieving similar and sometimes\\nbetter results.\\n\\x88Ensembles matter . Combining the prediction from multiple models can oﬀer large\\nimprovements in model performance.\\n17.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 211}, page_content='17.5. Further Reading 195\\n17.5.1 Books\\n\\x88Neural Network Methods in Natural Language Processing , 2017.\\nhttp://amzn.to/2vStiIS\\n\\x88Natural Language Processing, Artiﬁcial Intelligence A Modern Approach , 2009.\\nhttp://amzn.to/2fDPfF3\\n\\x88Language models for information retrieval, An Introduction to Information Retrieval , 2008.\\nhttp://amzn.to/2vAavQd\\n17.5.2 Papers\\n\\x88A Neural Probabilistic Language Model , NIPS, 2001.\\nhttps://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf\\n\\x88A Neural Probabilistic Language Model , JMLR, 2003.\\nhttp://www.jmlr.org/papers/v3/bengio03a.html\\n\\x88Connectionist language modeling for large vocabulary continuous speech recognition , 2002.\\nhttps://pdfs.semanticscholar.org/b4db/83366f925e9a1e1528ee9f6b41d7cd666f41.\\npdf\\n\\x88Recurrent neural network based language model , 2010.\\nhttp://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_\\nIS100722.pdf\\n\\x88Extensions of recurrent neural network language model , 2011.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 211}, page_content='http://ieeexplore.ieee.org/abstract/document/5947611/\\n\\x88Character-Aware Neural Language Model , 2015.\\nhttps://arxiv.org/abs/1508.06615\\n\\x88LSTM Neural Networks for Language Modeling , 2012.\\nhttps://pdfs.semanticscholar.org/f9a1/b3850dfd837793743565a8af95973d395a4e.\\npdf\\n\\x88Exploring the Limits of Language Modeling , 2016.\\nhttps://arxiv.org/abs/1602.02410\\n17.5.3 Articles\\n\\x88Language Model, Wikipedia.\\nhttps://en.wikipedia.org/wiki/Language_model\\n\\x88Neural net language models, Scholarpedia.\\nhttp://www.scholarpedia.org/article/Neural_net_language_models'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 212}, page_content='17.6. Summary 196\\n17.6 Summary\\nIn this chapter, you discovered language modeling for natural language processing tasks. Specif-\\nically, you learned:\\n\\x88That natural language is not formally speciﬁed and requires the use of statistical models\\nto learn from examples.\\n\\x88That statistical language models are central to many challenging natural language pro-\\ncessing tasks.\\n\\x88That state-of-the-art results are achieved using neural language models, speciﬁcally those\\nwith word embeddings and recurrent neural network algorithms.\\n17.6.1 Next\\nIn the next chapter, you will discover how you can develop a character-based neural language\\nmodel.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 213}, page_content='Chapter 18\\nHow to Develop a Character-Based\\nNeural Language Model\\nA language model predicts the next word in the sequence based on the speciﬁc words that have\\ncome before it in the sequence. It is also possible to develop language models at the character\\nlevel using neural networks. The beneﬁt of character-based language models is their small\\nvocabulary and ﬂexibility in handling any words, punctuation, and other document structure.\\nThis comes at the cost of requiring larger models that are slower to train. Nevertheless, in the\\nﬁeld of neural language models, character-based models oﬀer a lot of promise for a general,\\nﬂexible and powerful approach to language modeling. In this tutorial, you will discover how to\\ndevelop a character-based neural language model. After completing this tutorial, you will know:\\n\\x88How to prepare text for character-based language modeling.\\n\\x88How to develop a character-based language model using LSTMs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 213}, page_content='\\x88How to use a trained character-based language model to generate text.\\nLet’s get started.\\n18.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Sing a Song of Sixpence\\n2. Data Preparation\\n3. Train Language Model\\n4. Generate Text\\n197'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 214}, page_content=\"18.2. Sing a Song of Sixpence 198\\n18.2 Sing a Song of Sixpence\\nThe nursery rhyme Sing a Song of Sixpence is well known in the west. The ﬁrst verse is common,\\nbut there is also a 4 verse version that we will use to develop our character-based language\\nmodel. It is short, so ﬁtting the model will be fast, but not so short that we won’t see anything\\ninteresting. The complete 4 verse version we will use as source text is listed below.\\nSing a song of sixpence,\\nA pocket full of rye.\\nFour and twenty blackbirds,\\nBaked in a pie.\\nWhen the pie was opened\\nThe birds began to sing;\\nWasn 't that a dainty dish,\\nTo set before the king.\\nThe king was in his counting house,\\nCounting out his money;\\nThe queen was in the parlour,\\nEating bread and honey.\\nThe maid was in the garden,\\nHanging out the clothes,\\nWhen down came a blackbird\\nAnd pecked off her nose.\\nListing 18.1: Sing a Song of Sixpence nursery rhyme.\\nCopy the text and save it in a new ﬁle in your current working directory with the ﬁle name\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 214}, page_content='rhyme.txt .\\n18.3 Data Preparation\\nThe ﬁrst step is to prepare the text data. We will start by deﬁning the type of language model.\\n18.3.1 Language Model Design\\nA language model must be trained on the text, and in the case of a character-based language\\nmodel, the input and output sequences must be characters. The number of characters used\\nas input will also deﬁne the number of characters that will need to be provided to the model\\nin order to elicit the ﬁrst predicted character. After the ﬁrst character has been generated, it\\ncan be appended to the input sequence and used as input for the model to generate the next\\ncharacter.\\nLonger sequences oﬀer more context for the model to learn what character to output next\\nbut take longer to train and impose more burden on seeding the model when generating text.\\nWe will use an arbitrary length of 10 characters for this model. There is not a lot of text, and\\n10 characters is a few words. We can now transform the raw text into a form that our model'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 214}, page_content='can learn; speciﬁcally, input and output sequences of characters.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 215}, page_content=\"18.3. Data Preparation 199\\n18.3.2 Load Text\\nWe must load the text into memory so that we can work with it. Below is a function named\\nload doc() that will load a text ﬁle given a ﬁlename and return the loaded text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 18.2: Function to load a document into memory.\\nWe can call this function with the ﬁlename of the nursery rhyme rhyme.txt to load the text\\ninto memory. The contents of the ﬁle are then printed to screen as a sanity check.\\n# load text\\nraw_text = load_doc( 'rhyme.txt ')\\nprint(raw_text)\\nListing 18.3: Load the document into memory.\\n18.3.3 Clean Text\\nNext, we need to clean the loaded text. We will not do much to it on this example. Speciﬁcally,\\nwe will strip all of the new line characters so that we have one long sequence of characters\\nseparated only by white space.\\n# clean\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 215}, page_content=\"# clean\\ntokens = raw_text.split()\\nraw_text = ' '.join(tokens)\\nListing 18.4: Tokenize the loaded document.\\nYou may want to explore other methods for data cleaning, such as normalizing the case to\\nlowercase or removing punctuation in an eﬀort to reduce the ﬁnal vocabulary size and develop a\\nsmaller and leaner model.\\n18.3.4 Create Sequences\\nNow that we have a long list of characters, we can create our input-output sequences used to\\ntrain the model. Each input sequence will be 10 characters with one output character, making\\neach sequence 11 characters long. We can create the sequences by enumerating the characters\\nin the text, starting at the 11th character at index 10.\\n# organize into sequences of characters\\nlength = 10\\nsequences = list()\\nfor i in range(length, len(raw_text)):\\n# select sequence of tokens\\nseq = raw_text[i-length:i+1]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 216}, page_content=\"18.3. Data Preparation 200\\n# store\\nsequences.append(seq)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 18.5: Convert text into ﬁxed-length sequences.\\nRunning this snippet, we can see that we end up with just under 400 sequences of characters\\nfor training our language model.\\nTotal Sequences: 399\\nListing 18.6: Example output of converting text into ﬁxed-length sequences.\\n18.3.5 Save Sequences\\nFinally, we can save the prepared data to ﬁle so that we can load it later when we develop our\\nmodel. Below is a function save doc() that, given a list of strings and a ﬁlename, will save the\\nstrings to ﬁle, one per line.\\n# save tokens to file, one dialog per line\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nListing 18.7: Function to save sequences to ﬁle.\\nWe can call this function and save our prepared sequences to the ﬁlename char sequences.txt\\nin our current working directory.\\n# save sequences to file\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 216}, page_content=\"out_filename = 'char_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 18.8: Call function to save sequences to ﬁle.\\n18.3.6 Complete Example\\nTying all of this together, the complete code listing is provided below.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# save tokens to file, one dialog per line\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 217}, page_content=\"18.4. Train Language Model 201\\nfile.write(data)\\nfile.close()\\n# load text\\nraw_text = load_doc( 'rhyme.txt ')\\nprint(raw_text)\\n# clean\\ntokens = raw_text.split()\\nraw_text = ' '.join(tokens)\\n# organize into sequences of characters\\nlength = 10\\nsequences = list()\\nfor i in range(length, len(raw_text)):\\n# select sequence of tokens\\nseq = raw_text[i-length:i+1]\\n# store\\nsequences.append(seq)\\nprint( 'Total Sequences: %d '% len(sequences))\\n# save sequences to file\\nout_filename = 'char_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 18.9: Complete example of preparing the text data.\\nRun the example to create the char sequences.txt ﬁle. Take a look inside you should see\\nsomething like the following:\\nSing a song\\ning a song\\nng a song o\\ng a song of\\na song of\\na song of s\\nsong of si\\nsong of six\\nong of sixp\\nng of sixpe\\n...\\nListing 18.10: Sample of the output ﬁle.\\nWe are now ready to train our character-based neural language model.\\n18.4 Train Language Model\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 217}, page_content='In this section, we will develop a neural language model for the prepared sequence data. The\\nmodel will read encoded characters and predict the next character in the sequence. A Long\\nShort-Term Memory recurrent neural network hidden layer will be used to learn the context\\nfrom the input sequence in order to make the predictions.\\n18.4.1 Load Data\\nThe ﬁrst step is to load the prepared character sequence data from char sequences.txt . We\\ncan use the same load doc() function developed in the previous section. Once loaded, we split'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 218}, page_content=\"18.4. Train Language Model 202\\nthe text by new line to give a list of sequences ready to be encoded.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load\\nin_filename = 'char_sequences.txt '\\nraw_text = load_doc(in_filename)\\nlines = raw_text.split( '\\\\n ')\\nListing 18.11: Load the prepared text data.\\n18.4.2 Encode Sequences\\nThe sequences of characters must be encoded as integers. This means that each unique character\\nwill be assigned a speciﬁc integer value and each sequence of characters will be encoded as a\\nsequence of integers. We can create the mapping given a sorted set of unique characters in the\\nraw input data. The mapping is a dictionary of character values to integer values.\\nchars = sorted(list(set(raw_text)))\\nmapping = dict((c, i) for i, c in enumerate(chars))\\nListing 18.12: Create a mapping between chars and integers.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 218}, page_content=\"Next, we can process each sequence of characters one at a time and use the dictionary\\nmapping to look up the integer value for each character.\\nsequences = list()\\nfor line in lines:\\n# integer encode line\\nencoded_seq = [mapping[char] for char in line]\\n# store\\nsequences.append(encoded_seq)\\nListing 18.13: Integer encode sequences of characters.\\nThe result is a list of integer lists. We need to know the size of the vocabulary later. We can\\nretrieve this as the size of the dictionary mapping.\\n# vocabulary size\\nvocab_size = len(mapping)\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 18.14: Summarize the size of the vocabulary.\\nRunning this piece, we can see that there are 38 unique characters in the input sequence\\ndata.\\nVocabulary Size: 38\\nListing 18.15: Example output from summarizing the size of the vocabulary.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 219}, page_content='18.4. Train Language Model 203\\n18.4.3 Split Inputs and Output\\nNow that the sequences have been integer encoded, we can separate the columns into input and\\noutput sequences of characters. We can do this using a simple array slice.\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\nListing 18.16: Split sequences into input and output elements.\\nNext, we need to one hot encode each character. That is, each character becomes a vector as\\nlong as the vocabulary (38 elements) with a 1 marked for the speciﬁc character. This provides\\na more precise input representation for the network. It also provides a clear objective for the\\nnetwork to predict, where a probability distribution over characters can be output by the model\\nand compared to the ideal case of all 0 values with a 1 for the actual next character. We can\\nuse the tocategorical() function in the Keras API to one hot encode the input and output\\nsequences.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 219}, page_content='sequences.\\nsequences = [to_categorical(x, num_classes=vocab_size) for x in X]\\nX = array(sequences)\\ny = to_categorical(y, num_classes=vocab_size)\\nListing 18.17: Convert sequences into a format ready for training.\\nWe are now ready to ﬁt the model.\\n18.4.4 Fit Model\\nThe model is deﬁned with an input layer that takes sequences that have 10 time steps and 38\\nfeatures for the one hot encoded input sequences. Rather than specify these numbers, we use\\nthe second and third dimensions on the X input data. This is so that if we change the length of\\nthe sequences or size of the vocabulary, we do not need to change the model deﬁnition. The\\nmodel has a single LSTM hidden layer with 75 memory cells, chosen with a little trial and\\nerror. The model has a fully connected output layer that outputs one vector with a probability\\ndistribution across all characters in the vocabulary. A softmax activation function is used on'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 219}, page_content=\"the output layer to ensure the output has the properties of a probability distribution.\\n# define the model\\ndef define_model(X):\\nmodel = Sequential()\\nmodel.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 18.18: Deﬁne the language model.\\nThe model is learning a multiclass classiﬁcation problem, therefore we use the categorical log\\nloss intended for this type of problem. The eﬃcient Adam implementation of gradient descent\\nis used to optimize the model and accuracy is reported at the end of each batch update. The\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 220}, page_content='18.4. Train Language Model 204\\nmodel is ﬁt for 100 training epochs, again found with a little trial and error. Running this prints\\na summary of the deﬁned network as a sanity check.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nlstm_1 (LSTM) (None, 75) 34200\\n_________________________________________________________________\\ndense_1 (Dense) (None, 38) 2888\\n=================================================================\\nTotal params: 37,088\\nTrainable params: 37,088\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 18.19: Example output from summarizing the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .\\nFigure 18.1: Plot of the deﬁned character-based language model.\\n18.4.5 Save Model\\nAfter the model is ﬁt, we save it to ﬁle for later use. The Keras model API provides the save()'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 220}, page_content=\"function that we can use to save the model to a single ﬁle, including weights and topology\\ninformation.\\n# save the model to file\\nmodel.save( 'model.h5 ')\\nListing 18.20: Save the ﬁt model to ﬁle.\\nWe also save the mapping from characters to integers that we will need to encode any input\\nwhen using the model and decode any output from the model.\\n# save the mapping\\ndump(mapping, open( 'mapping.pkl ', 'wb '))\\nListing 18.21: Save the mapping of chars to integers to ﬁle.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 221}, page_content=\"18.4. Train Language Model 205\\n18.4.6 Complete Example\\nTying all of this together, the complete code listing for ﬁtting the character-based neural\\nlanguage model is listed below.\\nfrom numpy import array\\nfrom pickle import dump\\nfrom keras.utils import to_categorical\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# define the model\\ndef define_model(X):\\nmodel = Sequential()\\nmodel.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 221}, page_content=\"return model\\n# load\\nin_filename = 'char_sequences.txt '\\nraw_text = load_doc(in_filename)\\nlines = raw_text.split( '\\\\n ')\\n# integer encode sequences of characters\\nchars = sorted(list(set(raw_text)))\\nmapping = dict((c, i) for i, c in enumerate(chars))\\nsequences = list()\\nfor line in lines:\\n# integer encode line\\nencoded_seq = [mapping[char] for char in line]\\n# store\\nsequences.append(encoded_seq)\\n# vocabulary size\\nvocab_size = len(mapping)\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# separate into input and output\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\nsequences = [to_categorical(x, num_classes=vocab_size) for x in X]\\nX = array(sequences)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 222}, page_content=\"18.5. Generate Text 206\\ny = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(X)\\n# fit model\\nmodel.fit(X, y, epochs=100, verbose=2)\\n# save the model to file\\nmodel.save( 'model.h5 ')\\n# save the mapping\\ndump(mapping, open( 'mapping.pkl ', 'wb '))\\nListing 18.22: Complete example of training the language model.\\nRunning the example might take one minute. You will see that the model learns the problem\\nwell, perhaps too well for generating surprising sequences of characters.\\n...\\nEpoch 96/100\\n0s - loss: 0.2193 - acc: 0.9950\\nEpoch 97/100\\n0s - loss: 0.2124 - acc: 0.9950\\nEpoch 98/100\\n0s - loss: 0.2054 - acc: 0.9950\\nEpoch 99/100\\n0s - loss: 0.1982 - acc: 0.9950\\nEpoch 100/100\\n0s - loss: 0.1910 - acc: 0.9950\\nListing 18.23: Example output from training the language model.\\nAt the end of the run, you will have two ﬁles saved to the current working directory,\\nspeciﬁcally model.h5 andmapping.pkl . Next, we can look at using the learned model.\\n18.5 Generate Text\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 222}, page_content=\"18.5 Generate Text\\nWe will use the learned language model to generate new sequences of text that have the same\\nstatistical properties.\\n18.5.1 Load Model\\nThe ﬁrst step is to load the model saved to the ﬁle model.h5 . We can use the load model()\\nfunction from the Keras API.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\nListing 18.24: Load the saved model.\\nWe also need to load the pickled dictionary for mapping characters to integers from the ﬁle\\nmapping.pkl . We will use the Pickle API to load the object.\\n# load the mapping\\nmapping = load(open( 'mapping.pkl ', 'rb '))\\nListing 18.25: Load the saved mapping from chars to integers.\\nWe are now ready to use the loaded model.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 223}, page_content='18.5. Generate Text 207\\n18.5.2 Generate Characters\\nWe must provide sequences of 10 characters as input to the model in order to start the generation\\nprocess. We will pick these manually. A given input sequence will need to be prepared in the\\nsame way as preparing the training data for the model. First, the sequence of characters must\\nbe integer encoded using the loaded mapping.\\n# encode the characters as integers\\nencoded = [mapping[char] for char in in_text]\\nListing 18.26: Encode input text to integers.\\nNext, the integers need to be one hot encoded using the tocategorical() Keras function.\\nWe also need to reshape the sequence to be 3-dimensional, as we only have one sequence and\\nLSTMs require all input to be three dimensional (samples, time steps, features).\\n# one hot encode\\nencoded = to_categorical(encoded, num_classes=len(mapping))\\nencoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\\nListing 18.27: One hot encode the integer encoded text.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 223}, page_content=\"We can then use the model to predict the next character in the sequence. We use\\npredict classes() instead of predict() to directly select the integer for the character with\\nthe highest probability instead of getting the full probability distribution across the entire set of\\ncharacters.\\n# predict character\\nyhat = model.predict_classes(encoded, verbose=0)\\nListing 18.28: Predict the next character in the sequence.\\nWe can then decode this integer by looking up the mapping to see the character to which it\\nmaps.\\nout_char = ''\\nfor char, index in mapping.items():\\nif index == yhat:\\nout_char = char\\nbreak\\nListing 18.29: Map the predicted integer back to a character.\\nThis character can then be added to the input sequence. We then need to make sure that the\\ninput sequence is 10 characters by truncating the ﬁrst character from the input sequence text.\\nWe can use the padsequences() function from the Keras API that can perform this truncation\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 223}, page_content='operation. Putting all of this together, we can deﬁne a new function named generate seq()\\nfor using the loaded model to generate new sequences of text.\\n# generate a sequence of characters with a language model\\ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\\nin_text = seed_text\\n# generate a fixed number of characters\\nfor _ in range(n_chars):\\n# encode the characters as integers\\nencoded = [mapping[char] for char in in_text]\\n# truncate sequences to a fixed length'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 224}, page_content=\"18.5. Generate Text 208\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# one hot encode\\nencoded = to_categorical(encoded, num_classes=len(mapping))\\nencoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\\n# predict character\\nyhat = model.predict_classes(encoded, verbose=0)\\n# reverse map integer to character\\nout_char = ''\\nfor char, index in mapping.items():\\nif index == yhat:\\nout_char = char\\nbreak\\n# append to input\\nin_text += char\\nreturn in_text\\nListing 18.30: Function to predict a sequence of characters given seed text.\\n18.5.3 Complete Example\\nTying all of this together, the complete example for generating text using the ﬁt neural language\\nmodel is listed below.\\nfrom pickle import load\\nfrom keras.models import load_model\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.sequence import pad_sequences\\n# generate a sequence of characters with a language model\\ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 224}, page_content=\"in_text = seed_text\\n# generate a fixed number of characters\\nfor _ in range(n_chars):\\n# encode the characters as integers\\nencoded = [mapping[char] for char in in_text]\\n# truncate sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# one hot encode\\nencoded = to_categorical(encoded, num_classes=len(mapping))\\nencoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\\n# predict character\\nyhat = model.predict_classes(encoded, verbose=0)\\n# reverse map integer to character\\nout_char = ''\\nfor char, index in mapping.items():\\nif index == yhat:\\nout_char = char\\nbreak\\n# append to input\\nin_text += out_char\\nreturn in_text\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# load the mapping\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 225}, page_content=\"18.6. Further Reading 209\\nmapping = load(open( 'mapping.pkl ', 'rb '))\\n# test start of rhyme\\nprint(generate_seq(model, mapping, 10, 'Sing a son ', 20))\\n# test mid-line\\nprint(generate_seq(model, mapping, 10, 'king was i ', 20))\\n# test not in original\\nprint(generate_seq(model, mapping, 10, 'hello worl ', 20))\\nListing 18.31: Complete example of generating characters with the ﬁt model.\\nRunning the example generates three sequences of text. The ﬁrst is a test to see how the\\nmodel does at starting from the beginning of the rhyme. The second is a test to see how well it\\ndoes at beginning in the middle of a line. The ﬁnal example is a test to see how well it does\\nwith a sequence of characters never seen before.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nSing a song of sixpence, A poc\\nking was in his counting house\\nhello worls e pake wofey. The\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 225}, page_content='Listing 18.32: Example output from generating sequences of characters.\\nWe can see that the model did very well with the ﬁrst two examples, as we would expect.\\nWe can also see that the model still generated something for the new text, but it is nonsense.\\n18.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Sing a Song of Sixpence on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Sing_a_Song_of_Sixpence\\n\\x88Keras Utils API.\\nhttps://keras.io/utils/\\n\\x88Keras Sequence Processing API.\\nhttps://keras.io/preprocessing/sequence/\\n18.7 Summary\\nIn this tutorial, you discovered how to develop a character-based neural language model.\\nSpeciﬁcally, you learned:\\n\\x88How to prepare text for character-based language modeling.\\n\\x88How to develop a character-based language model using LSTMs.\\n\\x88How to use a trained character-based language model to generate text.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 226}, page_content='18.7. Summary 210\\n18.7.1 Next\\nIn the next chapter, you will discover how you can develop a word-based neural language model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 227}, page_content='Chapter 19\\nHow to Develop a Word-Based Neural\\nLanguage Model\\nLanguage modeling involves predicting the next word in a sequence given the sequence of words\\nalready present. A language model is a key element in many natural language processing models\\nsuch as machine translation and speech recognition. The choice of how the language model is\\nframed must match how the language model is intended to be used. In this tutorial, you will\\ndiscover how the framing of a language model aﬀects the skill of the model when generating\\nshort sequences from a nursery rhyme. After completing this tutorial, you will know:\\n\\x88The challenge of developing a good framing of a word-based language model for a given\\napplication.\\n\\x88How to develop one-word, two-word, and line-based framings for word-based language\\nmodels.\\n\\x88How to generate sequences using a ﬁt language model.\\nLet’s get started.\\n19.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Framing Language Modeling'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 227}, page_content='2. Jack and Jill Nursery Rhyme\\n3. Model 1: One-Word-In, One-Word-Out Sequences\\n4. Model 2: Line-by-Line Sequence\\n5. Model 3: Two-Words-In, One-Word-Out Sequence\\n211'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 228}, page_content='19.2. Framing Language Modeling 212\\n19.2 Framing Language Modeling\\nA statistical language model is learned from raw text and predicts the probability of the next\\nword in the sequence given the words already present in the sequence. Language models are\\na key component in larger models for challenging natural language processing problems, like\\nmachine translation and speech recognition. They can also be developed as standalone models\\nand used for generating new sequences that have the same statistical properties as the source\\ntext.\\nLanguage models both learn and predict one word at a time. The training of the network\\ninvolves providing sequences of words as input that are processed one at a time where a prediction\\ncan be made and learned for each input sequence. Similarly, when making predictions, the\\nprocess can be seeded with one or a few words, then predicted words can be gathered and\\npresented as input on subsequent predictions in order to build up a generated output sequence'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 228}, page_content='Therefore, each model will involve splitting the source text into input and output sequences,\\nsuch that the model can learn to predict words. There are many ways to frame the sequences\\nfrom a source text for language modeling. In this tutorial, we will explore 3 diﬀerent ways of\\ndeveloping word-based language models in the Keras deep learning library. There is no single\\nbest approach, just diﬀerent framings that may suit diﬀerent applications.\\n19.3 Jack and Jill Nursery Rhyme\\nJack and Jill is a simple nursery rhyme. It is comprised of 4 lines, as follows:\\nJack and Jill went up the hill\\nTo fetch a pail of water\\nJack fell down and broke his crown\\nAnd Jill came tumbling after\\nListing 19.1: Jack and Jill nursery rhyme.\\nWe will use this as our source text for exploring diﬀerent framings of a word-based language\\nmodel. We can deﬁne this text in Python as follows:\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n\\nJack fell down and broke his crown\\\\n'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 228}, page_content='And Jill came tumbling after\\\\n \"\"\"\\nListing 19.2: Sample text for this tutorial.\\n19.4 Model 1: One-Word-In, One-Word-Out Sequences\\nWe can start with a very simple model. Given one word as input, the model will learn to predict\\nthe next word in the sequence. For example:\\nX, y\\nJack, and\\nand, Jill\\nJill, went'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 229}, page_content='19.4. Model 1: One-Word-In, One-Word-Out Sequences 213\\n...\\nListing 19.3: Example of input and output pairs.\\nThe ﬁrst step is to encode the text as integers. Each lowercase word in the source text is\\nassigned a unique integer and we can convert the sequences of words to sequences of integers.\\nKeras provides the Tokenizer class that can be used to perform this encoding. First, the\\nTokenizer is ﬁt on the source text to develop the mapping from words to unique integers. Then\\nsequences of text can be converted to sequences of integers by calling the texts tosequences()\\nfunction.\\n# integer encode text\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\nencoded = tokenizer.texts_to_sequences([data])[0]\\nListing 19.4: Example of training a Tokenizer on the sample text.\\nWe will need to know the size of the vocabulary later for both deﬁning the word embedding\\nlayer in the model, and for encoding output words using a one hot encoding. The size of the'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 229}, page_content=\"vocabulary can be retrieved from the trained Tokenizer by accessing the word index attribute.\\n# determine the vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 19.5: Summarize the size of the vocabulary.\\nRunning this example, we can see that the size of the vocabulary is 21 words. We add one,\\nbecause we will need to specify the integer for the largest encoded word as an array index, e.g.\\nwords encoded 1 to 21 with array indicies 0 to 21 or 22 positions. Next, we need to create\\nsequences of words to ﬁt the model with one word as input and one word as output.\\n# create word -> word sequences\\nsequences = list()\\nfor i in range(1, len(encoded)):\\nsequence = encoded[i-1:i+1]\\nsequences.append(sequence)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 19.6: Example of encoding the source text.\\nRunning this piece shows that we have a total of 24 input-output pairs to train the network.\\nTotal Sequences: 24\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 229}, page_content='Total Sequences: 24\\nListing 19.7: Example of output of summarizing the encoded text.\\nWe can then split the sequences into input ( X) and output elements ( y). This is straightfor-\\nward as we only have two columns in the data.\\n# split into X and y elements\\nsequences = array(sequences)\\nX, y = sequences[:,0],sequences[:,1]\\nListing 19.8: Split the encoded text into input and output pairs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 230}, page_content='19.4. Model 1: One-Word-In, One-Word-Out Sequences 214\\nWe will ﬁt our model to predict a probability distribution across all words in the vocabulary.\\nThat means that we need to turn the output element from a single integer into a one hot\\nencoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value.\\nThis gives the network a ground truth to aim for from which we can calculate error and update\\nthe model. Keras provides the tocategorical() function that we can use to convert the\\ninteger to a one hot encoding while specifying the number of classes as the vocabulary size.\\n# one hot encode outputs\\ny = to_categorical(y, num_classes=vocab_size)\\nListing 19.9: One hot encode the output words.\\nWe are now ready to deﬁne the neural network model. The model uses a learned word\\nembedding in the input layer. This has one real-valued vector for each word in the vocabulary,\\nwhere each word vector has a speciﬁed length. In this case we will use a 10-dimensional'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 230}, page_content=\"projection. The input sequence contains a single word, therefore the input length=1 . The\\nmodel has a single hidden LSTM layer with 50 units. This is far more than is needed. The\\noutput layer is comprised of one neuron for each word in the vocabulary and uses a softmax\\nactivation function to ensure the output is normalized to look like a probability.\\n# define the model\\ndef define_model(vocab_size):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 19.10: Deﬁne and compile the language model.\\nThe structure of the network can be summarized as follows:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 230}, page_content='=================================================================\\nembedding_1 (Embedding) (None, 1, 10) 220\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 50) 12200\\n_________________________________________________________________\\ndense_1 (Dense) (None, 22) 1122\\n=================================================================\\nTotal params: 13,542\\nTrainable params: 13,542\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 19.11: Example output summarizing the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 231}, page_content='19.4. Model 1: One-Word-In, One-Word-Out Sequences 215\\nFigure 19.1: Plot of the deﬁned word-based language model.\\nWe will use this same general network structure for each example in this tutorial, with\\nminor changes to the learned embedding layer. We can compile and ﬁt the network on the\\nencoded text data. Technically, we are modeling a multiclass classiﬁcation problem (predict the\\nword in the vocabulary), therefore using the categorical cross entropy loss function. We use the\\neﬃcient Adam implementation of gradient descent and track accuracy at the end of each epoch.\\nThe model is ﬁt for 500 training epochs, again, perhaps more than is needed. The network\\nconﬁguration was not tuned for this and later experiments; an over-prescribed conﬁguration\\nwas chosen to ensure that we could focus on the framing of the language model.\\nAfter the model is ﬁt, we test it by passing it a given word from the vocabulary and'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 231}, page_content=\"having the model predict the next word. Here we pass in ‘ Jack’ by encoding it and calling\\nmodel.predict classes() to get the integer output for the predicted word. This is then looked\\nup in the vocabulary mapping to give the associated word.\\n# evaluate\\nin_text = 'Jack '\\nprint(in_text)\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\nencoded = array(encoded)\\nyhat = model.predict_classes(encoded, verbose=0)\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nprint(word)\\nListing 19.12: Evaluate the ﬁt language model.\\nThis process could then be repeated a few times to build up a generated sequence of words.\\nTo make this easier, we wrap up the behavior in a function that we can call by passing in our\\nmodel and the seed word.\\n# generate a sequence from the model\\ndef generate_seq(model, tokenizer, seed_text, n_words):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 232}, page_content=\"19.4. Model 1: One-Word-In, One-Word-Out Sequences 216\\nin_text, result = seed_text, seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\nencoded = array(encoded)\\n# predict a word in the vocabulary\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text, result = out_word, result + ' ' + out_word\\nreturn result\\nListing 19.13: Function to generate output sequences given a ﬁt model.\\nWe can tie all of this together. The complete code listing is provided below.\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils import to_categorical\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 232}, page_content=\"from keras.layers import Embedding\\n# generate a sequence from the model\\ndef generate_seq(model, tokenizer, seed_text, n_words):\\nin_text, result = seed_text, seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\nencoded = array(encoded)\\n# predict a word in the vocabulary\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text, result = out_word, result + ' ' + out_word\\nreturn result\\n# define the model\\ndef define_model(vocab_size):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 233}, page_content='19.4. Model 1: One-Word-In, One-Word-Out Sequences 217\\n# compile network\\nmodel.compile(loss= \\'categorical_crossentropy \\', optimizer= \\'adam \\', metrics=[ \\'accuracy \\'])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= \\'model.png \\', show_shapes=True)\\nreturn model\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n\\nJack fell down and broke his crown\\\\n\\nAnd Jill came tumbling after\\\\n \"\"\"\\n# integer encode text\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\nencoded = tokenizer.texts_to_sequences([data])[0]\\n# determine the vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( \\'Vocabulary Size: %d \\'% vocab_size)\\n# create word -> word sequences\\nsequences = list()\\nfor i in range(1, len(encoded)):\\nsequence = encoded[i-1:i+1]\\nsequences.append(sequence)\\nprint( \\'Total Sequences: %d \\'% len(sequences))\\n# split into X and y elements\\nsequences = array(sequences)\\nX, y = sequences[:,0],sequences[:,1]\\n# one hot encode outputs'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 233}, page_content=\"y = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(vocab_size)\\n# fit network\\nmodel.fit(X, y, epochs=500, verbose=2)\\n# evaluate\\nprint(generate_seq(model, tokenizer, 'Jack ', 6))\\nListing 19.14: Complete example of model1.\\nRunning the example prints the loss and accuracy each training epoch.\\n...\\nEpoch 496/500\\n0s - loss: 0.2358 - acc: 0.8750\\nEpoch 497/500\\n0s - loss: 0.2355 - acc: 0.8750\\nEpoch 498/500\\n0s - loss: 0.2352 - acc: 0.8750\\nEpoch 499/500\\n0s - loss: 0.2349 - acc: 0.8750\\nEpoch 500/500\\n0s - loss: 0.2346 - acc: 0.8750\\nListing 19.15: Example output of ﬁtting the language model.\\nWe can see that the model does not memorize the source sequences, likely because there is\\nsome ambiguity in the input sequences, for example:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 234}, page_content='19.5. Model 2: Line-by-Line Sequence 218\\njack => and\\njack => fell\\nListing 19.16: Example output of predicting the next word.\\nAnd so on. At the end of the run, Jack is passed in and a prediction or new sequence is\\ngenerated. We get a reasonable sequence as output that has some elements of the source.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nJack and jill came tumbling after down\\nListing 19.17: Example output of predicting a sequence of words.\\nThis is a good ﬁrst cut language model, but does not take full advantage of the LSTM’s\\nability to handle sequences of input and disambiguate some of the ambiguous pairwise sequences\\nby using a broader context.\\n19.5 Model 2: Line-by-Line Sequence\\nAnother approach is to split up the source text line-by-line, then break each line down into a\\nseries of words that build up. For example:\\nX, y\\n_, _, _, _, _, Jack, and\\n_, _, _, _, Jack, and, Jill'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 234}, page_content=\"_, _, _, Jack, and, Jill, went\\n_, _, Jack, and, Jill, went, up\\n_, Jack, and, Jill, went, up, the\\nJack, and, Jill, went, up, the, hill\\nListing 19.18: Example framing of the problem as sequences of words.\\nThis approach may allow the model to use the context of each line to help the model in those\\ncases where a simple one-word-in-and-out model creates ambiguity. In this case, this comes at\\nthe cost of predicting words across lines, which might be ﬁne for now if we are only interested\\nin modeling and generating lines of text. Note that in this representation, we will require a\\npadding of sequences to ensure they meet a ﬁxed length input. This is a requirement when\\nusing Keras. First, we can create the sequences of integers, line-by-line by using the Tokenizer\\nalready ﬁt on the source text.\\n# create line-based sequences\\nsequences = list()\\nfor line in data.split( '\\\\n '):\\nencoded = tokenizer.texts_to_sequences([line])[0]\\nfor i in range(1, len(encoded)):\\nsequence = encoded[:i+1]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 234}, page_content=\"sequences.append(sequence)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 19.19: Example of preparing sequences of words.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 235}, page_content=\"19.5. Model 2: Line-by-Line Sequence 219\\nNext, we can pad the prepared sequences. We can do this using the padsequences()\\nfunction provided in Keras. This ﬁrst involves ﬁnding the longest sequence, then using that as\\nthe length by which to pad-out all other sequences.\\n# pad input sequences\\nmax_length = max([len(seq) for seq in sequences])\\nsequences = pad_sequences(sequences, maxlen=max_length, padding= 'pre ')\\nprint( 'Max Sequence Length: %d '% max_length)\\nListing 19.20: Example of padding sequences of words.\\nNext, we can split the sequences into input and output elements, much like before.\\n# split into input and output elements\\nsequences = array(sequences)\\nX, y = sequences[:,:-1],sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\nListing 19.21: Example of preparing the input and output sequences.\\nThe model can then be deﬁned as before, except the input sequences are now longer than a\\nsingle word. Speciﬁcally, they are maxlength-1 in length, -1 because when we calculated the\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 235}, page_content=\"maximum length of sequences, they included the input and output elements.\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=max_length-1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 19.22: Deﬁne and compile the language model.\\nWe can use the model to generate new sequences as before. The generate seq() function\\ncan be updated to build up an input sequence by adding predictions to the list of input words\\neach iteration.\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 235}, page_content=\"encoded = tokenizer.texts_to_sequences([in_text])[0]\\n# pre-pad sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=max_length, padding= 'pre ')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 236}, page_content=\"19.5. Model 2: Line-by-Line Sequence 220\\nout_word = word\\nbreak\\n# append to input\\nin_text += ' ' + out_word\\nreturn in_text\\nListing 19.23: Function to generate sequences of words given input text.\\nTying all of this together, the complete code example is provided below.\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# pre-pad sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=max_length, padding= 'pre ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 236}, page_content='# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = \\'\\'\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += \\' \\' + out_word\\nreturn in_text\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=max_length-1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= \\'softmax \\'))\\n# compile network\\nmodel.compile(loss= \\'categorical_crossentropy \\', optimizer= \\'adam \\', metrics=[ \\'accuracy \\'])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= \\'model.png \\', show_shapes=True)\\nreturn model\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 237}, page_content='19.5. Model 2: Line-by-Line Sequence 221\\nJack fell down and broke his crown\\\\n\\nAnd Jill came tumbling after\\\\n \"\"\"\\n# prepare the tokenizer on the source text\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\n# determine the vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( \\'Vocabulary Size: %d \\'% vocab_size)\\n# create line-based sequences\\nsequences = list()\\nfor line in data.split( \\'\\\\n \\'):\\nencoded = tokenizer.texts_to_sequences([line])[0]\\nfor i in range(1, len(encoded)):\\nsequence = encoded[:i+1]\\nsequences.append(sequence)\\nprint( \\'Total Sequences: %d \\'% len(sequences))\\n# pad input sequences\\nmax_length = max([len(seq) for seq in sequences])\\nsequences = pad_sequences(sequences, maxlen=max_length, padding= \\'pre \\')\\nprint( \\'Max Sequence Length: %d \\'% max_length)\\n# split into input and output elements\\nsequences = array(sequences)\\nX, y = sequences[:,:-1],sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(vocab_size, max_length)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 237}, page_content=\"# fit network\\nmodel.fit(X, y, epochs=500, verbose=2)\\n# evaluate model\\nprint(generate_seq(model, tokenizer, max_length-1, 'Jack ', 4))\\nprint(generate_seq(model, tokenizer, max_length-1, 'Jill ', 4))\\nListing 19.24: Complete example of model2.\\nRunning the example achieves a better ﬁt on the source data. The added context has allowed\\nthe model to disambiguate some of the examples. There are still two lines of text that start\\nwith “ Jack” that may still be a problem for the network.\\n...\\nEpoch 496/500\\n0s - loss: 0.1039 - acc: 0.9524\\nEpoch 497/500\\n0s - loss: 0.1037 - acc: 0.9524\\nEpoch 498/500\\n0s - loss: 0.1035 - acc: 0.9524\\nEpoch 499/500\\n0s - loss: 0.1033 - acc: 0.9524\\nEpoch 500/500\\n0s - loss: 0.1032 - acc: 0.9524\\nListing 19.25: Example output of ﬁtting the language model.\\nAt the end of the run, we generate two sequences with diﬀerent seed words: Jack and Jill.\\nThe ﬁrst generated line looks good, directly matching the source text. The second is a bit\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 237}, page_content='strange. This makes sense, because the network only ever saw Jillwithin an input sequence,\\nnot at the beginning of the sequence, so it has forced an output to use the word Jill, i.e. the'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 238}, page_content='19.6. Model 3: Two-Words-In, One-Word-Out Sequence 222\\nlast line of the rhyme.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nJack fell down and broke\\nJill jill came tumbling after\\nListing 19.26: Example output of generating sequences of words.\\nThis was a good example of how the framing may result in better new lines, but not good\\npartial lines of input.\\n19.6 Model 3: Two-Words-In, One-Word-Out Sequence\\nWe can use an intermediate between the one-word-in and the whole-sentence-in approaches\\nand pass in a sub-sequences of words as input. This will provide a trade-oﬀ between the two\\nframings allowing new lines to be generated and for generation to be picked up mid line. We will\\nuse 3 words as input to predict one word as output. The preparation of the sequences is much\\nlike the ﬁrst example, except with diﬀerent oﬀsets in the source sequence arrays, as follows:\\n# encode 2 words -> 1 word\\nsequences = list()'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 238}, page_content=\"sequences = list()\\nfor i in range(2, len(encoded)):\\nsequence = encoded[i-2:i+1]\\nsequences.append(sequence)\\nListing 19.27: Example of preparing constrained sequence data.\\nThe complete example is listed below\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# pre-pad sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=max_length, padding= 'pre ')\\n# predict probabilities for each word\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 238}, page_content=\"yhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 239}, page_content='19.6. Model 3: Two-Words-In, One-Word-Out Sequence 223\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += \\' \\' + out_word\\nreturn in_text\\n# define the model\\ndef define_model(vocab_size, max_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 10, input_length=max_length-1))\\nmodel.add(LSTM(50))\\nmodel.add(Dense(vocab_size, activation= \\'softmax \\'))\\n# compile network\\nmodel.compile(loss= \\'categorical_crossentropy \\', optimizer= \\'adam \\', metrics=[ \\'accuracy \\'])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= \\'model.png \\', show_shapes=True)\\nreturn model\\n# source text\\ndata = \"\"\" Jack and Jill went up the hill\\\\n\\nTo fetch a pail of water\\\\n\\nJack fell down and broke his crown\\\\n\\nAnd Jill came tumbling after\\\\n \"\"\"\\n# integer encode sequences of words\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts([data])\\nencoded = tokenizer.texts_to_sequences([data])[0]\\n# retrieve vocabulary size'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 239}, page_content=\"vocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# encode 2 words -> 1 word\\nsequences = list()\\nfor i in range(2, len(encoded)):\\nsequence = encoded[i-2:i+1]\\nsequences.append(sequence)\\nprint( 'Total Sequences: %d '% len(sequences))\\n# pad sequences\\nmax_length = max([len(seq) for seq in sequences])\\nsequences = pad_sequences(sequences, maxlen=max_length, padding= 'pre ')\\nprint( 'Max Sequence Length: %d '% max_length)\\n# split into input and output elements\\nsequences = array(sequences)\\nX, y = sequences[:,:-1],sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\n# define model\\nmodel = define_model(vocab_size, max_length)\\n# fit network\\nmodel.fit(X, y, epochs=500, verbose=2)\\n# evaluate model\\nprint(generate_seq(model, tokenizer, max_length-1, 'Jack and ', 5))\\nprint(generate_seq(model, tokenizer, max_length-1, 'And Jill ', 3))\\nprint(generate_seq(model, tokenizer, max_length-1, 'fell down ', 5))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 239}, page_content=\"print(generate_seq(model, tokenizer, max_length-1, 'pail of ', 5))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 240}, page_content='19.7. Further Reading 224\\nListing 19.28: Complete example of model3.\\nRunning the example again gets a good ﬁt on the source text at around 95% accuracy.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\n...\\nEpoch 496/500\\n0s - loss: 0.0685 - acc: 0.9565\\nEpoch 497/500\\n0s - loss: 0.0685 - acc: 0.9565\\nEpoch 498/500\\n0s - loss: 0.0684 - acc: 0.9565\\nEpoch 499/500\\n0s - loss: 0.0684 - acc: 0.9565\\nEpoch 500/500\\n0s - loss: 0.0684 - acc: 0.9565\\nListing 19.29: Example output of ﬁtting the language model.\\nWe look at 4 generation examples, two start of line cases and two starting mid line.\\nJack and jill went up the hill\\nAnd Jill went up the\\nfell down and broke his crown and\\npail of water jack fell down and\\nListing 19.30: Example output of generating sequences of words.\\nThe ﬁrst start of line case generated correctly, but the second did not. The second case was'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 240}, page_content='an example from the 4th line, which is ambiguous with content from the ﬁrst line. Perhaps a\\nfurther expansion to 3 input words would be better. The two mid-line generation examples were\\ngenerated correctly, matching the source text.\\nWe can see that the choice of how the language model is framed and the requirements on\\nhow the model will be used must be compatible. That careful design is required when using\\nlanguage models in general, perhaps followed-up by spot testing with sequence generation to\\nconﬁrm model requirements have been met.\\n19.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Jack and Jill on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Jack_and_Jill_(nursery_rhyme)\\n\\x88Language Model on Wikpedia.\\nhttps://en.wikipedia.org/wiki/Language_model\\n\\x88Keras Embedding Layer API.\\nhttps://keras.io/layers/embeddings/#embedding'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 241}, page_content='19.8. Summary 225\\n\\x88Keras Text Processing API.\\nhttps://keras.io/preprocessing/text/\\n\\x88Keras Sequence Processing API.\\nhttps://keras.io/preprocessing/sequence/\\n\\x88Keras Utils API.\\nhttps://keras.io/utils/\\n19.8 Summary\\nIn this tutorial, you discovered how to develop diﬀerent word-based language models for a simple\\nnursery rhyme. Speciﬁcally, you learned:\\n\\x88The challenge of developing a good framing of a word-based language model for a given\\napplication.\\n\\x88How to develop one-word, two-word, and line-based framings for word-based language\\nmodels.\\n\\x88How to generate sequences using a ﬁt language model.\\n19.8.1 Next\\nIn the next chapter, you will discover how you can develop a word-based neural language model\\non a large corpus of text.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 242}, page_content='Chapter 20\\nProject: Develop a Neural Language\\nModel for Text Generation\\nA language model can predict the probability of the next word in the sequence, based on the\\nwords already observed in the sequence. Neural network models are a preferred method for\\ndeveloping statistical language models because they can use a distributed representation where\\ndiﬀerent words with similar meanings have similar representation and because they can use a\\nlarge context of recently observed words when making predictions. In this tutorial, you will\\ndiscover how to develop a statistical language model using deep learning in Python. After\\ncompleting this tutorial, you will know:\\n\\x88How to prepare text for developing a word-based language model.\\n\\x88How to design and ﬁt a neural language model with a learned embedding and an LSTM\\nhidden layer.\\n\\x88How to use the learned language model to generate new text with similar statistical\\nproperties as the source text.\\nLet’s get started.\\n20.1 Tutorial Overview'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 242}, page_content='This tutorial is divided into the following parts:\\n1. The Republic by Plato\\n2. Data Preparation\\n3. Train Language Model\\n4. Use Language Model\\n226'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 243}, page_content='20.2. The Republic by Plato 227\\n20.2 The Republic by Plato\\nThe Republic is the classical Greek philosopher Plato’s most famous work. It is structured as a\\ndialog (e.g. conversation) on the topic of order and justice within a city state The entire text is\\navailable for free in the public domain. It is available on the Project Gutenberg website in a\\nnumber of formats. You can download the ASCII text version of the entire book (or books)\\nhere (you might need to open the URL twice):\\n\\x88Download The Republic by Plato.\\nhttp://www.gutenberg.org/cache/epub/1497/pg1497.txt\\nDownload the book text and place it in your current working directly with the ﬁlename\\nrepublic.txt . Open the ﬁle in a text editor and delete the front and back matter. This\\nincludes details about the book at the beginning, a long analysis, and license information at the\\nend. The text should begin with:\\nBOOK I.\\nI went down yesterday to the Piraeus with Glaucon the son of Ariston, ...\\nAnd end with:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 243}, page_content='And end with:\\n... And it shall be well with us both in this life and in the pilgrimage of a thousand\\nyears which we have been describing.\\nSave the cleaned version as republic clean.txt in your current working directory. The\\nﬁle should be about 15,802 lines of text. Now we can develop a language model from this text.\\n20.3 Data Preparation\\nWe will start by preparing the data for modeling. The ﬁrst step is to look at the data.\\n20.3.1 Review the Text\\nOpen the text in an editor and just look at the text data. For example, here is the ﬁrst piece of\\ndialog:\\nBOOK I.\\nI went down yesterday to the Piraeus with Glaucon the son of Ariston, that I might\\noﬀer up my prayers to the goddess (Bendis, the Thracian Artemis.); and also because\\nI wanted to see in what manner they would celebrate the festival, which was a\\nnew thing. I was delighted with the procession of the inhabitants; but that of the\\nThracians was equally, if not more, beautiful. When we had ﬁnished our prayers'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 243}, page_content='and viewed the spectacle, we turned in the direction of the city; and at that instant\\nPolemarchus the son of Cephalus chanced to catch sight of us from a distance as we\\nwere starting on our way home, and told his servant to run and bid us wait for him.\\nThe servant took hold of me by the cloak behind, and said: Polemarchus desires you\\nto wait.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 244}, page_content='20.3. Data Preparation 228\\nI turned round, and asked him where his master was.\\nThere he is, said the youth, coming after you, if you will only wait.\\nCertainly we will, said Glaucon; and in a few minutes Polemarchus appeared, and\\nwith him Adeimantus, Glaucon’s brother, Niceratus the son of Nicias, and several\\nothers who had been at the procession.\\nPolemarchus said to me: I perceive, Socrates, that you and your companion are\\nalready on your way to the city.\\nYou are not far wrong, I said.\\n...\\nWhat do you see that we will need to handle in preparing the data? Here’s what I see from\\na quick look:\\n\\x88Book/Chapter headings (e.g. BOOK I. ).\\n\\x88Lots of punctuation (e.g. -,;-,?-, and more).\\n\\x88Strange names (e.g. Polemarchus ).\\n\\x88Some long monologues that go on for hundreds of lines.\\n\\x88Some quoted dialog (e.g. ‘...’).\\nThese observations, and more, suggest at ways that we may wish to prepare the text data.\\nThe speciﬁc way we prepare the data really depends on how we intend to model it, which in'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 244}, page_content='turn depends on how we intend to use it.\\n20.3.2 Language Model Design\\nIn this tutorial, we will develop a model of the text that we can then use to generate new\\nsequences of text. The language model will be statistical and will predict the probability of\\neach word given an input sequence of text. The predicted word will be fed in as input to in\\nturn generate the next word. A key design decision is how long the input sequences should be.\\nThey need to be long enough to allow the model to learn the context for the words to predict.\\nThis input length will also deﬁne the length of seed text used to generate new sequences when\\nwe use the model.\\nThere is no correct answer. With enough time and resources, we could explore the ability of\\nthe model to learn with diﬀerently sized input sequences. Instead, we will pick a length of 50\\nwords for the length of the input sequences, somewhat arbitrarily. We could process the data so'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 244}, page_content='that the model only ever deals with self-contained sentences and pad or truncate the text to\\nmeet this requirement for each input sequence. You could explore this as an extension to this\\ntutorial.\\nInstead, to keep the example brief, we will let all of the text ﬂow together and train the\\nmodel to predict the next word across sentences, paragraphs, and even books or chapters in the\\ntext. Now that we have a model design, we can look at transforming the raw text into sequences\\nof 100 input words to 1 output word, ready to ﬁt a model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 245}, page_content=\"20.3. Data Preparation 229\\n20.3.3 Load Text\\nThe ﬁrst step is to load the text into memory. We can develop a small function to load the\\nentire text ﬁle into memory and return it. The function is called load doc() and is listed below.\\nGiven a ﬁlename, it returns a sequence of loaded text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 20.1: Function to load text into memory.\\nUsing this function, we can load the cleaner version of the document in the ﬁle\\nrepublic clean.txt as follows:\\n# load document\\nin_filename = 'republic_clean.txt '\\ndoc = load_doc(in_filename)\\nprint(doc[:200])\\nListing 20.2: Example of loading the text into memory.\\nRunning this snippet loads the document and prints the ﬁrst 200 characters as a sanity\\ncheck.\\nBOOK I.\\nI went down yesterday to the Piraeus with Glaucon the son of Ariston,\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 245}, page_content='that I might offer up my prayers to the goddess (Bendis, the Thracian\\nArtemis.); and also because I wanted to see in what\\nListing 20.3: Example output of loading the text into memory.\\nSo far, so good. Next, let’s clean the text.\\n20.3.4 Clean Text\\nWe need to transform the raw text into a sequence of tokens or words that we can use as a\\nsource to train the model. Based on reviewing the raw text (above), below are some speciﬁc\\noperations we will perform to clean the text. You may want to explore more cleaning operations\\nyourself as an extension.\\n\\x88Replace ‘-’ with a white space so we can split words better.\\n\\x88Split words based on white space.\\n\\x88Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes\\n‘What’).\\n\\x88Remove all words that are not alphabetic to remove standalone punctuation tokens.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 246}, page_content=\"20.3. Data Preparation 230\\n\\x88Normalize all words to lowercase to reduce the vocabulary size.\\nVocabulary size is a big deal with language modeling. A smaller vocabulary results in a\\nsmaller model that trains faster. We can implement each of these cleaning operations in this\\norder in a function. Below is the function clean doc() that takes a loaded document as an\\nargument and returns an array of clean tokens.\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# replace '-- 'with a space ' '\\ndoc = doc.replace( '-- ', ' ')\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# make lower case\\ntokens = [word.lower() for word in tokens]\\nreturn tokens\\nListing 20.4: Function to clean text.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 246}, page_content=\"We can run this cleaning operation on our loaded document and print out some of the tokens\\nand statistics as a sanity check.\\n# clean document\\ntokens = clean_doc(doc)\\nprint(tokens[:200])\\nprint( 'Total Tokens: %d '% len(tokens))\\nprint( 'Unique Tokens: %d '% len(set(tokens)))\\nListing 20.5: Example of cleaning text.\\nFirst, we can see a nice list of tokens that look cleaner than the raw text. We could remove\\nthe ’Book I’ chapter markers and more, but this is a good start.\\n[ 'book ', 'i ', 'i ', 'went ', 'down ', 'yesterday ', 'to ', 'the ', 'piraeus ', 'with ', 'glaucon ',\\n'the ', 'son ', 'of ', 'ariston ', 'that ', 'i ', 'might ', 'offer ', 'up ', 'my ', 'prayers ',\\n'to ', 'the ', 'goddess ', 'bendis ', 'the ', 'thracian ', 'artemis ', 'and ', 'also ',\\n'because ', 'i ', 'wanted ', 'to ', 'see ', 'in ', 'what ', 'manner ', 'they ', 'would ',\\n'celebrate ', 'the ', 'festival ', 'which ', 'was ', 'a ', 'new ', 'thing ', 'i ', 'was ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 246}, page_content=\"'delighted ', 'with ', 'the ', 'procession ', 'of ', 'the ', 'inhabitants ', 'but ', 'that ',\\n'of ', 'the ', 'thracians ', 'was ', 'equally ', 'if ', 'not ', 'more ', 'beautiful ', 'when ',\\n'we ', 'had ', 'finished ', 'our ', 'prayers ', 'and ', 'viewed ', 'the ', 'spectacle ', 'we ',\\n'turned ', 'in ', 'the ', 'direction ', 'of ', 'the ', 'city ', 'and ', 'at ', 'that ',\\n'instant ', 'polemarchus ', 'the ', 'son ', 'of ', 'cephalus ', 'chanced ', 'to ', 'catch ',\\n'sight ', 'of ', 'us ', 'from ', 'a ', 'distance ', 'as ', 'we ', 'were ', 'starting ', 'on ',\\n'our ', 'way ', 'home ', 'and ', 'told ', 'his ', 'servant ', 'to ', 'run ', 'and ', 'bid ', 'us ',\\n'wait ', 'for ', 'him ', 'the ', 'servant ', 'took ', 'hold ', 'of ', 'me ', 'by ', 'the ',\\n'cloak ', 'behind ', 'and ', 'said ', 'polemarchus ', 'desires ', 'you ', 'to ', 'wait ', 'i ',\\n'turned ', 'round ', 'and ', 'asked ', 'him ', 'where ', 'his ', 'master ', 'was ', 'there ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 246}, page_content=\"'he ', 'is ', 'said ', 'the ', 'youth ', 'coming ', 'after ', 'you ', 'if ', 'you ', 'will ',\\n'only ', 'wait ', 'certainly ', 'we ', 'will ', 'said ', 'glaucon ', 'and ', 'in ', 'a ', 'few ',\\n'minutes ', 'polemarchus ', 'appeared ', 'and ', 'with ', 'him ', 'adeimantus ', 'glaucons ',\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 247}, page_content=\"20.3. Data Preparation 231\\n'brother ', 'niceratus ', 'the ', 'son ', 'of ', 'nicias ', 'and ', 'several ', 'others ',\\n'who ', 'had ', 'been ', 'at ', 'the ', 'procession ', 'polemarchus ', 'said ']\\nListing 20.6: Example output of tokenized and clean text.\\nWe also get some statistics about the clean document. We can see that there are just under\\n120,000 words in the clean text and a vocabulary of just under 7,500 words. This is smallish\\nand models ﬁt on this data should be manageable on modest hardware.\\nTotal Tokens: 118684\\nUnique Tokens: 7409\\nListing 20.7: Example output summarizing properties of the clean text.\\nNext, we can look at shaping the tokens into sequences and saving them to ﬁle.\\n20.3.5 Save Clean Text\\nWe can organize the long list of tokens into sequences of 50 input words and 1 output word.\\nThat is, sequences of 51 words. We can do this by iterating over the list of tokens from token 51\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 247}, page_content=\"onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of\\nthe list of tokens. We will transform the tokens into space-separated strings for later storage\\nin a ﬁle. The code to split the list of clean tokens into sequences with a length of 51 tokens is\\nlisted below.\\n# organize into sequences of tokens\\nlength = 50 + 1\\nsequences = list()\\nfor i in range(length, len(tokens)):\\n# select sequence of tokens\\nseq = tokens[i-length:i]\\n# convert into a line\\nline = ' '.join(seq)\\n# store\\nsequences.append(line)\\nprint( 'Total Sequences: %d '% len(sequences))\\nListing 20.8: Split document into sequences of text.\\nRunning this piece creates a long list of lines. Printing statistics on the list, we can see that\\nwe will have exactly 118,633 training patterns to ﬁt our model.\\nTotal Sequences: 118633\\nListing 20.9: Example output of splitting the document into sequences.\\nNext, we can save the sequences to a new ﬁle for later loading. We can deﬁne a new function\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 247}, page_content=\"for saving lines of text to a ﬁle. This new function is called save doc() and is listed below. It\\ntakes as input a list of lines and a ﬁlename. The lines are written, one per line, in ASCII format.\\n# save tokens to file, one dialog per line\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 248}, page_content=\"20.3. Data Preparation 232\\nListing 20.10: Function to save sequences of text to ﬁle.\\nWe can call this function and save our training sequences to the ﬁle republic sequences.txt .\\n# save sequences to file\\nout_filename = 'republic_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 20.11: Example of saving sequences to ﬁle.\\nTake a look at the ﬁle with your text editor. You will see that each line is shifted along one\\nword, with a new word at the end to be predicted; for example, here are the ﬁrst 3 lines in\\ntruncated form:\\nbook i i ... catch sight of\\ni i went ... sight of us\\ni went down ... of us from\\n...\\nListing 20.12: Example contents of sequences saved to ﬁle.\\n20.3.6 Complete Example\\nTying all of this together, the complete code listing is provided below.\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 248}, page_content=\"return text\\n# turn a doc into clean tokens\\ndef clean_doc(doc):\\n# replace '-- 'with a space ' '\\ndoc = doc.replace( '-- ', ' ')\\n# split into tokens by white space\\ntokens = doc.split()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\n# remove punctuation from each word\\ntokens = [re_punc.sub( '', w) for w in tokens]\\n# remove remaining tokens that are not alphabetic\\ntokens = [word for word in tokens if word.isalpha()]\\n# make lower case\\ntokens = [word.lower() for word in tokens]\\nreturn tokens\\n# save tokens to file, one dialog per line\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 249}, page_content=\"20.4. Train Language Model 233\\ndef save_doc(lines, filename):\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\n# load document\\nin_filename = 'republic_clean.txt '\\ndoc = load_doc(in_filename)\\nprint(doc[:200])\\n# clean document\\ntokens = clean_doc(doc)\\nprint(tokens[:200])\\nprint( 'Total Tokens: %d '% len(tokens))\\nprint( 'Unique Tokens: %d '% len(set(tokens)))\\n# organize into sequences of tokens\\nlength = 50 + 1\\nsequences = list()\\nfor i in range(length, len(tokens)):\\n# select sequence of tokens\\nseq = tokens[i-length:i]\\n# convert into a line\\nline = ' '.join(seq)\\n# store\\nsequences.append(line)\\nprint( 'Total Sequences: %d '% len(sequences))\\n# save sequences to file\\nout_filename = 'republic_sequences.txt '\\nsave_doc(sequences, out_filename)\\nListing 20.13: Complete example preparing text data for modeling.\\nYou should now have training data stored in the ﬁle republic sequences.txt in your\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 249}, page_content='current working directory. Next, let’s look at how to ﬁt a language model to this data.\\n20.4 Train Language Model\\nWe can now train a statistical language model from the prepared data. The model we will train\\nis a neural language model. It has a few unique characteristics:\\n\\x88It uses a distributed representation for words so that diﬀerent words with similar meanings\\nwill have a similar representation.\\n\\x88It learns the representation at the same time as learning the model.\\n\\x88It learns to predict the probability for the next word using the context of the last 100\\nwords.\\nSpeciﬁcally, we will use an Embedding Layer to learn the representation of words, and a\\nLong Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on\\ntheir context. Let’s start by loading our training data.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 250}, page_content=\"20.4. Train Language Model 234\\n20.4.1 Load Sequences\\nWe can load our training data using the load doc() function we developed in the previous\\nsection. Once loaded, we can split the data into separate training sequences by splitting based\\non new lines. The snippet below will load the republic sequences.txt data ﬁle from the\\ncurrent working directory.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\nListing 20.14: Load the clean sequences from ﬁle.\\nNext, we can encode the training data.\\n20.4.2 Encode Sequences\\nThe word embedding layer expects input sequences to be comprised of integers. We can map\\neach word in our vocabulary to a unique integer and encode our input sequences. Later, when\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 250}, page_content='we make predictions, we can convert the prediction to numbers and look up their associated\\nwords in the same mapping. To do this encoding, we will use the Tokenizer class in the Keras\\nAPI.\\nFirst, the Tokenizer must be trained on the entire training dataset, which means it ﬁnds\\nall of the unique words in the data and assigns each a unique integer. We can then use the ﬁt\\nTokenizer to encode all of the training sequences, converting each sequence from a list of words\\nto a list of integers.\\n# integer encode sequences of words\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nsequences = tokenizer.texts_to_sequences(lines)\\nListing 20.15: Train a tokenizer on the loaded sequences.\\nWe can access the mapping of words to integers as a dictionary attribute called word index\\non the Tokenizer object. We need to know the size of the vocabulary for deﬁning the embedding\\nlayer later. We can determine the vocabulary by calculating the size of the mapping dictionary.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 250}, page_content='Words are assigned values from 1 to the total number of words (e.g. 7,409). The Embedding\\nlayer needs to allocate a vector representation for each word in this vocabulary from index 1 to\\nthe largest index and because indexing of arrays is zero-oﬀset, the index of the word at the end\\nof the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length. Therefore,\\nwhen specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the\\nactual vocabulary.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 251}, page_content='20.4. Train Language Model 235\\n# vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\nListing 20.16: Calculate the size of the vocabulary.\\n20.4.3 Sequence Inputs and Output\\nNow that we have encoded the input sequences, we need to separate them into input ( X)and\\noutput ( y) elements. We can do this with array slicing. After separating, we need to one hot\\nencode the output word. This means converting it from an integer to a vector of 0 values, one\\nfor each word in the vocabulary, with a 1 to indicate the speciﬁc word at the index of the words\\ninteger value.\\nThis is so that the model learns to predict the probability distribution for the next word and\\nthe ground truth from which to learn from is 0 for all words except the actual word that comes\\nnext. Keras provides the tocategorical() that can be used to one hot encode the output\\nwords for each input-output sequence pair.\\nFinally, we need to specify to the Embedding layer how long input sequences are. We know'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 251}, page_content='that there are 50 words because we designed the model, but a good generic way to specify that\\nis to use the second dimension (number of columns) of the input data’s shape. That way, if\\nyou change the length of sequences when preparing data, you do not need to change this data\\nloading code; it is generic.\\n# separate into input and output\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\nseq_length = X.shape[1]\\nListing 20.17: Split text data into input and output sequences.\\n20.4.4 Fit Model\\nWe can now deﬁne and ﬁt our language model on the training data. The learned embedding\\nneeds to know the size of the vocabulary and the length of input sequences as previously\\ndiscussed. It also has a parameter to specify how many dimensions will be used to represent\\neach word. That is, the size of the embedding vector space.\\nCommon values are 50, 100, and 300. We will use 50 here, but consider testing smaller or'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 251}, page_content='larger values. We will use a two LSTM hidden layers with 100 memory cells each. More memory\\ncells and a deeper network may achieve better results.\\nA dense fully connected layer with 100 neurons connects to the LSTM hidden layers to\\ninterpret the features extracted from the sequence. The output layer predicts the next word as\\na single vector the size of the vocabulary with a probability for each word in the vocabulary. A\\nsoftmax activation function is used to ensure the outputs have the characteristics of normalized\\nprobabilities.\\n# define the model\\ndef define_model(vocab_size, seq_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 50, input_length=seq_length))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 252}, page_content=\"20.4. Train Language Model 236\\nmodel.add(LSTM(100, return_sequences=True))\\nmodel.add(LSTM(100))\\nmodel.add(Dense(100, activation= 'relu '))\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 20.18: Deﬁne the language model.\\nA summary of the deﬁned network is printed as a sanity check to ensure we have constructed\\nwhat we intended.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 50, 50) 370500\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 50, 100) 60400\\n_________________________________________________________________\\nlstm_2 (LSTM) (None, 100) 80400\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 252}, page_content='_________________________________________________________________\\ndense_1 (Dense) (None, 100) 10100\\n_________________________________________________________________\\ndense_2 (Dense) (None, 7410) 748410\\n=================================================================\\nTotal params: 1,269,810\\nTrainable params: 1,269,810\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 20.19: Example output from summarizing the deﬁned model.\\nA plot the deﬁned model is then saved to ﬁle with the name model.png .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 253}, page_content='20.4. Train Language Model 237\\nFigure 20.1: Plot of the deﬁned word-based language model.\\nThe model is compiled specifying the categorical cross entropy loss needed to ﬁt the model.\\nTechnically, the model is learning a multiclass classiﬁcation and this is the suitable loss function\\nfor this type of problem. The eﬃcient Adam implementation to mini-batch gradient descent\\nis used and accuracy is evaluated of the model. Finally, the model is ﬁt on the data for 100\\ntraining epochs with a modest batch size of 128 to speed things up. Training may take a few\\nhours on modern hardware without GPUs. You can speed it up with a larger batch size and/or\\nfewer training epochs.\\nDuring training, you will see a summary of performance, including the loss and accuracy\\nevaluated from the training data at the end of each batch update. You will get diﬀerent results,\\nbut perhaps an accuracy of just over 50% of predicting the next word in the sequence, which is'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 253}, page_content='not bad. We are not aiming for 100% accuracy (e.g. a model that memorized the text), but\\nrather a model that captures the essence of the text.\\n...\\nEpoch 96/100\\n118633/118633 [==============================] - 265s - loss: 2.0324 - acc: 0.5187\\nEpoch 97/100\\n118633/118633 [==============================] - 265s - loss: 2.0136 - acc: 0.5247\\nEpoch 98/100'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 254}, page_content=\"20.4. Train Language Model 238\\n118633/118633 [==============================] - 267s - loss: 1.9956 - acc: 0.5262\\nEpoch 99/100\\n118633/118633 [==============================] - 266s - loss: 1.9812 - acc: 0.5291\\nEpoch 100/100\\n118633/118633 [==============================] - 270s - loss: 1.9709 - acc: 0.5315\\nListing 20.20: Example output from training the language model.\\n20.4.5 Save Model\\nAt the end of the run, the trained model is saved to ﬁle. Here, we use the Keras model API to\\nsave the model to the ﬁle model.h5 in the current working directory. Later, when we load the\\nmodel to make predictions, we will also need the mapping of words to integers. This is in the\\nTokenizer object, and we can save that too using Pickle.\\n# save the model to file\\nmodel.save( 'model.h5 ')\\n# save the tokenizer\\ndump(tokenizer, open( 'tokenizer.pkl ', 'wb '))\\nListing 20.21: Save the ﬁt model and Tokenizer to ﬁle.\\n20.4.6 Complete Example\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 254}, page_content=\"We can put all of this together; the complete example for ﬁtting the language model is listed\\nbelow.\\nfrom numpy import array\\nfrom pickle import dump\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.utils import to_categorical\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# define the model\\ndef define_model(vocab_size, seq_length):\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, 50, input_length=seq_length))\\nmodel.add(LSTM(100, return_sequences=True))\\nmodel.add(LSTM(100))\\nmodel.add(Dense(100, activation= 'relu '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 255}, page_content=\"20.5. Use Language Model 239\\nmodel.add(Dense(vocab_size, activation= 'softmax '))\\n# compile network\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ', metrics=[ 'accuracy '])\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\n# integer encode sequences of words\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nsequences = tokenizer.texts_to_sequences(lines)\\n# vocabulary size\\nvocab_size = len(tokenizer.word_index) + 1\\n# separate into input and output\\nsequences = array(sequences)\\nX, y = sequences[:,:-1], sequences[:,-1]\\ny = to_categorical(y, num_classes=vocab_size)\\nseq_length = X.shape[1]\\n# define model\\nmodel = define_model(vocab_size, seq_length)\\n# fit model\\nmodel.fit(X, y, batch_size=128, epochs=100)\\n# save the model to file\\nmodel.save( 'model.h5 ')\\n# save the tokenizer\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 255}, page_content=\"dump(tokenizer, open( 'tokenizer.pkl ', 'wb '))\\nListing 20.22: Complete example training the language model.\\n20.5 Use Language Model\\nNow that we have a trained language model, we can use it. In this case, we can use it to generate\\nnew sequences of text that have the same statistical properties as the source text. This is not\\npractical, at least not for this example, but it gives a concrete example of what the language\\nmodel has learned. We will start by loading the training sequences again.\\n20.5.1 Load Data\\nWe can use the same code from the previous section to load the training data sequences of text.\\nSpeciﬁcally, the load doc() function.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 256}, page_content=\"20.5. Use Language Model 240\\nfile.close()\\nreturn text\\n# load cleaned text sequences\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\nListing 20.23: Load the clean sequences from ﬁle.\\nWe need the text so that we can choose a source sequence as input to the model for generating\\na new sequence of text. The model will require 50 words as input. Later, we will need to specify\\nthe expected length of input. We can determine this from the input sequences by calculating\\nthe length of one line of the loaded data and subtracting 1 for the expected output word that is\\nalso on the same line.\\nseq_length = len(lines[0].split()) - 1\\nListing 20.24: Calculate the expected input length.\\n20.5.2 Load Model\\nWe can now load the model from ﬁle. Keras provides the load model() function for loading\\nthe model, ready for use.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\nListing 20.25: Load the saved model from ﬁle.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 256}, page_content=\"We can also load the tokenizer from ﬁle using the Pickle API.\\n# load the tokenizer\\ntokenizer = load(open( 'tokenizer.pkl ', 'rb '))\\nListing 20.26: Load the saved Tokenizer from ﬁle.\\nWe are ready to use the loaded model.\\n20.5.3 Generate Text\\nThe ﬁrst step in generating text is preparing a seed input. We will select a random line of text\\nfrom the input text for this purpose. Once selected, we will print it so that we have some idea\\nof what was used.\\n# select a seed text\\nseed_text = lines[randint(0,len(lines))]\\nprint(seed_text + '\\\\n ')\\nListing 20.27: Select random examples as seed text.\\nNext, we can generate new words, one at a time. First, the seed text must be encoded to\\nintegers using the same tokenizer that we used when training the model.\\nencoded = tokenizer.texts_to_sequences([seed_text])[0]\\nListing 20.28: Encode the selected seed text.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 257}, page_content=\"20.5. Use Language Model 241\\nThe model can predict the next word directly by calling model.predict classes() that\\nwill return the index of the word with the highest probability.\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\nListing 20.29: Predict the next word in the sequence.\\nWe can then look up the index in the Tokenizer ’s mapping to get the associated word.\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\nListing 20.30: Map the predicted integer to a word in the known vocabulary.\\nWe can then append this word to the seed text and repeat the process. Importantly, the\\ninput sequence is going to get too long. We can truncate it to the desired length after the input\\nsequence has been encoded to integers. Keras provides the padsequences() function that we\\ncan use to perform this truncation.\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 257}, page_content=\"Listing 20.31: Pad the encoded sequence.\\nWe can wrap all of this into a function called generate seq() that takes as input the model,\\nthe tokenizer, input sequence length, the seed text, and the number of words to generate. It\\nthen returns a sequence of words generated by the model.\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, seq_length, seed_text, n_words):\\nresult = list()\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# truncate sequences to a fixed length\\nencoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += ' ' + out_word\\nresult.append(out_word)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 257}, page_content=\"return ' '.join(result)\\nListing 20.32: Function to generate a sequence of words given the model and seed text.\\nWe are now ready to generate a sequence of new words given some seed text.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 258}, page_content=\"20.5. Use Language Model 242\\n# generate new text\\ngenerated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\\nprint(generated)\\nListing 20.33: Example of generating a sequence of text.\\nPutting this all together, the complete code listing for generating text from the learned-\\nlanguage model is listed below.\\nfrom random import randint\\nfrom pickle import load\\nfrom keras.models import load_model\\nfrom keras.preprocessing.sequence import pad_sequences\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# generate a sequence from a language model\\ndef generate_seq(model, tokenizer, seq_length, seed_text, n_words):\\nresult = list()\\nin_text = seed_text\\n# generate a fixed number of words\\nfor _ in range(n_words):\\n# encode the text as integer\\nencoded = tokenizer.texts_to_sequences([in_text])[0]\\n# truncate sequences to a fixed length\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 258}, page_content=\"encoded = pad_sequences([encoded], maxlen=seq_length, truncating= 'pre ')\\n# predict probabilities for each word\\nyhat = model.predict_classes(encoded, verbose=0)\\n# map predicted word index to word\\nout_word = ''\\nfor word, index in tokenizer.word_index.items():\\nif index == yhat:\\nout_word = word\\nbreak\\n# append to input\\nin_text += ' ' + out_word\\nresult.append(out_word)\\nreturn ' '.join(result)\\n# load cleaned text sequences\\nin_filename = 'republic_sequences.txt '\\ndoc = load_doc(in_filename)\\nlines = doc.split( '\\\\n ')\\nseq_length = len(lines[0].split()) - 1\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# load the tokenizer\\ntokenizer = load(open( 'tokenizer.pkl ', 'rb '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 259}, page_content=\"20.6. Extensions 243\\n# select a seed text\\nseed_text = lines[randint(0,len(lines))]\\nprint(seed_text + '\\\\n ')\\n# generate new text\\ngenerated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\\nprint(generated)\\nListing 20.34: Complete example of generating sequences of text.\\nRunning the example ﬁrst prints the seed text.\\nwhen he said that a man when he grows old may learn many things for he can no more learn\\nmuch than he can run much youth is the time for any extraordinary toil of course and\\ntherefore calculation and geometry and all the other elements of instruction which are a\\nListing 20.35: Example output from selecting seed text.\\nThen 50 words of generated text are printed.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\npreparation for dialectic should be presented to the name of idle spendthrifts of whom the\\nother is the manifold and the unjust and is the best and the other which delighted to\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 259}, page_content='be the opening of the soul of the soul and the embroiderer will have to be said at\\nListing 20.36: Example output of generated text.\\nYou can see that the text seems reasonable. In fact, the addition of concatenation would\\nhelp in interpreting the seed and the generated text. Nevertheless, the generated text gets the\\nright kind of words in the right kind of order. Try running the example a few times to see other\\nexamples of generated text.\\n20.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Contrived Seed Text . Hand craft or select seed text and evaluate how the seed text\\nimpacts the generated text, speciﬁcally the initial words or sentences generated.\\n\\x88Simplify Vocabulary . Explore a simpler vocabulary, perhaps with stemmed words or\\nstop words removed.\\n\\x88Data Cleaning . Consider using more or less cleaning of the text, perhaps leave in some\\npunctuation or perhaps replacing all fancy names with one or a handful. Evaluate how'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 259}, page_content='these changes to the size of the vocabulary impact the generated text.\\n\\x88Tune Model . Tune the model, such as the size of the embedding or number of memory\\ncells in the hidden layer, to see if you can develop a better model.\\n\\x88Deeper Model . Extend the model to have multiple LSTM hidden layers, perhaps with\\ndropout to see if you can develop a better model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 260}, page_content='20.7. Further Reading 244\\n\\x88Develop Pre-Trained Embedding . Extend the model to use pre-trained Word2Vec\\nvectors to see if it results in a better model.\\n\\x88Use GloVe Embedding . Use the GloVe word embedding vectors with and without ﬁne\\ntuning by the network and evaluate how it impacts training and the generated words.\\n\\x88Sequence Length . Explore training the model with diﬀerent length input sequences,\\nboth shorter and longer, and evaluate how it impacts the quality of the generated text.\\n\\x88Reduce Scope . Consider training the model on one book (chapter) or a subset of\\nthe original text and evaluate the impact on training, training speed and the resulting\\ngenerated text.\\n\\x88Sentence-Wise Model . Split the raw data based on sentences and pad each sentence\\nto a ﬁxed length (e.g. the longest sentence length).\\nIf you explore any of these extensions, I’d love to know.\\n20.7 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88Project Gutenberg.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 260}, page_content='\\x88Project Gutenberg.\\nhttps://www.gutenberg.org/\\n\\x88The Republic by Plato on Project Gutenberg.\\nhttps://www.gutenberg.org/ebooks/1497\\n\\x88Republic (Plato) on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Republic_(Plato)\\n\\x88Language model on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Language_model\\n20.8 Summary\\nIn this tutorial, you discovered how to develop a word-based language model using a word\\nembedding and a recurrent neural network. Speciﬁcally, you learned:\\n\\x88How to prepare text for developing a word-based language model.\\n\\x88How to design and ﬁt a neural language model with a learned embedding and an LSTM\\nhidden layer.\\n\\x88How to use the learned language model to generate new text with similar statistical\\nproperties as the source text.\\n20.8.1 Next\\nThis is the ﬁnal chapter in the language modeling part. In the next part you will discover how\\nto develop automatic caption generation for photographs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 261}, page_content='Part VIII\\nImage Captioning\\n245'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 262}, page_content='Chapter 21\\nNeural Image Caption Generation\\nCaptioning an image involves generating a human readable textual description given an image,\\nsuch as a photograph. It is an easy problem for a human, but very challenging for a machine as\\nit involves both understanding the content of an image and how to translate this understanding\\ninto natural language. Recently, deep learning methods have displaced classical methods and\\nare achieving state-of-the-art results for the problem of automatically generating descriptions,\\ncalled captions , for images. In this chapter, you will discover how deep neural network models\\ncan be used to automatically generate descriptions for images, such as photographs. After\\ncompleting this chapter, you will know:\\n\\x88About the challenge of generating textual descriptions for images and the need to combine\\nbreakthroughs from computer vision and natural language processing.\\n\\x88About the elements that comprise a neural feature captioning model, namely the feature'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 262}, page_content='extractor and language model.\\n\\x88How the elements of the model can be arranged into an Encoder-Decoder, possibly with\\nthe use of an attention mechanism.\\nLet’s get started.\\n21.1 Overview\\nThis tutorial is divided into the following parts:\\n1. Describing an Image with Text\\n2. Neural Captioning Model\\n3. Encoder-Decoder Architecture\\n21.2 Describing an Image with Text\\nDescribing an image is the problem of generating a human-readable textual description of an\\nimage, such as a photograph of an object or scene. The problem is sometimes called automatic\\nimage annotation orimage tagging . It is an easy problem for a human, but very challenging for\\na machine.\\n246'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 263}, page_content='21.3. Neural Captioning Model 247\\nA quick glance at an image is suﬃcient for a human to point out and describe an\\nimmense amount of details about the visual scene. However, this remarkable ability\\nhas proven to be an elusive task for our visual recognition models\\n—Deep Visual-Semantic Alignments for Generating Image Descriptions , 2015.\\nA solution requires both that the content of the image be understood and translated to\\nmeaning in the terms of words, and that the words must string together to be comprehensible.\\nIt combines both computer vision and natural language processing and marks a true challenging\\nproblem in broader artiﬁcial intelligence.\\nAutomatically describing the content of an image is a fundamental problem in\\nartiﬁcial intelligence that connects computer vision and natural language processing.\\n—Show and Tell: A Neural Image Caption Generator , 2015.\\nFurther, the problems can range in diﬃculty; let’s look at three diﬀerent variations on the\\nproblem with examples.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 263}, page_content='\\x88Classify Image . Assign an image a class label from one of many known classes.\\n\\x88Describe Image . Generate a textual description of the contents image.\\n\\x88Annotate Image . Generate textual descriptions for speciﬁc regions on the image.\\nThe general problem can also be extended to describe images over time in video. In this\\nchapter, we will focus our attention on describing images, which we will describe as image\\ncaptioning .\\n21.3 Neural Captioning Model\\nNeural network models have come to dominate the ﬁeld of automatic caption generation;\\nthis is primarily because the methods are demonstrating state-of-the-art results. The two\\ndominant methods prior to end-to-end neural network models for generating image captions\\nwere template-based methods and nearest-neighbor-based methods and modifying existing\\ncaptions.\\nPrior to the use of neural networks for generating captions, two main approaches were\\ndominant. The ﬁrst involved generating caption templates which were ﬁlled in based'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 263}, page_content='on the results of object detections and attribute discovery. The second approach\\nwas based on ﬁrst retrieving similar captioned images from a large database then\\nmodifying these retrieved captions to ﬁt the query. [...] Both of these approaches\\nhave since fallen out of favour to the now dominant neural network methods.\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nNeural network models for captioning involve two main elements:\\n1. Feature Extraction.\\n2. Language Model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 264}, page_content='21.3. Neural Captioning Model 248\\n21.3.1 Feature Extraction Model\\nThe feature extraction model is a neural network that given an image is able to extract the\\nsalient features, often in the form of a ﬁxed-length vector. The extracted features are an internal\\nrepresentation of the image, not something directly intelligible. A deep convolutional neural\\nnetwork, or CNN, is used as the feature extraction submodel. This network can be trained\\ndirectly on the images in the image captioning dataset. Alternately, a pre-trained model, such\\nas a state-of-the-art model used for image classiﬁcation, can be used, or some hybrid where a\\npre-trained model is used and ﬁne tuned on the problem. It is popular to use top performing\\nmodels in the ImageNet dataset developed for the ILSVRC challenge, such as the Oxford Vision\\nGeometry Group model, called VGG for short.\\n[...] we explored several techniques to deal with overﬁtting. The most obvious way'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 264}, page_content='to not overﬁt is to initialize the weights of the CNN component of our system to a\\npretrained model (e.g., on ImageNet)\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nFigure 21.1: Feature Extractor\\n21.3.2 Language Model\\nGenerally, a language model predicts the probability of the next word in the sequence given\\nthe words already present in the sequence. For image captioning, the language model is a\\nneural network that given the extracted features from the network is capable of predicting the\\nsequence of words in the description and build up the description conditional on the words\\nthat have already been generated. It is popular to use a recurrent neural network, such as a\\nLong Short-Term Memory network, or LSTM, as the language model. Each output time step\\ngenerates a new word in the sequence. Each word that is generated is then encoded using a\\nword embedding (such as Word2Vec) and passed as input to the decoder for generating the\\nsubsequent word.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 265}, page_content='21.4. Encoder-Decoder Architecture 249\\nAn improvement to the model involves gathering the probability distribution of words\\nacross the vocabulary for the output sequence and searching it to generate multiple possible\\ndescriptions. These descriptions can be scored and ranked by likelihood. It is common to use a\\nBeam Search for this search. The language model can be trained standalone using pre-computed\\nfeatures extracted from the image dataset; it can be trained jointly with the feature extraction\\nnetwork, or some combination.\\nFigure 21.2: Language Model\\n21.4 Encoder-Decoder Architecture\\nA popular way to structure the sub-models is to use an Encoder-Decoder architecture where\\nboth models are trained jointly.\\n[the model] is based on a convolution neural network that encodes an image into\\na compact representation, followed by a recurrent neural network that generates\\na corresponding sentence. The model is trained to maximize the likelihood of the\\nsentence given the image.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 265}, page_content='—Show and Tell: A Neural Image Caption Generator , 2015.\\nThis is an architecture developed for machine translation where an input sequence, say in\\nFrench, is encoded as a ﬁxed-length vector by an encoder network. A separate decoder network\\nthen reads the encoding and generates an output sequence in the new language, say English.\\nA beneﬁt of this approach in addition to the impressive skill of the approach is that a single\\nend-to-end model can be trained on the problem. When adapted for image captioning, the\\nencoder network is a deep convolutional neural network, and the decoder network is a stack of\\nLSTM layers.\\n[in machine translation] An “encoder” RNN reads the source sentence and transforms\\nit into a rich ﬁxed-length vector representation, which in turn in used as the initial\\nhidden state of a “decoder” RNN that generates the target sentence. Here, we propose'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 266}, page_content='21.5. Further Reading 250\\nto follow this elegant recipe, replacing the encoder RNN by a deep convolution neural\\nnetwork (CNN).\\n—Show and Tell: A Neural Image Caption Generator , 2015.\\n21.4.1 Captioning Model with Attention\\nA limitation of the Encoder-Decoder architecture is that a single ﬁxed-length representation is\\nused to hold the extracted features. This was addressed in machine translation through the\\ndevelopment of attention across a richer encoding, allowing the decoder to learn where to place\\nattention as each word in the translation is generated. The approach of attention has also been\\nused to improve the performance of the Encoder-Decoder architecture for image captioning by\\nallowing the decoder to learn where to put attention in the image when generating each word in\\nthe description.\\nEncouraged by recent advances in caption generation and inspired by recent success\\nin employing attention in machine translation and object recognition we investigate'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 266}, page_content='models that can attend to salient part of an image while generating its caption.\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nA beneﬁt of this approach is that it is possible to visualize exactly where attention is placed\\nwhile generating each word in a description.\\nWe also show through visualization how the model is able to automatically learn\\nto ﬁx its gaze on salient objects while generating the corresponding words in the\\noutput sequence.\\n—Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\n21.5 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n21.5.1 Papers\\n\\x88Show and Tell: A Neural Image Caption Generator , 2015.\\nhttps://arxiv.org/abs/1411.4555\\n\\x88Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nhttps://arxiv.org/abs/1502.03044\\n\\x88Long-term recurrent convolutional networks for visual recognition and description , 2015.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 266}, page_content='https://arxiv.org/abs/1411.4389\\n\\x88Deep Visual-Semantic Alignments for Generating Image Descriptions , 2015.\\nhttps://arxiv.org/abs/1412.2306'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 267}, page_content='21.6. Summary 251\\n21.5.2 Articles\\n\\x88Automatic image annotation on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Automatic_image_annotation\\n\\x88Show and Tell: image captioning open sourced in TensorFlow , 2016.\\nhttps://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.\\nhtml\\n\\x88Presentation: Automated Image Captioning with ConvNets and Recurrent Nets , Andrej\\nKarpathy and Fei-Fei Li.\\nhttps://www.youtube.com/watch?v=xKt21ucdBY0\\n21.5.3 Projects\\n\\x88Deep Visual-Semantic Alignments for Generating Image Descriptions , 2015.\\nhttp://cs.stanford.edu/people/karpathy/deepimagesent/\\n\\x88NeuralTalk2: Eﬃcient Image Captioning code in Torch, runs on GPU , Andrej Karpathy.\\nhttps://github.com/karpathy/neuraltalk2\\n21.6 Summary\\nIn this chapter, you discovered how deep neural network models can be used to automatically\\ngenerate descriptions for images, such as photographs. Speciﬁcally, you learned:\\n\\x88About the challenge of generating textual descriptions for images and the need to combine'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 267}, page_content='breakthroughs from computer vision and natural language processing.\\n\\x88About the elements that comprise a neural feature captioning model, namely the feature\\nextractor and language model.\\n\\x88How the elements of the model can be arranged into an Encoder-Decoder, possibly with\\nthe use of an attention mechanism.\\n21.6.1 Next\\nIn the next chapter, you will discover how you can develop neural caption models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 268}, page_content='Chapter 22\\nNeural Network Models for Caption\\nGeneration\\nCaption generation is a challenging artiﬁcial intelligence problem that draws on both computer\\nvision and natural language processing. The encoder-decoder recurrent neural network architec-\\nture has been shown to be eﬀective at this problem. The implementation of this architecture\\ncan be distilled into inject and merge based models, and both make diﬀerent assumptions about\\nthe role of the recurrent neural network in addressing the problem. In this chapter, you will\\ndiscover the inject and merge architectures for the encoder-decoder recurrent neural network\\nmodels on caption generation. After reading this chapter, you will know:\\n\\x88The challenge of caption generation and the use of the encoder-decoder architecture.\\n\\x88The inject model that combines the encoded image with each word to generate the next\\nword in the caption.\\n\\x88The merge model that separately encodes the image and description which are decoded in'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 268}, page_content='order to generate the next word in the caption.\\nLet’s get started.\\n22.1 Image Caption Generation\\nThe problem of image caption generation involves outputting a readable and concise description\\nof the contents of a photograph. It is a challenging artiﬁcial intelligence problem as it requires\\nboth techniques from computer vision to interpret the contents of the photograph and techniques\\nfrom natural language processing to generate the textual description. Recently, deep learning\\nmethods have achieved state-of-the-art results on this challenging problem. The results are so\\nimpressive that this problem has become a standard demonstration problem for the capabilities\\nof deep learning.\\n22.1.1 Encoder-Decoder Architecture\\nA standard encoder-decoder recurrent neural network architecture is used to address the image\\ncaption generation problem. This involves two elements:\\n252'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 269}, page_content='22.2. Inject Model 253\\n\\x88Encoder : A network model that reads the photograph input and encodes the content\\ninto a ﬁxed-length vector using an internal representation.\\n\\x88Decoder : A network model that reads the encoded photograph and generates the textual\\ndescription output.\\nGenerally, a convolutional neural network is used to encode the images and a recurrent\\nneural network, such as a Long Short-Term Memory network, is used to either encode the\\ntext sequence generated so far, and/or generate the next word in the sequence. There are\\nmany ways to realize this architecture for the problem of caption generation. It is common\\nto use a pre-trained convolutional neural network model trained on a challenging photograph\\nclassiﬁcation problem to encode the photograph. The pre-trained model can be loaded, the\\noutput of the model removed, and the internal representation of the photograph used as the\\nencoding or internal representation of the input image.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 269}, page_content='It is also common to frame the problem such that the model generates one word of the\\noutput textual description, given both the photograph and the description generated so far\\nas input. In this framing, the model is called recursively until the entire output sequence is\\ngenerated.\\nFigure 22.1: Recursive Framing of the Caption Generation Model. Taken from Where to put\\nthe Image in an Image Caption Generator .\\nThis framing can be implemented using one of two architectures, called by Marc Tanti, et al.\\nas the inject and the merge models.\\n22.2 Inject Model\\nThe inject model combines the encoded form of the image with each word from the text description\\ngenerated so-far. The approach uses the recurrent neural network as a text generation model\\nthat uses a sequence of both image and word information as input in order to generate the next\\nword in the sequence.\\nIn these ’inject’ architectures, the image vector (usually derived from the activation'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 269}, page_content='values of a hidden layer in a convolutional neural network) is injected into the RNN,\\nfor example by treating the image vector on a par with a ’word’ and including it as\\npart of the caption preﬁx.\\n—Where to put the Image in an Image Caption Generator , 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 270}, page_content='22.3. Merge Model 254\\nFigure 22.2: Inject Architecture for Encoder-Decoder Model. Taken from What is the Role of\\nRecurrent Neural Networks (RNNs) in an Image Caption Generator? .\\nThis model combines the concerns of the image with each input word, requiring the encoder\\nto develop an encoding that incorporates both visual and linguistic information together.\\nIn an inject model, the RNN is trained to predict sequences based on histories\\nconsisting of both linguistic and perceptual features. Hence, in this model, the RNN\\nis primarily responsible for image-conditioned language generation.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\n22.3 Merge Model\\nThe merge model combines both the encoded form of the image input with the encoded form of\\nthe text description generated so far. The combination of these two encoded inputs is then used\\nby a very simple decoder model to generate the next word in the sequence. The approach uses'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 270}, page_content='the recurrent neural network only to encode the text generated so far.\\nIn the case of ‘merge’ architectures, the image is left out of the RNN subnetwork,\\nsuch that the RNN handles only the caption preﬁx, that is, handles only purely\\nlinguistic information. After the preﬁx has been vectorised, the image vector is then\\nmerged with the preﬁx vector in a separate ‘multimodal layer’ which comes after\\nthe RNN subnetwork\\n—Where to put the Image in an Image Caption Generator , 2017.\\nFigure 22.3: Merge Architecture for Encoder-Decoder Model. Taken from What is the Role of\\nRecurrent Neural Networks (RNNs) in an Image Caption Generator? .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 271}, page_content='22.4. More on the Merge Model 255\\nThis separates the concern of modeling the image input, the text input and the combining\\nand interpretation of the encoded inputs. As mentioned, it is common to use a pre-trained model\\nfor encoding the image, but similarly, this architecture also permits a pre-trained language\\nmodel to be used to encode the caption text input.\\n... in the merge architecture, RNNs in eﬀect encode linguistic representations,\\nwhich themselves constitute the input to a later prediction stage that comes after\\na multimodal layer. It is only at this late stage that image features are used to\\ncondition predictions\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nThere are multiple ways to combine the two encoded inputs, such as concatenation, multi-\\nplication, and addition, although experiments by Marc Tanti, et al. have shown addition to\\nwork better. Generally, Marc Tanti, et al. found the merge architecture to be more eﬀective'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 271}, page_content='compared to the inject approach.\\nOverall, the evidence suggests that delaying the merging of image features with\\nlinguistic encodings to a late stage in the architecture may be advantageous [...]\\nresults suggest that a merge architecture has a higher capacity than an inject\\narchitecture and can generate better quality captions with smaller layers.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\n22.4 More on the Merge Model\\nThe success of the merge model for the encoder-decoder architecture suggests that the role of\\nthe recurrent neural network is to encode input rather than generate output. This is a departure\\nfrom the common understanding where it is believed that the contribution of the recurrent\\nneural network is that of a generative model.\\nIf the RNN had the primary role of generating captions, then it would need to have\\naccess to the image in order to know what to generate. This does not seem to'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 271}, page_content='be the case as including the image into the RNN is not generally beneﬁcial to its\\nperformance as a caption generator.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nThe explicit comparison of the inject and merge models, and the success of merge over\\ninject for caption generation, raises the question of whether this approach translates to related\\nsequence-to-sequence generation problems. Instead of pre-trained models used to encode images,\\npre-trained language models could be used to encode source text in problems such as text\\nsummarization, question answering, and machine translation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 272}, page_content='22.5. Further Reading 256\\nWe would like to investigate whether similar changes in architecture would work in\\nsequence-to-sequence tasks such as machine translation, where instead of conditioning\\na language model on an image we are conditioning a target language model on\\nsentences in a source language.\\n—What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\n22.5 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Marc Tanti’s Blog.\\nhttps://geekyisawesome.blogspot.com.au/\\n\\x88Where to put the Image in an Image Caption Generator , 2017.\\nhttps://arxiv.org/abs/1703.09137\\n\\x88What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nhttps://arxiv.org/abs/1708.02043\\n22.6 Summary\\nIn this chapter, you discovered the inject and merge architectures for the encoder-decoder\\nrecurrent neural network model on caption generation. Speciﬁcally, you learned:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 272}, page_content='\\x88The challenge of caption generation and the use of the encoder-decoder architecture.\\n\\x88The inject model that combines the encoded image with each word to generate the next\\nword in the caption.\\n\\x88The merge model that separately encodes the image and description which are decoded in\\norder to generate the next word in the caption.\\n22.6.1 Next\\nIn the next chapter, you will discover how you can load and re-use a pre-trained deep computer\\nvision model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 273}, page_content='Chapter 23\\nHow to Load and Use a Pre-Trained\\nObject Recognition Model\\nConvolutional neural networks are now capable of outperforming humans on some computer\\nvision tasks, such as classifying images. That is, given a photograph of an object, answer the\\nquestion as to which of 1,000 speciﬁc objects the photograph shows. A competition-winning\\nmodel for this task is the VGG model by researchers at Oxford. What is important about this\\nmodel, besides its capability of classifying objects in photographs, is that the model weights\\nare freely available and can be loaded and used in your own models and applications. In this\\ntutorial, you will discover the VGG convolutional neural network models for image classiﬁcation.\\nAfter completing this tutorial, you will know:\\n\\x88About the ImageNet dataset and competition and the VGG winning models.\\n\\x88How to load the VGG model in Keras and summarize its structure.\\n\\x88How to use the loaded VGG model to classifying objects in ad hoc photographs.\\nLet’s get started.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 273}, page_content='Let’s get started.\\n23.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. ImageNet\\n2. The Oxford VGG Models\\n3. Load the VGG Model in Keras\\n4. Develop a Simple Photo Classiﬁer\\nNote, Keras makes use of the Python Imaging Library or PIL library for manipulating\\nimages. Installation on your system may vary.\\n257'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 274}, page_content='23.2. ImageNet 258\\n23.2 ImageNet\\nImageNet is a research project to develop a large database of images with annotations, e.g.\\nimages and their descriptions. The images and their annotations have been the basis for an\\nimage classiﬁcation challenge called the ImageNet Large Scale Visual Recognition Challenge\\nor ILSVRC since 2010. The result is that research organizations battle it out on pre-deﬁned\\ndatasets to see who has the best model for classifying the objects in images.\\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object\\ncategory classiﬁcation and detection on hundreds of object categories and millions\\nof images. The challenge has been run annually from 2010 to present, attracting\\nparticipation from more than ﬁfty institutions.\\n—ImageNet Large Scale Visual Recognition Challenge , 2015.\\nFor the classiﬁcation task, images must be classiﬁed into one of 1,000 diﬀerent categories.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 274}, page_content='For the last few years very deep convolutional neural network models have been used to win\\nthese challenges and results on the tasks have exceeded human performance.\\nFigure 23.1: Sample of Images from the ImageNet Dataset used in the ILSVRC Challenge.\\nTaken From ImageNet Large Scale Visual Recognition Challenge .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 275}, page_content='23.3. The Oxford VGG Models 259\\n23.3 The Oxford VGG Models\\nResearchers from the Oxford Visual Geometry Group, or VGG for short, participate in the\\nILSVRC challenge. In 2014, convolutional neural network models (CNN) developed by the\\nVGG won the image classiﬁcation tasks.\\nFigure 23.2: ILSVRC Results in 2014 for the Classiﬁcation task.\\nAfter the competition, the participants wrote up their ﬁndings in the paper Very Deep\\nConvolutional Networks for Large-Scale Image Recognition , 2014. They also made their models\\nand learned weights available online. This allowed other researchers and developers to use a\\nstate-of-the-art image classiﬁcation model in their own work and programs. This helped to fuel\\na rash of transfer learning work where pre-trained models are used with minor modiﬁcation\\non wholly new predictive modeling tasks, harnessing the state-of-the-art feature extraction\\ncapabilities of proven models.\\n... we come up with signiﬁcantly more accurate ConvNet architectures, which not'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 275}, page_content='only achieve the state-of-the-art accuracy on ILSVRC classiﬁcation and localisation\\ntasks, but are also applicable to other image recognition datasets, where they achieve\\nexcellent performance even when used as a part of a relatively simple pipelines (e.g.\\ndeep features classiﬁed by a linear SVM without ﬁne-tuning). We have released our\\ntwo best-performing models to facilitate further research.\\n—Very Deep Convolutional Networks for Large-Scale Image Recognition , 2014.\\nVGG released two diﬀerent CNN models, speciﬁcally a 16-layer model and a 19-layer model.\\nRefer to the paper for the full details of these models. The VGG models are not longer state-of-\\nthe-art by only a few percentage points. Nevertheless, they are very powerful models and useful\\nboth as image classiﬁers and as the basis for new models that use image inputs. In the next\\nsection, we will see how we can use the VGG model directly in Keras.\\n23.4 Load the VGG Model in Keras'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 275}, page_content='The VGG model can be loaded and used in the Keras deep learning library. Keras provides an\\nApplications interface for loading and using pre-trained models. Using this interface, you can'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 276}, page_content='23.4. Load the VGG Model in Keras 260\\ncreate a VGG model using the pre-trained weights provided by the Oxford group and use it as\\na starting point in your own model, or use it as a model directly for classifying images. In this\\ntutorial, we will focus on the use case of classifying new images using the VGG model. Keras\\nprovides both the 16-layer and 19-layer version via the VGG16 and VGG19 classes. Let’s focus\\non the VGG16 model. The model can be created as follows:\\nfrom keras.applications.vgg16 import VGG16\\nmodel = VGG16()\\nListing 23.1: Create the VGG16 model in Keras.\\nThat’s it. The ﬁrst time you run this example, Keras will download the weight ﬁles from\\nthe Internet and store them in the ∼/.keras/models directory. Note that the weights are\\nabout 528 megabytes, so the download may take a few minutes depending on the speed of your\\nInternet connection.\\nThe weights are only downloaded once. The next time you run the example, the weights are'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 276}, page_content='loaded locally and the model should be ready to use in seconds. We can use the standard Keras\\ntools for inspecting the model structure. For example, you can print a summary of the network\\nlayers as follows:\\nfrom keras.applications.vgg16 import VGG16\\nmodel = VGG16()\\nmodel.summary()\\nListing 23.2: Create and summarize the VGG16 model.\\nYou can see that the model is huge. You can also see that, by default, the model expects\\nimages as input with the size 224 x 224 pixels with 3 channels (e.g. color).\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 224, 224, 3) 0\\n_________________________________________________________________\\nblock1_conv1 (Conv2D) (None, 224, 224, 64) 1792\\n_________________________________________________________________\\nblock1_conv2 (Conv2D) (None, 224, 224, 64) 36928'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 276}, page_content='_________________________________________________________________\\nblock1_pool (MaxPooling2D) (None, 112, 112, 64) 0\\n_________________________________________________________________\\nblock2_conv1 (Conv2D) (None, 112, 112, 128) 73856\\n_________________________________________________________________\\nblock2_conv2 (Conv2D) (None, 112, 112, 128) 147584\\n_________________________________________________________________\\nblock2_pool (MaxPooling2D) (None, 56, 56, 128) 0\\n_________________________________________________________________\\nblock3_conv1 (Conv2D) (None, 56, 56, 256) 295168\\n_________________________________________________________________\\nblock3_conv2 (Conv2D) (None, 56, 56, 256) 590080\\n_________________________________________________________________\\nblock3_conv3 (Conv2D) (None, 56, 56, 256) 590080\\n_________________________________________________________________\\nblock3_pool (MaxPooling2D) (None, 28, 28, 256) 0\\n_________________________________________________________________'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 276}, page_content='block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160\\n_________________________________________________________________'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 277}, page_content='23.4. Load the VGG Model in Keras 261\\nblock4_conv2 (Conv2D) (None, 28, 28, 512) 2359808\\n_________________________________________________________________\\nblock4_conv3 (Conv2D) (None, 28, 28, 512) 2359808\\n_________________________________________________________________\\nblock4_pool (MaxPooling2D) (None, 14, 14, 512) 0\\n_________________________________________________________________\\nblock5_conv1 (Conv2D) (None, 14, 14, 512) 2359808\\n_________________________________________________________________\\nblock5_conv2 (Conv2D) (None, 14, 14, 512) 2359808\\n_________________________________________________________________\\nblock5_conv3 (Conv2D) (None, 14, 14, 512) 2359808\\n_________________________________________________________________\\nblock5_pool (MaxPooling2D) (None, 7, 7, 512) 0\\n_________________________________________________________________\\nflatten (Flatten) (None, 25088) 0\\n_________________________________________________________________\\nfc1 (Dense) (None, 4096) 102764544'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 277}, page_content=\"_________________________________________________________________\\nfc2 (Dense) (None, 4096) 16781312\\n_________________________________________________________________\\npredictions (Dense) (None, 1000) 4097000\\n=================================================================\\nTotal params: 138,357,544\\nTrainable params: 138,357,544\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 23.3: Output summary for the VGG16 model.\\nWe can also create a plot of the layers in the VGG model, as follows:\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.utils.vis_utils import plot_model\\nmodel = VGG16()\\nplot_model(model, to_file= 'vgg.png ')\\nListing 23.4: Create and plot the graph of the VGG16 model.\\nAgain, because the model is large, the plot is a little too large and perhaps unreadable.\\nNevertheless, it is provided below.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 278}, page_content='23.4. Load the VGG Model in Keras 262\\nFigure 23.3: Plot of Layers in the VGG Model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 279}, page_content='23.5. Develop a Simple Photo Classiﬁer 263\\nThe VGG() class takes a few arguments that may only interest you if you are looking to use\\nthe model in your own project, e.g. for transfer learning. For example:\\n\\x88include top(True): Whether or not to include the output layers for the model. You\\ndon’t need these if you are ﬁtting the model on your own problem.\\n\\x88weights (‘imagenet’ ): What weights to load. You can specify None to not load pre-\\ntrained weights if you are interested in training the model yourself from scratch.\\n\\x88input tensor (None ): A new input layer if you intend to ﬁt the model on new data of a\\ndiﬀerent size.\\n\\x88input shape (None ): The size of images that the model is expected to take if you change\\nthe input layer.\\n\\x88pooling (None): The type of pooling to use when you are training a new set of output\\nlayers.\\n\\x88classes (1000 ): The number of classes (e.g. size of output vector) for the model.\\nNext, let’s look at using the loaded VGG model to classify ad hoc photographs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 279}, page_content='23.5 Develop a Simple Photo Classiﬁer\\nLet’s develop a simple image classiﬁcation script.\\n23.5.1 Get a Sample Image\\nFirst, we need an image we can classify. You can download a random photograph of a coﬀee\\nmug from Flickr.\\nFigure 23.4: Coﬀee Mug. Photo by jfanaian , some rights reserved.\\nDownload the image and save it to your current working directory with the ﬁlename mug.jpg .'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 280}, page_content=\"23.5. Develop a Simple Photo Classiﬁer 264\\n23.5.2 Load the VGG Model\\nLoad the weights for the VGG-16 model, as we did in the previous section.\\nfrom keras.applications.vgg16 import VGG16\\n# load the model\\nmodel = VGG16()\\nListing 23.5: Create the VGG16 model.\\n23.5.3 Load and Prepare Image\\nNext, we can load the image as pixel data and prepare it to be presented to the network. Keras\\nprovides some tools to help with this step. First, we can use the load img() function to load\\nthe image and resize it to the required size of 224 x 224 pixels.\\nfrom keras.preprocessing.image import load_img\\n# load an image from file\\nimage = load_img( 'mug.jpg ', target_size=(224, 224))\\nListing 23.6: Load and resize the image.\\nNext, we can convert the pixels to a NumPy array so that we can work with it in Keras. We\\ncan use the imgtoarray() function for this.\\nfrom keras.preprocessing.image import img_to_array\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 280}, page_content='Listing 23.7: Convert the image pixels to a NumPy array.\\nThe network expects one or more images as input; that means the input array will need to\\nbe 4-dimensional: samples, rows, columns, and channels. We only have one sample (one image).\\nWe can reshape the array by calling reshape() and adding the extra dimension.\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\nListing 23.8: Reshape the NumPy array of pixels.\\nNext, the image pixels need to be prepared in the same way as the ImageNet training data\\nwas prepared. Speciﬁcally, from the paper:\\nThe only preprocessing we do is subtracting the mean RGB value, computed on the\\ntraining set, from each pixel.\\n—Very Deep Convolutional Networks for Large-Scale Image Recognition , 2014.\\nKeras provides a function called preprocess input() to prepare new input for the network.\\nfrom keras.applications.vgg16 import preprocess_input\\n# prepare the image for the VGG model'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 280}, page_content='image = preprocess_input(image)\\nListing 23.9: Pre-process the pixel data for the model.\\nWe are now ready to make a prediction for our loaded and prepared image.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 281}, page_content='23.5. Develop a Simple Photo Classiﬁer 265\\n23.5.4 Make a Prediction\\nWe can call the predict() function on the model in order to get a prediction of the probability\\nof the image belonging to each of the 1,000 known object types.\\n# predict the probability across all output classes\\nyhat = model.predict(image)\\nListing 23.10: Classify the image with the VGG16 model.\\nNearly there, now we need to interpret the probabilities.\\n23.5.5 Interpret Prediction\\nKeras provides a function to interpret the probabilities called decode predictions() . It can\\nreturn a list of classes and their probabilities in case you would like to present the top 3 objects\\nthat may be in the photo. We will just report the ﬁrst most likely object.\\nfrom keras.applications.vgg16 import decode_predictions\\n# convert the probabilities to class labels\\nlabel = decode_predictions(yhat)\\n# retrieve the most likely result, e.g. highest probability\\nlabel = label[0][0]\\n# print the classification'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 281}, page_content=\"print( '%s (%.2f%%) '% (label[1], label[2]*100))\\nListing 23.11: Interpret the prediction probabilities.\\nAnd that’s it.\\n23.5.6 Complete Example\\nTying all of this together, the complete example is listed below:\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.applications.vgg16 import decode_predictions\\nfrom keras.applications.vgg16 import VGG16\\n# load the model\\nmodel = VGG16()\\n# load an image from file\\nimage = load_img( 'mug.jpg ', target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# predict the probability across all output classes\\nyhat = model.predict(image)\\n# convert the probabilities to class labels\\nlabel = decode_predictions(yhat)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 281}, page_content='# retrieve the most likely result, e.g. highest probability\\nlabel = label[0][0]\\n# print the classification'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 282}, page_content=\"23.6. Further Reading 266\\nprint( '%s (%.2f%%) '% (label[1], label[2]*100))\\nListing 23.12: Complete example for classifying an image with the VGG model.\\nRunning the example, we can see that the image is correctly classiﬁed as a coﬀee mug with\\na 75% likelihood.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\ncoffee_mug (75.27%)\\nListing 23.13: Sample output of making a prediction for the image.\\n23.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88ImageNet.\\nhttp://www.image-net.org/\\n\\x88ImageNet on Wikipedia.\\nhttps://en.wikipedia.org/wiki/ImageNet\\n\\x88Very Deep Convolutional Networks for Large-Scale Image Recognition , 2015.\\nhttps://arxiv.org/abs/1409.1556\\n\\x88Very Deep Convolutional Networks for Large-Scale Visual Recognition , at Oxford.\\nhttp://www.robots.ox.ac.uk/ ~vgg/research/very_deep/\\n\\x88Building powerful image classiﬁcation models using very little data , 2016.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 282}, page_content='https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.\\nhtml\\n\\x88Keras Applications API.\\nhttps://keras.io/applications/\\n\\x88Keras weight ﬁles ﬁles.\\nhttps://github.com/fchollet/deep-learning-models/releases/\\n23.7 Summary\\nIn this tutorial, you discovered the VGG convolutional neural network models for image\\nclassiﬁcation. Speciﬁcally, you learned:\\n\\x88About the ImageNet dataset and competition and the VGG winning models.\\n\\x88How to load the VGG model in Keras and summarize its structure.\\n\\x88How to use the loaded VGG model to classifying objects in ad hoc photographs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 283}, page_content='23.7. Summary 267\\n23.7.1 Next\\nIn the next chapter, you will discover how you can evaluate generated text against a ground\\ntruth.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 284}, page_content='Chapter 24\\nHow to Evaluate Generated Text With\\nthe BLEU Score\\nBLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation\\nof text to one or more reference translations. Although developed for translation, it can be used\\nto evaluate text generated for a suite of natural language processing tasks. In this tutorial, you\\nwill discover the BLEU score for evaluating and scoring candidate text using the NLTK library\\nin Python. After completing this tutorial, you will know:\\n\\x88A gentle introduction to the BLEU score and an intuition for what is being calculated.\\n\\x88How you can calculate BLEU scores in Python using the NLTK library for sentences and\\ndocuments.\\n\\x88How you can use a suite of small examples to develop an intuition for how diﬀerences\\nbetween a candidate and reference text impact the ﬁnal BLEU score.\\nLet’s get started.\\n24.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Bilingual Evaluation Understudy Score'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 284}, page_content='2. Calculate BLEU Scores\\n3. Cumulative and Individual BLEU Scores\\n4. Worked Examples\\n24.2 Bilingual Evaluation Understudy Score\\nThe Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a\\ngenerated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a\\nperfect mismatch results in a score of 0.0. The score was developed for evaluating the predictions\\n268'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 285}, page_content='24.2. Bilingual Evaluation Understudy Score 269\\nmade by automatic machine translation systems. It is not perfect, but does oﬀer 5 compelling\\nbeneﬁts:\\n\\x88It is quick and inexpensive to calculate.\\n\\x88It is easy to understand.\\n\\x88It is language independent.\\n\\x88It correlates highly with human evaluation.\\n\\x88It has been widely adopted.\\nThe BLEU score was proposed by Kishore Papineni, et al. in their 2002 paper BLEU: a\\nMethod for Automatic Evaluation of Machine Translation . The approach works by counting\\nmatching n-grams in the candidate translation to n-grams in the reference text, where 1-gram\\nor unigram would be each token and a bigram comparison would be each word pair. The\\ncomparison is made regardless of word order.\\nThe primary programming task for a BLEU implementor is to compare n-grams of\\nthe candidate with the n-grams of the reference translation and count the number\\nof matches. These matches are position-independent. The more the matches, the\\nbetter the candidate translation is.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 285}, page_content='—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nThe counting of matching n-grams is modiﬁed to ensure that it takes the occurrence of the\\nwords in the reference text into account, not rewarding a candidate translation that generates\\nan abundance of reasonable words. This is referred to in the paper as modiﬁed n-gram precision.\\nUnfortunately, MT systems can overgenerate “reasonable” words, resulting in im-\\nprobable, but high-precision, translations [...] Intuitively the problem is clear: a\\nreference word should be considered exhausted after a matching candidate word is\\nidentiﬁed. We formalize this intuition as the modiﬁed unigram precision.\\n—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nThe score is for comparing sentences, but a modiﬁed version that normalizes n-grams by\\ntheir occurrence is also proposed for better scoring blocks of multiple sentences.\\nWe ﬁrst compute the n-gram matches sentence by sentence. Next, we add the clipped'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 285}, page_content='n-gram counts for all the candidate sentences and divide by the number of candidate\\nn-grams in the test corpus to compute a modiﬁed precision score, pn, for the entire\\ntest corpus.\\n—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nA perfect score is not possible in practice as a translation would have to match the reference\\nexactly. This is not even possible by human translators. The number and quality of the\\nreferences used to calculate the BLEU score means that comparing scores across datasets can\\nbe troublesome.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 286}, page_content='24.3. Calculate BLEU Scores 270\\nThe BLEU metric ranges from 0 to 1. Few translations will attain a score of 1\\nunless they are identical to a reference translation. For this reason, even a human\\ntranslator will not necessarily score 1. [...] on a test corpus of about 500 sentences\\n(40 general news stories), a human translator scored 0.3468 against four references\\nand scored 0.2571 against two references.\\n—BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.\\nIn addition to translation, we can use the BLEU score for other language generation problems\\nwith deep learning methods such as:\\n\\x88Language generation.\\n\\x88Image caption generation.\\n\\x88Text summarization.\\n\\x88Speech recognition.\\n\\x88And much more.\\n24.3 Calculate BLEU Scores\\nThe Python Natural Language Toolkit library, or NLTK, provides an implementation of the\\nBLEU score that you can use to evaluate your generated text against a reference.\\n24.3.1 Sentence BLEU Score'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 286}, page_content=\"NLTK provides the sentence bleu() function for evaluating a candidate sentence against one\\nor more reference sentences. The reference sentences must be provided as a list of sentences\\nwhere each reference is a list of tokens. The candidate sentence is provided as a list of tokens.\\nFor example:\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'a ', 'test '], [ 'this ', 'is ' 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.1: Example of calculating a sentence BLEU score.\\nRunning this example prints a perfect score as the candidate matches one of the references\\nexactly.\\n1.0\\nListing 24.2: Sample output of calculating the sentence BLEU score.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 287}, page_content=\"24.4. Cumulative and Individual BLEU Scores 271\\n24.3.2 Corpus BLEU Score\\nNLTK also provides a function called corpus bleu() for calculating the BLEU score for multiple\\nsentences such as a paragraph or a document. The references must be speciﬁed as a list of\\ndocuments where each document is a list of references and each alternative reference is a list of\\ntokens, e.g. a list of lists of lists of tokens. The candidate documents must be speciﬁed as a list\\nwhere each document is a list of tokens, e.g. a list of lists of tokens. This is a little confusing;\\nhere is an example of two references for one document.\\n# two references for one document\\nfrom nltk.translate.bleu_score import corpus_bleu\\nreferences = [[[ 'this ', 'is ', 'a ', 'test '], [ 'this ', 'is ' 'test ']]]\\ncandidates = [[ 'this ', 'is ', 'a ', 'test ']]\\nscore = corpus_bleu(references, candidates)\\nprint(score)\\nListing 24.3: Example of calculating a corpus BLEU score.\\nRunning the example prints a perfect score as before.\\n1.0\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 287}, page_content=\"1.0\\nListing 24.4: Sample output of calculating the corpus BLEU score.\\n24.4 Cumulative and Individual BLEU Scores\\nThe BLEU score calculations in NLTK allow you to specify the weighting of diﬀerent n-grams\\nin the calculation of the BLEU score. This gives you the ﬂexibility to calculate diﬀerent types\\nof BLEU score, such as individual and cumulative n-gram scores. Let’s take a look.\\n24.4.1 Individual n-gram Scores\\nAn individual n-gram score is the evaluation of just matching grams of a speciﬁc order, such\\nas single words (1-gram) or word pairs (2-gram or bigram). The weights are speciﬁed as a\\ntuple where each index refers to the gram order. To calculate the BLEU score only for 1-gram\\nmatches, you can specify a weight of 1 for 1-gram and 0 for 2, 3 and 4 (1, 0, 0, 0). For example:\\n# 1-gram individual BLEU\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'small ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 287}, page_content='score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\\nprint(score)\\nListing 24.5: Example of calculating an individual 1-gram BLEU score.\\nRunning this example prints a score of 0.5.\\n0.75\\nListing 24.6: Sample output of calculating an individual 1-gram BLEU score.\\nWe can repeat this example for individual n-grams from 1 to 4 as follows:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 288}, page_content=\"24.4. Cumulative and Individual BLEU Scores 272\\n# n-gram individual BLEU\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'a ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nprint( 'Individual 1-gram: %f '% sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\\nprint( 'Individual 2-gram: %f '% sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\\nprint( 'Individual 3-gram: %f '% sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\\nprint( 'Individual 4-gram: %f '% sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\\nListing 24.7: Example of calculating an individual n-gram BLEU scores.\\nRunning the example gives the following results.\\nIndividual 1-gram: 1.000000\\nIndividual 2-gram: 1.000000\\nIndividual 3-gram: 1.000000\\nIndividual 4-gram: 1.000000\\nListing 24.8: Sample output of calculating an individual n-gram BLEU scores.\\nAlthough we can calculate the individual BLEU scores, this is not how the method was\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 288}, page_content=\"intended to be used and the scores do not carry a lot of meaning, or seem that interpretable.\\n24.4.2 Cumulative n-gram Scores\\nCumulative scores refer to the calculation of individual n-gram scores at all orders from 1 to nand\\nweighting them by calculating the weighted geometric mean. By default, the sentence bleu()\\nand corpus bleu() scores calculate the cumulative 4-gram BLEU score, also called BLEU-4.\\nThe weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and\\n4-gram scores. For example:\\n# 4-gram cumulative BLEU\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'small ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nscore = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\\nprint(score)\\nListing 24.9: Example of calculating a cumulative 4-gram BLEU score.\\nRunning this example prints the following score:\\n0.707106781187\\nListing 24.10: Sample output of calculating a cumulative 4-gram BLEU score.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 288}, page_content=\"The cumulative and individual 1-gram BLEU use the same weights, e.g. (1, 0, 0, 0). The\\n2-gram weights assign a 50% to each of 1-gram and 2-gram and the 3-gram weights are 33%\\nfor each of the 1, 2 and 3-gram scores. Let’s make this concrete by calculating the cumulative\\nscores for BLEU-1, BLEU-2, BLEU-3 and BLEU-4:\\n# cumulative BLEU scores\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'this ', 'is ', 'small ', 'test ']]\\ncandidate = [ 'this ', 'is ', 'a ', 'test ']\\nprint( 'Cumulative 1-gram: %f '% sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 289}, page_content=\"24.5. Worked Examples 273\\nprint( 'Cumulative 2-gram: %f '% sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0,\\n0)))\\nprint( 'Cumulative 3-gram: %f '% sentence_bleu(reference, candidate, weights=(0.33, 0.33,\\n0.33, 0)))\\nprint( 'Cumulative 4-gram: %f '% sentence_bleu(reference, candidate, weights=(0.25, 0.25,\\n0.25, 0.25)))\\nListing 24.11: Example of calculating cumulative n-gram BLEU scores.\\nRunning the example prints the following scores. They are quite diﬀerent and more expressive\\nthan the They are quite diﬀerent and more expressive than the standalone individual n-gram\\nscores.\\nCumulative 1-gram: 0.750000\\nCumulative 2-gram: 0.500000\\nCumulative 3-gram: 0.632878\\nCumulative 4-gram: 0.707107\\nListing 24.12: Sample output of calculating cumulative n-gram BLEU scores.\\nIt is common to report the cumulative BLEU-1 to BLEU-4 scores when describing the skill\\nof a text generation system.\\n24.5 Worked Examples\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 289}, page_content=\"In this section, we try to develop further intuition for the BLEU score with some examples. We\\nwork at the sentence level with a single reference sentence of the following:\\nthe quick brown fox jumped over the lazy dog\\nListing 24.13: Sample text for the worked example of calculating BLEU scores.\\nFirst, let’s look at a perfect score.\\n# prefect match\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.14: Example of two matching cases.\\nRunning the example prints a perfect match.\\n1.0\\nListing 24.15: Sample output BLEU score for two matching cases.\\nNext, let’s change one word, ‘ quick ’ to ‘ fast’.\\n# one word different\\nfrom nltk.translate.bleu_score import sentence_bleu\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 289}, page_content=\"reference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'fast ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.16: Example of making one word diﬀerent.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 290}, page_content=\"24.5. Worked Examples 274\\nThis result is a slight drop in score.\\n0.7506238537503395\\nListing 24.17: Sample output BLEU score when making one word diﬀerent.\\nTry changing two words, both ‘ quick ’ to ‘ fast’ and ‘ lazy’ to ‘ sleepy ’.\\n# two words different\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'fast ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'sleepy ', 'dog ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.18: Example of making two words diﬀerent.\\nRunning the example, we can see a linear drop in skill.\\n0.4854917717073234\\nListing 24.19: Sample output BLEU score when making two words diﬀerent.\\nWhat about if all words are diﬀerent in the candidate?\\n# all words different\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 290}, page_content=\"candidate = [ 'a ', 'b ', 'c ', 'd ', 'e ', 'f ', 'g ', 'h ', 'i ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.20: Example of making all words diﬀerent.\\nWe get the worse possible score.\\n0.0\\nListing 24.21: Sample output BLEU score when making all words diﬀerent.\\nNow, let’s try a candidate that has fewer words than the reference (e.g. drop the last two\\nwords), but the words are all correct.\\n# shorter candidate\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.22: Example of making one case shorter.\\nThe score is much like the score when two words were wrong above.\\n0.7514772930752859\\nListing 24.23: Sample output BLEU score when making one case shorter.\\nHow about if we make the candidate two words longer than the reference?\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 291}, page_content=\"24.6. Further Reading 275\\n# longer candidate\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ',\\n'from ', 'space ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.24: Example of making one case longer.\\nAgain, we can see that our intuition holds and the score is something like two words wrong .\\n0.7860753021519787\\nListing 24.25: Sample output BLEU score when making one longer shorter.\\nFinally, let’s compare a candidate that is way too short: only two words in length.\\n# very short\\nfrom nltk.translate.bleu_score import sentence_bleu\\nreference = [[ 'the ', 'quick ', 'brown ', 'fox ', 'jumped ', 'over ', 'the ', 'lazy ', 'dog ']]\\ncandidate = [ 'the ', 'quick ']\\nscore = sentence_bleu(reference, candidate)\\nprint(score)\\nListing 24.26: Example of making one case too short.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 291}, page_content='Running this example ﬁrst prints a warning message indicating that the 3-gram and above\\npart of the evaluation (up to 4-gram) cannot be performed. This is fair given we only have\\n2-grams to work with in the candidate.\\nUserWarning:\\nCorpus/Sentence contains 0 counts of 3-gram overlaps.\\nBLEU scores might be undesirable; use SmoothingFunction().\\nwarnings.warn(_msg)\\nListing 24.27: Sample warning message.\\nNext, we can a score that is very low indeed.\\n0.0301973834223185\\nListing 24.28: Sample output BLEU score when making one case too short.\\nI encourage you to continue to play with examples. The math is pretty simple and I would\\nalso encourage you to read the paper and explore calculating the sentence-level score yourself in\\na spreadsheet.\\n24.6 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.\\n\\x88BLEU on Wikipedia.\\nhttps://en.wikipedia.org/wiki/BLEU\\n\\x88BLEU: a Method for Automatic Evaluation of Machine Translation , 2002.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 291}, page_content='http://www.aclweb.org/anthology/P02-1040.pdf'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 292}, page_content='24.7. Summary 276\\n\\x88Source code for nltk.translate.bleu score .\\nhttp://www.nltk.org/_modules/nltk/translate/bleu_score.html\\n\\x88nltk.translate package API Documentation.\\nhttp://www.nltk.org/api/nltk.translate.html\\n24.7 Summary\\nIn this tutorial, you discovered the BLEU score for evaluating and scoring candidate text to\\nreference text in machine translation and other language generation tasks. Speciﬁcally, you\\nlearned:\\n\\x88A gentle introduction to the BLEU score and an intuition for what is being calculated.\\n\\x88How you can calculate BLEU scores in Python using the NLTK library for sentences and\\ndocuments.\\n\\x88How to can use a suite of small examples to develop an intuition for how diﬀerences\\nbetween a candidate and reference text impact the ﬁnal BLEU score.\\n24.7.1 Next\\nIn the next chapter, you will discover how you can prepare data for training a caption generation\\nmodel.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 293}, page_content='Chapter 25\\nHow to Prepare a Photo Caption\\nDataset For Modeling\\nAutomatic photo captioning is a problem where a model must generate a human-readable textual\\ndescription given a photograph. It is a challenging problem in artiﬁcial intelligence that requires\\nboth image understanding from the ﬁeld of computer vision as well as language generation from\\nthe ﬁeld of natural language processing. It is now possible to develop your own image caption\\nmodels using deep learning and freely available datasets of photos and their descriptions. In this\\ntutorial, you will discover how to prepare photos and textual descriptions ready for developing\\na deep learning automatic photo caption generation model. After completing this tutorial, you\\nwill know:\\n\\x88About the Flickr8K dataset comprised of more than 8,000 photos and up to 5 captions for\\neach photo.\\n\\x88How to generally load and prepare photo and text data for modeling with deep learning.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 293}, page_content='\\x88How to speciﬁcally encode data for two diﬀerent types of deep learning models in Keras.\\nLet’s get started.\\n25.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Download the Flickr8K Dataset\\n2. How to Load Photographs\\n3. Pre-Calculate Photo Features\\n4. How to Load Descriptions\\n5. Prepare Description Text\\n6. Whole Description Sequence Model\\n7. Word-By-Word Model\\n8. Progressive Loading\\n277'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 294}, page_content='25.2. Download the Flickr8K Dataset 278\\n25.2 Download the Flickr8K Dataset\\nA good dataset to use when getting started with image captioning is the Flickr8K dataset. The\\nreason is that it is realistic and relatively small so that you can download it and build models on\\nyour workstation using a CPU. The deﬁnitive description of the dataset is in the paper Framing\\nImage Description as a Ranking Task: Data, Models and Evaluation Metrics from 2013. The\\nauthors describe the dataset as follows:\\nWe introduce a new benchmark collection for sentence-based image description and\\nsearch, consisting of 8,000 images that are each paired with ﬁve diﬀerent captions\\nwhich provide clear descriptions of the salient entities and events.\\n...\\nThe images were chosen from six diﬀerent Flickr groups, and tend not to contain\\nany well-known people or locations, but were manually selected to depict a variety\\nof scenes and situations.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 294}, page_content='—Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics , 2013.\\nThe dataset is available for free. You must complete a request form and the links to the\\ndataset will be emailed to you. I would love to link to them for you, but the email address\\nexpressly requests: Please do not redistribute the dataset . You can use the link below to request\\nthe dataset:\\n\\x88Dataset Request Form.\\nhttps://illinois.edu/fb/sec/1713398\\nWithin a short time, you will receive an email that contains links to two ﬁles:\\n\\x88Flickr8k Dataset.zip (1 Gigabyte) An archive of all photographs.\\n\\x88Flickr8k text.zip (2.2 Megabytes) An archive of all text descriptions for photographs.\\nDownload the datasets and unzip them into your current working directory. You will have\\ntwo directories:\\n\\x88Flicker8k Dataset : Contains more than 8000 photographs in JPEG format (yes the\\ndirectory name spells it ‘Flicker’ not ‘Flickr’).'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 294}, page_content='\\x88Flickr8k text : Contains a number of ﬁles containing diﬀerent sources of descriptions for\\nthe photographs.\\nNext, let’s look at how to load the images.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 295}, page_content=\"25.3. How to Load Photographs 279\\n25.3 How to Load Photographs\\nIn this section, we will develop some code to load the photos for use with the Keras deep learning\\nlibrary in Python. The image ﬁle names are unique image identiﬁers. For example, here is a\\nsample of image ﬁle names:\\n990890291_afc72be141.jpg\\n99171998_7cc800ceef.jpg\\n99679241_adc853a5c0.jpg\\n997338199_7343367d7f.jpg\\n997722733_0cb5439472.jpg\\nListing 25.1: Example of photographs ﬁlenames.\\nKeras provides the load img() function that can be used to load the image ﬁles directly as\\nan array of pixels.\\nfrom keras.preprocessing.image import load_img\\nimage = load_img( '990890291_afc72be141.jpg ')\\nListing 25.2: Example of loading a single photograph\\nThe pixel data needs to be converted to a NumPy array for use in Keras. We can use the\\nimgtoarray() Keras function to convert the loaded data.\\nfrom keras.preprocessing.image import img_to_array\\nimage = img_to_array(image)\\nListing 25.3: Example of converting a photograph to a NumPy array\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 295}, page_content=\"We may want to use a pre-deﬁned feature extraction model, such as a state-of-the-art deep\\nimage classiﬁcation network trained on Image net. The Oxford Visual Geometry Group (VGG)\\nmodel is popular for this purpose and is available in Keras. If we decide to use this pre-trained\\nmodel as a feature extractor in our model, we can pre-process the pixel data for the model by\\nusing the preprocess input() function in Keras, for example:\\nfrom keras.applications.vgg16 import preprocess_input\\n# reshape data into a single sample of an image\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\nListing 25.4: Prepare image data for the VGG16 model\\nWe may also want to force the loading of the photo to have the same pixel dimensions as the\\nVGG model, which are 224 x 224 pixels. We can do that in the call to load img() , for example:\\nimage = load_img( '990890291_afc72be141.jpg ', target_size=(224, 224))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 295}, page_content=\"Listing 25.5: Load an image in Keras to a speciﬁc size\\nWe may want to extract the unique image identiﬁer from the image ﬁlename. We can do\\nthat by splitting the ﬁlename string by the ‘.’ (period) character and retrieving the ﬁrst element\\nof the resulting array:\\nimage_id = filename.split( '. ')[0]\\nListing 25.6: Retrieve image identiﬁer from ﬁlename\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 296}, page_content=\"25.4. Pre-Calculate Photo Features 280\\nWe can tie all of this together and develop a function that, given the name of the directory\\ncontaining the photos, will load and pre-process all of the photos for the VGG model and return\\nthem in a dictionary keyed on their unique image identiﬁers.\\nfrom os import listdir\\nfrom os import path\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\ndef load_photos(directory):\\nimages = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get image id\\nimage_id = name.split( '. ')[0]\\nimages[image_id] = image\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 296}, page_content=\"return images\\n# load images\\ndirectory = 'Flicker8k_Dataset '\\nimages = load_photos(directory)\\nprint( 'Loaded Images: %d '% len(images))\\nListing 25.7: Complete example of loading photos from ﬁle.\\nRunning this example prints the number of loaded images. It takes a few minutes to run.\\nLoaded Images: 8091\\nListing 25.8: Example output of loading photos from ﬁle.\\n25.4 Pre-Calculate Photo Features\\nIt is possible to use a pre-trained model to extract the features from photos in the dataset and\\nstore the features to ﬁle. This is an eﬃciency that means that the language part of the model\\nthat turns features extracted from the photo into textual descriptions can be trained standalone\\nfrom the feature extraction model. The beneﬁt is that the very large pre-trained models do not\\nneed to be loaded, held in memory, and used to process each photo while training the language\\nmodel.\\nLater, the feature extraction model and language model can be put back together for making\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 296}, page_content='predictions on new photos. In this section, we will extend the photo loading behavior developed\\nin the previous section to load all photos, extract their features using a pre-trained VGG model,\\nand store the extracted features to a new ﬁle that can be loaded and used to train the language\\nmodel. The ﬁrst step is to load the VGG model. This model is provided directly in Keras and'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 297}, page_content=\"25.4. Pre-Calculate Photo Features 281\\ncan be loaded as follows. Note that this will download the 500-megabyte model weights to your\\ncomputer, which may take a few minutes.\\nfrom keras.applications.vgg16 import VGG16\\n# load the model\\nin_layer = Input(shape=(224, 224, 3))\\nmodel = VGG16(include_top=False, input_tensor=in_layer, pooling= 'avg ')\\nmodel.summary()\\nListing 25.9: Load the VGG mode.\\nThis will load the VGG 16-layer model. The two Dense output layers as well as the\\nclassiﬁcation output layer are removed from the model by setting include top=False . The\\noutput from the ﬁnal pooling layer is taken as the features extracted from the image. Next, we\\ncan walk over all images in the directory of images as in the previous section and call predict()\\nfunction on the model for each prepared image to get the extracted features. The features can\\nthen be stored in a dictionary keyed on the image id. The complete example is listed below.\\nfrom os import listdir\\nfrom os import path\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 297}, page_content='from os import path\\nfrom pickle import dump\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.layers import Input\\n# extract features from each photo in the directory\\ndef extract_features(directory):\\n# load the model\\nin_layer = Input(shape=(224, 224, 3))\\nmodel = VGG16(include_top=False, input_tensor=in_layer)\\nmodel.summary()\\n# extract features from each photo\\nfeatures = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\n# get image id'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 297}, page_content=\"# get image id\\nimage_id = name.split( '. ')[0]\\n# store feature\\nfeatures[image_id] = feature\\nprint( '>%s '% name)\\nreturn features\\n# extract features from all images\\ndirectory = 'Flicker8k_Dataset '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 298}, page_content=\"25.5. How to Load Descriptions 282\\nfeatures = extract_features(directory)\\nprint( 'Extracted Features: %d '% len(features))\\n# save to file\\ndump(features, open( 'features.pkl ', 'wb '))\\nListing 25.10: Complete example of pre-calculating VGG16 photo features.\\nThe example may take some time to complete, perhaps one hour. After all features are\\nextracted, the dictionary is stored in the ﬁle features.pkl in the current working directory.\\nThese features can then be loaded later and used as input for training a language model. You\\ncould experiment with other types of pre-trained models in Keras.\\n25.5 How to Load Descriptions\\nIt is important to take a moment to talk about the descriptions; there are a number available.\\nThe ﬁle Flickr8k.token.txt contains a list of image identiﬁers (used in the image ﬁlenames)\\nand tokenized descriptions. Each image has multiple descriptions. Below is a sample of the\\ndescriptions from the ﬁle showing 5 diﬀerent descriptions for a single image.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 298}, page_content=\"1305564994_00513f9a5b.jpg#0 A man in street racer armor be examine the tire of another\\nracer 's motorbike .\\n1305564994_00513f9a5b.jpg#1 Two racer drive a white bike down a road .\\n1305564994_00513f9a5b.jpg#2 Two motorist be ride along on their vehicle that be oddly\\ndesign and color .\\n1305564994_00513f9a5b.jpg#3 Two person be in a small race car drive by a green hill .\\n1305564994_00513f9a5b.jpg#4 Two person in race uniform in a street car .\\nListing 25.11: Sample of raw photo descriptions.\\nThe ﬁle ExpertAnnotations.txt indicates which of the descriptions for each image were\\nwritten by experts which were written by crowdsource workers asked to describe the image.\\nFinally, the ﬁle CrowdFlowerAnnotations.txt provides the frequency of crowd workers that\\nindicate whether captions suit each image. These frequencies can be interpreted probabilistically.\\nThe authors of the paper describe the annotations as follows:\\n... annotators were asked to write sentences that describe the depicted scenes,\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 298}, page_content=\"situations, events and entities (people, animals, other objects). We collected multiple\\ncaptions for each image because there is a considerable degree of variance in the way\\nmany images can be described.\\n—Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics , 2013.\\nThere are also lists of the photo identiﬁers to use in a train/test split so that you can compare\\nresults reported in the paper. The ﬁrst step is to decide which captions to use. The simplest\\napproach is to use the ﬁrst description for each photograph. First, we need a function to load\\nthe entire annotations ﬁle ( Flickr8k.token.txt ) into memory. Below is a function to do this\\ncalled load doc() that, given a ﬁlename, will return the document as a string.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 299}, page_content=\"25.5. How to Load Descriptions 283\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 25.12: Function for loading a ﬁle into memory.\\nWe can see from the sample of the ﬁle above that we need only split each line by white space\\nand take the ﬁrst element as the image identiﬁer and the rest as the image description. For\\nexample:\\n# split line by white space\\ntokens = line.split()\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\nListing 25.13: Example of splitting a line into an identiﬁer and a description.\\nWe can then clean up the image identiﬁer by removing the ﬁlename extension and the\\ndescription number.\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\nListing 25.14: Example of cleaning up the photo identiﬁer.\\nWe can also put the description tokens back together into a string for later processing.\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 299}, page_content=\"Listing 25.15: Example of converting the description tokens into a string.\\nWe can put all of this together into a function. Below deﬁnes the load descriptions()\\nfunction that will take the loaded ﬁle, process it line-by-line, and return a dictionary of image\\nidentiﬁers to their ﬁrst description.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 300}, page_content=\"25.6. Prepare Description Text 284\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# store the first description for each image\\nif image_id not in mapping:\\nmapping[image_id] = image_desc\\nreturn mapping\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\ndoc = load_doc(filename)\\ndescriptions = load_descriptions(doc)\\nprint( 'Loaded: %d '% len(descriptions))\\nListing 25.16: Complete example of loading photo descriptions.\\nRunning the example prints the number of loaded image descriptions.\\nLoaded: 8092\\nListing 25.17: Example output of loading photo descriptions.\\nThere are other ways to load descriptions that may turn out to be more accurate for the\\ndata. Use the above example as a starting point and let me know what you come up with.\\n25.6 Prepare Description Text\\nThe descriptions are tokenized; this means that each token is comprised of words separated by\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 300}, page_content=\"white space. It also means that punctuation are separated as their own tokens, such as periods\\n(‘.’) and apostrophes for word plurals (’s). It is a good idea to clean up the description text\\nbefore using it in a model. Some ideas of data cleaning we can form include:\\n\\x88Normalizing the case of all tokens to lowercase.\\n\\x88Remove all punctuation from tokens.\\n\\x88Removing all tokens that contain one or fewer characters (after punctuation is removed),\\ne.g. ‘a’ and hanging ‘s’ characters.\\nWe can implement these simple cleaning operations in a function that cleans each description\\nin the loaded dictionary from the previous section. Below deﬁnes the clean descriptions()\\nfunction that will clean each loaded description.\\n# clean description text\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor key, desc in descriptions.items():\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 300}, page_content='desc = [word.lower() for word in desc]\\n# remove punctuation from each word'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 301}, page_content=\"25.6. Prepare Description Text 285\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\\n# store as string\\ndescriptions[key] = ' '.join(desc)\\nListing 25.18: Function to clean photo descriptions.\\nWe can then save the clean text to ﬁle for later use by our model. Each line will contain the\\nimage identiﬁer followed by the clean description. Below deﬁnes the save doc() function for\\nsaving the cleaned descriptions to ﬁle.\\n# save descriptions to file, one per line\\ndef save_doc(descriptions, filename):\\nlines = list()\\nfor key, desc in mapping.items():\\nlines.append(key + ' ' + desc)\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nListing 25.19: Function to save clean descriptions.\\nPutting this all together with the loading of descriptions from the previous section, the\\ncomplete example is listed below.\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 301}, page_content=\"# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# store the first description for each image\\nif image_id not in mapping:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 302}, page_content=\"25.6. Prepare Description Text 286\\nmapping[image_id] = image_desc\\nreturn mapping\\n# clean description text\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor key, desc in descriptions.items():\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\\ndesc = [word.lower() for word in desc]\\n# remove punctuation from each word\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\\n# store as string\\ndescriptions[key] = ' '.join(desc)\\n# save descriptions to file, one per line\\ndef save_doc(descriptions, filename):\\nlines = list()\\nfor key, desc in descriptions.items():\\nlines.append(key + ' ' + desc)\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\n# load descriptions\\ndoc = load_doc(filename)\\n# parse descriptions\\ndescriptions = load_descriptions(doc)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 302}, page_content=\"print( 'Loaded: %d '% len(descriptions))\\n# clean descriptions\\nclean_descriptions(descriptions)\\n# summarize vocabulary\\nall_tokens = ' '.join(descriptions.values()).split()\\nvocabulary = set(all_tokens)\\nprint( 'Vocabulary Size: %d '% len(vocabulary))\\n# save descriptions\\nsave_doc(descriptions, 'descriptions.txt ')\\nListing 25.20: Complete example of cleaning photo descriptions.\\nRunning the example ﬁrst loads 8,092 descriptions, cleans them, summarizes the vocabulary\\nof 4,484 unique words, then saves them to a new ﬁle called descriptions.txt .\\nLoaded: 8092\\nVocabulary Size: 4484\\nListing 25.21: Example output of cleaning photo descriptions.\\nOpen the new ﬁle descriptions.txt in a text editor and review the contents. You should\\nsee somewhat readable descriptions of photos ready for modeling.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 303}, page_content='25.7. Whole Description Sequence Model 287\\n...\\n3139118874_599b30b116 two girls pose for picture at christmastime\\n2065875490_a46b58c12b person is walking on sidewalk and skeleton is on the left inside of\\nfence\\n2682382530_f9f8fd1e89 man in black shorts is stretching out his leg\\n3484019369_354e0b88c0 hockey team in red and white on the side of the ice rink\\n505955292_026f1489f2 boy rides horse\\nListing 25.22: Sample of clean photo descriptions.\\nThe vocabulary is still relatively large. To make modeling easier, especially the ﬁrst time\\naround, I would recommend further reducing the vocabulary by removing words that only\\nappear once or twice across all descriptions.\\n25.7 Whole Description Sequence Model\\nThere are many ways to model the caption generation problem. One naive way is to create a\\nmodel that outputs the entire textual description in a one-shot manner. This is a naive model\\nbecause it puts a heavy burden on the model to both interpret the meaning of the photograph'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 303}, page_content='and generate words, then arrange those words into the correct order.\\nThis is not unlike the language translation problem used in an Encoder-Decoder recurrent\\nneural network where the entire translated sentence is output one word at a time given an\\nencoding of the input sequence. Here we would use an encoding of the image to generate the\\noutput sentence instead. The image may be encoded using a pre-trained model used for image\\nclassiﬁcation, such as the VGG trained on the ImageNet model mentioned above.\\nThe output of the model would be a probability distribution over each word in the vocabulary.\\nThe sequence would be as long as the longest photo description. The descriptions would, therefore,\\nneed to be ﬁrst integer encoded where each word in the vocabulary is assigned a unique integer\\nand sequences of words would be replaced with sequences of integers. The integer sequences\\nwould then need to be one hot encoded to represent the idealized probability distribution'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 303}, page_content=\"over the vocabulary for each word in the sequence. We can use tools in Keras to prepare the\\ndescriptions for this type of model. The ﬁrst step is to load the mapping of image identiﬁers to\\nclean descriptions stored in descriptions.txt .\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 304}, page_content=\"25.7. Whole Description Sequence Model 288\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\nmapping[image_id] = ' '.join(image_desc)\\nreturn descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\nListing 25.23: Load the cleaned descriptions.\\nRunning this piece loads the 8,092 photo descriptions into a dictionary keyed on image\\nidentiﬁers. These identiﬁers can then be used to load each photo ﬁle for the corresponding\\ninputs to the model.\\nLoaded 8092\\nListing 25.24: Example output from loading the clean descriptions.\\nNext, we need to extract all of the description text so we can encode it.\\n# extract all text\\ndesc_text = list(descriptions.values())\\nListing 25.25: Convert loaded description text to a list.\\nWe can use the Keras Tokenizer class to consistently map each word in the vocabulary to\\nan integer. First, the object is created, then is ﬁt on the description text. The ﬁt tokenizer can\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 304}, page_content=\"later be saved to ﬁle for consistent decoding of the predictions back to vocabulary words.\\nfrom keras.preprocessing.text import Tokenizer\\n# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 25.26: Fit a Tokenizer of the photo description text.\\nNext, we can use the ﬁt tokenizer to encode the photo descriptions into sequences of integers.\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\nListing 25.27: Example of integer encoding the description text.\\nThe model will require all output sequences to have the same length for training. We can\\nachieve this by padding all encoded sequences to have the same length as the longest encoded\\nsequence. We can pad the sequences with 0 values after the list of words. Keras provides the\\npadsequences() function to pad the sequences.\\nfrom keras.preprocessing.sequence import pad_sequences\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 304}, page_content=\"# pad all sequences to a fixed length\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\npadded = pad_sequences(sequences, maxlen=max_length, padding= 'post ')\\nListing 25.28: Pad descriptions to a maximum length.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 305}, page_content=\"25.7. Whole Description Sequence Model 289\\nFinally, we can one hot encode the padded sequences to have one sparse vector for each word\\nin the sequence. Keras provides the tocategorical() function to perform this operation.\\nfrom keras.utils import to_categorical\\n# one hot encode\\ny = to_categorical(padded, num_classes=vocab_size)\\nListing 25.29: Example of one hot encoding output text.\\nOnce encoded, we can ensure that the sequence output data has the right shape for the\\nmodel.\\ny = y.reshape((len(descriptions), max_length, vocab_size))\\nprint(y.shape)\\nListing 25.30: Example of reshaping encoded text.\\nPutting all of this together, the complete example is listed below.\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 305}, page_content=\"return text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\ndescriptions[image_id] = ' '.join(image_desc)\\nreturn descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\n# extract all text\\ndesc_text = list(descriptions.values())\\n# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\n# pad all sequences to a fixed length\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 306}, page_content=\"25.8. Word-By-Word Model 290\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\npadded = pad_sequences(sequences, maxlen=max_length, padding= 'post ')\\n# one hot encode\\ny = to_categorical(padded, num_classes=vocab_size)\\ny = y.reshape((len(descriptions), max_length, vocab_size))\\nprint(y.shape)\\nListing 25.31: Complete example of data preparation for a whole sequence model.\\nRunning the example ﬁrst prints the number of loaded image descriptions (8,092 photos),\\nthe dataset vocabulary size (4,485 words), the length of the longest description (28 words), then\\nﬁnally the shape of the data for ﬁtting a prediction model in the form [samples, sequence length,\\nfeatures].\\nLoaded 8092\\nVocabulary Size: 4485\\nDescription Length: 28\\n(8092, 28, 4485)\\nListing 25.32: Example output from preparing data for a whole sequence prediction model.\\nAs mentioned, outputting the entire sequence may be challenging for the model. We will\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 306}, page_content='look at a simpler model in the next section.\\n25.8 Word-By-Word Model\\nA simpler model for generating a caption for photographs is to generate one word given both\\nthe image as input and the last word generated. This model would then have to be called\\nrecursively to generate each word in the description with previous predictions as input. Using\\nthe word as input, give the model a forced context for predicting the next word in the sequence.\\nThis is the model used in prior research, such as: Show and Tell: A Neural Image Caption\\nGenerator , 2015. A word embedding layer can be used to represent the input words. Like the\\nfeature extraction model for the photos, this too can be pre-trained either on a large corpus or\\non the dataset of all descriptions.\\nThe model would take a full sequence of words as input; the length of the sequence would be\\nthe maximum length of descriptions in the dataset. The model must be started with something.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 306}, page_content='One approach is to surround each photo description with special tags to signal the start and\\nend of the description, such as STARTDESC andENDDESC . For example, the description:\\nboy rides horse\\nListing 25.33: Example of a photo description.\\nWould become:\\nSTARTDESC boy rides horse ENDDESC\\nListing 25.34: Example of a wrapped photo description.\\nAnd would be fed to the model with the same image input to result in the following\\ninput-output word sequence pairs:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 307}, page_content=\"25.8. Word-By-Word Model 291\\nInput (X), Output (y)\\nSTARTDESC, boy\\nSTARTDESC, boy, rides\\nSTARTDESC, boy, rides, horse\\nSTARTDESC, boy, rides, horse ENDDESC\\nListing 25.35: Example input-output pairs for a wrapped description.\\nThe data preparation would begin much the same as was described in the previous section.\\nEach description must be integer encoded. After encoding, the sequences are split into multiple\\ninput and output pairs and only the output word ( y) is one hot encoded. This is because the\\nmodel is only required to predict the probability distribution of one word at a time. The code is\\nthe same up to the point where we calculate the maximum length of sequences.\\n...\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\n# extract all text\\ndesc_text = list(descriptions.values())\\n# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 307}, page_content=\"print( 'Vocabulary Size: %d '% vocab_size)\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\n# determine the maximum sequence length\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\nListing 25.36: Example of loading and encoding photo descriptions.\\nNext, we split the each integer encoded sequence into input and output pairs. Let’s step\\nthrough a single sequence called seq at the i’th word in the sequence, where imore than or\\nequal to 1. First, we take the ﬁrst i-1words as the input sequence and the i’th word as the\\noutput word.\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\nListing 25.37: Example of splitting a description sequence.\\nNext, the input sequence is padded to the maximum length of the input sequences. Pre-\\npadding is used (the default) so that new words appear at the end of the sequence, instead of\\nthe input beginning.\\n# pad input sequence\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 307}, page_content='in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\nListing 25.38: Example of padding a split description sequence.\\nThe output word is one hot encoded, much like in the previous section.\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\nListing 25.39: Example of one hot encoding the output word.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 308}, page_content=\"25.8. Word-By-Word Model 292\\nWe can put all of this together into a complete example to prepare description data for the\\nword-by-word model.\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\ndescriptions[image_id] = ' '.join(image_desc)\\nreturn descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\nprint( 'Loaded %d '% (len(descriptions)))\\n# extract all text\\ndesc_text = list(descriptions.values())\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 308}, page_content=\"# prepare tokenizer\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(desc_text)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# integer encode descriptions\\nsequences = tokenizer.texts_to_sequences(desc_text)\\n# determine the maximum sequence length\\nmax_length = max(len(s) for s in sequences)\\nprint( 'Description Length: %d '% max_length)\\nX, y = list(), list()\\nfor img_no, seq in enumerate(sequences):\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 309}, page_content='25.9. Progressive Loading 293\\n# store\\nX.append(in_seq)\\ny.append(out_seq)\\n# convert to numpy arrays\\nX, y = array(X), array(y)\\nprint(X.shape)\\nprint(y.shape)\\nListing 25.40: Complete example of data preparation for a word-by-word model.\\nRunning the example prints the same statistics, but prints the size of the resulting encoded\\ninput and output sequences. Note that the input of images must follow the exact same ordering\\nwhere the same photo is shown for each example drawn from a single description. One way\\nto do this would be to load the photo and store it for each example prepared from a single\\ndescription.\\nLoaded 8092\\nVocabulary Size: 4485\\nDescription Length: 28\\n(66456, 28)\\n(66456, 4485)\\nListing 25.41: Example output of data preparation for a word-by-word prediction model.\\n25.9 Progressive Loading\\nThe Flicr8K dataset of photos and descriptions can ﬁt into RAM, if you have a lot of RAM\\n(e.g. 8 Gigabytes or more), and most modern systems do. This is ﬁne if you want to ﬁt a deep'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 309}, page_content='learning model using the CPU. Alternately, if you want to ﬁt a model using a GPU, then you\\nwill not be able to ﬁt the data into memory of an average GPU video card. One solution is to\\nprogressively load the photos and descriptions as-needed by the model.\\nKeras supports progressively loaded datasets by using the fitgenerator() function on the\\nmodel. A generator is the term used to describe a function used to return batches of samples\\nfor the model to train on. This can be as simple as a standalone function, the name of which is\\npassed to the fitgenerator() function when ﬁtting the model. As a reminder, a model is ﬁt\\nfor multiple epochs, where one epoch is one pass through the entire training dataset, such as all\\nphotos. One epoch is comprised of multiple batches of examples where the model weights are\\nupdated at the end of each batch.\\nA generator must create and yield one batch of examples. For example, the average sentence'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 309}, page_content='length in the dataset is 11 words; that means that each photo will result in 11 examples for\\nﬁtting the model and two photos will result in about 22 examples on average. A good default\\nbatch size for modern hardware may be 32 examples, so that is about 2-3 photos worth of\\nexamples.\\nWe can write a custom generator to load a few photos and return the samples as a single\\nbatch. Let’s assume we are working with a word-by-word model described in the previous\\nsection that expects a sequence of words and a prepared image as input and predicts a single\\nword. Let’s design a data generator that given a loaded dictionary of image identiﬁers to clean\\ndescriptions, a trained tokenizer, and a maximum sequence length will load one-image worth of\\nexamples for each batch.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 310}, page_content=\"25.9. Progressive Loading 294\\nA generator must loop forever and yield each batch of samples. We can loop forever with\\na while loop and within this, loop over each image in the image directory. For each image\\nﬁlename, we can load the image and create all of the input-output sequence pairs from the\\nimage’s description. Below is the data generator function.\\ndef data_generator(mapping, tokenizer, max_length):\\n# loop for ever over images\\ndirectory = 'Flicker8k_Dataset '\\nwhile 1:\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = directory + '/ '+ name\\nimage, image_id = load_image(filename)\\n# create word sequences\\ndesc = mapping[image_id]\\nin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc, image)\\nyield [[in_img, in_seq], out_word]\\nListing 25.42: Example of a generator for progressive loading.\\nYou could extend it to take the name of the dataset directory as a parameter. The generator\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 310}, page_content=\"returns an array containing the inputs ( X) and output ( y) for the model. The input is comprised\\nof an array with two items for the input images and encoded word sequences. The outputs\\nare one hot encoded words. You can see that it calls a function called load photo() to load a\\nsingle photo and return the pixels and image identiﬁer. This is a simpliﬁed version of the photo\\nloading function developed at the beginning of this tutorial.\\n# load a single photo intended as input for the VGG feature extractor model\\ndef load_photo(filename):\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)[0]\\n# get image id\\nimage_id = filename.split( '/ ')[-1].split( '. ')[0]\\nreturn image, image_id\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 310}, page_content='Listing 25.43: Example of a function for loading and preparing a photo.\\nAnother function named create sequences() is called to create sequences of images, input\\nsequences of words, and output words that we then yield to the caller. This is a function that\\nincludes everything discussed in the previous section, and also creates copies of the image pixels,\\none for each input-output pair created from the photo’s description.\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, descriptions, images):\\nXimages, XSeq, y = list(), list(),list()\\nvocab_size = len(tokenizer.word_index) + 1\\nfor j in range(len(descriptions)):\\nseq = descriptions[j]\\nimage = images[j]\\n# integer encode\\nseq = tokenizer.texts_to_sequences([seq])[0]'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 311}, page_content='25.9. Progressive Loading 295\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# select\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nXimages.append(image)\\nXSeq.append(in_seq)\\ny.append(out_seq)\\nXimages, XSeq, y = array(Ximages), array(XSeq), array(y)\\nreturn Ximages, XSeq, y\\nListing 25.44: Example of a function for preparing description text.\\nPrior to preparing the model that uses the data generator, we must load the clean descriptions,\\nprepare the tokenizer, and calculate the maximum sequence length. All 3 of must be passed to\\nthedata generator() as parameters. We use the same load clean descriptions() function\\ndeveloped previously and a new create tokenizer() function that simpliﬁes the creation of\\nthe tokenizer. Tying all of this together, the complete data generator is listed below, ready for'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 311}, page_content=\"use to train a model.\\nfrom os import listdir\\nfrom os import path\\nfrom numpy import array\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename):\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# store\\ndescriptions[image_id] = ' '.join(image_desc)\\nreturn descriptions\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 312}, page_content=\"25.9. Progressive Loading 296\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = list(descriptions.values())\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# load a single photo intended as input for the VGG feature extractor model\\ndef load_photo(filename):\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)[0]\\n# get image id\\nimage_id = path.basename(filename).split( '. ')[0]\\nreturn image, image_id\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, desc, image):\\nXimages, XSeq, y = list(), list(),list()\\nvocab_size = len(tokenizer.word_index) + 1\\n# integer encode the description\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 312}, page_content=\"seq = tokenizer.texts_to_sequences([desc])[0]\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# select\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nXimages.append(image)\\nXSeq.append(in_seq)\\ny.append(out_seq)\\nXimages, XSeq, y = array(Ximages), array(XSeq), array(y)\\nreturn [Ximages, XSeq, y]\\n# data generator, intended to be used in a call to model.fit_generator()\\ndef data_generator(descriptions, tokenizer, max_length):\\n# loop for ever over images\\ndirectory = 'Flicker8k_Dataset '\\nwhile 1:\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage, image_id = load_photo(filename)\\n# create word sequences\\ndesc = descriptions[image_id]\\nin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc, image)\\nyield [[in_img, in_seq], out_word]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 313}, page_content=\"25.10. Further Reading 297\\n# load mapping of ids to descriptions\\ndescriptions = load_clean_descriptions( 'descriptions.txt ')\\n# integer encode sequences of words\\ntokenizer = create_tokenizer(descriptions)\\n# pad to fixed length\\nmax_length = max(len(s.split()) for s in list(descriptions.values()))\\nprint( 'Description Length: %d '% max_length)\\n# test the data generator\\ngenerator = data_generator(descriptions, tokenizer, max_length)\\ninputs, outputs = next(generator)\\nprint(inputs[0].shape)\\nprint(inputs[1].shape)\\nprint(outputs.shape)\\nListing 25.45: Complete example of progressive loading.\\nA data generator can be tested by calling the next() function. We can test the generator as\\nfollows.\\n# test the data generator\\ngenerator = data_generator(descriptions, tokenizer, max_length)\\ninputs, outputs = next(generator)\\nprint(inputs[0].shape)\\nprint(inputs[1].shape)\\nprint(outputs.shape)\\nListing 25.46: Example of testing the custom generator function.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 313}, page_content='Running the example prints the shape of the input and output example for a single batch\\n(e.g. 13 input-output pairs):\\n(13, 224, 224, 3)\\n(13, 28)\\n(13, 4485)\\nListing 25.47: Example output from testing the generator function.\\nThe generator can be used to ﬁt a model by calling the fitgenerator() function on the\\nmodel (instead of fit() ) and passing in the generator. We must also specify the number of\\nsteps or batches per epoch. We could estimate this as (10 x training dataset size), perhaps\\n70,000 if 7,000 images are used for training.\\n# define model\\n# ...\\n# fit model\\nmodel.fit_generator(data_generator(descriptions, tokenizer, max_length),\\nsteps_per_epoch=70000, ...)\\nListing 25.48: Example of using the progressive loading data generator when ﬁtting a Keras\\nmodel.\\n25.10 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 314}, page_content='25.11. Summary 298\\n25.10.1 Flickr8K Dataset\\n\\x88Framing image description as a ranking task: data, models and evaluation metrics (Home-\\npage).\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.\\nhtml\\n\\x88Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics ,\\n013.\\nhttps://www.jair.org/media/3994/live-3994-7274-jair.pdf\\n\\x88Dataset Request Form.\\nhttps://illinois.edu/fb/sec/1713398\\n\\x88Old Flicrk8K Homepage.\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html\\n25.10.2 API\\n\\x88Python Generators.\\nhttps://wiki.python.org/moin/Generators\\n\\x88Keras Model API.\\nhttps://keras.io/models/model/\\n\\x88Keras padsequences() API.\\nhttps://keras.io/preprocessing/sequence/#pad_sequences\\n\\x88Keras Tokenizer API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n\\x88Keras VGG16 API.\\nhttps://keras.io/applications/#vgg16\\n25.11 Summary\\nIn this tutorial, you discovered how to prepare photos and textual descriptions ready for'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 314}, page_content='developing an automatic photo caption generation model. Speciﬁcally, you learned:\\n\\x88About the Flickr8K dataset comprised of more than 8,000 photos and up to 5 captions for\\neach photo.\\n\\x88How to generally load and prepare photo and text data for modeling with deep learning.\\n\\x88How to speciﬁcally encode data for two diﬀerent types of deep learning models in Keras.\\n25.11.1 Next\\nIn the next chapter, you will discover how you can develop a model for automatic caption\\ngeneration.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 315}, page_content='Chapter 26\\nProject: Develop a Neural Image\\nCaption Generation Model\\nCaption generation is a challenging artiﬁcial intelligence problem where a textual description\\nmust be generated for a given photograph. It requires both methods from computer vision to\\nunderstand the content of the image and a language model from the ﬁeld of natural language\\nprocessing to turn the understanding of the image into words in the right order. Recently, deep\\nlearning methods have achieved state-of-the-art results on examples of this problem.\\nDeep learning methods have demonstrated state-of-the-art results on caption generation\\nproblems. What is most impressive about these methods is a single end-to-end model can be\\ndeﬁned to predict a caption, given a photo, instead of requiring sophisticated data preparation\\nor a pipeline of speciﬁcally designed models. In this tutorial, you will discover how to develop\\na photo captioning deep learning model from scratch. After completing this tutorial, you will\\nknow:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 315}, page_content='know:\\n\\x88How to prepare photo and text data for training a deep learning model.\\n\\x88How to design and train a deep learning caption generation model.\\n\\x88How to evaluate a train caption generation model and use it to caption entirely new\\nphotographs.\\nLet’s get started.\\n26.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. Photo and Caption Dataset\\n2. Prepare Photo Data\\n3. Prepare Text Data\\n4. Develop Deep Learning Model\\n5. Evaluate Model\\n6. Generate New Captions\\n299'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 316}, page_content='26.2. Photo and Caption Dataset 300\\n26.2 Photo and Caption Dataset\\nIn this tutorial, we will use the Flickr8k dataset. This dataset was introduced previously in\\nChapter 25. The dataset is available for free. You must complete a request form and the links to\\nthe dataset will be emailed to you. I would love to link to them for you, but the email address\\nexpressly requests: Please do not redistribute the dataset . You can use the link below to request\\nthe dataset:\\n\\x88Dataset Request Form.\\nhttps://illinois.edu/fb/sec/1713398\\nWithin a short time, you will receive an email that contains links to two ﬁles:\\n\\x88Flickr8k Dataset.zip (1 Gigabyte) An archive of all photographs.\\n\\x88Flickr8k text.zip (2.2 Megabytes) An archive of all text descriptions for photographs.\\nDownload the datasets and unzip them into your current working directory. You will have\\ntwo directories:\\n\\x88Flicker8k Dataset : Contains 8092 photographs in JPEG format (yes the directory name\\nspells it ‘Flicker’ not ‘Flickr’).'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 316}, page_content='\\x88Flickr8k text : Contains a number of ﬁles containing diﬀerent sources of descriptions for\\nthe photographs.\\nThe dataset has a pre-deﬁned training dataset (6,000 images), development dataset (1,000\\nimages), and test dataset (1,000 images). One measure that can be used to evaluate the skill of\\nthe model are BLEU scores. For reference, below are some ball-park BLEU scores for skillful\\nmodels when evaluated on the test dataset (taken from the 2017 paper Where to put the Image\\nin an Image Caption Generator ):\\n\\x88BLEU-1: 0.401 to 0.578.\\n\\x88BLEU-2: 0.176 to 0.390.\\n\\x88BLEU-3: 0.099 to 0.260.\\n\\x88BLEU-4: 0.059 to 0.170.\\nWe describe the BLEU metric more later when we work on evaluating our model. Next, let’s\\nlook at how to load the images.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 317}, page_content='26.3. Prepare Photo Data 301\\n26.3 Prepare Photo Data\\nWe will use a pre-trained model to interpret the content of the photos. There are many models\\nto choose from. In this case, we will use the Oxford Visual Geometry Group, or VGG, model\\nthat won the ImageNet competition in 2014. Keras provides this pre-trained model directly.\\nNote, the ﬁrst time you use this model, Keras will download the model weights from the Internet,\\nwhich are about 500 Megabytes. This may take a few minutes depending on your internet\\nconnection. Note the use of the VGG pre-trained model was introduced in Chapter 23.\\nWe could use this model as part of a broader image caption model. The problem is, it\\nis a large model and running each photo through the network every time we want to test a\\nnew language model conﬁguration (downstream) is redundant. Instead, we can pre-compute\\nthephoto features using the pre-trained model and save them to ﬁle. We can then load these'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 317}, page_content='features later and feed them into our model as the interpretation of a given photo in the dataset.\\nIt is no diﬀerent to running the photo through the full VGG model; it is just we will have done\\nit once in advance.\\nThis is an optimization that will make training our models faster and consume less memory.\\nWe can load the VGG model in Keras using the VGG class. We will remove the last layer from\\nthe loaded model, as this is the model used to predict a classiﬁcation for a photo. We are not\\ninterested in classifying images, but we are interested in the internal representation of the photo\\nright before a classiﬁcation is made. These are the features that the model has extracted from\\nthe photo.\\nKeras also provides tools for reshaping the loaded photo into the preferred size for the model\\n(e.g. 3 channel 224 x 224 pixel image). Below is a function named extract features() that,\\ngiven a directory name, will load each photo, prepare it for VGG, and collect the predicted'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 317}, page_content=\"features from the VGG model. The image features are a 1-dimensional 4,096 element vector.\\nThe function returns a dictionary of image identiﬁer to image features.\\n# extract features from each photo in the directory\\ndef extract_features(directory):\\n# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# summarize\\nmodel.summary()\\n# extract features from each photo\\nfeatures = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = directory + '/ '+ name\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\n# get image id\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 318}, page_content=\"26.3. Prepare Photo Data 302\\nimage_id = name.split( '. ')[0]\\n# store feature\\nfeatures[image_id] = feature\\nprint( '>%s '% name)\\nreturn features\\nListing 26.1: Function to extract photo features\\nWe can call this function to prepare the photo data for testing our models, then save the\\nresulting dictionary to a ﬁle named features.pkl . The complete example is listed below.\\nfrom os import listdir\\nfrom os import path\\nfrom pickle import dump\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.models import Model\\n# extract features from each photo in the directory\\ndef extract_features(directory):\\n# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# summarize\\nmodel.summary()\\n# extract features from each photo\\nfeatures = dict()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 318}, page_content=\"features = dict()\\nfor name in listdir(directory):\\n# load an image from file\\nfilename = path.join(directory, name)\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\n# get image id\\nimage_id = name.split( '. ')[0]\\n# store feature\\nfeatures[image_id] = feature\\nprint( '>%s '% name)\\nreturn features\\n# extract features from all images\\ndirectory = 'Flicker8k_Dataset '\\nfeatures = extract_features(directory)\\nprint( 'Extracted Features: %d '% len(features))\\n# save to file\\ndump(features, open( 'features.pkl ', 'wb '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 319}, page_content=\"26.4. Prepare Text Data 303\\nListing 26.2: Complete example of extracting photo features.\\nRunning this data preparation step may take a while depending on your hardware, perhaps\\none hour on the CPU with a modern workstation. At the end of the run, you will have\\nthe extracted features stored in features.pkl for later use. This ﬁle will be a few hundred\\nMegabytes in size.\\n26.4 Prepare Text Data\\nThe dataset contains multiple descriptions for each photograph and the text of the descriptions\\nrequires some minimal cleaning. Note, a fuller investigation into how this text data can\\nbe prepared was described in Chapter 25. First, we will load the ﬁle containing all of the\\ndescriptions.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\n# load descriptions\\ndoc = load_doc(filename)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 319}, page_content=\"Listing 26.3: Example of loading photo descriptions into memory\\nEach photo has a unique identiﬁer. This identiﬁer is used on the photo ﬁlename and in the\\ntext ﬁle of descriptions. Next, we will step through the list of photo descriptions. Below deﬁnes\\na function load descriptions() that, given the loaded document text, will return a dictionary\\nof photo identiﬁers to descriptions. Each photo identiﬁer maps to a list of one or more textual\\ndescriptions.\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# create the list if needed\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 320}, page_content=\"26.4. Prepare Text Data 304\\nif image_id not in mapping:\\nmapping[image_id] = list()\\n# store description\\nmapping[image_id].append(image_desc)\\nreturn mapping\\n# parse descriptions\\ndescriptions = load_descriptions(doc)\\nprint( 'Loaded: %d '% len(descriptions))\\nListing 26.4: Example of splitting descriptions from photo identiﬁers\\nNext, we need to clean the description text. The descriptions are already tokenized and easy\\nto work with. We will clean the text in the following ways in order to reduce the size of the\\nvocabulary of words we will need to work with:\\n\\x88Convert all words to lowercase.\\n\\x88Remove all punctuation.\\n\\x88Remove all words that are one character or less in length (e.g. ‘a’).\\n\\x88Remove all words with numbers in them.\\nBelow deﬁnes the clean descriptions() function that, given the dictionary of image\\nidentiﬁers to descriptions, steps through each description and cleans the text.\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 320}, page_content=\"re_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor key, desc_list in descriptions.items():\\nfor i in range(len(desc_list)):\\ndesc = desc_list[i]\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\\ndesc = [word.lower() for word in desc]\\n# remove punctuation from each token\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\\n# remove tokens with numbers in them\\ndesc = [word for word in desc if word.isalpha()]\\n# store as string\\ndesc_list[i] = ' '.join(desc)\\n# clean descriptions\\nclean_descriptions(descriptions)\\nListing 26.5: Example of cleaning description text\\nOnce cleaned, we can summarize the size of the vocabulary. Ideally, we want a vocabulary\\nthat is both expressive and as small as possible. A smaller vocabulary will result in a smaller\\nmodel that will train faster. For reference, we can transform the clean descriptions into a set\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 320}, page_content='and print its size to get an idea of the size of our dataset vocabulary.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 321}, page_content=\"26.4. Prepare Text Data 305\\n# convert the loaded descriptions into a vocabulary of words\\ndef to_vocabulary(descriptions):\\n# build a list of all description strings\\nall_desc = set()\\nfor key in descriptions.keys():\\n[all_desc.update(d.split()) for d in descriptions[key]]\\nreturn all_desc\\n# summarize vocabulary\\nvocabulary = to_vocabulary(descriptions)\\nprint( 'Vocabulary Size: %d '% len(vocabulary))\\nListing 26.6: Example of deﬁning the description text vocabulary\\nFinally, we can save the dictionary of image identiﬁers and descriptions to a new ﬁle\\nnamed descriptions.txt , with one image identiﬁer and description per line. Below deﬁnes the\\nsave doc() function that, given a dictionary containing the mapping of identiﬁers to descriptions\\nand a ﬁlename, saves the mapping to ﬁle.\\n# save descriptions to file, one per line\\ndef save_descriptions(descriptions, filename):\\nlines = list()\\nfor key, desc_list in descriptions.items():\\nfor desc in desc_list:\\nlines.append(key + ' ' + desc)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 321}, page_content=\"data = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\n# save descriptions\\nsave_doc(descriptions, 'descriptions.txt ')\\nListing 26.7: Example of saving clean descriptions to ﬁle\\nPutting this all together, the complete listing is provided below.\\nimport string\\nimport re\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# extract descriptions for images\\ndef load_descriptions(doc):\\nmapping = dict()\\n# process lines\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 322}, page_content=\"26.4. Prepare Text Data 306\\ntokens = line.split()\\nif len(line) < 2:\\ncontinue\\n# take the first token as the image id, the rest as the description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# remove filename from image id\\nimage_id = image_id.split( '. ')[0]\\n# convert description tokens back to string\\nimage_desc = ' '.join(image_desc)\\n# create the list if needed\\nif image_id not in mapping:\\nmapping[image_id] = list()\\n# store description\\nmapping[image_id].append(image_desc)\\nreturn mapping\\ndef clean_descriptions(descriptions):\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nfor _, desc_list in descriptions.items():\\nfor i in range(len(desc_list)):\\ndesc = desc_list[i]\\n# tokenize\\ndesc = desc.split()\\n# convert to lower case\\ndesc = [word.lower() for word in desc]\\n# remove punctuation from each token\\ndesc = [re_punc.sub( '', w) for w in desc]\\n# remove hanging 's 'and 'a '\\ndesc = [word for word in desc if len(word)>1]\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 322}, page_content=\"# remove tokens with numbers in them\\ndesc = [word for word in desc if word.isalpha()]\\n# store as string\\ndesc_list[i] = ' '.join(desc)\\n# convert the loaded descriptions into a vocabulary of words\\ndef to_vocabulary(descriptions):\\n# build a list of all description strings\\nall_desc = set()\\nfor key in descriptions.keys():\\n[all_desc.update(d.split()) for d in descriptions[key]]\\nreturn all_desc\\n# save descriptions to file, one per line\\ndef save_descriptions(descriptions, filename):\\nlines = list()\\nfor key, desc_list in descriptions.items():\\nfor desc in desc_list:\\nlines.append(key + ' ' + desc)\\ndata = '\\\\n '.join(lines)\\nfile = open(filename, 'w ')\\nfile.write(data)\\nfile.close()\\nfilename = 'Flickr8k_text/Flickr8k.token.txt '\\n# load descriptions\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 323}, page_content=\"26.5. Develop Deep Learning Model 307\\ndoc = load_doc(filename)\\n# parse descriptions\\ndescriptions = load_descriptions(doc)\\nprint( 'Loaded: %d '% len(descriptions))\\n# clean descriptions\\nclean_descriptions(descriptions)\\n# summarize vocabulary\\nvocabulary = to_vocabulary(descriptions)\\nprint( 'Vocabulary Size: %d '% len(vocabulary))\\n# save to file\\nsave_descriptions(descriptions, 'descriptions.txt ')\\nListing 26.8: Complete example of text data preparation.\\nRunning the example ﬁrst prints the number of loaded photo descriptions (8,092) and the\\nsize of the clean vocabulary (8,763 words).\\nLoaded: 8,092\\nVocabulary Size: 8,763\\nListing 26.9: Example output from preparing the text data\\nFinally, the clean descriptions are written to descriptions.txt . Taking a look at the ﬁle,\\nwe can see that the descriptions are ready for modeling. The order of descriptions in your ﬁle\\nmay vary.\\n2252123185_487f21e336 bunch on people are seated in stadium\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 323}, page_content='2252123185_487f21e336 crowded stadium is full of people watching an event\\n2252123185_487f21e336 crowd of people fill up packed stadium\\n2252123185_487f21e336 crowd sitting in an indoor stadium\\n2252123185_487f21e336 stadium full of people watch game\\n...\\nListing 26.10: Sample of text from the clean photo descriptions\\n26.5 Develop Deep Learning Model\\nIn this section, we will deﬁne the deep learning model and ﬁt it on the training dataset. This\\nsection is divided into the following parts:\\n1. Loading Data.\\n2. Deﬁning the Model.\\n3. Fitting the Model.\\n4. Complete Example.\\n26.5.1 Loading Data\\nFirst, we must load the prepared photo and text data so that we can use it to ﬁt the model.\\nWe are going to train the data on all of the photos and captions in the training dataset. While'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 324}, page_content=\"26.5. Develop Deep Learning Model 308\\ntraining, we are going to monitor the performance of the model on the development dataset and\\nuse that performance to decide when to save models to ﬁle.\\nThe train and development dataset have been predeﬁned in the Flickr 8k.trainImages.txt\\nand Flickr 8k.devImages.txt ﬁles respectively, that both contain lists of photo ﬁle names.\\nFrom these ﬁle names, we can extract the photo identiﬁers and use these identiﬁers to ﬁlter\\nphotos and descriptions for each set. The function load set() below will load a pre-deﬁned set\\nof identiﬁers given the train or development sets ﬁlename.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 324}, page_content=\"if len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\nListing 26.11: Functions for loading the photo description text and identiﬁers\\nNow, we can load the photos and descriptions using the pre-deﬁned set of train or development\\nidentiﬁers. Below is the function load clean descriptions() that loads the cleaned text\\ndescriptions from descriptions.txt for a given set of identiﬁers and returns a dictionary of\\nidentiﬁers to lists of text descriptions.\\nThe model we will develop will generate a caption given a photo, and the caption will be\\ngenerated one word at a time. The sequence of previously generated words will be provided as\\ninput. Therefore, we will need a ﬁrst word to kick-oﬀ the generation process and a last word to\\nsignal the end of the caption. We will use the strings startseq and endseq for this purpose.\\nThese tokens are added to the loaded descriptions as they are loaded. It is important to do this\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 324}, page_content=\"now before we encode the text so that the tokens are also encoded correctly.\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 325}, page_content=\"26.5. Develop Deep Learning Model 309\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\nListing 26.12: Function for loading the clean photo descriptions\\nNext, we can load the photo features for a given dataset. Below deﬁnes a function named\\nload photo features() that loads the entire set of photo descriptions, then returns the subset\\nof interest for a given set of photo identiﬁers. This is not very eﬃcient; nevertheless, this will\\nget us up and running quickly.\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\\n# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 325}, page_content=\"return features\\nListing 26.13: Function for loading pre-calculated photo features\\nWe can pause here and test everything developed so far. The complete code example is\\nlisted below.\\nfrom pickle import load\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 326}, page_content=\"26.5. Develop Deep Learning Model 310\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\\n# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\\n# load training dataset (6K)\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\\ntrain = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 326}, page_content=\"# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# photo features\\ntrain_features = load_photo_features( 'features.pkl ', train)\\nprint( 'Photos: train=%d '% len(train_features))\\nListing 26.14: Complete example of loading the prepared data.\\nRunning this example ﬁrst loads the 6,000 photo identiﬁers in the test dataset. These\\nfeatures are then used to ﬁlter and load the cleaned description text and the pre-computed\\nphoto features. We are nearly there.\\nDataset: 6,000\\nDescriptions: train=6,000\\nPhotos: train=6,000\\nListing 26.15: Example output from preparing the text data\\nThe description text will need to be encoded to numbers before it can be presented to\\nthe model as in input or compared to the model’s predictions. The ﬁrst step in encoding the\\ndata is to create a consistent mapping from words to unique integer values. Keras provides\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 327}, page_content=\"26.5. Develop Deep Learning Model 311\\ntheTokenizer class that can learn this mapping from the loaded description data. Below\\ndeﬁnes the tolines() to convert the dictionary of descriptions into a list of strings and the\\ncreate tokenizer() function that will ﬁt a Tokenizer given the loaded photo description text.\\n# convert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\nListing 26.16: Example of preparing the Tokenizer\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 327}, page_content='We can now encode the text. Each description will be split into words. The model will be\\nprovided one word and the photo and generate the next word. Then the ﬁrst two words of the\\ndescription will be provided to the model as input with the image to generate the next word.\\nThis is how the model will be trained. For example, the input sequence “ little girl running in\\nﬁeld” would be split into 6 input-output pairs to train the model:\\nX1, X2 (text sequence), y (word)\\nphoto startseq, little\\nphoto startseq, little, girl\\nphoto startseq, little, girl, running\\nphoto startseq, little, girl, running, in\\nphoto startseq, little, girl, running, in, field\\nphoto startseq, little, girl, running, in, field, endseq\\nListing 26.17: Example of how a photo description is transformed into input and output\\nsequences\\nLater, when the model is used to generate descriptions, the generated words will be con-\\ncatenated and recursively provided as input to generate a caption for an image. The function'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 327}, page_content='below named create sequences() , given the tokenizer, a maximum sequence length, and the\\ndictionary of all descriptions and photos, will transform the data into input-output pairs of data\\nfor training the model. There are two input arrays to the model: one for photo features and\\none for the encoded text. There is one output for the model which is the encoded next word in\\nthe text sequence.\\nThe input text is encoded as integers, which will be fed to a word embedding layer. The\\nphoto features will be fed directly to another part of the model. The model will output a\\nprediction, which will be a probability distribution over all words in the vocabulary. The\\noutput data will therefore be a one hot encoded version of each word, representing an idealized'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 328}, page_content='26.5. Develop Deep Learning Model 312\\nprobability distribution with 0 values at all word positions except the actual word position,\\nwhich has a value of 1.\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, descriptions, photos):\\nX1, X2, y = list(), list(), list()\\n# walk through each image identifier\\nfor key, desc_list in descriptions.items():\\n# walk through each description for the image\\nfor desc in desc_list:\\n# encode the sequence\\nseq = tokenizer.texts_to_sequences([desc])[0]\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nX1.append(photos[key][0])\\nX2.append(in_seq)\\ny.append(out_seq)\\nreturn array(X1), array(X2), array(y)'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 328}, page_content='Listing 26.18: Function for creating input and output sequences\\nWe will need to calculate the maximum number of words in the longest description. A short\\nhelper function named maxlength() is deﬁned below.\\n# calculate the length of the description with the most words\\ndef max_length(descriptions):\\nlines = to_lines(descriptions)\\nreturn max(len(d.split()) for d in lines)\\nListing 26.19: Function for calculating the maximum sequence length.\\nWe now have enough to load the data for the training and development datasets and\\ntransform the loaded data into input-output pairs for ﬁtting a deep learning model.\\n26.5.2 Deﬁning the Model\\nWe will deﬁne a deep learning based on the merge-model described by Marc Tanti, et al. in\\ntheir 2017 papers. Note, the merge model for image captioning was introduced in Chapter 22.\\nWe will describe the model in three parts:\\n\\x88Photo Feature Extractor . This is a 16-layer VGG model pre-trained on the ImageNet'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 328}, page_content='dataset. We have pre-processed the photos with the VGG model (without the output\\nlayer) and will use the extracted features predicted by this model as input.\\n\\x88Sequence Processor . This is a word embedding layer for handling the text input,\\nfollowed by a Long Short-Term Memory (LSTM) recurrent neural network layer.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 329}, page_content='26.5. Develop Deep Learning Model 313\\n\\x88Decoder (for lack of a better name). Both the feature extractor and sequence processor\\noutput a ﬁxed-length vector. These are merged together and processed by a Dense layer\\nto make a ﬁnal prediction.\\nThe Photo Feature Extractor model expects input photo features to be a vector of 4,096\\nelements. These are processed by a Dense layer to produce a 256 element representation of the\\nphoto. The Sequence Processor model expects input sequences with a pre-deﬁned length (34\\nwords) which are fed into an Embedding layer that uses a mask to ignore padded values. This is\\nfollowed by an LSTM layer with 256 memory units.\\nBoth the input models produce a 256 element vector. Further, both input models use\\nregularization in the form of 50% dropout. This is to reduce overﬁtting the training dataset, as\\nthis model conﬁguration learns very fast. The Decoder model merges the vectors from both'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 329}, page_content=\"input models using an addition operation. This is then fed to a Dense 256 neuron layer and then\\nto a ﬁnal output Dense layer that makes a softmax prediction over the entire output vocabulary\\nfor the next word in the sequence. The function below named define model() deﬁnes and\\nreturns the model ready to be ﬁt.\\n# define the captioning model\\ndef define_model(vocab_size, max_length):\\n# feature extractor model\\ninputs1 = Input(shape=(4096,))\\nfe1 = Dropout(0.5)(inputs1)\\nfe2 = Dense(256, activation= 'relu ')(fe1)\\n# sequence model\\ninputs2 = Input(shape=(max_length,))\\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\\nse2 = Dropout(0.5)(se1)\\nse3 = LSTM(256)(se2)\\n# decoder model\\ndecoder1 = add([fe2, se3])\\ndecoder2 = Dense(256, activation= 'relu ')(decoder1)\\noutputs = Dense(vocab_size, activation= 'softmax ')(decoder2)\\n# tie it together [image, seq] [word]\\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\\n# compile model\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 329}, page_content=\"# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ')\\n# summarize model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 26.20: Function to deﬁne the caption generation model.\\nA plot of the model is created and helps to better understand the structure of the network\\nand the two streams of input.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 330}, page_content=\"26.5. Develop Deep Learning Model 314\\nFigure 26.1: Plot of the deﬁned caption generation model.\\n26.5.3 Fitting the Model\\nNow that we know how to deﬁne the model, we can ﬁt it on the training dataset. The model\\nlearns fast and quickly overﬁts the training dataset. For this reason, we will monitor the skill\\nof the trained model on the holdout development dataset. When the skill of the model on the\\ndevelopment dataset improves at the end of an epoch, we will save the whole model to ﬁle.\\nAt the end of the run, we can then use the saved model with the best skill on the training\\ndataset as our ﬁnal model. We can do this by deﬁning a ModelCheckpoint in Keras and\\nspecifying it to monitor the minimum loss on the validation dataset and save the model to a ﬁle\\nthat has both the training and validation loss in the ﬁlename.\\n# define checkpoint callback\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 330}, page_content='Listing 26.21: Example of checkpoint conﬁguration.\\nWe can then specify the checkpoint in the call to fit() via the callbacks argument. We\\nmust also specify the development dataset in fit() via the validation data argument. We\\nwill only ﬁt the model for 20 epochs, but given the amount of training data, each epoch may\\ntake 30 minutes on modern hardware.\\n# fit model\\nmodel.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint],\\nvalidation_data=([X1test, X2test], ytest))\\nListing 26.22: Example of ﬁtting the caption generation model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 331}, page_content=\"26.5. Develop Deep Learning Model 315\\n26.5.4 Complete Example\\nThe complete example for ﬁtting the model on the training data is listed below. Note, running\\nthis example may require a machine with 8 or more Gigabytes of RAM. See the appendix for\\nusing AWS, if needed.\\nfrom numpy import array\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\nfrom keras.utils import plot_model\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers import LSTM\\nfrom keras.layers import Embedding\\nfrom keras.layers import Dropout\\nfrom keras.layers.merge import add\\nfrom keras.callbacks import ModelCheckpoint\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 331}, page_content=\"def load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 332}, page_content=\"26.5. Develop Deep Learning Model 316\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\\n# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\\n# covert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the length of the description with the most words\\ndef max_length(descriptions):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 332}, page_content='lines = to_lines(descriptions)\\nreturn max(len(d.split()) for d in lines)\\n# create sequences of images, input sequences and output words for an image\\ndef create_sequences(tokenizer, max_length, descriptions, photos):\\nX1, X2, y = list(), list(), list()\\n# walk through each image identifier\\nfor key, desc_list in descriptions.items():\\n# walk through each description for the image\\nfor desc in desc_list:\\n# encode the sequence\\nseq = tokenizer.texts_to_sequences([desc])[0]\\n# split one sequence into multiple X,y pairs\\nfor i in range(1, len(seq)):\\n# split into input and output pair\\nin_seq, out_seq = seq[:i], seq[i]\\n# pad input sequence\\nin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\\n# encode output sequence\\nout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\\n# store\\nX1.append(photos[key][0])'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 333}, page_content=\"26.5. Develop Deep Learning Model 317\\nX2.append(in_seq)\\ny.append(out_seq)\\nreturn array(X1), array(X2), array(y)\\n# define the captioning model\\ndef define_model(vocab_size, max_length):\\n# feature extractor model\\ninputs1 = Input(shape=(4096,))\\nfe1 = Dropout(0.5)(inputs1)\\nfe2 = Dense(256, activation= 'relu ')(fe1)\\n# sequence model\\ninputs2 = Input(shape=(max_length,))\\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\\nse2 = Dropout(0.5)(se1)\\nse3 = LSTM(256)(se2)\\n# decoder model\\ndecoder1 = add([fe2, se3])\\ndecoder2 = Dense(256, activation= 'relu ')(decoder1)\\noutputs = Dense(vocab_size, activation= 'softmax ')(decoder2)\\n# tie it together [image, seq] [word]\\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\\n# compile model\\nmodel.compile(loss= 'categorical_crossentropy ', optimizer= 'adam ')\\n# summarize model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load training dataset (6K)\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 333}, page_content=\"train = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# photo features\\ntrain_features = load_photo_features( 'features.pkl ', train)\\nprint( 'Photos: train=%d '% len(train_features))\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\nvocab_size = len(tokenizer.word_index) + 1\\nprint( 'Vocabulary Size: %d '% vocab_size)\\n# determine the maximum sequence length\\nmax_length = max_length(train_descriptions)\\nprint( 'Description Length: %d '% max_length)\\n# prepare sequences\\nX1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions,\\ntrain_features)\\n# load test set\\nfilename = 'Flickr8k_text/Flickr_8k.devImages.txt '\\ntest = load_set(filename)\\nprint( 'Dataset: %d '% len(test))\\n# descriptions\\ntest_descriptions = load_clean_descriptions( 'descriptions.txt ', test)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 333}, page_content=\"print( 'Descriptions: test=%d '% len(test_descriptions))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 334}, page_content=\"26.6. Evaluate Model 318\\n# photo features\\ntest_features = load_photo_features( 'features.pkl ', test)\\nprint( 'Photos: test=%d '% len(test_features))\\n# prepare sequences\\nX1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions,\\ntest_features)\\n# define the model\\nmodel = define_model(vocab_size, max_length)\\n# define checkpoint callback\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\\n# fit model\\nmodel.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint],\\nvalidation_data=([X1test, X2test], ytest))\\nListing 26.23: Complete example of training the caption generation model.\\nRunning the example ﬁrst prints a summary of the loaded training and development datasets.\\nDataset: 6,000\\nDescriptions: train=6,000\\nPhotos: train=6,000\\nVocabulary Size: 7,579\\nDescription Length: 34\\nDataset: 1,000\\nDescriptions: test=1,000\\nPhotos: test=1,000\\nTrain on 306,404 samples, validate on 50,903 samples\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 334}, page_content='Listing 26.24: Sample output from ﬁtting the caption generation model\\nAfter the summary of the model, we can get an idea of the total number of training and\\nvalidation (development) input-output pairs. The model then runs, saving the best model to\\n.h5ﬁles along the way. Note, that even on a modern CPU, each epoch may take 20 minutes.\\nYou may want to consider running the example on a GPU, such as on AWS. See the appendix\\nfor details on how to set this up. When I ran the example, the best model was saved at the end\\nof epoch 2 with a loss of 3.245 on the training dataset and a loss of 3.612 on the development\\ndataset.\\n26.6 Evaluate Model\\nOnce the model is ﬁt, we can evaluate the skill of its predictions on the holdout test dataset.\\nWe will evaluate a model by generating descriptions for all photos in the test dataset and\\nevaluating those predictions with a standard cost function. First, we need to be able to generate\\na description for a photo using a trained model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 334}, page_content='This involves passing in the start description token startseq , generating one word, then\\ncalling the model recursively with generated words as input until the end of sequence token\\nis reached endseq or the maximum description length is reached. The function below named\\ngenerate desc() implements this behavior and generates a textual description given a trained\\nmodel, and a given prepared photo as input. It calls the function word forid() in order to\\nmap an integer prediction back to a word.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 335}, page_content=\"26.6. Evaluate Model 319\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# generate a description for an image\\ndef generate_desc(model, tokenizer, photo, max_length):\\n# seed the generation process\\nin_text = 'startseq '\\n# iterate over the whole length of the sequence\\nfor i in range(max_length):\\n# integer encode input sequence\\nsequence = tokenizer.texts_to_sequences([in_text])[0]\\n# pad input\\nsequence = pad_sequences([sequence], maxlen=max_length)\\n# predict next word\\nyhat = model.predict([photo,sequence], verbose=0)\\n# convert probability to integer\\nyhat = argmax(yhat)\\n# map integer to word\\nword = word_for_id(yhat, tokenizer)\\n# stop if we cannot map the word\\nif word is None:\\nbreak\\n# append as input for generating the next word\\nin_text += ' ' + word\\n# stop if we predict the end of the sequence\\nif word == 'endseq ':\\nbreak\\nreturn in_text\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 335}, page_content=\"return in_text\\nListing 26.25: Functions for generating a description for a photo.\\nWhen generating and comparing photo descriptions, we will need to strip oﬀ the special\\nstart and end of sequence words. The function below named cleanup summary() will perform\\nthis operation.\\n# remove start/end sequence tokens from a summary\\ndef cleanup_summary(summary):\\n# remove start of sequence token\\nindex = summary.find( 'startseq ')\\nif index > -1:\\nsummary = summary[len( 'startseq '):]\\n# remove end of sequence token\\nindex = summary.find( 'endseq ')\\nif index > -1:\\nsummary = summary[:index]\\nreturn summary\\nListing 26.26: Functions to remove start and end of sequence words.\\nWe will generate predictions for all photos in the test dataset. The function below named\\nevaluate model() will evaluate a trained model against a given dataset of photo descriptions\\nand photo features. The actual and predicted descriptions are collected and evaluated collectively\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 336}, page_content=\"26.6. Evaluate Model 320\\nusing the corpus BLEU score that summarizes how close the generated text is to the expected\\ntext.\\n# evaluate the skill of the model\\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\\nactual, predicted = list(), list()\\n# step over the whole set\\nfor key, desc_list in descriptions.items():\\n# generate description\\nyhat = generate_desc(model, tokenizer, photos[key], max_length)\\n# clean up prediction\\nyhat = cleanup_summary(yhat)\\n# store actual and predicted\\nreferences = [cleanup_summary(d).split() for d in desc_list]\\nactual.append(references)\\npredicted.append(yhat.split())\\n# calculate BLEU score\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\\nprint( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 336}, page_content='Listing 26.27: Functions for evaluating a caption generation model.\\nBLEU scores are used in text translation for evaluating translated text against one or\\nmore reference translations. Here, we compare each generated description against all of the\\nreference descriptions for the photograph. We then calculate BLEU scores for 1, 2, 3 and 4\\ncumulative n-grams. The NLTK Python library implements the BLEU score calculation in the\\ncorpus bleu() function. A higher score close to 1.0 is better, a score closer to zero is worse.\\nNote that the BLEU score and NLTK API were introduced in Chapter 24.\\nWe can put all of this together with the functions from the previous section for loading the\\ndata. We ﬁrst need to load the training dataset in order to prepare a Tokenizer so that we\\ncan encode generated words as input sequences for the model. It is critical that we encode the\\ngenerated words using exactly the same encoding scheme as was used when training the model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 336}, page_content=\"We then use these functions for loading the test dataset. The complete example is listed below.\\nfrom numpy import argmax\\nfrom pickle import load\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\\nfrom nltk.translate.bleu_score import corpus_bleu\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 337}, page_content=\"26.6. Evaluate Model 321\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# load photo features\\ndef load_photo_features(filename, dataset):\\n# load all features\\nall_features = load(open(filename, 'rb '))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 337}, page_content='# filter features\\nfeatures = {k: all_features[k] for k in dataset}\\nreturn features\\n# covert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# calculate the length of the description with the most words'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 338}, page_content=\"26.6. Evaluate Model 322\\ndef max_length(descriptions):\\nlines = to_lines(descriptions)\\nreturn max(len(d.split()) for d in lines)\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# generate a description for an image\\ndef generate_desc(model, tokenizer, photo, max_length):\\n# seed the generation process\\nin_text = 'startseq '\\n# iterate over the whole length of the sequence\\nfor _ in range(max_length):\\n# integer encode input sequence\\nsequence = tokenizer.texts_to_sequences([in_text])[0]\\n# pad input\\nsequence = pad_sequences([sequence], maxlen=max_length)\\n# predict next word\\nyhat = model.predict([photo,sequence], verbose=0)\\n# convert probability to integer\\nyhat = argmax(yhat)\\n# map integer to word\\nword = word_for_id(yhat, tokenizer)\\n# stop if we cannot map the word\\nif word is None:\\nbreak\\n# append as input for generating the next word\\nin_text += ' ' + word\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 338}, page_content=\"# stop if we predict the end of the sequence\\nif word == 'endseq ':\\nbreak\\nreturn in_text\\n# remove start/end sequence tokens from a summary\\ndef cleanup_summary(summary):\\n# remove start of sequence token\\nindex = summary.find( 'startseq ')\\nif index > -1:\\nsummary = summary[len( 'startseq '):]\\n# remove end of sequence token\\nindex = summary.find( 'endseq ')\\nif index > -1:\\nsummary = summary[:index]\\nreturn summary\\n# evaluate the skill of the model\\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\\nactual, predicted = list(), list()\\n# step over the whole set\\nfor key, desc_list in descriptions.items():\\n# generate description\\nyhat = generate_desc(model, tokenizer, photos[key], max_length)\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 339}, page_content=\"26.6. Evaluate Model 323\\n# clean up prediction\\nyhat = cleanup_summary(yhat)\\n# store actual and predicted\\nreferences = [cleanup_summary(d).split() for d in desc_list]\\nactual.append(references)\\npredicted.append(yhat.split())\\n# calculate BLEU score\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\\nprint( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\\n# load training dataset (6K)\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\\ntrain = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\nvocab_size = len(tokenizer.word_index) + 1\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 339}, page_content=\"print( 'Vocabulary Size: %d '% vocab_size)\\n# determine the maximum sequence length\\nmax_length = max_length(train_descriptions)\\nprint( 'Description Length: %d '% max_length)\\n# load test set\\nfilename = 'Flickr8k_text/Flickr_8k.testImages.txt '\\ntest = load_set(filename)\\nprint( 'Dataset: %d '% len(test))\\n# descriptions\\ntest_descriptions = load_clean_descriptions( 'descriptions.txt ', test)\\nprint( 'Descriptions: test=%d '% len(test_descriptions))\\n# photo features\\ntest_features = load_photo_features( 'features.pkl ', test)\\nprint( 'Photos: test=%d '% len(test_features))\\n# load the model\\nfilename = 'model.h5 '\\nmodel = load_model(filename)\\n# evaluate model\\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\\nListing 26.28: Complete example of evaluating the caption generation model.\\nRunning the example prints the BLEU scores. We can see that the scores ﬁt within the\\nexpected range of a skillful model on the problem. The chosen model conﬁguration is by no\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 339}, page_content='means optimized.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nBLEU-1: 0.438805\\nBLEU-2: 0.230646'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 340}, page_content='26.7. Generate New Captions 324\\nBLEU-3: 0.150245\\nBLEU-4: 0.062847\\nListing 26.29: Sample output from evaluating the caption generation model\\n26.7 Generate New Captions\\nNow that we know how to develop and evaluate a caption generation model, how can we use it?\\nAlmost everything we need to generate captions for entirely new photographs is in the model\\nﬁle. We also need the Tokenizer for encoding generated words for the model while generating\\na sequence, and the maximum length of input sequences, used when we deﬁned the model (e.g.\\n34).\\nWe can hard code the maximum sequence length. With the encoding of text, we can create\\nthe tokenizer and save it to a ﬁle so that we can load it quickly whenever we need it without\\nneeding the entire Flickr8K dataset. An alternative would be to use our own vocabulary ﬁle\\nand mapping to integers function during training. We can create the Tokenizer as before and\\nsave it as a pickle ﬁle tokenizer.pkl . The complete example is listed below.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 340}, page_content=\"from keras.preprocessing.text import Tokenizer\\nfrom pickle import dump\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, 'r ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# load a pre-defined list of photo identifiers\\ndef load_set(filename):\\ndoc = load_doc(filename)\\ndataset = list()\\n# process line by line\\nfor line in doc.split( '\\\\n '):\\n# skip empty lines\\nif len(line) < 1:\\ncontinue\\n# get the image identifier\\nidentifier = line.split( '. ')[0]\\ndataset.append(identifier)\\nreturn set(dataset)\\n# load clean descriptions into memory\\ndef load_clean_descriptions(filename, dataset):\\n# load document\\ndoc = load_doc(filename)\\ndescriptions = dict()\\nfor line in doc.split( '\\\\n '):\\n# split line by white space\\ntokens = line.split()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 341}, page_content=\"26.7. Generate New Captions 325\\n# split id from description\\nimage_id, image_desc = tokens[0], tokens[1:]\\n# skip images not in the set\\nif image_id in dataset:\\n# create list\\nif image_id not in descriptions:\\ndescriptions[image_id] = list()\\n# wrap description in tokens\\ndesc = 'startseq '+ ' '.join(image_desc) + 'endseq '\\n# store\\ndescriptions[image_id].append(desc)\\nreturn descriptions\\n# covert a dictionary of clean descriptions to a list of descriptions\\ndef to_lines(descriptions):\\nall_desc = list()\\nfor key in descriptions.keys():\\n[all_desc.append(d) for d in descriptions[key]]\\nreturn all_desc\\n# fit a tokenizer given caption descriptions\\ndef create_tokenizer(descriptions):\\nlines = to_lines(descriptions)\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# load training dataset\\nfilename = 'Flickr8k_text/Flickr_8k.trainImages.txt '\\ntrain = load_set(filename)\\nprint( 'Dataset: %d '% len(train))\\n# descriptions\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 341}, page_content=\"# descriptions\\ntrain_descriptions = load_clean_descriptions( 'descriptions.txt ', train)\\nprint( 'Descriptions: train=%d '% len(train_descriptions))\\n# prepare tokenizer\\ntokenizer = create_tokenizer(train_descriptions)\\n# save the tokenizer\\ndump(tokenizer, open( 'tokenizer.pkl ', 'wb '))\\nListing 26.30: Complete example of preparing and saving the Tokenizer .\\nWe can now load the tokenizer whenever we need it without having to load the entire training\\ndataset of annotations. Now, let’s generate a description for a new photograph. Below is a new\\nphotograph that I chose randomly on Flickr (available under a permissive license)1.\\n1https://www.flickr.com/photos/bambe1964/7837618434/\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 342}, page_content=\"26.7. Generate New Captions 326\\nFigure 26.2: Photo of a dog at the beach. Photo by bambe1964 , some rights reserved.\\nWe will generate a description for it using our model. Download the photograph and save\\nit to your local directory with the ﬁlename example.jpg . First, we must load the Tokenizer\\nfrom tokenizer.pkl and deﬁne the maximum length of the sequence to generate, needed for\\npadding inputs.\\n# load the tokenizer\\ntokenizer = load(open( 'tokenizer.pkl ', 'rb '))\\n# pre-define the max sequence length (from training)\\nmax_length = 34\\nListing 26.31: Example of loading the saved Tokenizer\\nThen we must load the model, as before.\\n# load the model\\nmodel = load_model( 'model.h5 ')\\nListing 26.32: Example of loading the saved model\\nNext, we must load the photo we wish to describe and extract the features. We could do this\\nby re-deﬁning the model and adding the VGG-16 model to it, or we can use the VGG model to\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 342}, page_content='predict the features and use them as inputs to our existing model. We will do the latter and\\nuse a modiﬁed version of the extract features() function used during data preparation, but\\nadapted to work on a single photo.\\n# extract features from each photo in the directory\\ndef extract_features(filename):\\n# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# load the photo\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a NumPy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 343}, page_content=\"26.7. Generate New Captions 327\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\nreturn feature\\n# load and prepare the photograph\\nphoto = extract_features( 'example.jpg ')\\nListing 26.33: Example of extracting features for the provided photo.\\nWe can then generate a description using the generate desc() function deﬁned when\\nevaluating the model. The complete example for generating a description for an entirely new\\nstandalone photograph is listed below.\\nfrom pickle import load\\nfrom numpy import argmax\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.preprocessing.image import load_img\\nfrom keras.preprocessing.image import img_to_array\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.models import Model\\nfrom keras.models import load_model\\n# extract features from each photo in the directory\\ndef extract_features(filename):\\n# load the model\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 343}, page_content=\"# load the model\\nmodel = VGG16()\\n# re-structure the model\\nmodel.layers.pop()\\nmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\\n# load the photo\\nimage = load_img(filename, target_size=(224, 224))\\n# convert the image pixels to a numpy array\\nimage = img_to_array(image)\\n# reshape data for the model\\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\\n# prepare the image for the VGG model\\nimage = preprocess_input(image)\\n# get features\\nfeature = model.predict(image, verbose=0)\\nreturn feature\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# remove start/end sequence tokens from a summary\\ndef cleanup_summary(summary):\\n# remove start of sequence token\\nindex = summary.find( 'startseq ')\\nif index > -1:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 344}, page_content=\"26.7. Generate New Captions 328\\nsummary = summary[len( 'startseq '):]\\n# remove end of sequence token\\nindex = summary.find( 'endseq ')\\nif index > -1:\\nsummary = summary[:index]\\nreturn summary\\n# generate a description for an image\\ndef generate_desc(model, tokenizer, photo, max_length):\\n# seed the generation process\\nin_text = 'startseq '\\n# iterate over the whole length of the sequence\\nfor _ in range(max_length):\\n# integer encode input sequence\\nsequence = tokenizer.texts_to_sequences([in_text])[0]\\n# pad input\\nsequence = pad_sequences([sequence], maxlen=max_length)\\n# predict next word\\nyhat = model.predict([photo,sequence], verbose=0)\\n# convert probability to integer\\nyhat = argmax(yhat)\\n# map integer to word\\nword = word_for_id(yhat, tokenizer)\\n# stop if we cannot map the word\\nif word is None:\\nbreak\\n# append as input for generating the next word\\nin_text += ' ' + word\\n# stop if we predict the end of the sequence\\nif word == 'endseq ':\\nbreak\\nreturn in_text\\n# load the tokenizer\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 344}, page_content=\"tokenizer = load(open( 'tokenizer.pkl ', 'rb '))\\n# pre-define the max sequence length (from training)\\nmax_length = 34\\n# load the model\\nmodel = load_model( 'model.h5 ')\\n# load and prepare the photograph\\nphoto = extract_features( 'example.jpg ')\\n# generate description\\ndescription = generate_desc(model, tokenizer, photo, max_length)\\ndescription = cleanup_summary(description)\\nprint(description)\\nListing 26.34: Complete example of generating a description for a new photo.\\nIn this case, the description generated was as follows:\\ndog is running across the beach\\nListing 26.35: Sample output from generating a caption for the new photograph\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 345}, page_content='26.8. Extensions 329\\n26.8 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Alternate Pre-Trained Image Models . A small 16-layer VGG model was used for\\nfeature extraction. Consider exploring larger models that oﬀer better performance on the\\nImageNet dataset, such as Inception.\\n\\x88Smaller Vocabulary . A larger vocabulary of nearly eight thousand words was used in\\nthe development of the model. Many of the words supported may be misspellings or only\\nused once in the entire dataset. Reﬁne the vocabulary and reduce the size, perhaps by\\nhalf.\\n\\x88Pre-trained Word Vectors . The model learned the word vectors as part of ﬁtting the\\nmodel. Better performance may be achieved by using word vectors either pre-trained on\\nthe training dataset or trained on a much larger corpus of text, such as news articles or\\nWikipedia.\\n\\x88Train Word2Vec Vectors . Pre-train word vectors using Word2Vec on the description'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 345}, page_content='data and explore models that allow and don’t allow ﬁne tuning of the vectors during\\ntraining, then compare skill.\\n\\x88Tune Model . The conﬁguration of the model was not tuned on the problem. Explore\\nalternate conﬁgurations and see if you can achieve better performance.\\n\\x88Inject Architecture . Explore the inject architecture for caption generation and compare\\nperformance to the merge architecture used in this tutorial.\\n\\x88Alternate Framings . Explore alternate framings of the problems such as generating the\\nentire sequence from the photo alone.\\n\\x88Pre-Train Language Model . Pre-train a language model for generating description\\ntext, then use it in the caption generation model and evaluate the impact on model\\ntraining time and skill.\\n\\x88Truncate Descriptions . Only train the model on description at or below a speciﬁc\\nnumber of words and explore truncating long descriptions to a preferred length. Evaluate\\nthe impact on training time and model skill.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 345}, page_content='\\x88Alternate Measure . Explore alternate performance measures beside BLEU such as\\nROGUE. Compare scores for the same descriptions to develop an intuition for how the\\nmeasures diﬀer in practice.\\nIf you explore any of these extensions, I’d love to know.\\n26.9 Further Reading\\nThis section provides more resources on the topic if you are looking go deeper.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 346}, page_content='26.9. Further Reading 330\\n26.9.1 Caption Generation Papers\\n\\x88Show and Tell: A Neural Image Caption Generator , 2015.\\nhttps://arxiv.org/abs/1411.4555\\n\\x88Show, Attend and Tell: Neural Image Caption Generation with Visual Attention , 2015.\\nhttps://arxiv.org/abs/1502.03044\\n\\x88Where to put the Image in an Image Caption Generator , 2017.\\nhttps://arxiv.org/abs/1703.09137\\n\\x88What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator? ,\\n2017.\\nhttps://arxiv.org/abs/1708.02043\\n\\x88Automatic Description Generation from Images: A Survey of Models, Datasets, and\\nEvaluation Measures , 2016.\\nhttps://arxiv.org/abs/1601.03896\\n26.9.2 Flickr8K Dataset\\n\\x88Framing image description as a ranking task: data, models and evaluation metrics (Home-\\npage).\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.\\nhtml\\n\\x88Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics ,\\n2013.\\nhttps://www.jair.org/media/3994/live-3994-7274-jair.pdf\\n\\x88Dataset Request Form.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 346}, page_content='https://illinois.edu/fb/sec/1713398\\n\\x88Old Flicrk8K Homepage.\\nhttp://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html\\n26.9.3 API\\n\\x88Keras Model API.\\nhttps://keras.io/models/model/\\n\\x88Keras padsequences() API.\\nhttps://keras.io/preprocessing/sequence/#pad_sequences\\n\\x88Keras Tokenizer API.\\nhttps://keras.io/preprocessing/text/#tokenizer\\n\\x88Keras VGG16 API.\\nhttps://keras.io/applications/#vgg16'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 347}, page_content='26.10. Summary 331\\n\\x88Gensim Word2Vec API.\\nhttps://radimrehurek.com/gensim/models/word2vec.html\\n\\x88nltk.translate package API Documentation.\\nhttp://www.nltk.org/api/nltk.translate.html\\n26.10 Summary\\nIn this tutorial, you discovered how to develop a photo captioning deep learning model from\\nscratch. Speciﬁcally, you learned:\\n\\x88How to prepare photo and text data ready for training a deep learning model.\\n\\x88How to design and train a deep learning caption generation model.\\n\\x88How to evaluate a train caption generation model and use it to caption entirely new\\nphotographs.\\n26.10.1 Next\\nThis is the last chapter in the image captioning part. In the next part you will discover how to\\ndevelop neural machine translation models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 348}, page_content='Part IX\\nMachine Translation\\n332'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 349}, page_content='Chapter 27\\nNeural Machine Translation\\nOne of the earliest goals for computers was the automatic translation of text from one language\\nto another. Automatic or machine translation is perhaps one of the most challenging artiﬁcial\\nintelligence tasks given the ﬂuidity of human language. Classically, rule-based systems were\\nused for this task, which were replaced in the 1990s with statistical methods. More recently,\\ndeep neural network models achieve state-of-the-art results in a ﬁeld that is aptly named neural\\nmachine translation. In this chapter, you will discover the challenge of machine translation\\nand the eﬀectiveness of neural machine translation models. After reading this chapter, you will\\nknow:\\n\\x88Machine translation is challenging given the inherent ambiguity and ﬂexibility of human\\nlanguage.\\n\\x88Statistical machine translation replaces classical rule-based systems with models that learn\\nto translate from examples.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 349}, page_content='\\x88Neural machine translation models ﬁt a single model rather than a pipeline of ﬁne-tuned\\nmodels and currently achieve state-of-the-art results.\\nLet’s get started.\\n27.1 What is Machine Translation?\\nMachine translation is the task of automatically converting source text in one language to text\\nin another language.\\nIn a machine translation task, the input already consists of a sequence of symbols\\nin some language, and the computer program must convert this into a sequence of\\nsymbols in another language.\\n— Page 98, Deep Learning , 2016.\\nGiven a sequence of text in a source language, there is no one single best translation of that\\ntext to another language. This is because of the natural ambiguity and ﬂexibility of human\\nlanguage. This makes the challenge of automatic machine translation diﬃcult, perhaps one of\\nthe most diﬃcult in artiﬁcial intelligence:\\n333'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 350}, page_content='27.2. What is Statistical Machine Translation? 334\\nThe fact is that accurate translation requires background knowledge in order to\\nresolve ambiguity and establish the content of the sentence.\\n— Page 21, Artiﬁcial Intelligence, A Modern Approach , 3rd Edition, 2009.\\nClassical machine translation methods often involve rules for converting text in the source\\nlanguage to the target language. The rules are often developed by linguists and may operate at\\nthe lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study:\\nRule-based Machine Translation, or RBMT.\\nRBMT is characterized with the explicit use and manual creation of linguistically\\ninformed rules and representations.\\n— Page 133, Handbook of Natural Language Processing and Machine Translation , 2011.\\nThe key limitations of the classical machine translation approaches are both the expertise\\nrequired to develop the rules, and the vast number of rules and exceptions required.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 350}, page_content='27.2 What is Statistical Machine Translation?\\nStatistical machine translation, or SMT for short, is the use of statistical models that learn to\\ntranslate text from a source language to a target language given a large corpus of examples.\\nThis task of using a statistical model can be stated formally as follows:\\nGiven a sentence T in the target language, we seek the sentence S from which the\\ntranslator produced T. We know that our chance of error is minimized by choosing\\nthat sentence S that is most probable given T. Thus, we wish to choose S so as to\\nmaximize Pr(S|T).\\n—A Statistical Approach to Machine Translation , 1990.\\nThis formal speciﬁcation makes the maximizing of the probability of the output sequence\\ngiven the input sequence of text explicit. It also makes the notion of there being a suite of\\ncandidate translations explicit and the need for a search process or decoder to select the one\\nmost likely translation from the model’s output probability distribution.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 350}, page_content='Given a text in the source language, what is the most probable translation in the\\ntarget language? [...] how should one construct a statistical model that assigns high\\nprobabilities to “good” translations and low probabilities to “bad” translations?\\n— Page xiii, Syntax-based Statistical Machine Translation , 2017.\\nThe approach is data-driven, requiring only a corpus of examples with both source and target\\nlanguage text. This means linguists are not longer required to specify the rules of translation.\\nThis approach does not need a complex ontology of interlingua concepts, nor does it\\nneed handcrafted grammars of the source and target languages, nor a hand-labeled\\ntreebank. All it needs is data-sample translations from which a translation model\\ncan be learned.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 351}, page_content='27.3. What is Neural Machine Translation? 335\\n— Page 909, Artiﬁcial Intelligence, A Modern Approach , 3rd Edition, 2009.\\nQuickly, the statistical approach to machine translation outperformed the classical rule-based\\nmethods to become the de-facto standard set of techniques.\\nSince the inception of the ﬁeld at the end of the 1980s, the most popular models for\\nstatistical machine translation [...] have been sequence-based. In these models, the\\nbasic units of translation are words or sequences of words [...] These kinds of models\\nare simple and eﬀective, and they work well for man language pairs\\n—Syntax-based Statistical Machine Translation , 2017.\\nThe most widely used techniques were phrase-based and focus on translating sub-sequences\\nof the source text piecewise.\\nStatistical Machine Translation (SMT) has been the dominant translation paradigm\\nfor decades. Practical implementations of SMT are generally phrase-based systems'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 351}, page_content='(PBMT) which translate sequences of words or phrases where the lengths may diﬀer\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nAlthough eﬀective, statistical machine translation methods suﬀered from a narrow focus on\\nthe phrases being translated, losing the broader nature of the target text. The hard focus on\\ndata-driven approaches also meant that methods may have ignored important syntax distinctions\\nknown by linguists. Finally, the statistical approaches required careful tuning of each module in\\nthe translation pipeline.\\n27.3 What is Neural Machine Translation?\\nNeural machine translation, or NMT for short, is the use of neural network models to learn\\na statistical model for machine translation. The key beneﬁt to the approach is that a single\\nsystem can be trained directly on source and target text, no longer requiring the pipeline of\\nspecialized systems used in statistical machine learning.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 351}, page_content='Unlike the traditional phrase-based translation system which consists of many small\\nsub-components that are tuned separately, neural machine translation attempts to\\nbuild and train a single, large neural network that reads a sentence and outputs a\\ncorrect translation.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nAs such, neural machine translation systems are said to be end-to-end systems as only one\\nmodel is required for the translation.\\nThe strength of NMT lies in its ability to learn directly, in an end-to-end fashion,\\nthe mapping from input text to associated output text.\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 352}, page_content='27.3. What is Neural Machine Translation? 336\\n27.3.1 Encoder-Decoder Model\\nMultilayer Perceptron neural network models can be used for machine translation, although the\\nmodels are limited by a ﬁxed-length input sequence where the output must be the same length.\\nThese early models have been greatly improved upon recently through the use of recurrent\\nneural networks organized into an encoder-decoder architecture that allow for variable length\\ninput and output sequences.\\nAn encoder neural network reads and encodes a source sentence into a ﬁxed-length\\nvector. A decoder then outputs a translation from the encoded vector. The whole\\nencoder-decoder system, which consists of the encoder and the decoder for a language\\npair, is jointly trained to maximize the probability of a correct translation given a\\nsource sentence.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nKey to the encoder-decoder architecture is the ability of the model to encode the source'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 352}, page_content='text into an internal ﬁxed-length representation called the context vector. Interestingly, once\\nencoded, diﬀerent decoding systems could be used, in principle, to translate the context into\\ndiﬀerent languages.\\n... one model ﬁrst reads the input sequence and emits a data structure that\\nsummarizes the input sequence. We call this summary the “context” C. [...] A\\nsecond mode, usually an RNN, then reads the context C and generates a sentence in\\nthe target language.\\n— Page 461, Deep Learning , 2016.\\n27.3.2 Encoder-Decoders with Attention\\nAlthough eﬀective, the Encoder-Decoder architecture has problems with long sequences of text\\nto be translated. The problem stems from the ﬁxed-length internal representation that must\\nbe used to decode each word in the output sequence. The solution is the use of an attention\\nmechanism that allows the model to learn where to place attention on the input sequence as\\neach word of the output sequence is decoded.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 352}, page_content='Using a ﬁxed-sized representation to capture all the semantic details of a very long\\nsentence [...] is very diﬃcult. [...] A more eﬃcient approach, however, is to read\\nthe whole sentence or paragraph [...], then to produce the translated words one at\\na time, each time focusing on a diﬀerent part of the input sentence to gather the\\nsemantic details required to produce the next output word.\\n— Page 462, Deep Learning , 2016.\\nThe encoder-decoder recurrent neural network architecture with attention is currently the\\nstate-of-the-art on some benchmark problems for machine translation. And this architecture is\\nused in the heart of the Google Neural Machine Translation system, or GNMT, used in their\\nGoogle Translate service.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 353}, page_content='27.4. Further Reading 337\\n... current state-of-the-art machine translation systems are powered by models that\\nemploy attention.\\n— Page 209, Neural Network Methods in Natural Language Processing , 2017.\\nAlthough eﬀective, the neural machine translation systems still suﬀer some issues, such as\\nscaling to larger vocabularies of words and the slow speed of training the models. There are\\nthe current areas of focus for large production neural translation systems, such as the Google\\nsystem.\\nThree inherent weaknesses of Neural Machine Translation [...]: its slower training\\nand inference speed, ineﬀectiveness in dealing with rare words, and sometimes failure\\nto translate all words in the source sentence.\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\n27.4 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n27.4.1 Books\\n\\x88Neural Network Methods in Natural Language Processing , 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 353}, page_content='http://amzn.to/2wPrW37\\n\\x88Syntax-based Statistical Machine Translation , 2017.\\nhttp://amzn.to/2xCrl3p\\n\\x88Deep Learning , 2016.\\nhttp://amzn.to/2xBEsBJ\\n\\x88Statistical Machine Translation , 2010.\\nhttp://amzn.to/2xCe1vP\\n\\x88Handbook of Natural Language Processing and Machine Translation , 2011.\\nhttp://amzn.to/2jYUFfy\\n\\x88Artiﬁcial Intelligence, A Modern Approach , 3rd Edition, 2009.\\nhttp://amzn.to/2wUZesr\\n27.4.2 Papers\\n\\x88A Statistical Approach to Machine Translation , 1990.\\nhttps://dl.acm.org/citation.cfm?id=92860\\n\\x88Review Article: Example-based Machine Translation , 1999.\\nhttps://link.springer.com/article/10.1023/A:1008109312730'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 354}, page_content='27.5. Summary 338\\n\\x88Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\nhttps://arxiv.org/abs/1406.1078\\n\\x88Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nhttps://arxiv.org/abs/1409.0473\\n\\x88Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nhttps://arxiv.org/abs/1609.08144\\n\\x88Sequence to sequence learning with neural networks , 2014.\\nhttps://arxiv.org/abs/1409.3215\\n\\x88Recurrent Continuous Translation Models , 2013.\\nhttp://www.aclweb.org/anthology/D13-1176\\n\\x88Continuous space translation models for phrase-based statistical machine translation , 2013.\\nhttps://aclweb.org/anthology/C/C12/C12-2104.pdf\\n27.4.3 Additional\\n\\x88Machine Translation Archive.\\nhttp://www.mt-archive.info/\\n\\x88Neural machine translation on Wikipedia.\\nhttps://en.wikipedia.org/wiki/Neural_machine_translation\\n\\x88Neural Machine Translation, Statistical Machine Translation , 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 354}, page_content='https://arxiv.org/abs/1709.07809\\n27.5 Summary\\nIn this chapter, you discovered the challenge of machine translation and the eﬀectiveness of\\nneural machine translation models. Speciﬁcally, you learned:\\n\\x88Machine translation is challenging given the inherent ambiguity and ﬂexibility of human\\nlanguage.\\n\\x88Statistical machine translation replaces classical rule-based systems with models that learn\\nto translate from examples.\\n\\x88Neural machine translation models ﬁt a single model rather than a pipeline of ﬁne tuned\\nmodels and currently achieve state-of-the-art results.\\n27.5.1 Next\\nIn the next chapter, you will discover how you can design neural machine translation models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 355}, page_content='Chapter 28\\nWhat are Encoder-Decoder Models for\\nNeural Machine Translation\\nThe encoder-decoder architecture for recurrent neural networks is the standard neural machine\\ntranslation method that rivals and in some cases outperforms classical statistical machine\\ntranslation methods. This architecture is very new, having only been pioneered in 2014,\\nalthough, has been adopted as the core technology inside Google’s translate service. In this\\nchapter, you will discover the two seminal examples of the encoder-decoder model for neural\\nmachine translation. After reading this chapter, you will know:\\n\\x88The encoder-decoder recurrent neural network architecture is the core technology inside\\nGoogle’s translate service.\\n\\x88The so-called Sutskever model for direct end-to-end machine translation.\\n\\x88The so-called Cho model that extends the architecture with GRU units and an attention\\nmechanism.\\nLet’s get started.\\n28.1 Encoder-Decoder Architecture for NMT'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 355}, page_content='The Encoder-Decoder architecture with recurrent neural networks has become an eﬀective\\nand standard approach for both neural machine translation (NMT) and sequence-to-sequence\\n(seq2seq) prediction in general. The key beneﬁts of the approach are the ability to train a single\\nend-to-end model directly on source and target sentences and the ability to handle variable\\nlength input and output sequences of text. As evidence of the success of the method, the\\narchitecture is the core of the Google translation service.\\nOur model follows the common sequence-to-sequence learning framework with\\nattention. It has three components: an encoder network, a decoder network, and an\\nattention network.\\n—Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016\\n339'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 356}, page_content='28.2. Sutskever NMT Model 340\\nIn this chapter, we will take a closer look at two diﬀerent research projects that developed\\nthe same Encoder-Decoder architecture at the same time in 2014 and achieved results that put\\nthe spotlight on the approach. They are:\\n\\x88Sutskever NMT Model\\n\\x88Cho NMT Model\\n28.2 Sutskever NMT Model\\nIn this section, we will look at the neural machine translation model developed by Ilya Sutskever,\\net al. as described in their 2014 paper Sequence to Sequence Learning with Neural Networks .\\nWe will refer to it as the Sutskever NMT Model , for lack of a better name. This is an important\\npaper as it was one of the ﬁrst to introduce the Encoder-Decoder model for machine translation\\nand more generally sequence-to-sequence learning. It is an important model in the ﬁeld of\\nmachine translation as it was one of the ﬁrst neural machine translation systems to outperform\\na baseline statistical machine learning model on a large translation task.\\n28.2.1 Problem'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 356}, page_content='28.2.1 Problem\\nThe model was applied to English to French translation, speciﬁcally the WMT 2014 translation\\ntask. The translation task was processed one sentence at a time, and an end-of-sequence ( <EOS> )\\ntoken was added to the end of output sequences during training to signify the end of the\\ntranslated sequence. This allowed the model to be capable of predicting variable length output\\nsequences.\\nNote that we require that each sentence ends with a special end-of-sentence symbol\\n<EOS> , which enables the model to deﬁne a distribution over sequences of all possible\\nlengths.\\n—Sequence to Sequence Learning with Neural Networks , 2014.\\nThe model was trained on a subset of the 12 Million sentences in the dataset, comprised of\\n348 Million French words and 304 Million English words. This set was chosen because it was\\npre-tokenized. The source vocabulary was reduced to the 160,000 most frequent source English'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 356}, page_content='words and 80,000 of the most frequent target French words. All out-of-vocabulary words were\\nreplaced with the UNKtoken.\\n28.2.2 Model\\nAn Encoder-Decoder architecture was developed where an input sequence was read in entirety\\nand encoded to a ﬁxed-length internal representation. A decoder network then used this internal\\nrepresentation to output words until the end of sequence token was reached. LSTM networks\\nwere used for both the encoder and decoder.\\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to\\nobtain large ﬁxed-dimensional vector representation, and then to use another LSTM\\nto extract the output sequence from that vector'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 357}, page_content='28.2. Sutskever NMT Model 341\\n—Sequence to Sequence Learning with Neural Networks , 2014.\\nThe ﬁnal model was an ensemble of 5 deep learning models. A left-to-right beam search was\\nused during the inference of the translations.\\nFigure 28.1: Depiction of Sutskever Encoder-Decoder Model for Text Translation. Taken from\\nSequence to Sequence Learning with Neural Networks .\\n28.2.3 Model Conﬁguration\\nThe following provides a summary of the model conﬁguration taken from the paper:\\n\\x88Input sequences were reversed.\\n\\x88A 1000-dimensional word embedding layer was used to represent the input words.\\n\\x88Softmax was used on the output layer.\\n\\x88The input and output models had 4 layers with 1,000 units per layer.\\n\\x88The model was ﬁt for 7.5 epochs where some learning rate decay was performed.\\n\\x88A batch-size of 128 sequences was used during training.\\n\\x88Gradient clipping was used during training to mitigate the chance of gradient explosions.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 357}, page_content='\\x88Batches were comprised of sentences with roughly the same length to speed-up computation.\\nThe model was ﬁt on an 8-GPU machine where each layer was run on a diﬀerent GPU.\\nTraining took 10 days.\\nThe resulting implementation achieved a speed of 6,300 (both English and French)\\nwords per second with a minibatch size of 128. Training took about ten days with\\nthis implementation.\\n—Sequence to Sequence Learning with Neural Networks , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 358}, page_content='28.3. Cho NMT Model 342\\n28.2.4 Result\\nThe system achieved a BLEU score of 34.81, which is a good score compared to the baseline\\nscore developed with a statistical machine translation system of 33.30. Importantly, this is\\nthe ﬁrst example of a neural machine translation system that outperformed a phrase-based\\nstatistical machine translation baseline on a large scale problem.\\n... we obtained a BLEU score of 34.81 [...] This is by far the best result achieved by\\ndirect translation with large neural networks. For comparison, the BLEU score of\\nan SMT baseline on this dataset is 33.30\\n—Sequence to Sequence Learning with Neural Networks , 2014.\\nThe ﬁnal model was used to re-score the list of best translations and improved the score to\\n36.5 which brings it close to the best result at the time of 37.0.\\n28.3 Cho NMT Model\\nIn this section, we will look at the neural machine translation system described by Kyunghyun'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 358}, page_content='Cho, et al. in their 2014 paper titled Learning Phrase Representations using RNN Encoder-\\nDecoder for Statistical Machine Translation . We will refer to it as the Cho NMT Model model for\\nlack of a better name. Importantly, the Cho Model is used only to score candidate translations\\nand is not used directly for translation like the Sutskever model above. Although extensions to\\nthe work to better diagnose and improve the model do use it directly and alone for translation.\\n28.3.1 Problem\\nAs above, the problem is the English to French translation task from the WMT 2014 workshop.\\nThe source and target vocabulary were limited to the most frequent 15,000 French and English\\nwords which covers 93% of the dataset, and out of vocabulary words were replaced with UNK.\\n... called RNN Encoder-Decoder that consists of two recurrent neural networks\\n(RNN). One RNN encodes a sequence of symbols into a ﬁxed-length vector rep-\\nresentation, and the other decodes the representation into another sequence of'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 358}, page_content='symbols.\\n—Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 359}, page_content='28.3. Cho NMT Model 343\\nFigure 28.2: Depiction of the Encoder-Decoder architecture. Taken from Learning Phrase\\nRepresentations using RNN Encoder-Decoder for Statistical Machine Translation .\\nThe implementation does not use LSTM units; instead, a simpler recurrent neural network\\nunit is developed called the gated recurrent unit or GRU.\\n... we also propose a new type of hidden unit that has been motivated by the LSTM\\nunit but is much simpler to compute and implement.\\n—Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\n28.3.2 Model Conﬁguration\\nThe following provides a summary of the model conﬁguration taken from the paper:\\n\\x88A 100-dimensional word embedding was used to represent the input words.\\n\\x88The encoder and decoder were conﬁgured with 1 layer of 1000 GRU units.\\n\\x88500 Maxout units pooling 2 inputs were used after the decoder.\\n\\x88A batch size of 64 sentences was used during training.\\nThe model was trained for approximately 2 days.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 360}, page_content='28.3. Cho NMT Model 344\\n28.3.3 Extensions\\nIn the paper On the Properties of Neural Machine Translation: Encoder-Decoder Approaches ,\\nCho, et al. investigate the limitations of their model. They discover that performance degrades\\nquickly with the increase in the length of input sentences and with the number of words outside\\nof the vocabulary.\\nOur analysis revealed that the performance of the neural machine translation suﬀers\\nsigniﬁcantly from the length of sentences.\\n—On the Properties of Neural Machine Translation: Encoder-Decoder Approaches , 2014.\\nThey provide a useful graph of the performance of the model as the length of the sentence is\\nincreased that captures the graceful loss in skill with increased diﬃculty.\\nFigure 28.3: Loss in model skill with increased sentence length. Taken from On the Properties\\nof Neural Machine Translation: Encoder-Decoder Approaches .\\nTo address the problem of unknown words, they suggest dramatically increasing the vocabu-'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 360}, page_content='lary of known words during training. They address the problem of sentence length in a follow-up\\npaper titled Neural Machine Translation by Jointly Learning to Align and Translate in which\\nthey propose the use of an attention mechanism. Instead of encoding the input sentence to a\\nﬁxed length vector, a fuller representation of the encoded input is kept and the model learns to\\nuse to pay attention to diﬀerent parts of the input for each word output by the decoder.\\nEach time the proposed model generates a word in a translation, it (soft-)searches\\nfor a set of positions in a source sentence where the most relevant information is\\nconcentrated. The model then predicts a target word based on the context vectors\\nassociated with these source positions and all the previous generated target words.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2015.\\nA wealth of technical details are provided in the paper; for example:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 360}, page_content='\\x88A similarly conﬁgured model is used, although with bidirectional layers.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 361}, page_content='28.4. Further Reading 345\\n\\x88The data is prepared such that 30,000 of the most common words are kept in the vocabulary.\\n\\x88The model is ﬁrst trained with sentences with a length up to 20 words, then with sentences\\nwith a length up to 50 words.\\n\\x88A batch size of 80 sentences is used and the model was ﬁt for 4-6 epochs.\\n\\x88A beam search was used during the inference to ﬁnd the most likely sequence of words for\\neach translation.\\nThis time the model takes approximately 5 days to train. The code for this follow-up work\\nis also made available. As with the Sutskever, the model achieved results within the reach of\\nclassical phrase-based statistical approaches.\\nPerhaps more importantly, the proposed approach achieved a translation performance\\ncomparable to the existing phrase-based statistical machine translation. It is a\\nstriking result, considering that the proposed architecture, or the whole family of\\nneural machine translation, has only been proposed as recently as this year. We'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 361}, page_content='believe the architecture proposed here is a promising step toward better machine\\ntranslation and a better understanding of natural languages in general.\\n—Neural Machine Translation by Jointly Learning to Align and Translate , 2015.\\nKyunghyun Cho is also the author of a 2015 series of posts on the Nvidia developer blog on\\nthe topic of the encoder-decoder architecture for neural machine translation titled Introduction\\nto Neural Machine Translation with GPUs. The series provides a good introduction to the topic\\nand the model.\\n28.4 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nhttps://arxiv.org/abs/1609.08144\\n\\x88Sequence to Sequence Learning with Neural Networks , 2014.\\nhttps://arxiv.org/abs/1409.3215\\n\\x88Presentation for Sequence to Sequence Learning with Neural Networks , 2016.\\nhttps://www.youtube.com/watch?v=-uyXE7dY5H0'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 361}, page_content='\\x88Ilya Sutskever Homepage.\\nhttp://www.cs.toronto.edu/ ~ilya/\\n\\x88Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\nhttps://arxiv.org/abs/1406.1078'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 362}, page_content='28.5. Summary 346\\n\\x88Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nhttps://arxiv.org/abs/1409.0473\\n\\x88On the Properties of Neural Machine Translation: Encoder-Decoder Approaches , 2014.\\nhttps://arxiv.org/abs/1409.1259\\n\\x88Kyunghyun Cho Homepage.\\nhttp://www.kyunghyuncho.me/\\n\\x88Introduction to Neural Machine Translation with GPUs , 2015.\\nhttps://goo.gl/GmWjvX\\n28.5 Summary\\nIn this chapter, you discovered two examples of the encoder-decoder model for neural machine\\ntranslation. Speciﬁcally, you learned:\\n\\x88The encoder-decoder recurrent neural network architecture is the core technology inside\\nGoogle’s translate service.\\n\\x88The so-called Sutskever model for direct end-to-end machine translation.\\n\\x88The so-called Cho model that extends the architecture with GRU units and an attention\\nmechanism.\\n28.5.1 Next\\nIn the next chapter, you will discover how you can conﬁgure neural machine translation models.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 363}, page_content='Chapter 29\\nHow to Conﬁgure Encoder-Decoder\\nModels for Machine Translation\\nThe encoder-decoder architecture for recurrent neural networks is achieving state-of-the-art\\nresults on standard machine translation benchmarks and is being used in the heart of industrial\\ntranslation services. The model is simple, but given the large amount of data required to train it,\\ntuning the myriad of design decisions in the model in order get top performance on your problem\\ncan be practically intractable. Thankfully, research scientists have used Google-scale hardware\\nto do this work for us and provide a set of heuristics for how to conﬁgure the encoder-decoder\\nmodel for neural machine translation and for sequence prediction generally.\\nIn this chapter, you will discover the details of how to best conﬁgure an encoder-decoder\\nrecurrent neural network for neural machine translation and other natural language processing\\ntasks. After reading this chapter, you will know:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 363}, page_content='\\x88The Google study that investigated each model design decision in the encoder-decoder\\nmodel to isolate their eﬀects.\\n\\x88The results and recommendations for design decisions like word embeddings, encoder and\\ndecoder depth, and attention mechanisms.\\n\\x88A set of base model design decisions that can be used as a starting point on your own\\nsequence-to-sequence projects.\\nLet’s get started.\\n29.1 Encoder-Decoder Model for Neural Machine Trans-\\nlation\\nThe Encoder-Decoder architecture for recurrent neural networks is displacing classical phrase-\\nbased statistical machine translation systems for state-of-the-art results. As evidence, by their\\n2016 paper Google’s Neural Machine Translation System: Bridging the Gap between Human\\nand Machine Translation , Google now uses the approach in their core of their Google Translate\\nservice.\\nA problem with this architecture is that the models are large, in turn requiring very large'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 363}, page_content='datasets on which to train. This has the eﬀect of model training taking days or weeks and\\n347'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 364}, page_content='29.2. Baseline Model 348\\nrequiring computational resources that are generally very expensive. As such, little work has\\nbeen done on the impact of diﬀerent design choices on the model and their impact on model\\nskill. This problem is addressed explicitly by Denny Britz, et al. in their 2017 paper Massive\\nExploration of Neural Machine Translation Architectures . In the paper, they design a baseline\\nmodel for a standard English-to-German translation task and enumerate a suite of diﬀerent\\nmodel design choices and describe their impact on the skill of the model. They claim that\\nthe complete set of experiments consumed more than 250,000 GPU compute hours, which is\\nimpressive, to say the least.\\nWe report empirical results and variance numbers for several hundred experimental\\nruns, corresponding to over 250,000 GPU hours on the standard WMT English\\nto German translation task. Our experiments lead to novel insights and practical\\nadvice for building and extending NMT architectures.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 364}, page_content='—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nIn this chapter, we will look at some of the ﬁndings from this paper that we can use to tune\\nour own neural machine translation models, as well as sequence-to-sequence models in general.\\n29.2 Baseline Model\\nWe can start-oﬀ by describing the baseline model used as the starting point for all experiments.\\nA baseline model conﬁguration was chosen such that the model would perform reasonably well\\non the translation task.\\n\\x88Embedding: 512-dimensions.\\n\\x88RNN Cell: Gated Recurrent Unit or GRU.\\n\\x88Encoder: Bidirectional.\\n\\x88Encoder Depth: 2-layers (1 layer in each direction).\\n\\x88Decoder Depth: 2-layers.\\n\\x88Attention: Bahdanau-style.\\n\\x88Optimizer: Adam.\\n\\x88Dropout: 20% on input.\\nEach experiment started with the baseline model and varied one element in an attempt to\\nisolate the impact of the design decision on the model skill, in this case, BLEU scores.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 365}, page_content='29.3. Word Embedding Size 349\\nFigure 29.1: Encoder-Decoder Architecture for Neural Machine Translation. Taken from Massive\\nExploration of Neural Machine Translation Architectures .\\n29.3 Word Embedding Size\\nA word-embedding is used to represent words input to the encoder. This is a distributed\\nrepresentation where each word is mapped to a ﬁxed-sized vector of continuous values. The\\nbeneﬁt of this approach is that diﬀerent words with similar meaning will have a similar\\nrepresentation. This distributed representation is often learned while ﬁtting the model on the\\ntraining data. The embedding size deﬁnes the length of the vectors used to represent words. It\\nis generally believed that a larger dimensionality will result in a more expressive representation,\\nand in turn, better skill. Interestingly, the results show that the largest size tested did achieve\\nthe best results, but the beneﬁt of increasing the size was minor overall.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 365}, page_content='[results show] that 2048-dimensional embeddings yielded the overall best result, they\\nonly did so by a small margin. Even small 128-dimensional embeddings performed\\nsurprisingly well, while converging almost twice as quickly.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Start with a small embedding, such as 128, perhaps increase the size\\nlater for a minor lift in skill.\\n29.4 RNN Cell Type\\nThere are generally three types of recurrent neural network cells that are commonly used:\\n\\x88Simple RNN.\\n\\x88Long Short-Term Memory or LSTM.\\n\\x88Gated Recurrent Unit or GRU.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 366}, page_content='29.5. Encoder-Decoder Depth 350\\nThe LSTM was developed to address the vanishing gradient problem of the Simple RNN\\nthat limited the training of deep RNNs. The GRU was developed in an attempt to simplify the\\nLSTM. Results showed that both the GRU and LSTM were signiﬁcantly better than the Simple\\nRNN, but the LSTM was generally better overall.\\nIn our experiments, LSTM cells consistently outperformed GRU cells\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use LSTM RNN units in your model.\\n29.5 Encoder-Decoder Depth\\nGenerally, deeper networks are believed to achieve better performance than shallow networks.\\nThe key is to ﬁnd a balance between network depth, model skill, and training time. This is\\nbecause we generally do not have inﬁnite resources to train very deep networks if the beneﬁt\\nto skill is minor. The authors explore the depth of both the encoder and decoder models and'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 366}, page_content='the impact on model skill. When it comes to encoders, it was found that depth did not have\\na dramatic impact on skill and more surprisingly, a 1-layer bidirectional model performs only\\nslightly better than a 4-layer bidirectional conﬁguration. A two-layer bidirectional encoder\\nperformed slightly better than other conﬁgurations tested.\\nWe found no clear evidence that encoder depth beyond two layers is necessary.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use a 1-layer bidirectional encoder and extend to 2 bidirectional layers\\nfor a small lift in skill.\\nA similar story was seen when it came to decoders. The skill between decoders with 1, 2,\\nand 4 layers was diﬀerent by a small amount where a 4-layer decoder was slightly better. An\\n8-layer decoder did not converge under the test conditions.\\nOn the decoder side, deeper models outperformed shallower ones by a small margin.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 366}, page_content='—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use a 1-layer decoder as a starting point and use a 4-layer decoder for\\nbetter results.\\n29.6 Direction of Encoder Input\\nThe order of the sequence of source text can be provided to the encoder a number of ways:\\n\\x88Forward or as-normal.\\n\\x88Reversed.\\n\\x88Both forward and reversed at the same time.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 367}, page_content='29.7. Attention Mechanism 351\\nThe authors explored the impact of the order of the input sequence on model skill comparing\\nvarious unidirectional and bidirectional conﬁgurations. Generally, they conﬁrmed previous\\nﬁndings that a reversed sequence is better than a forward sequence and that bidirectional is\\nslightly better than a reversed sequence.\\n... bidirectional encoders generally outperform unidirectional encoders, but not by\\na large margin. The encoders with reversed source consistently outperform their\\nnon-reversed counterparts.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Use a reversed order input sequence or move to bidirectional for a\\nsmall lift in model skill.\\n29.7 Attention Mechanism\\nA problem with the naive Encoder-Decoder model is that the encoder maps the input to a\\nﬁxed-length internal representation from which the decoder must produce the entire output'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 367}, page_content='sequence. Attention is an improvement to the model that allows the decoder to pay attention\\nto diﬀerent words in the input sequence as it outputs each word in the output sequence. The\\nauthors look at a few variations on simple attention mechanisms. The results show that having\\nattention results in dramatically better performance than not having attention.\\nWhile we did expect the attention-based models to signiﬁcantly outperform those\\nwithout an attention mechanism, we were surprised by just how poorly the [no\\nattention] models fared.\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nThe simple weighted average style attention described by Bahdanau, et al. in their 2015\\npaper Neural machine translation by jointly learning to align and translate was found to perform\\nthe best.\\nRecommendation : Use attention and prefer the Bahdanau-style weighted average style\\nattention.\\n29.8 Inference'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 367}, page_content='29.8 Inference\\nIt is common in neural machine translation systems to use a beam-search to sample the\\nprobabilities for the words in the sequence output by the model. The wider the beam width, the\\nmore exhaustive the search, and, it is believed, the better the results. The results showed that\\na modest beam-width of 3-5 performed the best, which could be improved only very slightly\\nthrough the use of length penalties. The authors generally recommend tuning the beam width\\non each speciﬁc problem.\\nWe found that a well-tuned beam search is crucial to achieving good results, and\\nthat it leads to consistent gains of more than one BLEU point\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.\\nRecommendation : Start with a greedy search (beam=1) and tune based on your problem.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 368}, page_content='29.9. Final Model 352\\n29.9 Final Model\\nThe authors pull together their ﬁndings into a single best model and compare the results of this\\nmodel to other well-performing models and state-of-the-art results. The speciﬁc conﬁgurations\\nof this model are summarized in the table below, taken from the paper. These parameters may\\nbe taken as a good or best starting point when developing your own encoder-decoder model for\\nan NLP application.\\nFigure 29.2: Summary of Model Conﬁguration for the Final NMT Model. Taken from Massive\\nExploration of Neural Machine Translation Architectures .\\nThe results of the system were shown to be impressive and achieve skill close to state-of-the-art\\nwith a simpler model, which was not the goal of the paper.\\n... we do show that through careful hyperparameter tuning and good initialization,\\nit is possible to achieve state-of-the-art performance on standard WMT benchmarks\\n—Massive Exploration of Neural Machine Translation Architectures , 2017.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 368}, page_content='Importantly, the authors provide all of their code as an open source project called tf-seq2seq.\\nBecause two of the authors were members of the Google Brain residency program, their work\\nwas announced on the Google Research blog with the title Introducing tf-seq2seq: An Open\\nSource Sequence-to-Sequence Framework in TensorFlow , 2017.\\n29.10 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n\\x88Massive Exploration of Neural Machine Translation Architectures , 2017.\\nhttps://arxiv.org/abs/1703.03906'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 369}, page_content='29.11. Summary 353\\n\\x88Denny Britz Homepage.\\nhttp://blog.dennybritz.com/\\n\\x88WildML Blog.\\nhttp://www.wildml.com/\\n\\x88Introducing tf-seq2seq: An Open Source Sequence-to-Sequence Framework in TensorFlow ,\\n2017.\\nhttps://research.googleblog.com/2017/04/introducing-tf-seq2seq-open-source.\\nhtml\\n\\x88tf-seq2seq: A general-purpose encoder-decoder framework for TensorFlow.\\nhttps://github.com/google/seq2seq\\n\\x88tf-seq2seq Project Documentation.\\nhttps://google.github.io/seq2seq/\\n\\x88tf-seq2seq Tutorial: Neural Machine Translation Background.\\nhttps://google.github.io/seq2seq/nmt/\\n\\x88Neural machine translation by jointly learning to align and translate , 2015.\\nhttps://arxiv.org/abs/1409.0473\\n29.11 Summary\\nIn this chapter, you discovered how to best conﬁgure an encoder-decoder recurrent neural\\nnetwork for neural machine translation and other natural language processing tasks. Speciﬁcally,\\nyou learned:\\n\\x88The Google study that investigated each model design decision in the encoder-decoder\\nmodel to isolate their eﬀects.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 369}, page_content='\\x88The results and recommendations for design decisions like word embeddings, encoder and\\ndecoder depth, and attention mechanisms.\\n\\x88A set of base model design decisioning that can be used as a starting point on your own\\nsequence to sequence projects.\\n29.11.1 Next\\nIn the next chapter, you will discover how you can develop a neural machine translation model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 370}, page_content='Chapter 30\\nProject: Develop a Neural Machine\\nTranslation Model\\nMachine translation is a challenging task that traditionally involves large statistical models\\ndeveloped using highly sophisticated linguistic knowledge. Neural machine translation is the\\nuse of deep neural networks for the problem of machine translation. In this tutorial, you will\\ndiscover how to develop a neural machine translation system for translating German phrases to\\nEnglish. After completing this tutorial, you will know:\\n\\x88How to clean and prepare data ready to train a neural machine translation system.\\n\\x88How to develop an encoder-decoder model for machine translation.\\n\\x88How to use a trained model for inference on new input phrases and evaluate the model\\nskill.\\nLet’s get started.\\n30.1 Tutorial Overview\\nThis tutorial is divided into the following parts:\\n1. German to English Translation Dataset\\n2. Preparing the Text Data\\n3. Train Neural Translation Model\\n4. Evaluate Neural Translation Model'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 370}, page_content='30.2 German to English Translation Dataset\\nIn this tutorial, we will use a dataset of German to English terms used as the basis for ﬂashcards\\nfor language learning. The dataset is available from the ManyThings.org website, with examples\\ndrawn from the Tatoeba Project. The dataset is comprised of German phrases and their English\\ncounterparts and is intended to be used with the Anki ﬂashcard software.\\n354'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 371}, page_content='30.3. Preparing the Text Data 355\\n\\x88Download the English-German pairs dataset.\\nhttp://www.manythings.org/anki/deu-eng.zip\\nDownload the dataset to your current working directory and decompress it; for example:\\nunzip deu-eng.zip\\nListing 30.1: Unzip the dataset\\nYou will have a ﬁle called deu.txt that contains 152,820 pairs of English to German phases,\\none pair per line with a tab separating the language. For example, the ﬁrst 5 lines of the ﬁle\\nlook as follows:\\nHi. Hallo!\\nHi. GruB Gott!\\nRun! Lauf!\\nWow! Potzdonner!\\nWow! Donnerwetter!\\nListing 30.2: Sample of the raw dataset (with Unicode characters normalized).\\nWe will frame the prediction problem as given a sequence of words in German as input,\\ntranslate or predict the sequence of words in English. The model we will develop will be suitable\\nfor some beginner German phrases.\\n30.3 Preparing the Text Data\\nThe next step is to prepare the text data ready for modeling. Take a look at the raw data and'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 371}, page_content='note what you see that we might need to handle in a data cleaning operation. For example,\\nhere are some observations I note from reviewing the raw data:\\n\\x88There is punctuation.\\n\\x88The text contains uppercase and lowercase.\\n\\x88There are special characters in the German.\\n\\x88There are duplicate phrases in English with diﬀerent translations in German.\\n\\x88The ﬁle is ordered by sentence length with very long sentences toward the end of the ﬁle.\\nA good text cleaning procedure may handle some or all of these observations. Data\\npreparation is divided into two subsections:\\n1. Clean Text\\n2. Split Text'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 372}, page_content=\"30.3. Preparing the Text Data 356\\n30.3.1 Clean Text\\nFirst, we must load the data in a way that preserves the Unicode German characters. The\\nfunction below called load doc() will load the ﬁle as a blob of text.\\n# load doc into memory\\ndef load_doc(filename):\\n# open the file as read only\\nfile = open(filename, mode= 'rt ', encoding= 'utf-8 ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\nListing 30.3: Function to load a ﬁle into memory\\nEach line contains a single pair of phrases, ﬁrst English and then German, separated by a tab\\ncharacter. We must split the loaded text by line and then by phrase. The function topairs()\\nbelow will split the loaded text.\\n# split a loaded document into sentences\\ndef to_pairs(doc):\\nlines = doc.strip().split( '\\\\n ')\\npairs = [line.split( '\\\\t ') for line in lines]\\nreturn pairs\\nListing 30.4: Function to split lines into pairs\\nWe are now ready to clean each sentence. The speciﬁc cleaning operations we will perform\\nare as follows:\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 372}, page_content=\"are as follows:\\n\\x88Remove all non-printable characters.\\n\\x88Remove all punctuation characters.\\n\\x88Normalize all Unicode characters to ASCII (e.g. Latin characters).\\n\\x88Normalize the case to lowercase.\\n\\x88Remove any remaining tokens that are not alphabetic.\\nWe will perform these operations on each phrase for each pair in the loaded dataset. The\\nclean pairs() function below implements these operations.\\n# clean a list of lines\\ndef clean_pairs(lines):\\ncleaned = list()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nre_print = re.compile( '[^%s] '% re.escape(string.printable))\\nfor pair in lines:\\nclean_pair = list()\\nfor line in pair:\\n# normalize unicode characters\\nline = normalize( 'NFD ', line).encode( 'ascii ', 'ignore ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 373}, page_content=\"30.3. Preparing the Text Data 357\\nline = line.decode( 'UTF-8 ')\\n# tokenize on white space\\nline = line.split()\\n# convert to lowercase\\nline = [word.lower() for word in line]\\n# remove punctuation from each token\\nline = [re_punc.sub( '', w) for w in line]\\n# remove non-printable chars form each token\\nline = [re_print.sub( '', w) for w in line]\\n# remove tokens with numbers in them\\nline = [word for word in line if word.isalpha()]\\n# store as string\\nclean_pair.append( ' '.join(line))\\ncleaned.append(clean_pair)\\nreturn array(cleaned)\\nListing 30.5: Function to clean text\\nFinally, now that the data has been cleaned, we can save the list of phrase pairs to a ﬁle\\nready for use. The function save clean data() uses the pickle API to save the list of clean\\ntext to ﬁle. Pulling all of this together, the complete example is listed below.\\nimport string\\nimport re\\nfrom pickle import dump\\nfrom unicodedata import normalize\\nfrom numpy import array\\n# load doc into memory\\ndef load_doc(filename):\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 373}, page_content=\"# open the file as read only\\nfile = open(filename, mode= 'rt ', encoding= 'utf-8 ')\\n# read all text\\ntext = file.read()\\n# close the file\\nfile.close()\\nreturn text\\n# split a loaded document into sentences\\ndef to_pairs(doc):\\nlines = doc.strip().split( '\\\\n ')\\npairs = [line.split( '\\\\t ') for line in lines]\\nreturn pairs\\n# clean a list of lines\\ndef clean_pairs(lines):\\ncleaned = list()\\n# prepare regex for char filtering\\nre_punc = re.compile( '[%s] '% re.escape(string.punctuation))\\nre_print = re.compile( '[^%s] '% re.escape(string.printable))\\nfor pair in lines:\\nclean_pair = list()\\nfor line in pair:\\n# normalize unicode characters\\nline = normalize( 'NFD ', line).encode( 'ascii ', 'ignore ')\\nline = line.decode( 'UTF-8 ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 374}, page_content=\"30.3. Preparing the Text Data 358\\n# tokenize on white space\\nline = line.split()\\n# convert to lowercase\\nline = [word.lower() for word in line]\\n# remove punctuation from each token\\nline = [re_punc.sub( '', w) for w in line]\\n# remove non-printable chars form each token\\nline = [re_print.sub( '', w) for w in line]\\n# remove tokens with numbers in them\\nline = [word for word in line if word.isalpha()]\\n# store as string\\nclean_pair.append( ' '.join(line))\\ncleaned.append(clean_pair)\\nreturn array(cleaned)\\n# save a list of clean sentences to file\\ndef save_clean_data(sentences, filename):\\ndump(sentences, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\n# load dataset\\nfilename = 'deu.txt '\\ndoc = load_doc(filename)\\n# split into english-german pairs\\npairs = to_pairs(doc)\\n# clean sentences\\nclean_pairs = clean_pairs(pairs)\\n# save clean pairs to file\\nsave_clean_data(clean_pairs, 'english-german.pkl ')\\n# spot check\\nfor i in range(100):\\nprint( '[%s] => [%s] '% (clean_pairs[i,0], clean_pairs[i,1]))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 374}, page_content='Listing 30.6: Complete example of text data preparation.\\nRunning the example creates a new ﬁle in the current working directory with the cleaned\\ntext called english-german.pkl . Some examples of the clean text are printed for us to evaluate\\nat the end of the run to conﬁrm that the clean operations were performed as expected.\\n30.3.2 Split Text\\nThe clean data contains a little over 150,000 phrase pairs and some of the pairs toward the end\\nof the ﬁle are very long. This is a good number of examples for developing a small translation\\nmodel. The complexity of the model increases with the number of examples, length of phrases,\\nand size of the vocabulary. Although we have a good dataset for modeling translation, we will\\nsimplify the problem slightly to dramatically reduce the size of the model required, and in turn\\nthe training time required to ﬁt the model.\\nYou can explore developing a model on the fuller dataset as an extension; I would love to'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 374}, page_content='hear how you do. We will simplify the problem by reducing the dataset to the ﬁrst 10,000\\nexamples in the ﬁle; these will be the shortest phrases in the dataset. Further, we will then\\nstake the ﬁrst 9,000 of those as examples for training and the remaining 1,000 examples to test\\nthe ﬁt model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 375}, page_content=\"30.4. Train Neural Translation Model 359\\nBelow is the complete example of loading the clean data, splitting it, and saving the split\\nportions of data to new ﬁles.\\nfrom pickle import load\\nfrom pickle import dump\\nfrom numpy.random import shuffle\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# save a list of clean sentences to file\\ndef save_clean_data(sentences, filename):\\ndump(sentences, open(filename, 'wb '))\\nprint( 'Saved: %s '% filename)\\n# load dataset\\nraw_dataset = load_clean_sentences( 'english-german.pkl ')\\n# reduce dataset size\\nn_sentences = 10000\\ndataset = raw_dataset[:n_sentences, :]\\n# random shuffle\\nshuffle(dataset)\\n# split into train/test\\ntrain, test = dataset[:9000], dataset[9000:]\\n# save\\nsave_clean_data(dataset, 'english-german-both.pkl ')\\nsave_clean_data(train, 'english-german-train.pkl ')\\nsave_clean_data(test, 'english-german-test.pkl ')\\nListing 30.7: Complete example of splitting text data.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 375}, page_content=\"Running the example creates three new ﬁles: the english-german-both.pkl that contains\\nall of the train and test examples that we can use to deﬁne the parameters of the problem,\\nsuch as max phrase lengths and the vocabulary, and the english-german-train.pkl and\\nenglish-german-test.pkl ﬁles for the train and test dataset. We are now ready to start\\ndeveloping our translation model.\\n30.4 Train Neural Translation Model\\nIn this section, we will develop the translation model. This involves both loading and preparing\\nthe clean text data ready for modeling and deﬁning and training the model on the prepared\\ndata. Let’s start oﬀ by loading the datasets so that we can prepare the data. The function\\nbelow named load clean sentences() can be used to load the train, test, and both datasets\\nin turn.\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 375}, page_content=\"train = load_clean_sentences( 'english-german-train.pkl ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 376}, page_content=\"30.4. Train Neural Translation Model 360\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\nListing 30.8: Load cleaned data from ﬁle.\\nWe will use the both or combination of the train and test datasets to deﬁne the maximum\\nlength and vocabulary of the problem. This is for simplicity. Alternately, we could deﬁne these\\nproperties from the training dataset alone and truncate examples in the test set that are too\\nlong or have words that are out of the vocabulary. We can use the Keras Tokenize class to\\nmap words to integers, as needed for modeling. We will use separate tokenizer for the English\\nsequences and the German sequences. The function below-named create tokenizer() will\\ntrain a tokenizer on a list of phrases.\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\nListing 30.9: Fit a tokenizer on the clean text data.\\nSimilarly, the function named maxlength() below will ﬁnd the length of the longest sequence\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 376}, page_content=\"in a list of phrases.\\n# max sentence length\\ndef max_length(lines):\\nreturn max(len(line.split()) for line in lines)\\nListing 30.10: Calculate the maximum sequence length.\\nWe can call these functions with the combined dataset to prepare tokenizers, vocabulary\\nsizes, and maximum lengths for both the English and German phrases.\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\nprint( 'English Vocabulary Size: %d '% eng_vocab_size)\\nprint( 'English Max Length: %d '% (eng_length))\\n# prepare german tokenizer\\nger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\nprint( 'German Vocabulary Size: %d '% ger_vocab_size)\\nprint( 'German Max Length: %d '% (ger_length))\\nListing 30.11: Prepare Tokenizers for source and target sequences.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 376}, page_content='We are now ready to prepare the training dataset. Each input and output sequence must be\\nencoded to integers and padded to the maximum phrase length. This is because we will use a\\nword embedding for the input sequences and one hot encode the output sequences The function\\nbelow named encode sequences() will perform these operations and return the result.\\n# encode and pad sequences\\ndef encode_sequences(tokenizer, length, lines):\\n# integer encode sequences\\nX = tokenizer.texts_to_sequences(lines)\\n# pad sequences with 0 values'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 377}, page_content=\"30.4. Train Neural Translation Model 361\\nX = pad_sequences(X, maxlen=length, padding= 'post ')\\nreturn X\\nListing 30.12: Function to encode and pad sequences.\\nThe output sequence needs to be one hot encoded. This is because the model will predict\\nthe probability of each word in the vocabulary as output. The function encode output() below\\nwill one hot encode English output sequences.\\n# one hot encode target sequence\\ndef encode_output(sequences, vocab_size):\\nylist = list()\\nfor sequence in sequences:\\nencoded = to_categorical(sequence, num_classes=vocab_size)\\nylist.append(encoded)\\ny = array(ylist)\\ny = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\\nreturn y\\nListing 30.13: One hot encode output sequences.\\nWe can make use of these two functions and prepare both the train and test dataset ready\\nfor training the model.\\n# prepare training data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 377}, page_content='trainY = encode_output(trainY, eng_vocab_size)\\n# prepare validation data\\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\\ntestY = encode_output(testY, eng_vocab_size)\\nListing 30.14: Prepare training and test data for modeling.\\nWe are now ready to deﬁne the model. We will use an encoder-decoder LSTM model on\\nthis problem. In this architecture, the input sequence is encoded by a front-end model called\\nthe encoder then decoded word by word by a backend model called the decoder. The function\\ndefine model() below deﬁnes the model and takes a number of arguments used to conﬁgure\\nthe model, such as the size of the input and output vocabularies, the maximum length of input\\nand output phrases, and the number of memory units used to conﬁgure the model.\\nThe model is trained using the eﬃcient Adam approach to stochastic gradient descent and'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 377}, page_content=\"minimizes the categorical loss function because we have framed the prediction problem as\\nmulticlass classiﬁcation. The model conﬁguration was not optimized for this problem, meaning\\nthat there is plenty of opportunity for you to tune it and lift the skill of the translations. I\\nwould love to see what you can come up with.\\n# define NMT model\\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\\nmodel = Sequential()\\nmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\\nmodel.add(LSTM(n_units))\\nmodel.add(RepeatVector(tar_timesteps))\\nmodel.add(LSTM(n_units, return_sequences=True))\\nmodel.add(TimeDistributed(Dense(tar_vocab, activation= 'softmax ')))\\n# compile model\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 378}, page_content=\"30.4. Train Neural Translation Model 362\\nmodel.compile(optimizer= 'adam ', loss= 'categorical_crossentropy ')\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\nListing 30.15: Deﬁne and summarize the model.\\nFinally, we can train the model. We train the model for 30 epochs and a batch size of\\n64 examples. We use checkpointing to ensure that each time the model skill on the test set\\nimproves, the model is saved to ﬁle.\\n# fit model\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\\nmodel.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),\\ncallbacks=[checkpoint], verbose=2)\\nListing 30.16: Fit the deﬁned model and save models using checkpointing.\\nWe can tie all of this together and ﬁt the neural translation model. The complete working\\nexample is listed below.\\nfrom pickle import load\\nfrom numpy import array\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 378}, page_content=\"from keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\nfrom keras.utils.vis_utils import plot_model\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM\\nfrom keras.layers import Dense\\nfrom keras.layers import Embedding\\nfrom keras.layers import RepeatVector\\nfrom keras.layers import TimeDistributed\\nfrom keras.callbacks import ModelCheckpoint\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# max sentence length\\ndef max_length(lines):\\nreturn max(len(line.split()) for line in lines)\\n# encode and pad sequences\\ndef encode_sequences(tokenizer, length, lines):\\n# integer encode sequences\\nX = tokenizer.texts_to_sequences(lines)\\n# pad sequences with 0 values\\nX = pad_sequences(X, maxlen=length, padding= 'post ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 379}, page_content=\"30.4. Train Neural Translation Model 363\\nreturn X\\n# one hot encode target sequence\\ndef encode_output(sequences, vocab_size):\\nylist = list()\\nfor sequence in sequences:\\nencoded = to_categorical(sequence, num_classes=vocab_size)\\nylist.append(encoded)\\ny = array(ylist)\\ny = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\\nreturn y\\n# define NMT model\\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\\nmodel = Sequential()\\nmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\\nmodel.add(LSTM(n_units))\\nmodel.add(RepeatVector(tar_timesteps))\\nmodel.add(LSTM(n_units, return_sequences=True))\\nmodel.add(TimeDistributed(Dense(tar_vocab, activation= 'softmax ')))\\n# compile model\\nmodel.compile(optimizer= 'adam ', loss= 'categorical_crossentropy ')\\n# summarize defined model\\nmodel.summary()\\nplot_model(model, to_file= 'model.png ', show_shapes=True)\\nreturn model\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 379}, page_content=\"train = load_clean_sentences( 'english-german-train.pkl ')\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\nprint( 'English Vocabulary Size: %d '% eng_vocab_size)\\nprint( 'English Max Length: %d '% (eng_length))\\n# prepare german tokenizer\\nger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\nprint( 'German Vocabulary Size: %d '% ger_vocab_size)\\nprint( 'German Max Length: %d '% (ger_length))\\n# prepare training data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\\ntrainY = encode_output(trainY, eng_vocab_size)\\n# prepare validation data\\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 379}, page_content=\"testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\\ntestY = encode_output(testY, eng_vocab_size)\\n# define model\\nmodel = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\\n# fit model\\ncheckpoint = ModelCheckpoint( 'model.h5 ', monitor= 'val_loss ', verbose=1,\\nsave_best_only=True, mode= 'min ')\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 380}, page_content='30.4. Train Neural Translation Model 364\\nmodel.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),\\ncallbacks=[checkpoint], verbose=2)\\nListing 30.17: Complete example of training the neural machine translation model.\\nRunning the example ﬁrst prints a summary of the parameters of the dataset such as\\nvocabulary size and maximum phrase lengths.\\nEnglish Vocabulary Size: 2404\\nEnglish Max Length: 5\\nGerman Vocabulary Size: 3856\\nGerman Max Length: 10\\nListing 30.18: Summary of the loaded data\\nNext, a summary of the deﬁned model is printed, allowing us to conﬁrm the model conﬁgu-\\nration.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 10, 256) 987136\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 256) 525312\\n_________________________________________________________________'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 380}, page_content='repeat_vector_1 (RepeatVecto (None, 5, 256) 0\\n_________________________________________________________________\\nlstm_2 (LSTM) (None, 5, 256) 525312\\n_________________________________________________________________\\ntime_distributed_1 (TimeDist (None, 5, 2404) 617828\\n=================================================================\\nTotal params: 2,655,588\\nTrainable params: 2,655,588\\nNon-trainable params: 0\\n_________________________________________________________________\\nListing 30.19: Summary of the deﬁned model\\nA plot of the model is also created providing another perspective on the model conﬁguration.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 381}, page_content='30.4. Train Neural Translation Model 365\\nFigure 30.1: Plot of the deﬁned neural machine translation model\\nNext, the model is trained. Each epoch takes about 30 seconds on modern CPU hardware;\\nno GPU is required. During the run, the model will be saved to the ﬁle model.h5 , ready for\\ninference in the next step.\\n...\\nEpoch 26/30\\nEpoch 00025: val_loss improved from 2.20048 to 2.19976, saving model to model.h5\\n17s - loss: 0.7114 - val_loss: 2.1998\\nEpoch 27/30\\nEpoch 00026: val_loss improved from 2.19976 to 2.18255, saving model to model.h5\\n17s - loss: 0.6532 - val_loss: 2.1826\\nEpoch 28/30\\nEpoch 00027: val_loss did not improve\\n17s - loss: 0.5970 - val_loss: 2.1970\\nEpoch 29/30\\nEpoch 00028: val_loss improved from 2.18255 to 2.17872, saving model to model.h5\\n17s - loss: 0.5474 - val_loss: 2.1787\\nEpoch 30/30\\nEpoch 00029: val_loss did not improve\\n17s - loss: 0.5023 - val_loss: 2.1823'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 382}, page_content=\"30.5. Evaluate Neural Translation Model 366\\nListing 30.20: Summary output from training the neural machine translation model.\\n30.5 Evaluate Neural Translation Model\\nWe will evaluate the model on the train and the test dataset. The model should perform very\\nwell on the train dataset and ideally have been generalized to perform well on the test dataset.\\nIdeally, we would use a separate validation dataset to help with model selection during training\\ninstead of the test set. You can try this as an extension. The clean datasets must be loaded\\nand prepared as before.\\n...\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\\ntrain = load_clean_sentences( 'english-german-train.pkl ')\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\n# prepare german tokenizer\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 382}, page_content=\"ger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\n# prepare data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\\ntestX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\\nListing 30.21: Load and prepare data.\\nNext, the best model saved during training must be loaded.\\n# load model\\nmodel = load_model( 'model.h5 ')\\nListing 30.22: Load and the saved model.\\nEvaluation involves two steps: ﬁrst generating a translated output sequence, and then\\nrepeating this process for many input examples and summarizing the skill of the model across\\nmultiple cases. Starting with inference, the model can predict the entire output sequence in a\\none-shot manner.\\ntranslation = model.predict(source, verbose=0)\\nListing 30.23: Predict the target sequence given the source sequence.\\nThis will be a sequence of integers that we can enumerate and lookup in the tokenizer to map\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 382}, page_content='back to words. The function below, named word forid() , will perform this reverse mapping.\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 383}, page_content=\"30.5. Evaluate Neural Translation Model 367\\nreturn None\\nListing 30.24: Map a predicted word index to the word in the vocabulary.\\nWe can perform this mapping for each integer in the translation and return the result as a\\nstring of words. The function predict sequence() below performs this operation for a single\\nencoded source phrase.\\n# generate target given source sequence\\ndef predict_sequence(model, tokenizer, source):\\nprediction = model.predict(source, verbose=0)[0]\\nintegers = [argmax(vector) for vector in prediction]\\ntarget = list()\\nfor i in integers:\\nword = word_for_id(i, tokenizer)\\nif word is None:\\nbreak\\ntarget.append(word)\\nreturn ' '.join(target)\\nListing 30.25: Predict and interpret the target sequence.\\nNext, we can repeat this for each source phrase in a dataset and compare the predicted result\\nto the expected target phrase in English. We can print some of these comparisons to screen to\\nget an idea of how the model performs in practice. We will also calculate the BLEU scores to\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 383}, page_content=\"get a quantitative idea of how well the model has performed. The evaluate model() function\\nbelow implements this, calling the above predict sequence() function for each phrase in a\\nprovided dataset.\\n# evaluate the skill of the model\\ndef evaluate_model(model, tokenizer, sources, raw_dataset):\\nactual, predicted = list(), list()\\nfor i, source in enumerate(sources):\\n# translate encoded source text\\nsource = source.reshape((1, source.shape[0]))\\ntranslation = predict_sequence(model, eng_tokenizer, source)\\nraw_target, raw_src = raw_dataset[i]\\nif i < 10:\\nprint( 'src=[%s], target=[%s], predicted=[%s] '% (raw_src, raw_target, translation))\\nactual.append(raw_target.split())\\npredicted.append(translation.split())\\n# calculate BLEU score\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 383}, page_content=\"print( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\\nListing 30.26: Function to evaluate a ﬁt model.\\nWe can tie all of this together and evaluate the loaded model on both the training and test\\ndatasets. The complete code listing is provided below.\\nfrom pickle import load\\nfrom numpy import argmax\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import load_model\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 384}, page_content=\"30.5. Evaluate Neural Translation Model 368\\nfrom nltk.translate.bleu_score import corpus_bleu\\n# load a clean dataset\\ndef load_clean_sentences(filename):\\nreturn load(open(filename, 'rb '))\\n# fit a tokenizer\\ndef create_tokenizer(lines):\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(lines)\\nreturn tokenizer\\n# max sentence length\\ndef max_length(lines):\\nreturn max(len(line.split()) for line in lines)\\n# encode and pad sequences\\ndef encode_sequences(tokenizer, length, lines):\\n# integer encode sequences\\nX = tokenizer.texts_to_sequences(lines)\\n# pad sequences with 0 values\\nX = pad_sequences(X, maxlen=length, padding= 'post ')\\nreturn X\\n# map an integer to a word\\ndef word_for_id(integer, tokenizer):\\nfor word, index in tokenizer.word_index.items():\\nif index == integer:\\nreturn word\\nreturn None\\n# generate target given source sequence\\ndef predict_sequence(model, tokenizer, source):\\nprediction = model.predict(source, verbose=0)[0]\\nintegers = [argmax(vector) for vector in prediction]\\ntarget = list()\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 384}, page_content=\"target = list()\\nfor i in integers:\\nword = word_for_id(i, tokenizer)\\nif word is None:\\nbreak\\ntarget.append(word)\\nreturn ' '.join(target)\\n# evaluate the skill of the model\\ndef evaluate_model(model, sources, raw_dataset):\\nactual, predicted = list(), list()\\nfor i, source in enumerate(sources):\\n# translate encoded source text\\nsource = source.reshape((1, source.shape[0]))\\ntranslation = predict_sequence(model, eng_tokenizer, source)\\nraw_target, raw_src = raw_dataset[i]\\nif i < 10:\\nprint( 'src=[%s], target=[%s], predicted=[%s] '% (raw_src, raw_target, translation))\\nactual.append(raw_target.split())\\npredicted.append(translation.split())\\n# calculate BLEU score\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 385}, page_content=\"30.5. Evaluate Neural Translation Model 369\\nprint( 'BLEU-1: %f '% corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\\nprint( 'BLEU-2: %f '% corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\\nprint( 'BLEU-3: %f '% corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\\nprint( 'BLEU-4: %f '% corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\\n# load datasets\\ndataset = load_clean_sentences( 'english-german-both.pkl ')\\ntrain = load_clean_sentences( 'english-german-train.pkl ')\\ntest = load_clean_sentences( 'english-german-test.pkl ')\\n# prepare english tokenizer\\neng_tokenizer = create_tokenizer(dataset[:, 0])\\neng_vocab_size = len(eng_tokenizer.word_index) + 1\\neng_length = max_length(dataset[:, 0])\\n# prepare german tokenizer\\nger_tokenizer = create_tokenizer(dataset[:, 1])\\nger_vocab_size = len(ger_tokenizer.word_index) + 1\\nger_length = max_length(dataset[:, 1])\\n# prepare data\\ntrainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 385}, page_content=\"testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\\n# load model\\nmodel = load_model( 'model.h5 ')\\n# test on some training sequences\\nprint( 'train ')\\nevaluate_model(model, trainX, train)\\n# test on some test sequences\\nprint( 'test ')\\nevaluate_model(model, testX, test)\\nListing 30.27: Complete example of translating text with a ﬁt neural machine translation model.\\nRunning the example ﬁrst prints examples of source text, expected and predicted translations,\\nas well as scores for the training dataset, followed by the test dataset. Your speciﬁc results will\\ndiﬀer given the random shuﬄing of the dataset and the stochastic nature of neural networks.\\nLooking at the results for the test dataset ﬁrst, we can see that the translations are readable\\nand mostly correct. For example: ‘ ich liebe dich ’ was correctly translated to ‘ i love you ’.\\nWe can also see that the translations were not perfect, with ‘ ich konnte nicht gehen ’ translated\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 385}, page_content='toi cant go instead of the expected ‘ i couldnt walk ’. We can also see the BLEU-4 score of 0.51,\\nwhich provides an upper bound on what we might expect from this model.\\nNote : Given the stochastic nature of neural networks, your speciﬁc results may vary. Consider\\nrunning the example a few times.\\nsrc=[ich liebe dich], target=[i love you], predicted=[i love you]\\nsrc=[ich sagte du sollst den mund halten], target=[i said shut up], predicted=[i said stop\\nup]\\nsrc=[wie geht es eurem vater], target=[hows your dad], predicted=[hows your dad]\\nsrc=[das gefallt mir], target=[i like that], predicted=[i like that]\\nsrc=[ich gehe immer zu fu], target=[i always walk], predicted=[i will to]\\nsrc=[ich konnte nicht gehen], target=[i couldnt walk], predicted=[i cant go]\\nsrc=[er ist sehr jung], target=[he is very young], predicted=[he is very young]\\nsrc=[versucht es doch einfach], target=[just try it], predicted=[just try it]\\nsrc=[sie sind jung], target=[youre young], predicted=[youre young]'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 385}, page_content='src=[er ging surfen], target=[he went surfing], predicted=[he went surfing]'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 386}, page_content='30.6. Extensions 370\\nBLEU-1: 0.085682\\nBLEU-2: 0.284191\\nBLEU-3: 0.459090\\nBLEU-4: 0.517571\\nListing 30.28: Sample output translation on the training dataset.\\nLooking at the results on the test set, do see readable translations, which is not an easy task.\\nFor example, we see ‘ ich mag dich nicht ’ correctly translated to ‘ i dont like you ’. We also see\\nsome poor translations and a good case that the model could support from further tuning, such\\nas ‘ich bin etwas beschwipst ’ translated as ‘ i a bit bit ’ instead of the expected im a bit tipsy A\\nBLEU-4 score of 0.076238 was achieved, providing a baseline skill to improve upon with further\\nimprovements to the model.\\nsrc=[tom erblasste], target=[tom turned pale], predicted=[tom went pale]\\nsrc=[bring mich nach hause], target=[take me home], predicted=[let us at]\\nsrc=[ich bin etwas beschwipst], target=[im a bit tipsy], predicted=[i a bit bit]\\nsrc=[das ist eine frucht], target=[its a fruit], predicted=[thats a a]'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 386}, page_content='src=[ich bin pazifist], target=[im a pacifist], predicted=[im am]\\nsrc=[unser plan ist aufgegangen], target=[our plan worked], predicted=[who is a man]\\nsrc=[hallo tom], target=[hi tom], predicted=[hello tom]\\nsrc=[sei nicht nervos], target=[dont be nervous], predicted=[dont be crazy]\\nsrc=[ich mag dich nicht], target=[i dont like you], predicted=[i dont like you]\\nsrc=[tom stellte eine falle], target=[tom set a trap], predicted=[tom has a cough]\\nBLEU-1: 0.082088\\nBLEU-2: 0.006182\\nBLEU-3: 0.046129\\nBLEU-4: 0.076238\\nListing 30.29: Sample output translation on the test dataset.\\n30.6 Extensions\\nThis section lists some ideas for extending the tutorial that you may wish to explore.\\n\\x88Data Cleaning . Diﬀerent data cleaning operations could be performed on the data, such\\nas not removing punctuation or normalizing case, or perhaps removing duplicate English\\nphrases.\\n\\x88Vocabulary . The vocabulary could be reﬁned, perhaps removing words used less than 5\\nor 10 times in the dataset and replaced with unk.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 386}, page_content='\\x88More Data . The dataset used to ﬁt the model could be expanded to 50,000, 100,000\\nphrases, or more.\\n\\x88Input Order . The order of input phrases could be reversed, which has been reported to\\nlift skill, or a Bidirectional input layer could be used.\\n\\x88Layers . The encoder and/or the decoder models could be expanded with additional layers\\nand trained for more epochs, providing more representational capacity for the model.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 387}, page_content='30.7. Further Reading 371\\n\\x88Units . The number of memory units in the encoder and decoder could be increased,\\nproviding more representational capacity for the model.\\n\\x88Regularization . The model could use regularization, such as weight or activation\\nregularization, or the use of dropout on the LSTM layers.\\n\\x88Pre-Trained Word Vectors . Pre-trained word vectors could be used in the model.\\n\\x88Alternate Measure . Explore alternate performance measures beside BLEU such as\\nROGUE. Compare scores for the same translations to develop an intuition for how the\\nmeasures diﬀer in practice.\\n\\x88Recursive Model . A recursive formulation of the model could be used where the next\\nword in the output sequence could be conditional on the input sequence and the output\\nsequence generated so far.\\nIf you explore any of these extensions, I’d love to know.\\n30.7 Further Reading\\nThis section provides more resources on the topic if you are looking to go deeper.\\n30.7.1 Dataset\\n\\x88Tab-delimited Bilingual Sentence Pairs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 387}, page_content='http://www.manythings.org/anki/\\n\\x88German - English deu-eng.zip .\\nhttp://www.manythings.org/anki/deu-eng.zip\\n30.7.2 Neural Machine Translation\\n\\x88Google’s Neural Machine Translation System: Bridging the Gap between Human and\\nMachine Translation , 2016.\\nhttps://arxiv.org/abs/1609.08144\\n\\x88Sequence to Sequence Learning with Neural Networks , 2014.\\nhttps://arxiv.org/abs/1409.3215\\n\\x88Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\\nTranslation , 2014.\\nhttps://arxiv.org/abs/1406.1078\\n\\x88Neural Machine Translation by Jointly Learning to Align and Translate , 2014.\\nhttps://arxiv.org/abs/1409.0473\\n\\x88On the Properties of Neural Machine Translation: Encoder-Decoder Approaches , 2014.\\nhttps://arxiv.org/abs/1409.1259\\n\\x88Massive Exploration of Neural Machine Translation Architectures , 2017.\\nhttps://arxiv.org/abs/1703.03906'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 388}, page_content='30.8. Summary 372\\n30.8 Summary\\nIn this tutorial, you discovered how to develop a neural machine translation system for translating\\nGerman phrases to English. Speciﬁcally, you learned:\\n\\x88How to clean and prepare data ready to train a neural machine translation system.\\n\\x88How to develop an encoder-decoder model for machine translation.\\n\\x88How to use a trained model for inference on new input phrases and evaluate the model\\nskill.\\n30.8.1 Next\\nThis is the ﬁnal chapter for the machine translation part. In the next part you will discover\\nhelpful information in the appendix.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 389}, page_content='Part X\\nAppendix\\n373'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 390}, page_content='Appendix A\\nGetting Help\\nThis is just the beginning of your journey with deep learning for natural language processing in\\nPython. As you start to work on projects and expand your existing knowledge of the techniques,\\nyou may need help. This appendix points out some of the best sources of help.\\nA.1 Oﬃcial Keras Destinations\\nThis section lists the oﬃcial Keras sites that you may ﬁnd helpful.\\n\\x88Keras Oﬃcial Blog.\\nhttps://blog.keras.io/\\n\\x88Keras API Documentation.\\nhttps://keras.io/\\n\\x88Keras Source Code Project.\\nhttps://github.com/fchollet/keras\\nA.2 Where to Get Help with Keras\\nThis section lists the 9 best places I know where you can get help with Keras and LSTMs.\\n\\x88Keras Users Google Group.\\nhttps://groups.google.com/forum/#!forum/keras-users\\n\\x88Keras Slack Channel (you must request to join).\\nhttps://keras-slack-autojoin.herokuapp.com/\\n\\x88Keras on Gitter.\\nhttps://gitter.im/Keras-io/Lobby#\\n\\x88Keras tag on StackOverﬂow.\\nhttps://stackoverflow.com/questions/tagged/keras\\n\\x88Keras tag on CrossValidated.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 390}, page_content='https://stats.stackexchange.com/questions/tagged/keras\\n374'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 391}, page_content='A.3. Where to Get Help with Natural Language 375\\n\\x88Keras tag on DataScience.\\nhttps://datascience.stackexchange.com/questions/tagged/keras\\n\\x88Keras Topic on Quora.\\nhttps://www.quora.com/topic/Keras\\n\\x88Keras Github Issues.\\nhttps://github.com/fchollet/keras/issues\\n\\x88Keras on Twitter.\\nhttps://twitter.com/hashtag/keras\\nA.3 Where to Get Help with Natural Language\\nThis section lists the best places I know where you can get help with natural language processing.\\n\\x88Language Technology Subreddit.\\nhttps://www.reddit.com/r/LanguageTechnology/\\n\\x88NLP Tag on StackOverﬂow.\\nhttps://stackoverflow.com/questions/tagged/nlp\\n\\x88Linguistics Stack Exchange.\\nhttps://linguistics.stackexchange.com/\\n\\x88Natural Language Processing Topic on Quora.\\nhttps://www.quora.com/topic/Natural-Language-Processing\\nA.4 How to Ask Questions\\nKnowing where to get help is the ﬁrst step, but you need to know how to get the most out of\\nthese resources. Below are some tips that you can use:'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 391}, page_content='\\x88Boil your question down to the simplest form. E.g. not something broad like my model\\ndoes not work orhow does x work .\\n\\x88Search for answers before asking questions.\\n\\x88Provide complete code and error messages.\\n\\x88Boil your code down to the smallest possible working example that demonstrates the issue.\\nA.5 Contact the Author\\nYou are not alone. If you ever have any questions about deep learning, natural language\\nprocessing, or this book, please contact me directly. I will do my best to help.\\nJason Brownlee\\nJason@MachineLearningMastery.com'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 392}, page_content='Appendix B\\nHow to Setup a Workstation for Deep\\nLearning\\nIt can be diﬃcult to install a Python machine learning environment on some platforms. Python\\nitself must be installed ﬁrst and then there are many packages to install, and it can be confusing\\nfor beginners. In this tutorial, you will discover how to setup a Python machine learning\\ndevelopment environment using Anaconda.\\nAfter completing this tutorial, you will have a working Python environment to begin learning,\\npracticing, and developing machine learning and deep learning software. These instructions are\\nsuitable for Windows, Mac OS X, and Linux platforms. I will demonstrate them on OS X, so\\nyou may see some mac dialogs and ﬁle extensions.\\nB.1 Overview\\nIn this tutorial, we will cover the following steps:\\n1. Download Anaconda\\n2. Install Anaconda\\n3. Start and Update Anaconda\\n4. Install Deep Learning Libraries\\nNote: The speciﬁc versions may diﬀer as the software and libraries are updated frequently.\\nB.2 Download Anaconda'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 392}, page_content='In this step, we will download the Anaconda Python package for your platform. Anaconda is a\\nfree and easy-to-use environment for scientiﬁc Python.\\n\\x881. Visit the Anaconda homepage.\\nhttps://www.continuum.io/\\n\\x882. Click Anaconda from the menu and click Download to go to the download page.\\nhttps://www.continuum.io/downloads\\n376'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 393}, page_content='B.2. Download Anaconda 377\\nFigure B.1: Click Anaconda and Download.\\n\\x883. Choose the download suitable for your platform (Windows, OSX, or Linux):\\n–Choose Python 3.6\\n–Choose the Graphical Installer'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 394}, page_content='B.3. Install Anaconda 378\\nFigure B.2: Choose Anaconda Download for Your Platform.\\nThis will download the Anaconda Python package to your workstation. I’m on OS X, so I\\nchose the OS X version. The ﬁle is about 426 MB. You should have a ﬁle with a name like:\\nAnaconda3-4.4.0-MacOSX-x86_64.pkg\\nListing B.1: Example ﬁlename on Mac OS X.\\nB.3 Install Anaconda\\nIn this step, we will install the Anaconda Python software on your system. This step assumes\\nyou have suﬃcient administrative privileges to install software on your system.\\n\\x881. Double click the downloaded ﬁle.\\n\\x882. Follow the installation wizard.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 395}, page_content='B.3. Install Anaconda 379\\nFigure B.3: Anaconda Python Installation Wizard.\\nInstallation is quick and painless. There should be no tricky questions or sticking points.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 396}, page_content='B.4. Start and Update Anaconda 380\\nFigure B.4: Anaconda Python Installation Wizard Writing Files.\\nThe installation should take less than 10 minutes and take up a little more than 1 GB of\\nspace on your hard drive.\\nB.4 Start and Update Anaconda\\nIn this step, we will conﬁrm that your Anaconda Python environment is up to date. Anaconda\\ncomes with a suite of graphical tools called Anaconda Navigator. You can start Anaconda\\nNavigator by opening it from your application launcher.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 397}, page_content='B.4. Start and Update Anaconda 381\\nFigure B.5: Anaconda Navigator GUI.\\nYou can use the Anaconda Navigator and graphical development environments later; for now,\\nI recommend starting with the Anaconda command line environment called conda. Conda is\\nfast, simple, it’s hard for error messages to hide, and you can quickly conﬁrm your environment\\nis installed and working correctly.\\n\\x881. Open a terminal (command line window).\\n\\x882. Conﬁrm conda is installed correctly, by typing:\\nconda -V\\nListing B.2: Check the conda version.\\nYou should see the following (or something similar):\\nconda 4.3.21\\nListing B.3: Example conda version.\\n\\x883. Conﬁrm Python is installed correctly by typing:\\npython -V\\nListing B.4: Check the Python version.\\nYou should see the following (or something similar):\\nPython 3.6.1 :: Anaconda 4.4.0 (x86_64)\\nListing B.5: Example Python version.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 398}, page_content=\"B.4. Start and Update Anaconda 382\\nIf the commands do not work or have an error, please check the documentation for help for\\nyour platform. See some of the resources in the Further Reading section.\\n\\x884. Conﬁrm your conda environment is up-to-date, type:\\nconda update conda\\nconda update anaconda\\nListing B.6: Update conda and anaconda.\\nYou may need to install some packages and conﬁrm the updates.\\n\\x885. Conﬁrm your SciPy environment.\\nThe script below will print the version number of the key SciPy libraries you require for\\nmachine learning development, speciﬁcally: SciPy, NumPy, Matplotlib, Pandas, Statsmodels,\\nand Scikit-learn. You can type python and type the commands in directly. Alternatively, I\\nrecommend opening a text editor and copy-pasting the script into your editor.\\n# scipy\\nimport scipy\\nprint( 'scipy: %s '% scipy.__version__)\\n# numpy\\nimport numpy\\nprint( 'numpy: %s '% numpy.__version__)\\n# matplotlib\\nimport matplotlib\\nprint( 'matplotlib: %s '% matplotlib.__version__)\\n# pandas\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 398}, page_content=\"# pandas\\nimport pandas\\nprint( 'pandas: %s '% pandas.__version__)\\n# statsmodels\\nimport statsmodels\\nprint( 'statsmodels: %s '% statsmodels.__version__)\\n# scikit-learn\\nimport sklearn\\nprint( 'sklearn: %s '% sklearn.__version__)\\nListing B.7: Code to check that key Python libraries are installed.\\nSave the script as a ﬁle with the name: versions.py . On the command line, change your\\ndirectory to where you saved the script and type:\\npython versions.py\\nListing B.8: Run the script from the command line.\\nYou should see output like the following:\\nscipy: 0.19.1\\nnumpy: 1.13.3\\nmatplotlib: 2.1.0\\npandas: 0.20.3\\nstatsmodels: 0.8.0\\nsklearn: 0.19.0\\nListing B.9: Sample output of versions script.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 399}, page_content='B.5. Install Deep Learning Libraries 383\\nB.5 Install Deep Learning Libraries\\nIn this step, we will install Python libraries used for deep learning, speciﬁcally: Theano,\\nTensorFlow, and Keras. Note: I recommend using Keras for deep learning and Keras only\\nrequires one of Theano or TensorFlow to be installed. You do not need both. There may be\\nproblems installing TensorFlow on some Windows machines.\\n\\x881. Install the Theano deep learning library by typing:\\nconda install theano\\nListing B.10: Install Theano with conda.\\n\\x882. Install the TensorFlow deep learning library by typing:\\nconda install -c conda-forge tensorflow\\nListing B.11: Install TensorFlow with conda.\\nAlternatively, you may choose to install using pipand a speciﬁc version of TensorFlow for\\nyour platform.\\n\\x883. Install Keras by typing:\\npip install keras\\nListing B.12: Install Keras with pip.\\n\\x884. Conﬁrm your deep learning environment is installed and working correctly.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 399}, page_content=\"Create a script that prints the version numbers of each library, as we did before for the SciPy\\nenvironment.\\n# theano\\nimport theano\\nprint( 'theano: %s '% theano.__version__)\\n# tensorflow\\nimport tensorflow\\nprint( 'tensorflow: %s '% tensorflow.__version__)\\n# keras\\nimport keras\\nprint( 'keras: %s '% keras.__version__)\\nListing B.13: Code to check that key deep learning libraries are installed.\\nSave the script to a ﬁle deep versions.py . Run the script by typing:\\npython deep_versions.py\\nListing B.14: Run script from the command line.\\nYou should see output like:\\ntheano: 0.9.0\\ntensorflow: 1.3.0\\nkeras: 2.0.8\\nListing B.15: Sample output of the deep learning versions script.\"),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 400}, page_content='B.6. Further Reading 384\\nB.6 Further Reading\\nThis section provides resources if you want to know more about Anaconda.\\n\\x88Anaconda homepage.\\nhttps://www.continuum.io/\\n\\x88Anaconda Navigator.\\nhttps://docs.continuum.io/anaconda/navigator.html\\n\\x88The conda command line tool.\\nhttp://conda.pydata.org/docs/index.html\\n\\x88Instructions for installing TensorFlow in Anaconda.\\nhttps://www.tensorflow.org/get_started/os_setup#anaconda_installation\\nB.7 Summary\\nCongratulations, you now have a working Python development environment for machine learning\\nand deep learning. You can now learn and practice machine learning and deep learning on your\\nworkstation.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 401}, page_content='Appendix C\\nHow to Use Deep Learning in the\\nCloud\\nLarge deep learning models require a lot of compute time to run. You can run them on your\\nCPU but it can take hours or days to get a result. If you have access to a GPU on your desktop,\\nyou can drastically speed up the training time of your deep learning models. In this project you\\nwill discover how you can get access to GPUs to speed up the training of your deep learning\\nmodels by using the Amazon Web Service (AWS) infrastructure. For less than a dollar per hour\\nand often a lot cheaper you can use this service from your workstation or laptop. After working\\nthrough this project you will know:\\n\\x88How to create an account and log-in to Amazon Web Service.\\n\\x88How to launch a server instance for deep learning.\\n\\x88How to conﬁgure a server instance for faster deep learning on the GPU.\\nLet’s get started.\\nC.1 Overview\\nThe process is quite simple because most of the work has already been done for us. Below is an\\noverview of the process.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 401}, page_content='\\x88Setup Your AWS Account.\\n\\x88Launch Your Server Instance.\\n\\x88Login and Run Your Code.\\n\\x88Close Your Server Instance.\\nNote, it costs money to use a virtual server instance on Amazon . The cost is low for\\nmodel development (e.g. less than one US dollar per hour), which is why this is so attractive,\\nbut it is not free. The server instance runs Linux. It is desirable although not required that you\\nknow how to navigate Linux or a Unix-like environment. We’re just running our Python scripts,\\nso no advanced skills are needed.\\nNote: The speciﬁc versions may diﬀer as the software and libraries are updated frequently.\\n385'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 402}, page_content='C.2. Setup Your AWS Account 386\\nC.2 Setup Your AWS Account\\nYou need an account on Amazon Web Services1.\\n\\x881. You can create account by the Amazon Web Services portal and click Sign in to the\\nConsole . From there you can sign in using an existing Amazon account or create a new\\naccount.\\nFigure C.1: AWS Sign-in Button\\n\\x882. You will need to provide your details as well as a valid credit card that Amazon can\\ncharge. The process is a lot quicker if you are already an Amazon customer and have your\\ncredit card on ﬁle.\\n1https://aws.amazon.com'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 403}, page_content='C.3. Launch Your Server Instance 387\\nFigure C.2: AWS Sign-In Form\\nOnce you have an account you can log into the Amazon Web Services console. You will see\\na range of diﬀerent services that you can access.\\nC.3 Launch Your Server Instance\\nNow that you have an AWS account, you want to launch an EC2 virtual server instance on\\nwhich you can run Keras. Launching an instance is as easy as selecting the image to load and\\nstarting the virtual server. Thankfully there is already an image available that has almost\\neverything we need it is called the Deep Learning AMI Amazon Linux Version and was\\ncreated and is maintained by Amazon. Let’s launch it as an instance.\\n\\x881. Login to your AWS console if you have not already.\\nhttps://console.aws.amazon.com/console/home'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 404}, page_content='C.3. Launch Your Server Instance 388\\nFigure C.3: AWS Console\\n\\x882. Click on EC2 for launching a new virtual server.\\n\\x883. Select US West Orgeon from the drop-down in the top right hand corner. This is\\nimportant otherwise you will not be able to ﬁnd the image we plan to use.\\n\\x884. Click the Launch Instance button.\\n\\x885. Click Community AMIs . An AMI is an Amazon Machine Image. It is a frozen instance\\nof a server that you can select and instantiate on a new virtual server.\\nFigure C.4: Community AMIs\\n\\x886. Enter ami-df77b6a7 in the Search community AMIs search box and press enter (this\\nis the current AMI id for v3.3 but the AMI may have been updated since, you check for a\\nmore recent id2). You should be presented with a single result.\\n2https://aws.amazon.com/marketplace/pp/B01M0AXXQB'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 405}, page_content='C.3. Launch Your Server Instance 389\\nFigure C.5: Select a Speciﬁc AMI\\n\\x887. Click Select to choose the AMI in the search result.\\n\\x888. Now you need to select the hardware on which to run the image. Scroll down and select\\ntheg2.2xlarge hardware. This includes a GPU that we can use to signiﬁcantly increase\\nthe training speed of our models. The choice of hardware will impact the price, I generally\\nrecommend g2 and p2 hardware. See the AMI page for estimated pricing per hour for\\ndiﬀerent hardware conﬁgurations3.\\nFigure C.6: Select g2.2xlarge Hardware\\n\\x889. Click Review and Launch to ﬁnalize the conﬁguration of your server instance.\\n\\x8810. Click the Launch button.\\n\\x8811. Select Your Key Pair.\\nIf you have a key pair because you have used EC2 before, select Choose an existing key pair\\nand choose your key pair from the list. Then check I acknowledge... . If you do not have a key\\npair, select the option Create a new key pair and enter a Key pair name such as keras-keypair.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 405}, page_content='Click the Download Key Pair button.\\n3https://aws.amazon.com/marketplace/pp/B01M0AXXQB'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 406}, page_content='C.3. Launch Your Server Instance 390\\nFigure C.7: Select Your Key Pair\\n\\x8812. Open a Terminal and change directory to where you downloaded your key pair.\\n\\x8813. If you have not already done so, restrict the access permissions on your key pair ﬁle.\\nThis is required as part of the SSH access to your server. For example, open a terminal on\\nyour workstation and type:\\ncd Downloads\\nchmod 600 keras-aws-keypair.pem\\nListing C.1: Change Permissions of Your Key Pair File.\\n\\x8814. Click Launch Instances . If this is your ﬁrst time using AWS, Amazon may have to\\nvalidate your request and this could take up to 2 hours (often just a few minutes).\\n\\x8815. Click View Instances to review the status of your instance.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 407}, page_content='C.4. Login, Conﬁgure and Run 391\\nFigure C.8: Review Your Running Instance\\nYour server is now running and ready for you to log in.\\nC.4 Login, Conﬁgure and Run\\nNow that you have launched your server instance, it is time to log in and start using it.\\n\\x881. Click View Instances in your Amazon EC2 console if you have not done so already.\\n\\x882. Copy the Public IP (down the bottom of the screen in Description) to your clipboard.\\nIn this example my IP address is 54.186.97.77 .Do not use this IP address, it will\\nnot work as your server IP address will be diﬀerent .\\n\\x883. Open a Terminal and change directory to where you downloaded your key pair. Login\\nto your server using SSH, for example:\\nssh -i keras-aws-keypair.pem ec2-user@54.186.97.77\\nListing C.2: Log-in To Your AWS Instance.\\n\\x884. If prompted, type yesand press enter.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 408}, page_content='C.5. Build and Run Models on AWS 392\\nYou are now logged into your server.\\nFigure C.9: Log in Screen for Your AWS Server\\nWe need to make two small changes before we can start using Keras. This will just take a\\nminute. You will have to do these changes each time you start the instance.\\nC.4.1 Update Keras\\nUpdate to a speciﬁc version of Keras known to work on this conﬁguration, at the time of writing\\nthe latest version of Keras is version 2.0.8. We can specify this version as part of the upgrade of\\nKeras via pip.\\nsudo pip install --upgrade keras==2.0.8\\nListing C.3: Update Keras Using pip.\\nYou can also conﬁrm that Keras is installed and is working correctly by typing:\\npython -c \"import keras; print(keras.__version__)\"\\nListing C.4: Script To Check Keras Conﬁguration.\\nYou should see:\\nUsing TensorFlow backend.\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcublas.so.7.5 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 408}, page_content='libcudnn.so.5 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcufft.so.7.5 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcuda.so.1 locally\\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library\\nlibcurand.so.7.5 locally\\n2.0.8\\nListing C.5: Sample Output of Script to Check Keras Conﬁguration.\\nYou are now free to run your code.\\nC.5 Build and Run Models on AWS\\nThis section oﬀers some tips for running your code on AWS.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 409}, page_content='C.6. Close Your EC2 Instance 393\\nC.5.1 Copy Scripts and Data to AWS\\nYou can get started quickly by copying your ﬁles to your running AWS instance. For example,\\nyou can copy the examples provided with this book to your AWS instance using the scp\\ncommand as follows:\\nscp -i keras-aws-keypair.pem -r src ec2-user@54.186.97.77:~/\\nListing C.6: Example for Copying Sample Code to AWS.\\nThis will copy the entire src/ directory to your home directory on your AWS instance. You\\ncan easily adapt this example to get your larger datasets from your workstation onto your AWS\\ninstance. Note that Amazon may impose charges for moving very large amounts of data in and\\nout of your AWS instance. Refer to Amazon documentation for relevant charges.\\nC.5.2 Run Models on AWS\\nYou can run your scripts on your AWS instance as per normal:\\npython filename.py\\nListing C.7: Example of Running a Python script on AWS.\\nYou are using AWS to create large neural network models that may take hours or days to'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 409}, page_content='train. As such, it is a better idea to run your scripts as a background job. This allows you to\\nclose your terminal and your workstation while your AWS instance continues to run your script.\\nYou can easily run your script as a background process as follows:\\nnohup /path/to/script >/path/to/script.log 2>&1 < /dev/null &\\nListing C.8: Run Script as a Background Process.\\nYou can then check the status and results in your script.log ﬁle later.\\nC.6 Close Your EC2 Instance\\nWhen you are ﬁnished with your work you must close your instance. Remember you are charged\\nby the amount of time that you use the instance. It is cheap, but you do not want to leave an\\ninstance on if you are not using it.\\n\\x881. Log out of your instance at the terminal, for example you can type:\\nexit\\nListing C.9: Log-out of Server Instance.\\n\\x882. Log in to your AWS account with your web browser.\\n\\x883. Click EC2.\\n\\x884. Click Instances from the left-hand side menu.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 410}, page_content='C.6. Close Your EC2 Instance 394\\nFigure C.10: Review Your List of Running Instances\\n\\x885. Select your running instance from the list (it may already be selected if you only have\\none running instance).\\nFigure C.11: Select Your Running AWS Instance\\n\\x886. Click the Actions button and select Instance State and choose Terminate . Conﬁrm\\nthat you want to terminate your running instance.\\nIt may take a number of seconds for the instance to close and to be removed from your list\\nof instances.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 411}, page_content='C.7. Tips and Tricks for Using Keras on AWS 395\\nC.7 Tips and Tricks for Using Keras on AWS\\nBelow are some tips and tricks for getting the most out of using Keras on AWS instances.\\n\\x88Design a suite of experiments to run beforehand . Experiments can take a long\\ntime to run and you are paying for the time you use. Make time to design a batch of\\nexperiments to run on AWS. Put each in a separate ﬁle and call them in turn from another\\nscript. This will allow you to answer multiple questions from one long run, perhaps\\novernight.\\n\\x88Always close your instance at the end of your experiments . You do not want to\\nbe surprised with a very large AWS bill.\\n\\x88Try spot instances for a cheaper but less reliable option . Amazon sell unused\\ntime on their hardware at a much cheaper price, but at the cost of potentially having your\\ninstance closed at any second. If you are learning or your experiments are not critical, this\\nmight be an ideal option for you. You can access spot instances from the Spot Instance'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 411}, page_content='option on the left hand side menu in your EC2 web console.\\nC.8 Further Reading\\nBelow is a list of resources to learn more about AWS and developing deep learning models in\\nthe cloud.\\n\\x88An introduction to Amazon Elastic Compute Cloud (EC2) if you are new to all of this.\\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html\\n\\x88An introduction to Amazon Machine Images (AMI).\\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\\n\\x88Deep Learning AMI Amazon Linux Version on the AMI Marketplace.\\nhttps://aws.amazon.com/marketplace/pp/B01M0AXXQB\\nC.9 Summary\\nIn this lesson you discovered how you can develop and evaluate your large deep learning models\\nin Keras using GPUs on the Amazon Web Service. You learned:\\n\\x88Amazon Web Services with their Elastic Compute Cloud oﬀers an aﬀordable way to run\\nlarge deep learning models on GPU hardware.\\n\\x88How to setup and launch an EC2 server for deep learning experiments.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 411}, page_content='\\x88How to update the Keras version on the server and conﬁrm that the system is working\\ncorrectly.\\n\\x88How to run Keras experiments on AWS instances in batch as background tasks.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 412}, page_content='Part XI\\nConclusions\\n396'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 413}, page_content='How Far You Have Come\\nYou made it. Well done. Take a moment and look back at how far you have come. You now\\nknow:\\n1. What natural language processing is and why it is challenging.\\n2. What deep learning is and how it is diﬀerent from other machine learning methods.\\n3. The promise of deep learning methods for natural language processing problems.\\n4. How to prepare text data for modeling using best-of-breed Python libraries.\\n5. How to develop distributed representations of text using word embedding models.\\n6.How to develop a bag-of-words model, a representation technique that can be used for\\nmachine learning and deep learning methods.\\n7.How to develop a neural sentiment analysis model for automatically predicting the class\\nlabel for a text document.\\n8.How to develop a neural language model, required for any text generating neural network.\\n9.How to develop a photo captioning system to automatically generate textual descriptions\\nof photographs.'),\n",
       " Document(metadata={'source': 'Deep_Learning_for_Natural_Language_processing.pdf', 'page': 413}, page_content='of photographs.\\n10.How to develop a neural machine translation system for translating text from one language\\nto another.\\nDon’t make light of this. You have come a long way in a short amount of time. You have\\ndeveloped the important and valuable skill of being able to implement and work through natural\\nlanguage prediction problems using deep learning in Python. You can now conﬁdently bring\\ndeep learning models to your own natural language processing problems. The sky’s the limit.\\nThank You!\\nI want to take a moment and sincerely thank you for letting me help you start your deep learning\\nfor natural language processing journey. I hope you keep learning and have fun as you continue\\nto master machine learning.\\nJason Brownlee\\n2017\\n397')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector Emnedding And Vectors store\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs[:20],OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONTENTS iii\\n3 Promise of Deep Learning for Natural Language 16\\n3.1 Promise of Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.2 Promise of Drop-in Replacement Models . . . . . . . . . . . . . . . . . . . . . . 17\\n3.3 Promise of New NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.4 Promise of Feature Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.5 Promise of Continued Improvement . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.6 Promise of End-to-End Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n3.7 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4 How to Develop Deep Learning Models With Keras 21\\n4.1 Keras Model Life-Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Natural Language Processing\"\n",
    "result = db.similarity_search(query)\n",
    "result[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import  Ollama\n",
    "#load Ollama\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Answer the following question based only on the provided context.\n",
    "        Think step by step before providing a detailed answer.\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "        Question:{input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##chain Introduction\n",
    "## create Stuff Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000022303AB0580>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Retriever Chain : This chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents\n",
    "    (and original inputs) are then passed to an LLM to generate a response.\n",
    "    \"\"\"\n",
    "    from langchain.chains import create_retrieval_chain\n",
    "    retrieval_chain = create_retrieval_chain(retriever,documents_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"Bag-Of-Word\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided is related to deep learning for natural language processing, specifically the use of bag-of-words models in NLP. Therefore, the answer to the question \"What is a bag-of-words model?\" can be inferred as follows:\n",
      "\n",
      "A bag-of-words model is a type of supervised machine learning model that represents a document or a sequence of words as a set of features, where each feature corresponds to the presence or absence of a particular word in the document. In other words, a bag-of-words model treats each word in a document as an independent feature, without considering the order or context of the words.\n",
      "\n",
      "For example, if we have a document that contains the following sentence: \"The quick brown fox jumps over the lazy dog\", a bag-of-words model would represent this sentence as a set of 5 features:\n",
      "\n",
      "1. The\n",
      "2. quick\n",
      "3. brown\n",
      "4. fox\n",
      "5. jumps\n",
      "\n",
      "Each feature indicates the presence or absence of a particular word in the sentence. This type of representation is useful for tasks such as text classification, where the goal is to classify a document into a predefined category based on its content.\n",
      "\n",
      "Bag-of-words models are commonly used in NLP because they are simple and easy to implement, and they can capture some aspects of language structure. However, they have some limitations, such as ignoring the order and context of words, which can lead to poor performance in certain tasks.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
